{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1 = Path(HERE).resolve().parents[1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3921050</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[9593141, 5451723, 1098538, 7467572, 8519057, ...</td>\n",
       "      <td>6.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL270476</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[13333824, 16015625, 6095547, 11013180, 106090...</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3664128</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "      <td>[9238077, 2038275, 1578989, 1109593, 7780264, ...</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4456250</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[2167236, 3827191, 5190786, 7944578, 19973311,...</td>\n",
       "      <td>5.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2408818</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2078792, 2722242, 5084638, 18495690, 845386, ...</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL3921050  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       CHEMBL270476  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3664128  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4456250  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL2408818  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, ...   \n",
       "3  [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC1  \n",
       "0  [9593141, 5451723, 1098538, 7467572, 8519057, ...           6.17  \n",
       "1  [13333824, 16015625, 6095547, 11013180, 106090...           6.80  \n",
       "2  [9238077, 2038275, 1578989, 1109593, 7780264, ...           7.62  \n",
       "3  [2167236, 3827191, 5190786, 7944578, 19973311,...           5.26  \n",
       "4  [2078792, 2722242, 5084638, 18495690, 845386, ...           6.32  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1/\"HDAC1_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type</th>\n",
       "      <th>Standard_Value_HDAC1</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL327146</td>\n",
       "      <td>O=C(CCCCCC(C(=O)Nc1ccc2ncccc2c1)C(=O)Nc1ccc2nc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL116620</td>\n",
       "      <td>O=C(/C=C/c1cccc(C(C(=O)Nc2ccccc2)C(=O)Nc2ccccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2093007</td>\n",
       "      <td>C/C=C1\\NC(=O)[C@@H](CSC)NC(=O)[C@@H](C(C)C)CC(...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6300.00</td>\n",
       "      <td>5.20</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL316457</td>\n",
       "      <td>CC(C)c1cc(C(C)C)c(S(=O)(=O)Nc2ccc(/C=C/C(=O)NO...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>600.00</td>\n",
       "      <td>6.22</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL269692</td>\n",
       "      <td>O=C(NCc1ccc(C(=O)NO)cc1)OCc1cccnc1</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3000.00</td>\n",
       "      <td>5.52</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4649511</td>\n",
       "      <td>O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3.30</td>\n",
       "      <td>8.48</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL4637976</td>\n",
       "      <td>O=C(CCCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>130.00</td>\n",
       "      <td>6.89</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL4638227</td>\n",
       "      <td>CC(=O)Nc1ccc2c(c1)CN(C(=O)[C@H](N)Cc1ccc(Cl)cc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>8800.00</td>\n",
       "      <td>5.06</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL4444219</td>\n",
       "      <td>CCCNNC(=O)/C=C/c1ccc(CNCCc2c(C)[nH]c3ccccc23)cc1</td>\n",
       "      <td>IC50</td>\n",
       "      <td>4.85</td>\n",
       "      <td>8.43</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>28.00</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0          CHEMBL327146  O=C(CCCCCC(C(=O)Nc1ccc2ncccc2c1)C(=O)Nc1ccc2nc...   \n",
       "1          CHEMBL116620  O=C(/C=C/c1cccc(C(C(=O)Nc2ccccc2)C(=O)Nc2ccccc...   \n",
       "2         CHEMBL2093007  C/C=C1\\NC(=O)[C@@H](CSC)NC(=O)[C@@H](C(C)C)CC(...   \n",
       "3          CHEMBL316457  CC(C)c1cc(C(C)C)c(S(=O)(=O)Nc2ccc(/C=C/C(=O)NO...   \n",
       "4          CHEMBL269692                 O=C(NCc1ccc(C(=O)NO)cc1)OCc1cccnc1   \n",
       "...                 ...                                                ...   \n",
       "4487      CHEMBL4649511        O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO   \n",
       "4488      CHEMBL4637976       O=C(CCCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO   \n",
       "4489      CHEMBL4638227  CC(=O)Nc1ccc2c(c1)CN(C(=O)[C@H](N)Cc1ccc(Cl)cc...   \n",
       "4490      CHEMBL4444219   CCCNNC(=O)/C=C/c1ccc(CNCCc2c(C)[nH]c3ccccc23)cc1   \n",
       "4491      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "\n",
       "      type  Standard_Value_HDAC1  pChEMBL_HDAC1          label  \n",
       "0     IC50                  1.00           9.00  Single points  \n",
       "1     IC50                  1.00           9.00  Single points  \n",
       "2     IC50               6300.00           5.20  Single points  \n",
       "3     IC50                600.00           6.22  Single points  \n",
       "4     IC50               3000.00           5.52  Single points  \n",
       "...    ...                   ...            ...            ...  \n",
       "4487  IC50                  3.30           8.48    Dual-binder  \n",
       "4488  IC50                130.00           6.89  Single points  \n",
       "4489  IC50               8800.00           5.06  Single points  \n",
       "4490  IC50                  4.85           8.43  Single points  \n",
       "4491    Ki                 28.00           7.55    Dual-binder  \n",
       "\n",
       "[4492 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1/\"HDAC1_dataset.csv\", )\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4250302</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5240905, 10949433, 5178223, 6083913, 2157755,...</td>\n",
       "      <td>10.25</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL483893</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[2331191, 10511, 137380, 311785, 4030911, 7589...</td>\n",
       "      <td>7.83</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL3655914</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[8142946, 2478511, 10872982, 45638894, 4495368...</td>\n",
       "      <td>6.69</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL467876</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[668904, 10511, 2475943, 2183376, 3858537, 376...</td>\n",
       "      <td>7.40</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4458544</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[5976924, 3821889, 137380, 5264283, 1293667, 6...</td>\n",
       "      <td>7.37</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                           fp_MACCS  \\\n",
       "4487      CHEMBL4250302  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4488       CHEMBL483893  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4489      CHEMBL3655914  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4490       CHEMBL467876  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4491      CHEMBL4458544  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_Morgan3  \\\n",
       "4487  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4488  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4489  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4490  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4491  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MorganF  \\\n",
       "4487  [1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4488  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "4489  [1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "4490  [1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "4491  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                                fp_MAP4  pChEMBL_HDAC1  \\\n",
       "4487  [5240905, 10949433, 5178223, 6083913, 2157755,...          10.25   \n",
       "4488  [2331191, 10511, 137380, 311785, 4030911, 7589...           7.83   \n",
       "4489  [8142946, 2478511, 10872982, 45638894, 4495368...           6.69   \n",
       "4490  [668904, 10511, 2475943, 2183376, 3858537, 376...           7.40   \n",
       "4491  [5976924, 3821889, 137380, 5264283, 1293667, 6...           7.37   \n",
       "\n",
       "               label  \n",
       "4487   Single points  \n",
       "4488     Dual-binder  \n",
       "4489  Semi-selective  \n",
       "4490   Single points  \n",
       "4491     Dual-binder  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3921050</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[9593141, 5451723, 1098538, 7467572, 8519057, ...</td>\n",
       "      <td>6.17</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL270476</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[13333824, 16015625, 6095547, 11013180, 106090...</td>\n",
       "      <td>6.80</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3664128</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "      <td>[9238077, 2038275, 1578989, 1109593, 7780264, ...</td>\n",
       "      <td>7.62</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4456250</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[2167236, 3827191, 5190786, 7944578, 19973311,...</td>\n",
       "      <td>5.26</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL3921050  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       CHEMBL270476  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3664128  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4456250  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, ...   \n",
       "3  [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC1  \\\n",
       "0  [9593141, 5451723, 1098538, 7467572, 8519057, ...           6.17   \n",
       "1  [13333824, 16015625, 6095547, 11013180, 106090...           6.80   \n",
       "2  [9238077, 2038275, 1578989, 1109593, 7780264, ...           7.62   \n",
       "3  [2167236, 3827191, 5190786, 7944578, 19973311,...           5.26   \n",
       "\n",
       "           label  Class  \n",
       "0  Single points    0.0  \n",
       "1  Single points    0.0  \n",
       "2  Single points    0.0  \n",
       "3  Single points    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for activity\n",
    "df[\"activity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as active if pchembl value is >=6.6 0 otherwise\n",
    "df.loc[df[df.pChEMBL_HDAC1 >= 6.6].index, \"activity\"] = 1.0\n",
    "\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"pChEMBL_HDAC1\"].values\n",
    "Y_cat =  df[\"activity\"].values\n",
    "Y_class = df['Class'].values\n",
    "indices =  np.array(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['pChEMBL_HDAC1'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['pChEMBL_HDAC1'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.683104     0.021265\n",
      "1                    TP       202.600000     9.477459\n",
      "2                    TN       173.000000     8.137704\n",
      "3                    FP        40.100000     5.173651\n",
      "4                    FN        33.500000     3.894440\n",
      "5              Accuracy         0.836143     0.015890\n",
      "6             Precision         0.834744     0.020126\n",
      "7           Sensitivity         0.857837     0.018346\n",
      "8           Specificity         0.811790     0.023305\n",
      "9              F1 score         0.846027     0.016609\n",
      "10  F1 score (weighted)         0.835958     0.015952\n",
      "11     F1 score (macro)         0.835253     0.015811\n",
      "12    Balanced Accuracy         0.834818     0.015701\n",
      "13                  MCC         0.671119     0.031278\n",
      "14                  NPV         0.837860     0.016747\n",
      "15              ROC_AUC         0.834818     0.015701\n",
      "CPU times: user 3min 31s, sys: 199 ms, total: 3min 31s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=16,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6) , 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=16, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=16, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 15:44:36,393] A new study created in memory with name: RFRegressor\n",
      "[I 2023-12-19 15:46:19,501] Trial 0 finished with value: 0.6593277125817122 and parameters: {'n_estimators': 882}. Best is trial 0 with value: 0.6593277125817122.\n",
      "[I 2023-12-19 15:47:06,860] Trial 1 finished with value: 0.6595631944145943 and parameters: {'n_estimators': 333}. Best is trial 1 with value: 0.6595631944145943.\n",
      "[I 2023-12-19 15:48:13,621] Trial 2 finished with value: 0.659567485985713 and parameters: {'n_estimators': 451}. Best is trial 2 with value: 0.659567485985713.\n",
      "[I 2023-12-19 15:48:58,269] Trial 3 finished with value: 0.6598410246877591 and parameters: {'n_estimators': 299}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 15:50:43,301] Trial 4 finished with value: 0.6596743102534252 and parameters: {'n_estimators': 662}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 15:53:13,507] Trial 5 finished with value: 0.6593471455795259 and parameters: {'n_estimators': 847}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 15:55:20,498] Trial 6 finished with value: 0.6594545206241517 and parameters: {'n_estimators': 692}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 15:57:42,989] Trial 7 finished with value: 0.6593276209055934 and parameters: {'n_estimators': 784}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:00:29,954] Trial 8 finished with value: 0.659333522803759 and parameters: {'n_estimators': 918}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:02:18,600] Trial 9 finished with value: 0.6594544312555483 and parameters: {'n_estimators': 588}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:02:39,667] Trial 10 finished with value: 0.6572454493590468 and parameters: {'n_estimators': 116}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:03:39,948] Trial 11 finished with value: 0.6595371500109467 and parameters: {'n_estimators': 332}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:03:58,332] Trial 12 finished with value: 0.6567421094811581 and parameters: {'n_estimators': 100}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:05:34,471] Trial 13 finished with value: 0.659479285682618 and parameters: {'n_estimators': 527}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:06:21,302] Trial 14 finished with value: 0.659417799848151 and parameters: {'n_estimators': 262}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:08:08,173] Trial 15 finished with value: 0.6595207781832674 and parameters: {'n_estimators': 594}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:10:12,341] Trial 16 finished with value: 0.6593459854442616 and parameters: {'n_estimators': 701}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:11:24,131] Trial 17 finished with value: 0.6596070814969858 and parameters: {'n_estimators': 412}. Best is trial 3 with value: 0.6598410246877591.\n",
      "[I 2023-12-19 16:12:03,646] Trial 18 finished with value: 0.6598630278345474 and parameters: {'n_estimators': 217}. Best is trial 18 with value: 0.6598630278345474.\n",
      "[I 2023-12-19 16:12:41,273] Trial 19 finished with value: 0.6596253135160677 and parameters: {'n_estimators': 208}. Best is trial 18 with value: 0.6598630278345474.\n",
      "[I 2023-12-19 16:13:16,519] Trial 20 finished with value: 0.6597890505799601 and parameters: {'n_estimators': 192}. Best is trial 18 with value: 0.6598630278345474.\n",
      "[I 2023-12-19 16:13:55,908] Trial 21 finished with value: 0.6597828437926848 and parameters: {'n_estimators': 220}. Best is trial 18 with value: 0.6598630278345474.\n",
      "[I 2023-12-19 16:14:30,810] Trial 22 finished with value: 0.6596747746517118 and parameters: {'n_estimators': 183}. Best is trial 18 with value: 0.6598630278345474.\n",
      "[I 2023-12-19 16:15:28,718] Trial 23 finished with value: 0.6596621695166494 and parameters: {'n_estimators': 316}. Best is trial 18 with value: 0.6598630278345474.\n",
      "[I 2023-12-19 16:16:44,496] Trial 24 finished with value: 0.6596304764215881 and parameters: {'n_estimators': 425}. Best is trial 18 with value: 0.6598630278345474.\n",
      "[I 2023-12-19 16:17:15,523] Trial 25 finished with value: 0.6592127762142448 and parameters: {'n_estimators': 160}. Best is trial 18 with value: 0.6598630278345474.\n",
      "[I 2023-12-19 16:18:10,192] Trial 26 finished with value: 0.6599194417857677 and parameters: {'n_estimators': 304}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:18:59,166] Trial 27 finished with value: 0.6590884112870693 and parameters: {'n_estimators': 278}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:20:08,518] Trial 28 finished with value: 0.6595989122245979 and parameters: {'n_estimators': 389}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:21:34,966] Trial 29 finished with value: 0.6595928831821918 and parameters: {'n_estimators': 493}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:22:23,093] Trial 30 finished with value: 0.6592311133294879 and parameters: {'n_estimators': 269}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:22:50,673] Trial 31 finished with value: 0.6591577609775822 and parameters: {'n_estimators': 154}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:23:56,208] Trial 32 finished with value: 0.6597614538288543 and parameters: {'n_estimators': 367}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:24:39,120] Trial 33 finished with value: 0.6596950678662388 and parameters: {'n_estimators': 239}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:25:36,575] Trial 34 finished with value: 0.6596836951974873 and parameters: {'n_estimators': 315}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:26:59,734] Trial 35 finished with value: 0.659528081183465 and parameters: {'n_estimators': 473}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:29:52,782] Trial 36 finished with value: 0.659361478671852 and parameters: {'n_estimators': 995}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:30:57,030] Trial 37 finished with value: 0.6597138321969078 and parameters: {'n_estimators': 362}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:31:26,916] Trial 38 finished with value: 0.6592565281967289 and parameters: {'n_estimators': 170}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:32:17,357] Trial 39 finished with value: 0.6594131685814814 and parameters: {'n_estimators': 285}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:32:56,736] Trial 40 finished with value: 0.6597828437926848 and parameters: {'n_estimators': 220}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:33:36,545] Trial 41 finished with value: 0.6595955652776432 and parameters: {'n_estimators': 225}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:34:12,312] Trial 42 finished with value: 0.6596273009898544 and parameters: {'n_estimators': 194}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:34:37,523] Trial 43 finished with value: 0.6582387350624639 and parameters: {'n_estimators': 136}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:35:34,400] Trial 44 finished with value: 0.6596102731740082 and parameters: {'n_estimators': 325}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:35:52,968] Trial 45 finished with value: 0.6567120340492589 and parameters: {'n_estimators': 103}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:36:36,992] Trial 46 finished with value: 0.6596700644113433 and parameters: {'n_estimators': 241}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:37:29,164] Trial 47 finished with value: 0.6596770813230182 and parameters: {'n_estimators': 291}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:38:33,251] Trial 48 finished with value: 0.6597002757604006 and parameters: {'n_estimators': 357}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:38:56,931] Trial 49 finished with value: 0.6586056737552642 and parameters: {'n_estimators': 140}. Best is trial 26 with value: 0.6599194417857677.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6599\n",
      "\tBest params:\n",
      "\t\tn_estimators: 304\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.681289\n",
      "1                    TP  401.000000\n",
      "2                    TN  328.000000\n",
      "3                    FP   98.000000\n",
      "4                    FN   72.000000\n",
      "5              Accuracy    0.810901\n",
      "6             Precision    0.803607\n",
      "7           Sensitivity    0.847780\n",
      "8           Specificity    0.770000\n",
      "9              F1 score    0.825103\n",
      "10  F1 score (weighted)    0.810454\n",
      "11     F1 score (macro)    0.809646\n",
      "12    Balanced Accuracy    0.808867\n",
      "13                  MCC    0.620663\n",
      "14                  NPV    0.820000\n",
      "15              ROC_AUC    0.808867\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_0_cat = np.where((y_pred_rf_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 16:41:20,427] Trial 50 finished with value: 0.6563318710879945 and parameters: {'n_estimators': 757}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:42:03,593] Trial 51 finished with value: 0.6562866291969623 and parameters: {'n_estimators': 232}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:42:41,871] Trial 52 finished with value: 0.6562083894750074 and parameters: {'n_estimators': 201}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:43:19,498] Trial 53 finished with value: 0.6561183945798204 and parameters: {'n_estimators': 204}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:44:07,545] Trial 54 finished with value: 0.6564324490953268 and parameters: {'n_estimators': 255}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:45:00,865] Trial 55 finished with value: 0.6563967715735664 and parameters: {'n_estimators': 299}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:46:54,250] Trial 56 finished with value: 0.6562616322341576 and parameters: {'n_estimators': 621}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:48:09,521] Trial 57 finished with value: 0.65570266684346 and parameters: {'n_estimators': 432}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:48:34,907] Trial 58 finished with value: 0.6537196056111554 and parameters: {'n_estimators': 132}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:49:06,777] Trial 59 finished with value: 0.6559151619096059 and parameters: {'n_estimators': 178}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:50:19,313] Trial 60 finished with value: 0.6565629450333005 and parameters: {'n_estimators': 396}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:51:22,997] Trial 61 finished with value: 0.6570511314122255 and parameters: {'n_estimators': 360}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:52:11,493] Trial 62 finished with value: 0.656293050685514 and parameters: {'n_estimators': 262}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:53:15,637] Trial 63 finished with value: 0.6569138578320148 and parameters: {'n_estimators': 349}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:53:57,615] Trial 64 finished with value: 0.6562821239392911 and parameters: {'n_estimators': 229}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:54:54,167] Trial 65 finished with value: 0.656683673011308 and parameters: {'n_estimators': 311}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:56:29,629] Trial 66 finished with value: 0.6559548776160424 and parameters: {'n_estimators': 539}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:57:02,365] Trial 67 finished with value: 0.6555736955015481 and parameters: {'n_estimators': 168}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:57:51,505] Trial 68 finished with value: 0.6562046294231028 and parameters: {'n_estimators': 273}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 16:59:22,320] Trial 69 finished with value: 0.6559105847428767 and parameters: {'n_estimators': 496}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:00:31,880] Trial 70 finished with value: 0.6569170854248576 and parameters: {'n_estimators': 384}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:01:34,332] Trial 71 finished with value: 0.6567225630002682 and parameters: {'n_estimators': 336}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:02:43,414] Trial 72 finished with value: 0.6571058606278961 and parameters: {'n_estimators': 378}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:04:06,346] Trial 73 finished with value: 0.6558119738195062 and parameters: {'n_estimators': 448}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:04:46,119] Trial 74 finished with value: 0.6560858038367806 and parameters: {'n_estimators': 207}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:05:40,538] Trial 75 finished with value: 0.6560989235400589 and parameters: {'n_estimators': 293}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:06:26,390] Trial 76 finished with value: 0.6563466573233441 and parameters: {'n_estimators': 256}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:07:07,867] Trial 77 finished with value: 0.6561636131097041 and parameters: {'n_estimators': 225}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:08:06,768] Trial 78 finished with value: 0.6566308103486219 and parameters: {'n_estimators': 333}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:08:30,566] Trial 79 finished with value: 0.6536475566537305 and parameters: {'n_estimators': 124}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:09:44,141] Trial 80 finished with value: 0.6562560229859727 and parameters: {'n_estimators': 409}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:10:51,446] Trial 81 finished with value: 0.6570957502881385 and parameters: {'n_estimators': 364}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:11:47,511] Trial 82 finished with value: 0.6566417045397925 and parameters: {'n_estimators': 307}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:12:51,560] Trial 83 finished with value: 0.6569150379198341 and parameters: {'n_estimators': 348}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:13:27,179] Trial 84 finished with value: 0.6560574750086796 and parameters: {'n_estimators': 186}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:13:56,435] Trial 85 finished with value: 0.6553683670473653 and parameters: {'n_estimators': 157}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:14:48,691] Trial 86 finished with value: 0.6561855015864841 and parameters: {'n_estimators': 282}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:15:35,883] Trial 87 finished with value: 0.6563562211159345 and parameters: {'n_estimators': 252}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:16:16,607] Trial 88 finished with value: 0.6563020445992344 and parameters: {'n_estimators': 213}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:17:35,021] Trial 89 finished with value: 0.655754356291107 and parameters: {'n_estimators': 425}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:20:01,518] Trial 90 finished with value: 0.6565438462117561 and parameters: {'n_estimators': 851}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:20:47,709] Trial 91 finished with value: 0.6564486003810843 and parameters: {'n_estimators': 245}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:21:45,039] Trial 92 finished with value: 0.6566757826860021 and parameters: {'n_estimators': 320}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:22:36,330] Trial 93 finished with value: 0.6561100836890719 and parameters: {'n_estimators': 277}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:23:10,134] Trial 94 finished with value: 0.6561321547097254 and parameters: {'n_estimators': 188}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:23:52,073] Trial 95 finished with value: 0.6562322213217227 and parameters: {'n_estimators': 224}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:24:20,218] Trial 96 finished with value: 0.6550724933539662 and parameters: {'n_estimators': 150}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:25:04,708] Trial 97 finished with value: 0.6562146463967472 and parameters: {'n_estimators': 242}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:26:12,125] Trial 98 finished with value: 0.6571040240520095 and parameters: {'n_estimators': 370}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:27:05,846] Trial 99 finished with value: 0.656440686900633 and parameters: {'n_estimators': 298}. Best is trial 26 with value: 0.6599194417857677.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6599\n",
      "\tBest params:\n",
      "\t\tn_estimators: 304\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.681289    0.690470\n",
      "1                    TP  401.000000  407.000000\n",
      "2                    TN  328.000000  333.000000\n",
      "3                    FP   98.000000   89.000000\n",
      "4                    FN   72.000000   70.000000\n",
      "5              Accuracy    0.810901    0.823137\n",
      "6             Precision    0.803607    0.820565\n",
      "7           Sensitivity    0.847780    0.853249\n",
      "8           Specificity    0.770000    0.789100\n",
      "9              F1 score    0.825103    0.836588\n",
      "10  F1 score (weighted)    0.810454    0.822827\n",
      "11     F1 score (macro)    0.809646    0.821930\n",
      "12    Balanced Accuracy    0.808867    0.821175\n",
      "13                  MCC    0.620663    0.644604\n",
      "14                  NPV    0.820000    0.826300\n",
      "15              ROC_AUC    0.808867    0.821175\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_1_cat = np.where((y_pred_rf_1 >= 6.6), 1, 0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 17:28:38,461] Trial 100 finished with value: 0.6577270594718856 and parameters: {'n_estimators': 478}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:29:38,345] Trial 101 finished with value: 0.6571670645185546 and parameters: {'n_estimators': 346}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:30:49,729] Trial 102 finished with value: 0.6575059733850119 and parameters: {'n_estimators': 397}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:31:46,635] Trial 103 finished with value: 0.6571177078355015 and parameters: {'n_estimators': 322}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:32:33,842] Trial 104 finished with value: 0.6568369569086736 and parameters: {'n_estimators': 268}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:33:29,529] Trial 105 finished with value: 0.6570390700999298 and parameters: {'n_estimators': 311}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:34:08,334] Trial 106 finished with value: 0.6563502094008048 and parameters: {'n_estimators': 212}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:34:39,029] Trial 107 finished with value: 0.6557301391153916 and parameters: {'n_estimators': 170}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:34:59,596] Trial 108 finished with value: 0.6528769934589077 and parameters: {'n_estimators': 100}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:35:50,478] Trial 109 finished with value: 0.6570608304478377 and parameters: {'n_estimators': 294}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:36:32,569] Trial 110 finished with value: 0.6562570823100546 and parameters: {'n_estimators': 238}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:37:20,003] Trial 111 finished with value: 0.6569702303986207 and parameters: {'n_estimators': 265}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:38:23,124] Trial 112 finished with value: 0.6570200829688596 and parameters: {'n_estimators': 355}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:40:07,546] Trial 113 finished with value: 0.6580837925962759 and parameters: {'n_estimators': 579}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:41:07,093] Trial 114 finished with value: 0.657195273769269 and parameters: {'n_estimators': 341}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:41:57,862] Trial 115 finished with value: 0.657201975099697 and parameters: {'n_estimators': 288}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:42:31,744] Trial 116 finished with value: 0.6560888488011227 and parameters: {'n_estimators': 194}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:43:29,677] Trial 117 finished with value: 0.6571177078355015 and parameters: {'n_estimators': 322}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:44:37,218] Trial 118 finished with value: 0.6573953367752521 and parameters: {'n_estimators': 374}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:45:15,177] Trial 119 finished with value: 0.6561712563189347 and parameters: {'n_estimators': 222}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:46:34,505] Trial 120 finished with value: 0.6575305441813929 and parameters: {'n_estimators': 449}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:47:18,448] Trial 121 finished with value: 0.6568337519689358 and parameters: {'n_estimators': 252}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:47:54,988] Trial 122 finished with value: 0.6559255522669719 and parameters: {'n_estimators': 199}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:48:20,700] Trial 123 finished with value: 0.6547812843310644 and parameters: {'n_estimators': 137}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:48:52,128] Trial 124 finished with value: 0.6557411819210748 and parameters: {'n_estimators': 179}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:50:57,547] Trial 125 finished with value: 0.6579290040157385 and parameters: {'n_estimators': 716}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:51:45,696] Trial 126 finished with value: 0.6568071045923143 and parameters: {'n_estimators': 278}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:52:27,310] Trial 127 finished with value: 0.6563056601818121 and parameters: {'n_estimators': 237}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:55:13,695] Trial 128 finished with value: 0.6580555526045364 and parameters: {'n_estimators': 957}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:56:05,703] Trial 129 finished with value: 0.6571268835190398 and parameters: {'n_estimators': 301}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:57:16,415] Trial 130 finished with value: 0.6575482284480241 and parameters: {'n_estimators': 402}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 17:59:01,847] Trial 131 finished with value: 0.6578796041221513 and parameters: {'n_estimators': 622}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:00:55,376] Trial 132 finished with value: 0.6578976684912415 and parameters: {'n_estimators': 647}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:01:33,336] Trial 133 finished with value: 0.6562547425089236 and parameters: {'n_estimators': 210}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:02:02,122] Trial 134 finished with value: 0.6550945582857353 and parameters: {'n_estimators': 159}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:04:20,630] Trial 135 finished with value: 0.6580007233787286 and parameters: {'n_estimators': 795}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:05:21,387] Trial 136 finished with value: 0.6569973746759888 and parameters: {'n_estimators': 335}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:06:07,169] Trial 137 finished with value: 0.6569702303986208 and parameters: {'n_estimators': 265}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:08:15,643] Trial 138 finished with value: 0.6580245090481015 and parameters: {'n_estimators': 746}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:10:09,981] Trial 139 finished with value: 0.6578777508570923 and parameters: {'n_estimators': 667}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:11:41,060] Trial 140 finished with value: 0.657886925571645 and parameters: {'n_estimators': 516}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:12:22,484] Trial 141 finished with value: 0.6562942870207993 and parameters: {'n_estimators': 236}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:12:55,160] Trial 142 finished with value: 0.6556124031592627 and parameters: {'n_estimators': 182}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:13:46,232] Trial 143 finished with value: 0.6569558547179928 and parameters: {'n_estimators': 282}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:14:23,721] Trial 144 finished with value: 0.6561616449935872 and parameters: {'n_estimators': 221}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:15:08,083] Trial 145 finished with value: 0.6568785134626836 and parameters: {'n_estimators': 254}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:15:44,719] Trial 146 finished with value: 0.6560713292278553 and parameters: {'n_estimators': 197}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:16:40,966] Trial 147 finished with value: 0.6570695060791298 and parameters: {'n_estimators': 318}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:17:45,303] Trial 148 finished with value: 0.6570005511438157 and parameters: {'n_estimators': 356}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:18:38,748] Trial 149 finished with value: 0.657081946387994 and parameters: {'n_estimators': 305}. Best is trial 26 with value: 0.6599194417857677.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6599\n",
      "\tBest params:\n",
      "\t\tn_estimators: 304\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.681289    0.690470    0.678922\n",
      "1                    TP  401.000000  407.000000  374.000000\n",
      "2                    TN  328.000000  333.000000  362.000000\n",
      "3                    FP   98.000000   89.000000   90.000000\n",
      "4                    FN   72.000000   70.000000   73.000000\n",
      "5              Accuracy    0.810901    0.823137    0.818687\n",
      "6             Precision    0.803607    0.820565    0.806034\n",
      "7           Sensitivity    0.847780    0.853249    0.836689\n",
      "8           Specificity    0.770000    0.789100    0.800900\n",
      "9              F1 score    0.825103    0.836588    0.821076\n",
      "10  F1 score (weighted)    0.810454    0.822827    0.818642\n",
      "11     F1 score (macro)    0.809646    0.821930    0.818655\n",
      "12    Balanced Accuracy    0.808867    0.821175    0.818787\n",
      "13                  MCC    0.620663    0.644604    0.637896\n",
      "14                  NPV    0.820000    0.826300    0.832200\n",
      "15              ROC_AUC    0.808867    0.821175    0.818787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_2_cat = np.where((y_pred_rf_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 18:19:51,597] Trial 150 finished with value: 0.6503365093690437 and parameters: {'n_estimators': 380}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:20:49,950] Trial 151 finished with value: 0.6500517237562272 and parameters: {'n_estimators': 330}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:21:41,954] Trial 152 finished with value: 0.6497152742282378 and parameters: {'n_estimators': 289}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:22:28,233] Trial 153 finished with value: 0.6496370045592766 and parameters: {'n_estimators': 257}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:23:09,810] Trial 154 finished with value: 0.649464335576962 and parameters: {'n_estimators': 237}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:23:49,323] Trial 155 finished with value: 0.6493060842602841 and parameters: {'n_estimators': 214}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:24:36,481] Trial 156 finished with value: 0.6492865527505336 and parameters: {'n_estimators': 268}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:25:41,225] Trial 157 finished with value: 0.6500085686401277 and parameters: {'n_estimators': 360}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:26:35,937] Trial 158 finished with value: 0.6501922486428227 and parameters: {'n_estimators': 309}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:27:06,203] Trial 159 finished with value: 0.6484881300244075 and parameters: {'n_estimators': 173}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:28:05,870] Trial 160 finished with value: 0.6499639449098997 and parameters: {'n_estimators': 343}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:29:13,628] Trial 161 finished with value: 0.6502947150659001 and parameters: {'n_estimators': 379}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:30:11,102] Trial 162 finished with value: 0.6500173164292196 and parameters: {'n_estimators': 326}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:31:50,355] Trial 163 finished with value: 0.6502748181580873 and parameters: {'n_estimators': 568}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:33:06,917] Trial 164 finished with value: 0.6504991482352063 and parameters: {'n_estimators': 428}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:34:18,238] Trial 165 finished with value: 0.6506314305711125 and parameters: {'n_estimators': 407}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:35:10,086] Trial 166 finished with value: 0.6498245684489807 and parameters: {'n_estimators': 299}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:36:02,622] Trial 167 finished with value: 0.6495681827286135 and parameters: {'n_estimators': 279}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:36:47,511] Trial 168 finished with value: 0.6496479543472006 and parameters: {'n_estimators': 249}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:37:21,946] Trial 169 finished with value: 0.6493851905581146 and parameters: {'n_estimators': 200}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:38:02,191] Trial 170 finished with value: 0.6493741982416161 and parameters: {'n_estimators': 225}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:38:37,969] Trial 171 finished with value: 0.6493698396627793 and parameters: {'n_estimators': 193}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:39:05,590] Trial 172 finished with value: 0.6476917264528161 and parameters: {'n_estimators': 150}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:39:37,132] Trial 173 finished with value: 0.6485904927877634 and parameters: {'n_estimators': 176}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:39:58,067] Trial 174 finished with value: 0.6478334292583197 and parameters: {'n_estimators': 118}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:40:36,442] Trial 175 finished with value: 0.649226664341489 and parameters: {'n_estimators': 213}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:41:38,279] Trial 176 finished with value: 0.6499639449098997 and parameters: {'n_estimators': 343}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:42:20,187] Trial 177 finished with value: 0.6494984162519802 and parameters: {'n_estimators': 239}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:43:43,515] Trial 178 finished with value: 0.6505137804476265 and parameters: {'n_estimators': 467}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:44:51,802] Trial 179 finished with value: 0.6506137394942486 and parameters: {'n_estimators': 392}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:45:46,265] Trial 180 finished with value: 0.65018197101421 and parameters: {'n_estimators': 315}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:46:19,281] Trial 181 finished with value: 0.6494214214186017 and parameters: {'n_estimators': 190}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:47:00,698] Trial 182 finished with value: 0.6493741982416161 and parameters: {'n_estimators': 225}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:47:39,408] Trial 183 finished with value: 0.6493375803723032 and parameters: {'n_estimators': 209}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:48:07,692] Trial 184 finished with value: 0.6479881690797935 and parameters: {'n_estimators': 158}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:49:13,072] Trial 185 finished with value: 0.6500825816897355 and parameters: {'n_estimators': 367}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:49:58,670] Trial 186 finished with value: 0.6496724714699045 and parameters: {'n_estimators': 255}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:50:48,825] Trial 187 finished with value: 0.6495804450290865 and parameters: {'n_estimators': 278}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:51:31,400] Trial 188 finished with value: 0.649464335576962 and parameters: {'n_estimators': 237}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:52:25,040] Trial 189 finished with value: 0.6498706509349095 and parameters: {'n_estimators': 296}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:52:59,300] Trial 190 finished with value: 0.6490879802965686 and parameters: {'n_estimators': 184}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:53:58,662] Trial 191 finished with value: 0.6500610428574192 and parameters: {'n_estimators': 338}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:54:57,887] Trial 192 finished with value: 0.6500728536335137 and parameters: {'n_estimators': 324}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:56:02,541] Trial 193 finished with value: 0.6499541270258946 and parameters: {'n_estimators': 354}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:56:57,310] Trial 194 finished with value: 0.650205480587123 and parameters: {'n_estimators': 311}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:57:43,446] Trial 195 finished with value: 0.6493370614060445 and parameters: {'n_estimators': 267}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:58:20,767] Trial 196 finished with value: 0.6493851905581146 and parameters: {'n_estimators': 200}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 18:58:51,022] Trial 197 finished with value: 0.6479228930675119 and parameters: {'n_estimators': 167}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:01:15,263] Trial 198 finished with value: 0.6503084695013769 and parameters: {'n_estimators': 811}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:02:07,429] Trial 199 finished with value: 0.649715274228238 and parameters: {'n_estimators': 289}. Best is trial 26 with value: 0.6599194417857677.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6599\n",
      "\tBest params:\n",
      "\t\tn_estimators: 304\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.681289    0.690470    0.678922    0.680260\n",
      "1                    TP  401.000000  407.000000  374.000000  409.000000\n",
      "2                    TN  328.000000  333.000000  362.000000  345.000000\n",
      "3                    FP   98.000000   89.000000   90.000000   75.000000\n",
      "4                    FN   72.000000   70.000000   73.000000   70.000000\n",
      "5              Accuracy    0.810901    0.823137    0.818687    0.838710\n",
      "6             Precision    0.803607    0.820565    0.806034    0.845041\n",
      "7           Sensitivity    0.847780    0.853249    0.836689    0.853862\n",
      "8           Specificity    0.770000    0.789100    0.800900    0.821400\n",
      "9              F1 score    0.825103    0.836588    0.821076    0.849429\n",
      "10  F1 score (weighted)    0.810454    0.822827    0.818642    0.838645\n",
      "11     F1 score (macro)    0.809646    0.821930    0.818655    0.837888\n",
      "12    Balanced Accuracy    0.808867    0.821175    0.818787    0.837645\n",
      "13                  MCC    0.620663    0.644604    0.637896    0.675828\n",
      "14                  NPV    0.820000    0.826300    0.832200    0.831300\n",
      "15              ROC_AUC    0.808867    0.821175    0.818787    0.837645\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_3_cat = np.where((y_pred_rf_3 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 19:03:19,927] Trial 200 finished with value: 0.6579080781421153 and parameters: {'n_estimators': 367}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:04:34,476] Trial 201 finished with value: 0.6583565179458077 and parameters: {'n_estimators': 408}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:05:49,542] Trial 202 finished with value: 0.6588999891511336 and parameters: {'n_estimators': 429}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:06:58,511] Trial 203 finished with value: 0.6583549159055766 and parameters: {'n_estimators': 385}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:07:36,216] Trial 204 finished with value: 0.6570031847921725 and parameters: {'n_estimators': 222}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:08:36,241] Trial 205 finished with value: 0.6578488167528025 and parameters: {'n_estimators': 334}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:10:05,749] Trial 206 finished with value: 0.658527611494758 and parameters: {'n_estimators': 505}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:11:08,183] Trial 207 finished with value: 0.6577711854134629 and parameters: {'n_estimators': 351}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:12:21,794] Trial 208 finished with value: 0.6585552977804701 and parameters: {'n_estimators': 416}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:13:43,502] Trial 209 finished with value: 0.658782686796411 and parameters: {'n_estimators': 444}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:14:38,528] Trial 210 finished with value: 0.6579793685615887 and parameters: {'n_estimators': 316}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:15:43,206] Trial 211 finished with value: 0.6578010168266413 and parameters: {'n_estimators': 358}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:16:51,366] Trial 212 finished with value: 0.6585010318105701 and parameters: {'n_estimators': 390}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:17:59,506] Trial 213 finished with value: 0.6580432504916255 and parameters: {'n_estimators': 374}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:18:38,227] Trial 214 finished with value: 0.6565904308656436 and parameters: {'n_estimators': 211}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:19:48,753] Trial 215 finished with value: 0.6586153637722002 and parameters: {'n_estimators': 394}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:20:33,401] Trial 216 finished with value: 0.6574518313395401 and parameters: {'n_estimators': 250}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:21:32,033] Trial 217 finished with value: 0.6578934294778984 and parameters: {'n_estimators': 330}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:23:07,338] Trial 218 finished with value: 0.6587679124678549 and parameters: {'n_estimators': 537}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:23:41,791] Trial 219 finished with value: 0.6568807776196196 and parameters: {'n_estimators': 196}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:24:36,017] Trial 220 finished with value: 0.6578211595982486 and parameters: {'n_estimators': 301}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:25:16,400] Trial 221 finished with value: 0.6571665402466375 and parameters: {'n_estimators': 229}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:25:56,333] Trial 222 finished with value: 0.6567854553541553 and parameters: {'n_estimators': 218}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:26:29,373] Trial 223 finished with value: 0.656354965485735 and parameters: {'n_estimators': 187}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:27:14,840] Trial 224 finished with value: 0.6572518893464442 and parameters: {'n_estimators': 239}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:29:05,668] Trial 225 finished with value: 0.6592795426746194 and parameters: {'n_estimators': 617}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:29:41,870] Trial 226 finished with value: 0.6568940893114886 and parameters: {'n_estimators': 207}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:30:30,445] Trial 227 finished with value: 0.6573918380508778 and parameters: {'n_estimators': 263}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:30:56,713] Trial 228 finished with value: 0.6569526826569215 and parameters: {'n_estimators': 137}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:31:27,372] Trial 229 finished with value: 0.6564393935682448 and parameters: {'n_estimators': 174}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:32:33,362] Trial 230 finished with value: 0.658078638939693 and parameters: {'n_estimators': 369}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:33:56,635] Trial 231 finished with value: 0.658497971151578 and parameters: {'n_estimators': 459}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:34:58,308] Trial 232 finished with value: 0.6577479024070854 and parameters: {'n_estimators': 344}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:35:48,452] Trial 233 finished with value: 0.657429444814781 and parameters: {'n_estimators': 283}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:37:52,026] Trial 234 finished with value: 0.6593620223250486 and parameters: {'n_estimators': 694}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:39:08,013] Trial 235 finished with value: 0.658495349862967 and parameters: {'n_estimators': 415}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:39:48,270] Trial 236 finished with value: 0.6570031847921725 and parameters: {'n_estimators': 222}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:41:13,908] Trial 237 finished with value: 0.6585117312674188 and parameters: {'n_estimators': 484}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:42:53,335] Trial 238 finished with value: 0.6587854168429279 and parameters: {'n_estimators': 562}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:44:12,045] Trial 239 finished with value: 0.6587555672277189 and parameters: {'n_estimators': 446}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:45:22,095] Trial 240 finished with value: 0.6586153637722001 and parameters: {'n_estimators': 394}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:46:39,044] Trial 241 finished with value: 0.6588999891511336 and parameters: {'n_estimators': 429}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:48:01,858] Trial 242 finished with value: 0.6584372515624638 and parameters: {'n_estimators': 458}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:49:24,896] Trial 243 finished with value: 0.6585754697207099 and parameters: {'n_estimators': 480}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:50:45,727] Trial 244 finished with value: 0.6588489091051264 and parameters: {'n_estimators': 440}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:51:57,774] Trial 245 finished with value: 0.6584142790428864 and parameters: {'n_estimators': 404}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:54:06,701] Trial 246 finished with value: 0.6592206019150109 and parameters: {'n_estimators': 719}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:55:34,254] Trial 247 finished with value: 0.6585238501446476 and parameters: {'n_estimators': 498}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:56:11,202] Trial 248 finished with value: 0.6571044668839724 and parameters: {'n_estimators': 201}. Best is trial 26 with value: 0.6599194417857677.\n",
      "[I 2023-12-19 19:57:07,116] Trial 249 finished with value: 0.6580005199829401 and parameters: {'n_estimators': 321}. Best is trial 26 with value: 0.6599194417857677.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6599\n",
      "\tBest params:\n",
      "\t\tn_estimators: 304\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681289    0.690470    0.678922    0.680260   \n",
      "1                    TP  401.000000  407.000000  374.000000  409.000000   \n",
      "2                    TN  328.000000  333.000000  362.000000  345.000000   \n",
      "3                    FP   98.000000   89.000000   90.000000   75.000000   \n",
      "4                    FN   72.000000   70.000000   73.000000   70.000000   \n",
      "5              Accuracy    0.810901    0.823137    0.818687    0.838710   \n",
      "6             Precision    0.803607    0.820565    0.806034    0.845041   \n",
      "7           Sensitivity    0.847780    0.853249    0.836689    0.853862   \n",
      "8           Specificity    0.770000    0.789100    0.800900    0.821400   \n",
      "9              F1 score    0.825103    0.836588    0.821076    0.849429   \n",
      "10  F1 score (weighted)    0.810454    0.822827    0.818642    0.838645   \n",
      "11     F1 score (macro)    0.809646    0.821930    0.818655    0.837888   \n",
      "12    Balanced Accuracy    0.808867    0.821175    0.818787    0.837645   \n",
      "13                  MCC    0.620663    0.644604    0.637896    0.675828   \n",
      "14                  NPV    0.820000    0.826300    0.832200    0.831300   \n",
      "15              ROC_AUC    0.808867    0.821175    0.818787    0.837645   \n",
      "\n",
      "          Set4  \n",
      "0     0.674801  \n",
      "1   413.000000  \n",
      "2   339.000000  \n",
      "3    84.000000  \n",
      "4    63.000000  \n",
      "5     0.836485  \n",
      "6     0.830986  \n",
      "7     0.867647  \n",
      "8     0.801400  \n",
      "9     0.848921  \n",
      "10    0.836168  \n",
      "11    0.835370  \n",
      "12    0.834533  \n",
      "13    0.671662  \n",
      "14    0.843300  \n",
      "15    0.834533  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_4_cat = np.where((y_pred_rf_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 19:57:59,714] Trial 250 finished with value: 0.6668360520104661 and parameters: {'n_estimators': 250}. Best is trial 250 with value: 0.6668360520104661.\n",
      "[I 2023-12-19 19:58:44,951] Trial 251 finished with value: 0.6667645505988367 and parameters: {'n_estimators': 248}. Best is trial 250 with value: 0.6668360520104661.\n",
      "[I 2023-12-19 19:59:32,217] Trial 252 finished with value: 0.6668127990554794 and parameters: {'n_estimators': 252}. Best is trial 250 with value: 0.6668360520104661.\n",
      "[I 2023-12-19 20:00:15,046] Trial 253 finished with value: 0.6667549694527258 and parameters: {'n_estimators': 247}. Best is trial 250 with value: 0.6668360520104661.\n",
      "[I 2023-12-19 20:01:03,539] Trial 254 finished with value: 0.6669004019733945 and parameters: {'n_estimators': 262}. Best is trial 254 with value: 0.6669004019733945.\n",
      "[I 2023-12-19 20:01:52,563] Trial 255 finished with value: 0.6668071607709283 and parameters: {'n_estimators': 257}. Best is trial 254 with value: 0.6669004019733945.\n",
      "[I 2023-12-19 20:02:38,296] Trial 256 finished with value: 0.6667342755654039 and parameters: {'n_estimators': 256}. Best is trial 254 with value: 0.6669004019733945.\n",
      "[I 2023-12-19 20:03:24,047] Trial 257 finished with value: 0.6667342755654039 and parameters: {'n_estimators': 256}. Best is trial 254 with value: 0.6669004019733945.\n",
      "[I 2023-12-19 20:04:10,662] Trial 258 finished with value: 0.6667018835408289 and parameters: {'n_estimators': 255}. Best is trial 254 with value: 0.6669004019733945.\n",
      "[I 2023-12-19 20:04:59,672] Trial 259 finished with value: 0.6668709688659701 and parameters: {'n_estimators': 259}. Best is trial 254 with value: 0.6669004019733945.\n",
      "[I 2023-12-19 20:05:45,207] Trial 260 finished with value: 0.6667342755654039 and parameters: {'n_estimators': 256}. Best is trial 254 with value: 0.6669004019733945.\n",
      "[I 2023-12-19 20:06:31,747] Trial 261 finished with value: 0.6669004019733945 and parameters: {'n_estimators': 262}. Best is trial 254 with value: 0.6669004019733945.\n",
      "[I 2023-12-19 20:07:18,767] Trial 262 finished with value: 0.6669004019733946 and parameters: {'n_estimators': 262}. Best is trial 262 with value: 0.6669004019733946.\n",
      "[I 2023-12-19 20:08:08,884] Trial 263 finished with value: 0.6671485908904083 and parameters: {'n_estimators': 269}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:08:56,266] Trial 264 finished with value: 0.6669410322248706 and parameters: {'n_estimators': 263}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:09:43,724] Trial 265 finished with value: 0.6669410322248704 and parameters: {'n_estimators': 263}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:10:31,630] Trial 266 finished with value: 0.6669004019733945 and parameters: {'n_estimators': 262}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:11:21,412] Trial 267 finished with value: 0.6670356836769649 and parameters: {'n_estimators': 267}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:12:08,838] Trial 268 finished with value: 0.6668647917298127 and parameters: {'n_estimators': 260}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:12:58,318] Trial 269 finished with value: 0.6669973126394197 and parameters: {'n_estimators': 265}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:13:45,477] Trial 270 finished with value: 0.6668647917298127 and parameters: {'n_estimators': 260}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:14:33,496] Trial 271 finished with value: 0.6670232177295178 and parameters: {'n_estimators': 266}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:15:23,426] Trial 272 finished with value: 0.6670232177295178 and parameters: {'n_estimators': 266}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:16:11,052] Trial 273 finished with value: 0.6669973126394197 and parameters: {'n_estimators': 265}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:16:59,615] Trial 274 finished with value: 0.6669973126394197 and parameters: {'n_estimators': 265}. Best is trial 263 with value: 0.6671485908904083.\n",
      "[I 2023-12-19 20:17:47,011] Trial 275 finished with value: 0.6671485908904085 and parameters: {'n_estimators': 269}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:18:35,128] Trial 276 finished with value: 0.6671102633053181 and parameters: {'n_estimators': 270}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:19:24,948] Trial 277 finished with value: 0.6671102633053181 and parameters: {'n_estimators': 270}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:20:14,147] Trial 278 finished with value: 0.6671079888566526 and parameters: {'n_estimators': 271}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:21:03,641] Trial 279 finished with value: 0.666836065948565 and parameters: {'n_estimators': 276}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:21:53,829] Trial 280 finished with value: 0.666910685248199 and parameters: {'n_estimators': 275}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:22:45,222] Trial 281 finished with value: 0.6669558204787215 and parameters: {'n_estimators': 279}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:23:34,913] Trial 282 finished with value: 0.6669332096216374 and parameters: {'n_estimators': 278}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:24:26,301] Trial 283 finished with value: 0.6669237992673195 and parameters: {'n_estimators': 274}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:25:15,372] Trial 284 finished with value: 0.6669437184855148 and parameters: {'n_estimators': 281}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:26:05,382] Trial 285 finished with value: 0.6668944120543018 and parameters: {'n_estimators': 277}. Best is trial 275 with value: 0.6671485908904085.\n",
      "[I 2023-12-19 20:26:57,705] Trial 286 finished with value: 0.6672200029123736 and parameters: {'n_estimators': 284}. Best is trial 286 with value: 0.6672200029123736.\n",
      "[I 2023-12-19 20:27:47,653] Trial 287 finished with value: 0.6672200029123735 and parameters: {'n_estimators': 284}. Best is trial 286 with value: 0.6672200029123736.\n",
      "[I 2023-12-19 20:28:40,096] Trial 288 finished with value: 0.6669437184855148 and parameters: {'n_estimators': 281}. Best is trial 286 with value: 0.6672200029123736.\n",
      "[I 2023-12-19 20:29:31,085] Trial 289 finished with value: 0.6672437034079877 and parameters: {'n_estimators': 286}. Best is trial 289 with value: 0.6672437034079877.\n",
      "[I 2023-12-19 20:30:24,468] Trial 290 finished with value: 0.6672871290461534 and parameters: {'n_estimators': 288}. Best is trial 290 with value: 0.6672871290461534.\n",
      "[I 2023-12-19 20:31:17,422] Trial 291 finished with value: 0.6675737200677461 and parameters: {'n_estimators': 296}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:32:10,468] Trial 292 finished with value: 0.6674443925069176 and parameters: {'n_estimators': 289}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:33:03,334] Trial 293 finished with value: 0.6675551576240537 and parameters: {'n_estimators': 295}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:33:56,432] Trial 294 finished with value: 0.6674443925069176 and parameters: {'n_estimators': 289}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:34:47,135] Trial 295 finished with value: 0.6673993498125108 and parameters: {'n_estimators': 291}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:35:40,604] Trial 296 finished with value: 0.6675737200677461 and parameters: {'n_estimators': 296}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:36:32,002] Trial 297 finished with value: 0.6669437184855148 and parameters: {'n_estimators': 281}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:37:26,485] Trial 298 finished with value: 0.6674878888043281 and parameters: {'n_estimators': 294}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:38:18,798] Trial 299 finished with value: 0.6675161578873843 and parameters: {'n_estimators': 297}. Best is trial 291 with value: 0.6675737200677461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6676\n",
      "\tBest params:\n",
      "\t\tn_estimators: 296\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681289    0.690470    0.678922    0.680260   \n",
      "1                    TP  401.000000  407.000000  374.000000  409.000000   \n",
      "2                    TN  328.000000  333.000000  362.000000  345.000000   \n",
      "3                    FP   98.000000   89.000000   90.000000   75.000000   \n",
      "4                    FN   72.000000   70.000000   73.000000   70.000000   \n",
      "5              Accuracy    0.810901    0.823137    0.818687    0.838710   \n",
      "6             Precision    0.803607    0.820565    0.806034    0.845041   \n",
      "7           Sensitivity    0.847780    0.853249    0.836689    0.853862   \n",
      "8           Specificity    0.770000    0.789100    0.800900    0.821400   \n",
      "9              F1 score    0.825103    0.836588    0.821076    0.849429   \n",
      "10  F1 score (weighted)    0.810454    0.822827    0.818642    0.838645   \n",
      "11     F1 score (macro)    0.809646    0.821930    0.818655    0.837888   \n",
      "12    Balanced Accuracy    0.808867    0.821175    0.818787    0.837645   \n",
      "13                  MCC    0.620663    0.644604    0.637896    0.675828   \n",
      "14                  NPV    0.820000    0.826300    0.832200    0.831300   \n",
      "15              ROC_AUC    0.808867    0.821175    0.818787    0.837645   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.674801    0.657296  \n",
      "1   413.000000  392.000000  \n",
      "2   339.000000  352.000000  \n",
      "3    84.000000   86.000000  \n",
      "4    63.000000   69.000000  \n",
      "5     0.836485    0.827586  \n",
      "6     0.830986    0.820084  \n",
      "7     0.867647    0.850325  \n",
      "8     0.801400    0.803700  \n",
      "9     0.848921    0.834931  \n",
      "10    0.836168    0.827441  \n",
      "11    0.835370    0.827244  \n",
      "12    0.834533    0.826989  \n",
      "13    0.671662    0.655082  \n",
      "14    0.843300    0.836100  \n",
      "15    0.834533    0.826989  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_5_cat = np.where((y_pred_rf_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 20:39:18,463] Trial 300 finished with value: 0.6551146089680631 and parameters: {'n_estimators': 299}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:40:09,680] Trial 301 finished with value: 0.6550797960245603 and parameters: {'n_estimators': 296}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:41:02,807] Trial 302 finished with value: 0.6550068011702933 and parameters: {'n_estimators': 292}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:41:56,802] Trial 303 finished with value: 0.655138445090795 and parameters: {'n_estimators': 300}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:42:46,661] Trial 304 finished with value: 0.6550804751817786 and parameters: {'n_estimators': 295}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:43:38,514] Trial 305 finished with value: 0.6549767641697335 and parameters: {'n_estimators': 303}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:44:27,952] Trial 306 finished with value: 0.6548816481013293 and parameters: {'n_estimators': 287}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:45:23,221] Trial 307 finished with value: 0.6548877622589856 and parameters: {'n_estimators': 313}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:46:13,873] Trial 308 finished with value: 0.6548582335643551 and parameters: {'n_estimators': 289}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:47:08,303] Trial 309 finished with value: 0.6549767641697335 and parameters: {'n_estimators': 303}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:47:58,355] Trial 310 finished with value: 0.6549827522074627 and parameters: {'n_estimators': 285}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:48:53,264] Trial 311 finished with value: 0.6549190653703882 and parameters: {'n_estimators': 310}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:49:40,784] Trial 312 finished with value: 0.6546261965737017 and parameters: {'n_estimators': 277}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:50:32,778] Trial 313 finished with value: 0.6548113465811284 and parameters: {'n_estimators': 290}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:51:28,064] Trial 314 finished with value: 0.6548877622589856 and parameters: {'n_estimators': 313}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:52:16,628] Trial 315 finished with value: 0.6547794981368906 and parameters: {'n_estimators': 279}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:53:07,612] Trial 316 finished with value: 0.6550083493406669 and parameters: {'n_estimators': 294}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:53:58,821] Trial 317 finished with value: 0.6546505198052555 and parameters: {'n_estimators': 278}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:54:52,292] Trial 318 finished with value: 0.6549029207315045 and parameters: {'n_estimators': 315}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:55:45,781] Trial 319 finished with value: 0.6550804751817787 and parameters: {'n_estimators': 295}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:56:33,924] Trial 320 finished with value: 0.654754002776609 and parameters: {'n_estimators': 281}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:57:17,287] Trial 321 finished with value: 0.6542948169054015 and parameters: {'n_estimators': 238}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:58:09,543] Trial 322 finished with value: 0.654962912389877 and parameters: {'n_estimators': 307}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:58:58,642] Trial 323 finished with value: 0.6545867907572966 and parameters: {'n_estimators': 271}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 20:59:50,190] Trial 324 finished with value: 0.6550209813744247 and parameters: {'n_estimators': 293}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:00:31,729] Trial 325 finished with value: 0.6542948169054015 and parameters: {'n_estimators': 238}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:01:29,567] Trial 326 finished with value: 0.6548632926114742 and parameters: {'n_estimators': 324}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:02:17,769] Trial 327 finished with value: 0.6545099200352728 and parameters: {'n_estimators': 272}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:03:10,293] Trial 328 finished with value: 0.6549402767028133 and parameters: {'n_estimators': 306}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:04:00,541] Trial 329 finished with value: 0.6545867907572966 and parameters: {'n_estimators': 271}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:04:51,972] Trial 330 finished with value: 0.6548816481013293 and parameters: {'n_estimators': 287}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:05:34,303] Trial 331 finished with value: 0.6542948169054015 and parameters: {'n_estimators': 238}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:06:31,799] Trial 332 finished with value: 0.6548997028934084 and parameters: {'n_estimators': 325}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:07:18,999] Trial 333 finished with value: 0.6546553218905801 and parameters: {'n_estimators': 275}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:08:13,346] Trial 334 finished with value: 0.655138445090795 and parameters: {'n_estimators': 300}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:08:54,746] Trial 335 finished with value: 0.6542674011839023 and parameters: {'n_estimators': 242}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:09:46,308] Trial 336 finished with value: 0.6548582335643551 and parameters: {'n_estimators': 289}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:10:33,157] Trial 337 finished with value: 0.6545068855246307 and parameters: {'n_estimators': 267}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:11:30,068] Trial 338 finished with value: 0.6550522523549305 and parameters: {'n_estimators': 318}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:12:18,750] Trial 339 finished with value: 0.6546505198052555 and parameters: {'n_estimators': 278}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:13:11,469] Trial 340 finished with value: 0.6549402767028133 and parameters: {'n_estimators': 306}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:13:54,619] Trial 341 finished with value: 0.6542542267771698 and parameters: {'n_estimators': 240}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:14:44,662] Trial 342 finished with value: 0.6548973939298114 and parameters: {'n_estimators': 291}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:15:31,845] Trial 343 finished with value: 0.654524568203093 and parameters: {'n_estimators': 268}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:16:26,857] Trial 344 finished with value: 0.6549272014110101 and parameters: {'n_estimators': 305}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:17:10,308] Trial 345 finished with value: 0.6543539784932485 and parameters: {'n_estimators': 245}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:17:58,069] Trial 346 finished with value: 0.6545099200352728 and parameters: {'n_estimators': 272}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:18:54,117] Trial 347 finished with value: 0.6546688421409761 and parameters: {'n_estimators': 330}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:19:46,308] Trial 348 finished with value: 0.6549412145125006 and parameters: {'n_estimators': 286}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:20:30,312] Trial 349 finished with value: 0.654409306491487 and parameters: {'n_estimators': 251}. Best is trial 291 with value: 0.6675737200677461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6676\n",
      "\tBest params:\n",
      "\t\tn_estimators: 296\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681289    0.690470    0.678922    0.680260   \n",
      "1                    TP  401.000000  407.000000  374.000000  409.000000   \n",
      "2                    TN  328.000000  333.000000  362.000000  345.000000   \n",
      "3                    FP   98.000000   89.000000   90.000000   75.000000   \n",
      "4                    FN   72.000000   70.000000   73.000000   70.000000   \n",
      "5              Accuracy    0.810901    0.823137    0.818687    0.838710   \n",
      "6             Precision    0.803607    0.820565    0.806034    0.845041   \n",
      "7           Sensitivity    0.847780    0.853249    0.836689    0.853862   \n",
      "8           Specificity    0.770000    0.789100    0.800900    0.821400   \n",
      "9              F1 score    0.825103    0.836588    0.821076    0.849429   \n",
      "10  F1 score (weighted)    0.810454    0.822827    0.818642    0.838645   \n",
      "11     F1 score (macro)    0.809646    0.821930    0.818655    0.837888   \n",
      "12    Balanced Accuracy    0.808867    0.821175    0.818787    0.837645   \n",
      "13                  MCC    0.620663    0.644604    0.637896    0.675828   \n",
      "14                  NPV    0.820000    0.826300    0.832200    0.831300   \n",
      "15              ROC_AUC    0.808867    0.821175    0.818787    0.837645   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.674801    0.657296    0.656789  \n",
      "1   413.000000  392.000000  408.000000  \n",
      "2   339.000000  352.000000  340.000000  \n",
      "3    84.000000   86.000000   89.000000  \n",
      "4    63.000000   69.000000   62.000000  \n",
      "5     0.836485    0.827586    0.832036  \n",
      "6     0.830986    0.820084    0.820926  \n",
      "7     0.867647    0.850325    0.868085  \n",
      "8     0.801400    0.803700    0.792500  \n",
      "9     0.848921    0.834931    0.843847  \n",
      "10    0.836168    0.827441    0.831652  \n",
      "11    0.835370    0.827244    0.831069  \n",
      "12    0.834533    0.826989    0.830313  \n",
      "13    0.671662    0.655082    0.663654  \n",
      "14    0.843300    0.836100    0.845800  \n",
      "15    0.834533    0.826989    0.830313  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_6_cat = np.where((y_pred_rf_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 21:21:27,805] Trial 350 finished with value: 0.6577221620838902 and parameters: {'n_estimators': 292}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:22:18,078] Trial 351 finished with value: 0.6572568230653318 and parameters: {'n_estimators': 270}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:23:11,304] Trial 352 finished with value: 0.6578621551884746 and parameters: {'n_estimators': 312}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:23:57,813] Trial 353 finished with value: 0.6572821051443701 and parameters: {'n_estimators': 268}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:24:51,153] Trial 354 finished with value: 0.6577221620838902 and parameters: {'n_estimators': 292}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:25:35,234] Trial 355 finished with value: 0.6574244033166716 and parameters: {'n_estimators': 245}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:26:28,113] Trial 356 finished with value: 0.6579531422091186 and parameters: {'n_estimators': 308}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:27:18,453] Trial 357 finished with value: 0.6573817955444585 and parameters: {'n_estimators': 279}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:28:15,883] Trial 358 finished with value: 0.6576715391949524 and parameters: {'n_estimators': 330}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:29:00,808] Trial 359 finished with value: 0.6572778256770152 and parameters: {'n_estimators': 255}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:29:51,831] Trial 360 finished with value: 0.6576542708584705 and parameters: {'n_estimators': 289}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:30:33,870] Trial 361 finished with value: 0.6573295097442761 and parameters: {'n_estimators': 230}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:33:13,116] Trial 362 finished with value: 0.6579996355254012 and parameters: {'n_estimators': 905}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:34:00,177] Trial 363 finished with value: 0.6572821051443701 and parameters: {'n_estimators': 268}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:34:51,378] Trial 364 finished with value: 0.6577725002414818 and parameters: {'n_estimators': 298}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:35:46,636] Trial 365 finished with value: 0.6579160976378101 and parameters: {'n_estimators': 313}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:36:31,431] Trial 366 finished with value: 0.6574076161995461 and parameters: {'n_estimators': 252}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:37:19,712] Trial 367 finished with value: 0.6574631997015532 and parameters: {'n_estimators': 278}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:38:06,150] Trial 368 finished with value: 0.6571341044887677 and parameters: {'n_estimators': 267}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:38:48,462] Trial 369 finished with value: 0.6573295097442762 and parameters: {'n_estimators': 230}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:39:38,562] Trial 370 finished with value: 0.6577221620838902 and parameters: {'n_estimators': 292}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:40:37,295] Trial 371 finished with value: 0.6575788304544568 and parameters: {'n_estimators': 326}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:41:21,890] Trial 372 finished with value: 0.6573812936651576 and parameters: {'n_estimators': 248}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:42:09,258] Trial 373 finished with value: 0.6574886189727696 and parameters: {'n_estimators': 280}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:43:04,050] Trial 374 finished with value: 0.6579040620132608 and parameters: {'n_estimators': 303}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:43:50,548] Trial 375 finished with value: 0.6571551737628278 and parameters: {'n_estimators': 266}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:44:45,273] Trial 376 finished with value: 0.6579004612699526 and parameters: {'n_estimators': 315}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:45:35,086] Trial 377 finished with value: 0.6576287766591038 and parameters: {'n_estimators': 287}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:46:19,632] Trial 378 finished with value: 0.6574643993354814 and parameters: {'n_estimators': 250}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:47:06,151] Trial 379 finished with value: 0.6570640122444461 and parameters: {'n_estimators': 265}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:47:57,964] Trial 380 finished with value: 0.6577909549121671 and parameters: {'n_estimators': 300}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:48:38,628] Trial 381 finished with value: 0.657286970006268 and parameters: {'n_estimators': 229}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:49:37,623] Trial 382 finished with value: 0.6577783827868234 and parameters: {'n_estimators': 333}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:50:26,247] Trial 383 finished with value: 0.6573817955444585 and parameters: {'n_estimators': 279}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:51:14,186] Trial 384 finished with value: 0.6570755576735751 and parameters: {'n_estimators': 264}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:52:08,544] Trial 385 finished with value: 0.6579032714896538 and parameters: {'n_estimators': 305}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:52:59,641] Trial 386 finished with value: 0.65765162578987 and parameters: {'n_estimators': 285}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:53:43,341] Trial 387 finished with value: 0.6573624944199793 and parameters: {'n_estimators': 242}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:54:30,169] Trial 388 finished with value: 0.6570461540531944 and parameters: {'n_estimators': 261}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:57:23,105] Trial 389 finished with value: 0.6581859256723208 and parameters: {'n_estimators': 994}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:58:18,525] Trial 390 finished with value: 0.6578168253928647 and parameters: {'n_estimators': 318}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:59:08,856] Trial 391 finished with value: 0.6576735991345236 and parameters: {'n_estimators': 290}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 21:59:57,379] Trial 392 finished with value: 0.6573362177854492 and parameters: {'n_estimators': 274}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:00:40,636] Trial 393 finished with value: 0.6574435489916922 and parameters: {'n_estimators': 244}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:01:33,402] Trial 394 finished with value: 0.657772500241482 and parameters: {'n_estimators': 298}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:02:31,650] Trial 395 finished with value: 0.6576559408708323 and parameters: {'n_estimators': 335}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:03:23,712] Trial 396 finished with value: 0.6574827568770667 and parameters: {'n_estimators': 276}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:04:06,342] Trial 397 finished with value: 0.6573643403832445 and parameters: {'n_estimators': 254}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:04:59,051] Trial 398 finished with value: 0.6577305122605276 and parameters: {'n_estimators': 295}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:05:40,470] Trial 399 finished with value: 0.6573295097442761 and parameters: {'n_estimators': 230}. Best is trial 291 with value: 0.6675737200677461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6676\n",
      "\tBest params:\n",
      "\t\tn_estimators: 296\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681289    0.690470    0.678922    0.680260   \n",
      "1                    TP  401.000000  407.000000  374.000000  409.000000   \n",
      "2                    TN  328.000000  333.000000  362.000000  345.000000   \n",
      "3                    FP   98.000000   89.000000   90.000000   75.000000   \n",
      "4                    FN   72.000000   70.000000   73.000000   70.000000   \n",
      "5              Accuracy    0.810901    0.823137    0.818687    0.838710   \n",
      "6             Precision    0.803607    0.820565    0.806034    0.845041   \n",
      "7           Sensitivity    0.847780    0.853249    0.836689    0.853862   \n",
      "8           Specificity    0.770000    0.789100    0.800900    0.821400   \n",
      "9              F1 score    0.825103    0.836588    0.821076    0.849429   \n",
      "10  F1 score (weighted)    0.810454    0.822827    0.818642    0.838645   \n",
      "11     F1 score (macro)    0.809646    0.821930    0.818655    0.837888   \n",
      "12    Balanced Accuracy    0.808867    0.821175    0.818787    0.837645   \n",
      "13                  MCC    0.620663    0.644604    0.637896    0.675828   \n",
      "14                  NPV    0.820000    0.826300    0.832200    0.831300   \n",
      "15              ROC_AUC    0.808867    0.821175    0.818787    0.837645   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.674801    0.657296    0.656789    0.662378  \n",
      "1   413.000000  392.000000  408.000000  416.000000  \n",
      "2   339.000000  352.000000  340.000000  333.000000  \n",
      "3    84.000000   86.000000   89.000000   80.000000  \n",
      "4    63.000000   69.000000   62.000000   70.000000  \n",
      "5     0.836485    0.827586    0.832036    0.833148  \n",
      "6     0.830986    0.820084    0.820926    0.838710  \n",
      "7     0.867647    0.850325    0.868085    0.855967  \n",
      "8     0.801400    0.803700    0.792500    0.806300  \n",
      "9     0.848921    0.834931    0.843847    0.847251  \n",
      "10    0.836168    0.827441    0.831652    0.832975  \n",
      "11    0.835370    0.827244    0.831069    0.831713  \n",
      "12    0.834533    0.826989    0.830313    0.831131  \n",
      "13    0.671662    0.655082    0.663654    0.663636  \n",
      "14    0.843300    0.836100    0.845800    0.826300  \n",
      "15    0.834533    0.826989    0.830313    0.831131  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_7_cat = np.where((y_pred_rf_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 22:06:42,000] Trial 400 finished with value: 0.664079801758339 and parameters: {'n_estimators': 310}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:07:29,473] Trial 401 finished with value: 0.6646020569786777 and parameters: {'n_estimators': 269}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:08:19,230] Trial 402 finished with value: 0.6643178372728918 and parameters: {'n_estimators': 283}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:09:03,307] Trial 403 finished with value: 0.6645188225644499 and parameters: {'n_estimators': 253}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:09:56,435] Trial 404 finished with value: 0.6641455527965686 and parameters: {'n_estimators': 307}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:10:52,516] Trial 405 finished with value: 0.6637110742836949 and parameters: {'n_estimators': 322}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:11:43,669] Trial 406 finished with value: 0.6642683898393049 and parameters: {'n_estimators': 284}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:12:27,997] Trial 407 finished with value: 0.6646856952306688 and parameters: {'n_estimators': 268}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:13:09,709] Trial 408 finished with value: 0.664438835608206 and parameters: {'n_estimators': 234}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:14:01,611] Trial 409 finished with value: 0.6642802506226662 and parameters: {'n_estimators': 297}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:14:45,476] Trial 410 finished with value: 0.6644942216390666 and parameters: {'n_estimators': 259}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:15:38,398] Trial 411 finished with value: 0.6641484299192981 and parameters: {'n_estimators': 288}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:16:35,928] Trial 412 finished with value: 0.6632954104468226 and parameters: {'n_estimators': 340}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:17:30,119] Trial 413 finished with value: 0.6639033861234502 and parameters: {'n_estimators': 313}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:18:16,743] Trial 414 finished with value: 0.6646020569786775 and parameters: {'n_estimators': 269}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:18:58,440] Trial 415 finished with value: 0.6645127793326397 and parameters: {'n_estimators': 247}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:19:37,567] Trial 416 finished with value: 0.664346796338985 and parameters: {'n_estimators': 224}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:20:28,171] Trial 417 finished with value: 0.664175754217655 and parameters: {'n_estimators': 285}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:21:19,865] Trial 418 finished with value: 0.6641905367837351 and parameters: {'n_estimators': 301}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:22:06,847] Trial 419 finished with value: 0.6646617845649474 and parameters: {'n_estimators': 266}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:23:04,029] Trial 420 finished with value: 0.6636482217853389 and parameters: {'n_estimators': 325}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:23:48,237] Trial 421 finished with value: 0.6645165992083207 and parameters: {'n_estimators': 245}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:24:37,395] Trial 422 finished with value: 0.6645178490656939 and parameters: {'n_estimators': 279}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:25:29,570] Trial 423 finished with value: 0.6641934978166089 and parameters: {'n_estimators': 302}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:26:14,779] Trial 424 finished with value: 0.6644269881399725 and parameters: {'n_estimators': 257}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:27:03,350] Trial 425 finished with value: 0.6645542404010711 and parameters: {'n_estimators': 278}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:28:00,899] Trial 426 finished with value: 0.6638274262352739 and parameters: {'n_estimators': 318}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:28:52,400] Trial 427 finished with value: 0.6641972351814517 and parameters: {'n_estimators': 295}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:29:35,488] Trial 428 finished with value: 0.6644259036849969 and parameters: {'n_estimators': 235}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:30:21,246] Trial 429 finished with value: 0.6645819617304263 and parameters: {'n_estimators': 263}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:31:10,762] Trial 430 finished with value: 0.6643905543090128 and parameters: {'n_estimators': 282}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:33:40,343] Trial 431 finished with value: 0.6640119840108222 and parameters: {'n_estimators': 856}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:34:23,194] Trial 432 finished with value: 0.6645156875492917 and parameters: {'n_estimators': 250}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:35:25,288] Trial 433 finished with value: 0.6634699353060436 and parameters: {'n_estimators': 342}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:36:19,059] Trial 434 finished with value: 0.6641455527965686 and parameters: {'n_estimators': 307}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:37:05,409] Trial 435 finished with value: 0.6645397106618269 and parameters: {'n_estimators': 271}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:37:43,862] Trial 436 finished with value: 0.6642027851287874 and parameters: {'n_estimators': 220}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:38:37,041] Trial 437 finished with value: 0.6641761140442541 and parameters: {'n_estimators': 291}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:39:23,552] Trial 438 finished with value: 0.6644506580629999 and parameters: {'n_estimators': 258}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:40:18,006] Trial 439 finished with value: 0.6638274262352739 and parameters: {'n_estimators': 318}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:41:06,832] Trial 440 finished with value: 0.6645014661762085 and parameters: {'n_estimators': 280}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:41:50,621] Trial 441 finished with value: 0.6647084685543898 and parameters: {'n_estimators': 240}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:42:42,220] Trial 442 finished with value: 0.6642802506226662 and parameters: {'n_estimators': 297}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:43:29,529] Trial 443 finished with value: 0.6645441919795954 and parameters: {'n_estimators': 270}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:44:21,638] Trial 444 finished with value: 0.6641585899299246 and parameters: {'n_estimators': 290}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:45:02,155] Trial 445 finished with value: 0.6644127422783623 and parameters: {'n_estimators': 252}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:45:59,535] Trial 446 finished with value: 0.6637293877165507 and parameters: {'n_estimators': 321}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:46:47,472] Trial 447 finished with value: 0.664539710661827 and parameters: {'n_estimators': 271}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:47:42,156] Trial 448 finished with value: 0.6641290490194987 and parameters: {'n_estimators': 306}. Best is trial 291 with value: 0.6675737200677461.\n",
      "[I 2023-12-19 22:48:21,157] Trial 449 finished with value: 0.6642414236769254 and parameters: {'n_estimators': 227}. Best is trial 291 with value: 0.6675737200677461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6676\n",
      "\tBest params:\n",
      "\t\tn_estimators: 296\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681289    0.690470    0.678922    0.680260   \n",
      "1                    TP  401.000000  407.000000  374.000000  409.000000   \n",
      "2                    TN  328.000000  333.000000  362.000000  345.000000   \n",
      "3                    FP   98.000000   89.000000   90.000000   75.000000   \n",
      "4                    FN   72.000000   70.000000   73.000000   70.000000   \n",
      "5              Accuracy    0.810901    0.823137    0.818687    0.838710   \n",
      "6             Precision    0.803607    0.820565    0.806034    0.845041   \n",
      "7           Sensitivity    0.847780    0.853249    0.836689    0.853862   \n",
      "8           Specificity    0.770000    0.789100    0.800900    0.821400   \n",
      "9              F1 score    0.825103    0.836588    0.821076    0.849429   \n",
      "10  F1 score (weighted)    0.810454    0.822827    0.818642    0.838645   \n",
      "11     F1 score (macro)    0.809646    0.821930    0.818655    0.837888   \n",
      "12    Balanced Accuracy    0.808867    0.821175    0.818787    0.837645   \n",
      "13                  MCC    0.620663    0.644604    0.637896    0.675828   \n",
      "14                  NPV    0.820000    0.826300    0.832200    0.831300   \n",
      "15              ROC_AUC    0.808867    0.821175    0.818787    0.837645   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.674801    0.657296    0.656789    0.662378    0.686148  \n",
      "1   413.000000  392.000000  408.000000  416.000000  412.000000  \n",
      "2   339.000000  352.000000  340.000000  333.000000  337.000000  \n",
      "3    84.000000   86.000000   89.000000   80.000000   89.000000  \n",
      "4    63.000000   69.000000   62.000000   70.000000   61.000000  \n",
      "5     0.836485    0.827586    0.832036    0.833148    0.833148  \n",
      "6     0.830986    0.820084    0.820926    0.838710    0.822355  \n",
      "7     0.867647    0.850325    0.868085    0.855967    0.871036  \n",
      "8     0.801400    0.803700    0.792500    0.806300    0.791100  \n",
      "9     0.848921    0.834931    0.843847    0.847251    0.845996  \n",
      "10    0.836168    0.827441    0.831652    0.832975    0.832711  \n",
      "11    0.835370    0.827244    0.831069    0.831713    0.831979  \n",
      "12    0.834533    0.826989    0.830313    0.831131    0.831058  \n",
      "13    0.671662    0.655082    0.663654    0.663636    0.665593  \n",
      "14    0.843300    0.836100    0.845800    0.826300    0.846700  \n",
      "15    0.834533    0.826989    0.830313    0.831131    0.831058  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_8_cat = np.where((y_pred_rf_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 22:49:18,123] Trial 450 finished with value: 0.6770450722722531 and parameters: {'n_estimators': 285}. Best is trial 450 with value: 0.6770450722722531.\n",
      "[I 2023-12-19 22:50:17,751] Trial 451 finished with value: 0.6776311526539046 and parameters: {'n_estimators': 337}. Best is trial 451 with value: 0.6776311526539046.\n",
      "[I 2023-12-19 22:51:16,865] Trial 452 finished with value: 0.6776925124064742 and parameters: {'n_estimators': 338}. Best is trial 452 with value: 0.6776925124064742.\n",
      "[I 2023-12-19 22:52:17,642] Trial 453 finished with value: 0.6777109446434624 and parameters: {'n_estimators': 339}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 22:53:17,588] Trial 454 finished with value: 0.6775726005439362 and parameters: {'n_estimators': 346}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 22:54:18,507] Trial 455 finished with value: 0.6775980617130125 and parameters: {'n_estimators': 347}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 22:55:20,779] Trial 456 finished with value: 0.6775130317450438 and parameters: {'n_estimators': 359}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 22:56:24,863] Trial 457 finished with value: 0.6774497224531719 and parameters: {'n_estimators': 355}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 22:57:27,064] Trial 458 finished with value: 0.6774054482791042 and parameters: {'n_estimators': 356}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 22:58:33,244] Trial 459 finished with value: 0.6776404050458313 and parameters: {'n_estimators': 366}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 22:59:34,615] Trial 460 finished with value: 0.6775801189341722 and parameters: {'n_estimators': 362}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:00:41,174] Trial 461 finished with value: 0.6775888297612302 and parameters: {'n_estimators': 364}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:01:44,306] Trial 462 finished with value: 0.6774054482791042 and parameters: {'n_estimators': 356}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:02:46,723] Trial 463 finished with value: 0.6775688398344386 and parameters: {'n_estimators': 363}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:03:49,124] Trial 464 finished with value: 0.6774519607364846 and parameters: {'n_estimators': 357}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:04:53,851] Trial 465 finished with value: 0.6775760330865912 and parameters: {'n_estimators': 365}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:05:56,206] Trial 466 finished with value: 0.6774054482791041 and parameters: {'n_estimators': 356}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:07:00,061] Trial 467 finished with value: 0.6775457541419286 and parameters: {'n_estimators': 370}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:08:03,882] Trial 468 finished with value: 0.6775760330865912 and parameters: {'n_estimators': 365}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:09:08,985] Trial 469 finished with value: 0.6775760330865912 and parameters: {'n_estimators': 365}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:10:13,301] Trial 470 finished with value: 0.6776160431794638 and parameters: {'n_estimators': 368}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:11:17,786] Trial 471 finished with value: 0.6776175044215077 and parameters: {'n_estimators': 369}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:12:21,551] Trial 472 finished with value: 0.6775381971380494 and parameters: {'n_estimators': 374}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:13:25,694] Trial 473 finished with value: 0.6775381971380494 and parameters: {'n_estimators': 374}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:14:28,645] Trial 474 finished with value: 0.6775632992889685 and parameters: {'n_estimators': 373}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:15:35,629] Trial 475 finished with value: 0.6775642042618407 and parameters: {'n_estimators': 371}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:16:37,346] Trial 476 finished with value: 0.6775766841894473 and parameters: {'n_estimators': 372}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:17:41,817] Trial 477 finished with value: 0.6775381971380495 and parameters: {'n_estimators': 374}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:18:47,950] Trial 478 finished with value: 0.6776210965233034 and parameters: {'n_estimators': 375}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:19:53,211] Trial 479 finished with value: 0.677587788992553 and parameters: {'n_estimators': 377}. Best is trial 453 with value: 0.6777109446434624.\n",
      "[I 2023-12-19 23:21:01,080] Trial 480 finished with value: 0.6777305135698143 and parameters: {'n_estimators': 385}. Best is trial 480 with value: 0.6777305135698143.\n",
      "[I 2023-12-19 23:22:06,535] Trial 481 finished with value: 0.6775999212957718 and parameters: {'n_estimators': 378}. Best is trial 480 with value: 0.6777305135698143.\n",
      "[I 2023-12-19 23:23:15,233] Trial 482 finished with value: 0.6777882335249166 and parameters: {'n_estimators': 387}. Best is trial 482 with value: 0.6777882335249166.\n",
      "[I 2023-12-19 23:24:21,599] Trial 483 finished with value: 0.6777305135698143 and parameters: {'n_estimators': 385}. Best is trial 482 with value: 0.6777882335249166.\n",
      "[I 2023-12-19 23:25:31,224] Trial 484 finished with value: 0.6777866675488861 and parameters: {'n_estimators': 384}. Best is trial 482 with value: 0.6777882335249166.\n",
      "[I 2023-12-19 23:26:40,470] Trial 485 finished with value: 0.6777963392976842 and parameters: {'n_estimators': 390}. Best is trial 485 with value: 0.6777963392976842.\n",
      "[I 2023-12-19 23:27:50,275] Trial 486 finished with value: 0.6778724633357124 and parameters: {'n_estimators': 393}. Best is trial 486 with value: 0.6778724633357124.\n",
      "[I 2023-12-19 23:28:59,296] Trial 487 finished with value: 0.677929539433957 and parameters: {'n_estimators': 397}. Best is trial 487 with value: 0.677929539433957.\n",
      "[I 2023-12-19 23:30:09,601] Trial 488 finished with value: 0.677929539433957 and parameters: {'n_estimators': 397}. Best is trial 487 with value: 0.677929539433957.\n",
      "[I 2023-12-19 23:31:18,390] Trial 489 finished with value: 0.6781729631388023 and parameters: {'n_estimators': 407}. Best is trial 489 with value: 0.6781729631388023.\n",
      "[I 2023-12-19 23:32:30,217] Trial 490 finished with value: 0.6778980417416933 and parameters: {'n_estimators': 396}. Best is trial 489 with value: 0.6781729631388023.\n",
      "[I 2023-12-19 23:33:40,126] Trial 491 finished with value: 0.6778724633357124 and parameters: {'n_estimators': 393}. Best is trial 489 with value: 0.6781729631388023.\n",
      "[I 2023-12-19 23:34:47,986] Trial 492 finished with value: 0.6778606413731028 and parameters: {'n_estimators': 398}. Best is trial 489 with value: 0.6781729631388023.\n",
      "[I 2023-12-19 23:35:58,639] Trial 493 finished with value: 0.6778606413731028 and parameters: {'n_estimators': 398}. Best is trial 489 with value: 0.6781729631388023.\n",
      "[I 2023-12-19 23:37:10,394] Trial 494 finished with value: 0.678089441009368 and parameters: {'n_estimators': 408}. Best is trial 489 with value: 0.6781729631388023.\n",
      "[I 2023-12-19 23:38:20,676] Trial 495 finished with value: 0.6781508307549375 and parameters: {'n_estimators': 406}. Best is trial 489 with value: 0.6781729631388023.\n",
      "[I 2023-12-19 23:39:32,470] Trial 496 finished with value: 0.6780513687450183 and parameters: {'n_estimators': 411}. Best is trial 489 with value: 0.6781729631388023.\n",
      "[I 2023-12-19 23:40:43,288] Trial 497 finished with value: 0.6780513687450183 and parameters: {'n_estimators': 411}. Best is trial 489 with value: 0.6781729631388023.\n",
      "[I 2023-12-19 23:41:57,123] Trial 498 finished with value: 0.6780503880885765 and parameters: {'n_estimators': 414}. Best is trial 489 with value: 0.6781729631388023.\n",
      "[I 2023-12-19 23:43:08,262] Trial 499 finished with value: 0.6780503880885765 and parameters: {'n_estimators': 414}. Best is trial 489 with value: 0.6781729631388023.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6782\n",
      "\tBest params:\n",
      "\t\tn_estimators: 407\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681289    0.690470    0.678922    0.680260   \n",
      "1                    TP  401.000000  407.000000  374.000000  409.000000   \n",
      "2                    TN  328.000000  333.000000  362.000000  345.000000   \n",
      "3                    FP   98.000000   89.000000   90.000000   75.000000   \n",
      "4                    FN   72.000000   70.000000   73.000000   70.000000   \n",
      "5              Accuracy    0.810901    0.823137    0.818687    0.838710   \n",
      "6             Precision    0.803607    0.820565    0.806034    0.845041   \n",
      "7           Sensitivity    0.847780    0.853249    0.836689    0.853862   \n",
      "8           Specificity    0.770000    0.789100    0.800900    0.821400   \n",
      "9              F1 score    0.825103    0.836588    0.821076    0.849429   \n",
      "10  F1 score (weighted)    0.810454    0.822827    0.818642    0.838645   \n",
      "11     F1 score (macro)    0.809646    0.821930    0.818655    0.837888   \n",
      "12    Balanced Accuracy    0.808867    0.821175    0.818787    0.837645   \n",
      "13                  MCC    0.620663    0.644604    0.637896    0.675828   \n",
      "14                  NPV    0.820000    0.826300    0.832200    0.831300   \n",
      "15              ROC_AUC    0.808867    0.821175    0.818787    0.837645   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.674801    0.657296    0.656789    0.662378    0.686148    0.656025  \n",
      "1   413.000000  392.000000  408.000000  416.000000  412.000000  395.000000  \n",
      "2   339.000000  352.000000  340.000000  333.000000  337.000000  340.000000  \n",
      "3    84.000000   86.000000   89.000000   80.000000   89.000000  103.000000  \n",
      "4    63.000000   69.000000   62.000000   70.000000   61.000000   61.000000  \n",
      "5     0.836485    0.827586    0.832036    0.833148    0.833148    0.817575  \n",
      "6     0.830986    0.820084    0.820926    0.838710    0.822355    0.793173  \n",
      "7     0.867647    0.850325    0.868085    0.855967    0.871036    0.866228  \n",
      "8     0.801400    0.803700    0.792500    0.806300    0.791100    0.767500  \n",
      "9     0.848921    0.834931    0.843847    0.847251    0.845996    0.828092  \n",
      "10    0.836168    0.827441    0.831652    0.832975    0.832711    0.817052  \n",
      "11    0.835370    0.827244    0.831069    0.831713    0.831979    0.816890  \n",
      "12    0.834533    0.826989    0.830313    0.831131    0.831058    0.816861  \n",
      "13    0.671662    0.655082    0.663654    0.663636    0.665593    0.637377  \n",
      "14    0.843300    0.836100    0.845800    0.826300    0.846700    0.847900  \n",
      "15    0.834533    0.826989    0.830313    0.831131    0.831058    0.816861  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_9_cat = np.where((y_pred_rf_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6782\n",
      "\tBest params:\n",
      "\t\tn_estimators: 407\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.018239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>202.900000</td>\n",
       "      <td>8.862530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>172.700000</td>\n",
       "      <td>6.832114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>40.400000</td>\n",
       "      <td>3.835507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>33.200000</td>\n",
       "      <td>3.938415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.836146</td>\n",
       "      <td>0.011753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.833810</td>\n",
       "      <td>0.015976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.859201</td>\n",
       "      <td>0.018004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.810450</td>\n",
       "      <td>0.016031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.846208</td>\n",
       "      <td>0.013750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.835960</td>\n",
       "      <td>0.011761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.835255</td>\n",
       "      <td>0.011566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.834832</td>\n",
       "      <td>0.011450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.671191</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.838900</td>\n",
       "      <td>0.016222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.834832</td>\n",
       "      <td>0.011450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.684966     0.018239\n",
       "1                    TP       202.900000     8.862530\n",
       "2                    TN       172.700000     6.832114\n",
       "3                    FP        40.400000     3.835507\n",
       "4                    FN        33.200000     3.938415\n",
       "5              Accuracy         0.836146     0.011753\n",
       "6             Precision         0.833810     0.015976\n",
       "7           Sensitivity         0.859201     0.018004\n",
       "8           Specificity         0.810450     0.016031\n",
       "9              F1 score         0.846208     0.013750\n",
       "10  F1 score (weighted)         0.835960     0.011761\n",
       "11     F1 score (macro)         0.835255     0.011566\n",
       "12    Balanced Accuracy         0.834832     0.011450\n",
       "13                  MCC         0.671191     0.023022\n",
       "14                  NPV         0.838900     0.016222\n",
       "15              ROC_AUC         0.834832     0.011450"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.681289</td>\n",
       "      <td>0.690470</td>\n",
       "      <td>0.678922</td>\n",
       "      <td>0.680260</td>\n",
       "      <td>0.674801</td>\n",
       "      <td>0.657296</td>\n",
       "      <td>0.656789</td>\n",
       "      <td>0.662378</td>\n",
       "      <td>0.686148</td>\n",
       "      <td>0.656025</td>\n",
       "      <td>0.672438</td>\n",
       "      <td>0.013102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>402.700000</td>\n",
       "      <td>12.719626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>362.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>340.900000</td>\n",
       "      <td>9.982763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>88.300000</td>\n",
       "      <td>8.056054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>4.771443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.810901</td>\n",
       "      <td>0.823137</td>\n",
       "      <td>0.818687</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.836485</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.833148</td>\n",
       "      <td>0.833148</td>\n",
       "      <td>0.817575</td>\n",
       "      <td>0.827141</td>\n",
       "      <td>0.009191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.803607</td>\n",
       "      <td>0.820565</td>\n",
       "      <td>0.806034</td>\n",
       "      <td>0.845041</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.820084</td>\n",
       "      <td>0.820926</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.822355</td>\n",
       "      <td>0.793173</td>\n",
       "      <td>0.820148</td>\n",
       "      <td>0.015944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.847780</td>\n",
       "      <td>0.853249</td>\n",
       "      <td>0.836689</td>\n",
       "      <td>0.853862</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.850325</td>\n",
       "      <td>0.868085</td>\n",
       "      <td>0.855967</td>\n",
       "      <td>0.871036</td>\n",
       "      <td>0.866228</td>\n",
       "      <td>0.857087</td>\n",
       "      <td>0.010982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.789100</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.821400</td>\n",
       "      <td>0.801400</td>\n",
       "      <td>0.803700</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.767500</td>\n",
       "      <td>0.794390</td>\n",
       "      <td>0.016364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.825103</td>\n",
       "      <td>0.836588</td>\n",
       "      <td>0.821076</td>\n",
       "      <td>0.849429</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>0.834931</td>\n",
       "      <td>0.843847</td>\n",
       "      <td>0.847251</td>\n",
       "      <td>0.845996</td>\n",
       "      <td>0.828092</td>\n",
       "      <td>0.838123</td>\n",
       "      <td>0.010519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.822827</td>\n",
       "      <td>0.818642</td>\n",
       "      <td>0.838645</td>\n",
       "      <td>0.836168</td>\n",
       "      <td>0.827441</td>\n",
       "      <td>0.831652</td>\n",
       "      <td>0.832975</td>\n",
       "      <td>0.832711</td>\n",
       "      <td>0.817052</td>\n",
       "      <td>0.826857</td>\n",
       "      <td>0.009247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.809646</td>\n",
       "      <td>0.821930</td>\n",
       "      <td>0.818655</td>\n",
       "      <td>0.837888</td>\n",
       "      <td>0.835370</td>\n",
       "      <td>0.827244</td>\n",
       "      <td>0.831069</td>\n",
       "      <td>0.831713</td>\n",
       "      <td>0.831979</td>\n",
       "      <td>0.816890</td>\n",
       "      <td>0.826238</td>\n",
       "      <td>0.009099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.808867</td>\n",
       "      <td>0.821175</td>\n",
       "      <td>0.818787</td>\n",
       "      <td>0.837645</td>\n",
       "      <td>0.834533</td>\n",
       "      <td>0.826989</td>\n",
       "      <td>0.830313</td>\n",
       "      <td>0.831131</td>\n",
       "      <td>0.831058</td>\n",
       "      <td>0.816861</td>\n",
       "      <td>0.825736</td>\n",
       "      <td>0.009016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.620663</td>\n",
       "      <td>0.644604</td>\n",
       "      <td>0.637896</td>\n",
       "      <td>0.675828</td>\n",
       "      <td>0.671662</td>\n",
       "      <td>0.655082</td>\n",
       "      <td>0.663654</td>\n",
       "      <td>0.663636</td>\n",
       "      <td>0.665593</td>\n",
       "      <td>0.637377</td>\n",
       "      <td>0.653600</td>\n",
       "      <td>0.017776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.826300</td>\n",
       "      <td>0.832200</td>\n",
       "      <td>0.831300</td>\n",
       "      <td>0.843300</td>\n",
       "      <td>0.836100</td>\n",
       "      <td>0.845800</td>\n",
       "      <td>0.826300</td>\n",
       "      <td>0.846700</td>\n",
       "      <td>0.847900</td>\n",
       "      <td>0.835590</td>\n",
       "      <td>0.009913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.808867</td>\n",
       "      <td>0.821175</td>\n",
       "      <td>0.818787</td>\n",
       "      <td>0.837645</td>\n",
       "      <td>0.834533</td>\n",
       "      <td>0.826989</td>\n",
       "      <td>0.830313</td>\n",
       "      <td>0.831131</td>\n",
       "      <td>0.831058</td>\n",
       "      <td>0.816861</td>\n",
       "      <td>0.825736</td>\n",
       "      <td>0.009016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.681289    0.690470    0.678922    0.680260   \n",
       "1                    TP  401.000000  407.000000  374.000000  409.000000   \n",
       "2                    TN  328.000000  333.000000  362.000000  345.000000   \n",
       "3                    FP   98.000000   89.000000   90.000000   75.000000   \n",
       "4                    FN   72.000000   70.000000   73.000000   70.000000   \n",
       "5              Accuracy    0.810901    0.823137    0.818687    0.838710   \n",
       "6             Precision    0.803607    0.820565    0.806034    0.845041   \n",
       "7           Sensitivity    0.847780    0.853249    0.836689    0.853862   \n",
       "8           Specificity    0.770000    0.789100    0.800900    0.821400   \n",
       "9              F1 score    0.825103    0.836588    0.821076    0.849429   \n",
       "10  F1 score (weighted)    0.810454    0.822827    0.818642    0.838645   \n",
       "11     F1 score (macro)    0.809646    0.821930    0.818655    0.837888   \n",
       "12    Balanced Accuracy    0.808867    0.821175    0.818787    0.837645   \n",
       "13                  MCC    0.620663    0.644604    0.637896    0.675828   \n",
       "14                  NPV    0.820000    0.826300    0.832200    0.831300   \n",
       "15              ROC_AUC    0.808867    0.821175    0.818787    0.837645   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.674801    0.657296    0.656789    0.662378    0.686148    0.656025   \n",
       "1   413.000000  392.000000  408.000000  416.000000  412.000000  395.000000   \n",
       "2   339.000000  352.000000  340.000000  333.000000  337.000000  340.000000   \n",
       "3    84.000000   86.000000   89.000000   80.000000   89.000000  103.000000   \n",
       "4    63.000000   69.000000   62.000000   70.000000   61.000000   61.000000   \n",
       "5     0.836485    0.827586    0.832036    0.833148    0.833148    0.817575   \n",
       "6     0.830986    0.820084    0.820926    0.838710    0.822355    0.793173   \n",
       "7     0.867647    0.850325    0.868085    0.855967    0.871036    0.866228   \n",
       "8     0.801400    0.803700    0.792500    0.806300    0.791100    0.767500   \n",
       "9     0.848921    0.834931    0.843847    0.847251    0.845996    0.828092   \n",
       "10    0.836168    0.827441    0.831652    0.832975    0.832711    0.817052   \n",
       "11    0.835370    0.827244    0.831069    0.831713    0.831979    0.816890   \n",
       "12    0.834533    0.826989    0.830313    0.831131    0.831058    0.816861   \n",
       "13    0.671662    0.655082    0.663654    0.663636    0.665593    0.637377   \n",
       "14    0.843300    0.836100    0.845800    0.826300    0.846700    0.847900   \n",
       "15    0.834533    0.826989    0.830313    0.831131    0.831058    0.816861   \n",
       "\n",
       "           ave        std  \n",
       "0     0.672438   0.013102  \n",
       "1   402.700000  12.719626  \n",
       "2   340.900000   9.982763  \n",
       "3    88.300000   8.056054  \n",
       "4    67.100000   4.771443  \n",
       "5     0.827141   0.009191  \n",
       "6     0.820148   0.015944  \n",
       "7     0.857087   0.010982  \n",
       "8     0.794390   0.016364  \n",
       "9     0.838123   0.010519  \n",
       "10    0.826857   0.009247  \n",
       "11    0.826238   0.009099  \n",
       "12    0.825736   0.009016  \n",
       "13    0.653600   0.017776  \n",
       "14    0.835590   0.009913  \n",
       "15    0.825736   0.009016  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_rf0</th>\n",
       "      <th>y_pred_rf1</th>\n",
       "      <th>y_pred_rf2</th>\n",
       "      <th>y_pred_rf3</th>\n",
       "      <th>y_pred_rf4</th>\n",
       "      <th>y_pred_rf_ave</th>\n",
       "      <th>y_pred_rf_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3921050</td>\n",
       "      <td>0</td>\n",
       "      <td>6.17</td>\n",
       "      <td>5.745405</td>\n",
       "      <td>5.777961</td>\n",
       "      <td>5.752727</td>\n",
       "      <td>5.792752</td>\n",
       "      <td>5.737715</td>\n",
       "      <td>5.829427</td>\n",
       "      <td>0.153480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL270476</td>\n",
       "      <td>1</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.513853</td>\n",
       "      <td>6.673002</td>\n",
       "      <td>6.596345</td>\n",
       "      <td>6.755946</td>\n",
       "      <td>6.560680</td>\n",
       "      <td>6.649971</td>\n",
       "      <td>0.102994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3664128</td>\n",
       "      <td>2</td>\n",
       "      <td>7.62</td>\n",
       "      <td>6.985934</td>\n",
       "      <td>6.982142</td>\n",
       "      <td>6.807652</td>\n",
       "      <td>7.017887</td>\n",
       "      <td>6.862733</td>\n",
       "      <td>7.046058</td>\n",
       "      <td>0.267192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4456250</td>\n",
       "      <td>3</td>\n",
       "      <td>5.26</td>\n",
       "      <td>5.871720</td>\n",
       "      <td>5.953489</td>\n",
       "      <td>5.916302</td>\n",
       "      <td>5.820229</td>\n",
       "      <td>5.850442</td>\n",
       "      <td>5.778697</td>\n",
       "      <td>0.235959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2408818</td>\n",
       "      <td>4</td>\n",
       "      <td>6.32</td>\n",
       "      <td>6.294889</td>\n",
       "      <td>6.190686</td>\n",
       "      <td>6.228387</td>\n",
       "      <td>6.184382</td>\n",
       "      <td>6.180012</td>\n",
       "      <td>6.233059</td>\n",
       "      <td>0.055351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4250302</td>\n",
       "      <td>4487</td>\n",
       "      <td>10.25</td>\n",
       "      <td>9.101310</td>\n",
       "      <td>8.978829</td>\n",
       "      <td>9.015004</td>\n",
       "      <td>9.116962</td>\n",
       "      <td>8.946996</td>\n",
       "      <td>9.234850</td>\n",
       "      <td>0.458075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL483893</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.83</td>\n",
       "      <td>8.022598</td>\n",
       "      <td>7.975680</td>\n",
       "      <td>7.753303</td>\n",
       "      <td>7.905447</td>\n",
       "      <td>7.946143</td>\n",
       "      <td>7.905528</td>\n",
       "      <td>0.090511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL3655914</td>\n",
       "      <td>4489</td>\n",
       "      <td>6.69</td>\n",
       "      <td>6.682162</td>\n",
       "      <td>6.682975</td>\n",
       "      <td>6.750413</td>\n",
       "      <td>6.694152</td>\n",
       "      <td>6.696839</td>\n",
       "      <td>6.699424</td>\n",
       "      <td>0.023423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL467876</td>\n",
       "      <td>4490</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.177964</td>\n",
       "      <td>7.187125</td>\n",
       "      <td>7.201744</td>\n",
       "      <td>7.161826</td>\n",
       "      <td>7.233759</td>\n",
       "      <td>7.227070</td>\n",
       "      <td>0.080472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4458544</td>\n",
       "      <td>4491</td>\n",
       "      <td>7.37</td>\n",
       "      <td>6.824521</td>\n",
       "      <td>6.871947</td>\n",
       "      <td>6.827862</td>\n",
       "      <td>6.761970</td>\n",
       "      <td>6.654754</td>\n",
       "      <td>6.885176</td>\n",
       "      <td>0.227474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_rf0  y_pred_rf1  \\\n",
       "0         CHEMBL3921050            0     6.17    5.745405    5.777961   \n",
       "1          CHEMBL270476            1     6.80    6.513853    6.673002   \n",
       "2         CHEMBL3664128            2     7.62    6.985934    6.982142   \n",
       "3         CHEMBL4456250            3     5.26    5.871720    5.953489   \n",
       "4         CHEMBL2408818            4     6.32    6.294889    6.190686   \n",
       "...                 ...          ...      ...         ...         ...   \n",
       "4487      CHEMBL4250302         4487    10.25    9.101310    8.978829   \n",
       "4488       CHEMBL483893         4488     7.83    8.022598    7.975680   \n",
       "4489      CHEMBL3655914         4489     6.69    6.682162    6.682975   \n",
       "4490       CHEMBL467876         4490     7.40    7.177964    7.187125   \n",
       "4491      CHEMBL4458544         4491     7.37    6.824521    6.871947   \n",
       "\n",
       "      y_pred_rf2  y_pred_rf3  y_pred_rf4  y_pred_rf_ave  y_pred_rf_std  \n",
       "0       5.752727    5.792752    5.737715       5.829427       0.153480  \n",
       "1       6.596345    6.755946    6.560680       6.649971       0.102994  \n",
       "2       6.807652    7.017887    6.862733       7.046058       0.267192  \n",
       "3       5.916302    5.820229    5.850442       5.778697       0.235959  \n",
       "4       6.228387    6.184382    6.180012       6.233059       0.055351  \n",
       "...          ...         ...         ...            ...            ...  \n",
       "4487    9.015004    9.116962    8.946996       9.234850       0.458075  \n",
       "4488    7.753303    7.905447    7.946143       7.905528       0.090511  \n",
       "4489    6.750413    6.694152    6.696839       6.699424       0.023423  \n",
       "4490    7.201744    7.161826    7.233759       7.227070       0.080472  \n",
       "4491    6.827862    6.761970    6.654754       6.885176       0.227474  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=1121218, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where((y_pred_optimized_rf >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "rf_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.6803 with a standard deviation of 0.0201\n",
      "RF optimized model r2_score 0.6822 with a standard deviation of 0.0204\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_reg, \"OUTPUT/rf_reg.joblib\")\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.666572     0.018482\n",
      "1                    TP       196.200000     5.921711\n",
      "2                    TN       170.600000     5.777350\n",
      "3                    FP        42.500000     5.060742\n",
      "4                    FN        39.900000     6.297266\n",
      "5              Accuracy         0.816563     0.011010\n",
      "6             Precision         0.822140     0.018438\n",
      "7           Sensitivity         0.831347     0.023445\n",
      "8           Specificity         0.800840     0.019826\n",
      "9              F1 score         0.826372     0.011142\n",
      "10  F1 score (weighted)         0.816510     0.010956\n",
      "11     F1 score (macro)         0.815876     0.010971\n",
      "12    Balanced Accuracy         0.816093     0.010824\n",
      "13                  MCC         0.632586     0.022058\n",
      "14                  NPV         0.810840     0.026209\n",
      "15              ROC_AUC         0.816093     0.010824\n",
      "CPU times: user 43.5 s, sys: 156 ms, total: 43.7 s\n",
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=16,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=16,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=16,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 23:53:27,190] A new study created in memory with name: lgbmRegressor\n",
      "[I 2023-12-19 23:53:30,752] Trial 0 finished with value: 0.6601916797955442 and parameters: {'n_estimators': 808, 'learning_rate': 0.17006849684275036, 'max_depth': 6, 'max_bin': 235, 'num_leaves': 552}. Best is trial 0 with value: 0.6601916797955442.\n",
      "[I 2023-12-19 23:53:32,723] Trial 1 finished with value: 0.6503410065411449 and parameters: {'n_estimators': 73, 'learning_rate': 0.18130944829005818, 'max_depth': 12, 'max_bin': 299, 'num_leaves': 190}. Best is trial 0 with value: 0.6601916797955442.\n",
      "[I 2023-12-19 23:53:40,916] Trial 2 finished with value: 0.6240651300172317 and parameters: {'n_estimators': 432, 'learning_rate': 0.016830317475350274, 'max_depth': 8, 'max_bin': 264, 'num_leaves': 363}. Best is trial 0 with value: 0.6601916797955442.\n",
      "[I 2023-12-19 23:53:44,411] Trial 3 finished with value: 0.6565936931808236 and parameters: {'n_estimators': 598, 'learning_rate': 0.1211702986291031, 'max_depth': 4, 'max_bin': 269, 'num_leaves': 654}. Best is trial 0 with value: 0.6601916797955442.\n",
      "[I 2023-12-19 23:53:45,999] Trial 4 finished with value: 0.5686110841286796 and parameters: {'n_estimators': 205, 'learning_rate': 0.10588720986143454, 'max_depth': 3, 'max_bin': 294, 'num_leaves': 466}. Best is trial 0 with value: 0.6601916797955442.\n",
      "[I 2023-12-19 23:53:49,839] Trial 5 finished with value: 0.6126206988769527 and parameters: {'n_estimators': 523, 'learning_rate': 0.04184982429641622, 'max_depth': 4, 'max_bin': 182, 'num_leaves': 502}. Best is trial 0 with value: 0.6601916797955442.\n",
      "[I 2023-12-19 23:53:54,507] Trial 6 finished with value: 0.6695787433924656 and parameters: {'n_estimators': 825, 'learning_rate': 0.17173225705018763, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 402}. Best is trial 6 with value: 0.6695787433924656.\n",
      "[I 2023-12-19 23:53:57,178] Trial 7 finished with value: 0.6563427437572269 and parameters: {'n_estimators': 328, 'learning_rate': 0.17161715580948805, 'max_depth': 5, 'max_bin': 234, 'num_leaves': 250}. Best is trial 6 with value: 0.6695787433924656.\n",
      "[I 2023-12-19 23:53:59,566] Trial 8 finished with value: 0.5791452757484102 and parameters: {'n_estimators': 78, 'learning_rate': 0.03907067472651009, 'max_depth': 10, 'max_bin': 176, 'num_leaves': 614}. Best is trial 6 with value: 0.6695787433924656.\n",
      "[I 2023-12-19 23:54:05,914] Trial 9 finished with value: 0.47739612279257315 and parameters: {'n_estimators': 350, 'learning_rate': 0.005655414374421114, 'max_depth': 7, 'max_bin': 227, 'num_leaves': 58}. Best is trial 6 with value: 0.6695787433924656.\n",
      "[I 2023-12-19 23:54:10,463] Trial 10 finished with value: 0.6694858776005221 and parameters: {'n_estimators': 883, 'learning_rate': 0.19900460543642856, 'max_depth': 12, 'max_bin': 191, 'num_leaves': 737}. Best is trial 6 with value: 0.6695787433924656.\n",
      "[I 2023-12-19 23:54:14,349] Trial 11 finished with value: 0.6658670582514585 and parameters: {'n_estimators': 877, 'learning_rate': 0.1971890897599454, 'max_depth': 12, 'max_bin': 199, 'num_leaves': 713}. Best is trial 6 with value: 0.6695787433924656.\n",
      "[I 2023-12-19 23:54:19,497] Trial 12 finished with value: 0.6788934227485494 and parameters: {'n_estimators': 717, 'learning_rate': 0.1422182309083948, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 744}. Best is trial 12 with value: 0.6788934227485494.\n",
      "[I 2023-12-19 23:54:23,872] Trial 13 finished with value: 0.6714753029994729 and parameters: {'n_estimators': 697, 'learning_rate': 0.13948279568382374, 'max_depth': 10, 'max_bin': 156, 'num_leaves': 356}. Best is trial 12 with value: 0.6788934227485494.\n",
      "[I 2023-12-19 23:54:28,433] Trial 14 finished with value: 0.6726204955396945 and parameters: {'n_estimators': 687, 'learning_rate': 0.13412231259612706, 'max_depth': 9, 'max_bin': 159, 'num_leaves': 287}. Best is trial 12 with value: 0.6788934227485494.\n",
      "[I 2023-12-19 23:54:32,539] Trial 15 finished with value: 0.6738689742744391 and parameters: {'n_estimators': 695, 'learning_rate': 0.1442230599467104, 'max_depth': 9, 'max_bin': 153, 'num_leaves': 231}. Best is trial 12 with value: 0.6788934227485494.\n",
      "[I 2023-12-19 23:54:41,665] Trial 16 finished with value: 0.6791533812612165 and parameters: {'n_estimators': 704, 'learning_rate': 0.07650748468059432, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 110}. Best is trial 16 with value: 0.6791533812612165.\n",
      "[I 2023-12-19 23:54:47,649] Trial 17 finished with value: 0.6806928917699904 and parameters: {'n_estimators': 608, 'learning_rate': 0.08496584269069102, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 87}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:54:53,627] Trial 18 finished with value: 0.6730885263129041 and parameters: {'n_estimators': 560, 'learning_rate': 0.07935128159599286, 'max_depth': 8, 'max_bin': 250, 'num_leaves': 47}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:55:00,201] Trial 19 finished with value: 0.6788862526839801 and parameters: {'n_estimators': 615, 'learning_rate': 0.08522817514103637, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 132}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:55:06,545] Trial 20 finished with value: 0.6712852133783211 and parameters: {'n_estimators': 447, 'learning_rate': 0.07155789158836706, 'max_depth': 9, 'max_bin': 213, 'num_leaves': 159}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:55:12,017] Trial 21 finished with value: 0.6803249608533527 and parameters: {'n_estimators': 755, 'learning_rate': 0.10157387203142906, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 112}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:55:17,201] Trial 22 finished with value: 0.6749328637903089 and parameters: {'n_estimators': 785, 'learning_rate': 0.10041488099302207, 'max_depth': 11, 'max_bin': 173, 'num_leaves': 95}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:55:25,237] Trial 23 finished with value: 0.6740233049761453 and parameters: {'n_estimators': 646, 'learning_rate': 0.0762852372501178, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 33}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:55:31,681] Trial 24 finished with value: 0.6674071960979571 and parameters: {'n_estimators': 753, 'learning_rate': 0.06014307543410293, 'max_depth': 7, 'max_bin': 217, 'num_leaves': 126}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:55:37,616] Trial 25 finished with value: 0.6758863558672664 and parameters: {'n_estimators': 521, 'learning_rate': 0.09538796966578691, 'max_depth': 10, 'max_bin': 246, 'num_leaves': 299}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:55:42,947] Trial 26 finished with value: 0.6736293811694726 and parameters: {'n_estimators': 617, 'learning_rate': 0.11406163017405965, 'max_depth': 9, 'max_bin': 182, 'num_leaves': 195}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:55:48,926] Trial 27 finished with value: 0.6725769122052613 and parameters: {'n_estimators': 757, 'learning_rate': 0.09472251174523263, 'max_depth': 8, 'max_bin': 201, 'num_leaves': 119}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:55:57,320] Trial 28 finished with value: 0.6740883701908873 and parameters: {'n_estimators': 391, 'learning_rate': 0.06779355640651938, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 82}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:56:02,124] Trial 29 finished with value: 0.6687521605986516 and parameters: {'n_estimators': 810, 'learning_rate': 0.0873724000491621, 'max_depth': 6, 'max_bin': 233, 'num_leaves': 178}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:56:09,425] Trial 30 finished with value: 0.6741895020799478 and parameters: {'n_estimators': 528, 'learning_rate': 0.05826200443005308, 'max_depth': 10, 'max_bin': 246, 'num_leaves': 266}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:56:15,206] Trial 31 finished with value: 0.6748490413521806 and parameters: {'n_estimators': 736, 'learning_rate': 0.12162836722017148, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 568}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:56:20,044] Trial 32 finished with value: 0.6745763831308293 and parameters: {'n_estimators': 664, 'learning_rate': 0.10992819099857887, 'max_depth': 9, 'max_bin': 219, 'num_leaves': 215}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:56:24,945] Trial 33 finished with value: 0.6762579715967701 and parameters: {'n_estimators': 816, 'learning_rate': 0.14712140738719226, 'max_depth': 11, 'max_bin': 186, 'num_leaves': 325}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:56:31,578] Trial 34 finished with value: 0.6765121455401588 and parameters: {'n_estimators': 726, 'learning_rate': 0.09046439948210454, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 461}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:56:35,430] Trial 35 finished with value: 0.6711180769661601 and parameters: {'n_estimators': 586, 'learning_rate': 0.15577659743001976, 'max_depth': 8, 'max_bin': 191, 'num_leaves': 165}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:56:42,276] Trial 36 finished with value: 0.6784429747842298 and parameters: {'n_estimators': 652, 'learning_rate': 0.11892603048920018, 'max_depth': 9, 'max_bin': 227, 'num_leaves': 413}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:56:47,743] Trial 37 finished with value: 0.6792350228190756 and parameters: {'n_estimators': 844, 'learning_rate': 0.10082178876893126, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 88}. Best is trial 17 with value: 0.6806928917699904.\n",
      "[I 2023-12-19 23:56:54,102] Trial 38 finished with value: 0.6820079319553323 and parameters: {'n_estimators': 852, 'learning_rate': 0.10340146316890783, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 84}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:01,967] Trial 39 finished with value: 0.6770238057509415 and parameters: {'n_estimators': 899, 'learning_rate': 0.10074688301828431, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 65}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:07,885] Trial 40 finished with value: 0.6785807043467781 and parameters: {'n_estimators': 843, 'learning_rate': 0.10443465939151295, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 154}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:14,298] Trial 41 finished with value: 0.6748698432582061 and parameters: {'n_estimators': 778, 'learning_rate': 0.08544316694348247, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 122}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:19,552] Trial 42 finished with value: 0.6771128464262599 and parameters: {'n_estimators': 841, 'learning_rate': 0.1087999561546378, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 91}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:25,457] Trial 43 finished with value: 0.6731942036432693 and parameters: {'n_estimators': 850, 'learning_rate': 0.12474414543838477, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 90}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:33,939] Trial 44 finished with value: 0.6788138535106636 and parameters: {'n_estimators': 790, 'learning_rate': 0.0957934621084326, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 195}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:35,514] Trial 45 finished with value: 0.5506780021779033 and parameters: {'n_estimators': 215, 'learning_rate': 0.08143980546852023, 'max_depth': 3, 'max_bin': 267, 'num_leaves': 33}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:40,772] Trial 46 finished with value: 0.6803293098247133 and parameters: {'n_estimators': 857, 'learning_rate': 0.10902162168794137, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 139}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:45,862] Trial 47 finished with value: 0.6768535517235028 and parameters: {'n_estimators': 862, 'learning_rate': 0.13019787620496295, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 72}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:51,734] Trial 48 finished with value: 0.6785434251069018 and parameters: {'n_estimators': 897, 'learning_rate': 0.11424969926373214, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 243}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:57:56,179] Trial 49 finished with value: 0.6570406865180325 and parameters: {'n_estimators': 810, 'learning_rate': 0.10577523326065119, 'max_depth': 4, 'max_bin': 259, 'num_leaves': 215}. Best is trial 38 with value: 0.6820079319553323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 852\n",
      "\t\tlearning_rate: 0.10340146316890783\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 285\n",
      "\t\tnum_leaves: 84\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.682897\n",
      "1                    TP  396.000000\n",
      "2                    TN  341.000000\n",
      "3                    FP   85.000000\n",
      "4                    FN   77.000000\n",
      "5              Accuracy    0.819800\n",
      "6             Precision    0.823285\n",
      "7           Sensitivity    0.837209\n",
      "8           Specificity    0.800500\n",
      "9              F1 score    0.830189\n",
      "10  F1 score (weighted)    0.819701\n",
      "11     F1 score (macro)    0.819123\n",
      "12    Balanced Accuracy    0.818839\n",
      "13                  MCC    0.638376\n",
      "14                  NPV    0.815800\n",
      "15              ROC_AUC    0.818839\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_0_cat = np.where((y_pred_lgbm_0>= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 23:58:03,668] Trial 50 finished with value: 0.6678424912504921 and parameters: {'n_estimators': 771, 'learning_rate': 0.11603625043579888, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 141}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:58:08,697] Trial 51 finished with value: 0.6620695953086513 and parameters: {'n_estimators': 709, 'learning_rate': 0.09935212280261914, 'max_depth': 10, 'max_bin': 279, 'num_leaves': 107}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:58:13,695] Trial 52 finished with value: 0.6609467292792116 and parameters: {'n_estimators': 860, 'learning_rate': 0.08897305336961962, 'max_depth': 10, 'max_bin': 238, 'num_leaves': 67}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:58:19,975] Trial 53 finished with value: 0.6646756963804248 and parameters: {'n_estimators': 742, 'learning_rate': 0.07755178770584908, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 148}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:58:25,596] Trial 54 finished with value: 0.6624634002887535 and parameters: {'n_estimators': 670, 'learning_rate': 0.12669827799096806, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 30}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:58:30,881] Trial 55 finished with value: 0.6621392346998706 and parameters: {'n_estimators': 820, 'learning_rate': 0.1089821793991776, 'max_depth': 10, 'max_bin': 299, 'num_leaves': 108}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:58:35,927] Trial 56 finished with value: 0.6654041899551663 and parameters: {'n_estimators': 789, 'learning_rate': 0.10161070647007214, 'max_depth': 9, 'max_bin': 213, 'num_leaves': 58}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:58:42,676] Trial 57 finished with value: 0.6650888414081624 and parameters: {'n_estimators': 628, 'learning_rate': 0.09365383906689392, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 183}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:58:50,327] Trial 58 finished with value: 0.6615113849956649 and parameters: {'n_estimators': 700, 'learning_rate': 0.07252253716362067, 'max_depth': 10, 'max_bin': 228, 'num_leaves': 129}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:58:56,013] Trial 59 finished with value: 0.6673676317099163 and parameters: {'n_estimators': 565, 'learning_rate': 0.08133708819812073, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 102}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:01,287] Trial 60 finished with value: 0.6621252201204764 and parameters: {'n_estimators': 490, 'learning_rate': 0.09086600338399832, 'max_depth': 9, 'max_bin': 212, 'num_leaves': 52}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:06,401] Trial 61 finished with value: 0.6601837316278927 and parameters: {'n_estimators': 721, 'learning_rate': 0.13771752196404774, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 711}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:11,838] Trial 62 finished with value: 0.6637659869220015 and parameters: {'n_estimators': 757, 'learning_rate': 0.12499358647443934, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 601}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:15,769] Trial 63 finished with value: 0.6543448245827204 and parameters: {'n_estimators': 873, 'learning_rate': 0.11752024237981487, 'max_depth': 8, 'max_bin': 188, 'num_leaves': 658}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:20,867] Trial 64 finished with value: 0.6607813182057447 and parameters: {'n_estimators': 693, 'learning_rate': 0.1323341118547926, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 529}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:29,705] Trial 65 finished with value: 0.6593305778431661 and parameters: {'n_estimators': 834, 'learning_rate': 0.06684831080978089, 'max_depth': 7, 'max_bin': 218, 'num_leaves': 427}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:35,107] Trial 66 finished with value: 0.6652203531866997 and parameters: {'n_estimators': 742, 'learning_rate': 0.0960895177468489, 'max_depth': 11, 'max_bin': 236, 'num_leaves': 162}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:40,334] Trial 67 finished with value: 0.6653795707024057 and parameters: {'n_estimators': 795, 'learning_rate': 0.10786926499827904, 'max_depth': 10, 'max_bin': 177, 'num_leaves': 81}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:46,251] Trial 68 finished with value: 0.6603548989790287 and parameters: {'n_estimators': 676, 'learning_rate': 0.08545604388364529, 'max_depth': 9, 'max_bin': 195, 'num_leaves': 271}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:52,326] Trial 69 finished with value: 0.6660326416563289 and parameters: {'n_estimators': 636, 'learning_rate': 0.1144597254737823, 'max_depth': 11, 'max_bin': 202, 'num_leaves': 373}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-19 23:59:57,557] Trial 70 finished with value: 0.6641495012061279 and parameters: {'n_estimators': 588, 'learning_rate': 0.1020452054238739, 'max_depth': 10, 'max_bin': 291, 'num_leaves': 323}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:05,588] Trial 71 finished with value: 0.6696207118787723 and parameters: {'n_estimators': 611, 'learning_rate': 0.08454165372706783, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 136}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:11,452] Trial 72 finished with value: 0.6633572930528415 and parameters: {'n_estimators': 559, 'learning_rate': 0.09011671525557162, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 118}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:18,543] Trial 73 finished with value: 0.6644989568288662 and parameters: {'n_estimators': 443, 'learning_rate': 0.0791318552411967, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 205}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:24,717] Trial 74 finished with value: 0.6642359056473557 and parameters: {'n_estimators': 761, 'learning_rate': 0.07241512047384227, 'max_depth': 10, 'max_bin': 231, 'num_leaves': 178}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:31,025] Trial 75 finished with value: 0.6693911065786902 and parameters: {'n_estimators': 716, 'learning_rate': 0.09798806876155325, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 85}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:36,069] Trial 76 finished with value: 0.6623345505784445 and parameters: {'n_estimators': 876, 'learning_rate': 0.11145322502118768, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 144}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:40,458] Trial 77 finished with value: 0.6622150619512561 and parameters: {'n_estimators': 654, 'learning_rate': 0.12033834260491251, 'max_depth': 9, 'max_bin': 208, 'num_leaves': 53}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:47,437] Trial 78 finished with value: 0.6579633929047976 and parameters: {'n_estimators': 269, 'learning_rate': 0.10301145138533693, 'max_depth': 11, 'max_bin': 190, 'num_leaves': 494}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:52,396] Trial 79 finished with value: 0.6630025036652984 and parameters: {'n_estimators': 398, 'learning_rate': 0.09274990466898782, 'max_depth': 10, 'max_bin': 241, 'num_leaves': 228}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:53,472] Trial 80 finished with value: 0.5237601442375951 and parameters: {'n_estimators': 53, 'learning_rate': 0.08571629913456397, 'max_depth': 6, 'max_bin': 289, 'num_leaves': 103}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:00:59,672] Trial 81 finished with value: 0.6667618269409425 and parameters: {'n_estimators': 789, 'learning_rate': 0.09475171197867426, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 190}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:01:05,486] Trial 82 finished with value: 0.6644788755314467 and parameters: {'n_estimators': 829, 'learning_rate': 0.10687936101164446, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 166}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:01:11,293] Trial 83 finished with value: 0.6669638133318511 and parameters: {'n_estimators': 800, 'learning_rate': 0.09757450448469228, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 129}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:01:18,519] Trial 84 finished with value: 0.6650480783893741 and parameters: {'n_estimators': 770, 'learning_rate': 0.08179931528267426, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 84}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:01:24,113] Trial 85 finished with value: 0.6626155450457312 and parameters: {'n_estimators': 820, 'learning_rate': 0.0887071497546721, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 113}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:01:29,569] Trial 86 finished with value: 0.6602056638480176 and parameters: {'n_estimators': 856, 'learning_rate': 0.1118843098173388, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 64}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:01:35,770] Trial 87 finished with value: 0.6656829240503062 and parameters: {'n_estimators': 734, 'learning_rate': 0.10485162218999461, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 38}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:01:38,864] Trial 88 finished with value: 0.6494758247873349 and parameters: {'n_estimators': 139, 'learning_rate': 0.07539129018356786, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 151}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:01:44,982] Trial 89 finished with value: 0.6633749696277301 and parameters: {'n_estimators': 897, 'learning_rate': 0.09641619859363265, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 96}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:01:52,022] Trial 90 finished with value: 0.6624886888208028 and parameters: {'n_estimators': 514, 'learning_rate': 0.09022415728116606, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 73}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:01:57,700] Trial 91 finished with value: 0.6679356038140163 and parameters: {'n_estimators': 806, 'learning_rate': 0.10628711790406864, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 172}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:02:03,459] Trial 92 finished with value: 0.6623974593077575 and parameters: {'n_estimators': 866, 'learning_rate': 0.10085341149391693, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 147}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:02:09,602] Trial 93 finished with value: 0.6655697854771565 and parameters: {'n_estimators': 849, 'learning_rate': 0.11342324114276259, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 127}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:02:15,243] Trial 94 finished with value: 0.6626792173037652 and parameters: {'n_estimators': 828, 'learning_rate': 0.12343278344229161, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 200}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:02:19,035] Trial 95 finished with value: 0.6451907862073358 and parameters: {'n_estimators': 679, 'learning_rate': 0.10425146513311968, 'max_depth': 5, 'max_bin': 300, 'num_leaves': 111}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:02:24,014] Trial 96 finished with value: 0.6675439559001753 and parameters: {'n_estimators': 771, 'learning_rate': 0.11783775879429573, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 47}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:02:30,027] Trial 97 finished with value: 0.6685319705860989 and parameters: {'n_estimators': 883, 'learning_rate': 0.09283716949429414, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 153}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:02:34,794] Trial 98 finished with value: 0.656502054356618 and parameters: {'n_estimators': 844, 'learning_rate': 0.0994428245814337, 'max_depth': 10, 'max_bin': 184, 'num_leaves': 95}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:02:41,430] Trial 99 finished with value: 0.6620488015457926 and parameters: {'n_estimators': 749, 'learning_rate': 0.08485988187611225, 'max_depth': 9, 'max_bin': 195, 'num_leaves': 134}. Best is trial 38 with value: 0.6820079319553323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 852\n",
      "\t\tlearning_rate: 0.10340146316890783\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 285\n",
      "\t\tnum_leaves: 84\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.682897    0.713345\n",
      "1                    TP  396.000000  399.000000\n",
      "2                    TN  341.000000  341.000000\n",
      "3                    FP   85.000000   81.000000\n",
      "4                    FN   77.000000   78.000000\n",
      "5              Accuracy    0.819800    0.823137\n",
      "6             Precision    0.823285    0.831250\n",
      "7           Sensitivity    0.837209    0.836478\n",
      "8           Specificity    0.800500    0.808100\n",
      "9              F1 score    0.830189    0.833856\n",
      "10  F1 score (weighted)    0.819701    0.823099\n",
      "11     F1 score (macro)    0.819123    0.822398\n",
      "12    Balanced Accuracy    0.818839    0.822267\n",
      "13                  MCC    0.638376    0.644814\n",
      "14                  NPV    0.815800    0.813800\n",
      "15              ROC_AUC    0.818839    0.822267\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_1_cat = np.where((y_pred_lgbm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:02:49,716] Trial 100 finished with value: 0.669693822793139 and parameters: {'n_estimators': 783, 'learning_rate': 0.11133682589852359, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 224}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:02:54,924] Trial 101 finished with value: 0.6729844384529687 and parameters: {'n_estimators': 891, 'learning_rate': 0.1286693661600422, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 243}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:03:00,241] Trial 102 finished with value: 0.672297671560398 and parameters: {'n_estimators': 838, 'learning_rate': 0.12025835156752807, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 259}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:03:06,340] Trial 103 finished with value: 0.6740707727484434 and parameters: {'n_estimators': 899, 'learning_rate': 0.10831865816906915, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 182}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:03:11,608] Trial 104 finished with value: 0.6664727225692261 and parameters: {'n_estimators': 806, 'learning_rate': 0.11414722499444957, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 162}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:03:17,004] Trial 105 finished with value: 0.6705564527502486 and parameters: {'n_estimators': 707, 'learning_rate': 0.10404083192893646, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 659}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:03:22,170] Trial 106 finished with value: 0.6694260262288393 and parameters: {'n_estimators': 865, 'learning_rate': 0.09758172934939498, 'max_depth': 10, 'max_bin': 281, 'num_leaves': 209}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:03:30,404] Trial 107 finished with value: 0.6718630416990439 and parameters: {'n_estimators': 729, 'learning_rate': 0.09195697475221543, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 738}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:03:37,123] Trial 108 finished with value: 0.6779699941480655 and parameters: {'n_estimators': 610, 'learning_rate': 0.08742003966766634, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 305}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:03:43,295] Trial 109 finished with value: 0.671420911880536 and parameters: {'n_estimators': 876, 'learning_rate': 0.07744515951510733, 'max_depth': 10, 'max_bin': 269, 'num_leaves': 81}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:03:49,108] Trial 110 finished with value: 0.6717972555724571 and parameters: {'n_estimators': 818, 'learning_rate': 0.1016243199808863, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 243}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:03:54,347] Trial 111 finished with value: 0.6681903220450106 and parameters: {'n_estimators': 641, 'learning_rate': 0.10904830188656348, 'max_depth': 9, 'max_bin': 217, 'num_leaves': 120}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:00,718] Trial 112 finished with value: 0.6668079079643399 and parameters: {'n_estimators': 663, 'learning_rate': 0.11981156352713127, 'max_depth': 8, 'max_bin': 210, 'num_leaves': 400}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:05,535] Trial 113 finished with value: 0.6635166757334221 and parameters: {'n_estimators': 576, 'learning_rate': 0.11597305746376145, 'max_depth': 9, 'max_bin': 215, 'num_leaves': 422}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:10,922] Trial 114 finished with value: 0.6697300038657061 and parameters: {'n_estimators': 688, 'learning_rate': 0.12545854320902075, 'max_depth': 10, 'max_bin': 163, 'num_leaves': 569}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:15,889] Trial 115 finished with value: 0.6687306536455699 and parameters: {'n_estimators': 634, 'learning_rate': 0.13490862465533807, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 193}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:21,899] Trial 116 finished with value: 0.6708125689167967 and parameters: {'n_estimators': 596, 'learning_rate': 0.0947195059572269, 'max_depth': 10, 'max_bin': 273, 'num_leaves': 642}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:27,340] Trial 117 finished with value: 0.6698942594996501 and parameters: {'n_estimators': 842, 'learning_rate': 0.09892805627443349, 'max_depth': 9, 'max_bin': 205, 'num_leaves': 711}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:32,062] Trial 118 finished with value: 0.6666226220602871 and parameters: {'n_estimators': 549, 'learning_rate': 0.13000764105068066, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 100}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:37,404] Trial 119 finished with value: 0.6650132330275532 and parameters: {'n_estimators': 651, 'learning_rate': 0.1053439737192435, 'max_depth': 10, 'max_bin': 277, 'num_leaves': 339}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:41,945] Trial 120 finished with value: 0.6633750298675731 and parameters: {'n_estimators': 762, 'learning_rate': 0.11150639084609949, 'max_depth': 7, 'max_bin': 286, 'num_leaves': 142}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:48,881] Trial 121 finished with value: 0.6781456901342805 and parameters: {'n_estimators': 628, 'learning_rate': 0.08729353753959523, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 298}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:04:57,189] Trial 122 finished with value: 0.6738781360585545 and parameters: {'n_estimators': 609, 'learning_rate': 0.08307951826772651, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 289}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:05:01,997] Trial 123 finished with value: 0.6673774220904076 and parameters: {'n_estimators': 709, 'learning_rate': 0.14329856046746903, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 360}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:05:09,207] Trial 124 finished with value: 0.6728573596563372 and parameters: {'n_estimators': 628, 'learning_rate': 0.08000528000181688, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 486}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:05:14,796] Trial 125 finished with value: 0.6717566959309167 and parameters: {'n_estimators': 663, 'learning_rate': 0.08780174404109717, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 70}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:05:21,371] Trial 126 finished with value: 0.6734055727679344 and parameters: {'n_estimators': 796, 'learning_rate': 0.09279683304319265, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 275}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:05:28,407] Trial 127 finished with value: 0.6693288060619128 and parameters: {'n_estimators': 694, 'learning_rate': 0.11657962562988997, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 117}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:05:33,839] Trial 128 finished with value: 0.6697755931343202 and parameters: {'n_estimators': 852, 'learning_rate': 0.10183551504976773, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 528}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:05:38,793] Trial 129 finished with value: 0.6640794092540625 and parameters: {'n_estimators': 735, 'learning_rate': 0.15372952253669786, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 175}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:05:46,397] Trial 130 finished with value: 0.6732507116914004 and parameters: {'n_estimators': 829, 'learning_rate': 0.12271665694144698, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 456}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:05:52,952] Trial 131 finished with value: 0.6737627155052738 and parameters: {'n_estimators': 610, 'learning_rate': 0.09532148878886605, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 311}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:05:59,594] Trial 132 finished with value: 0.6741742288837765 and parameters: {'n_estimators': 676, 'learning_rate': 0.08848213231296935, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 293}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:06:06,340] Trial 133 finished with value: 0.675075795624129 and parameters: {'n_estimators': 578, 'learning_rate': 0.0831176363049737, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 343}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:06:13,367] Trial 134 finished with value: 0.6779540852107957 and parameters: {'n_estimators': 620, 'learning_rate': 0.08712881356999781, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 307}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:06:19,094] Trial 135 finished with value: 0.6771760727471505 and parameters: {'n_estimators': 651, 'learning_rate': 0.10754369715690079, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 276}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:06:25,965] Trial 136 finished with value: 0.6752636609821258 and parameters: {'n_estimators': 532, 'learning_rate': 0.07663965763140619, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 235}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:06:31,654] Trial 137 finished with value: 0.6688794252859107 and parameters: {'n_estimators': 872, 'learning_rate': 0.09805625309351212, 'max_depth': 10, 'max_bin': 221, 'num_leaves': 255}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:06:39,774] Trial 138 finished with value: 0.6721114395677594 and parameters: {'n_estimators': 718, 'learning_rate': 0.07085573034863946, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 158}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:06:44,590] Trial 139 finished with value: 0.6672841399012021 and parameters: {'n_estimators': 600, 'learning_rate': 0.09057054720363178, 'max_depth': 9, 'max_bin': 208, 'num_leaves': 133}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:06:49,511] Trial 140 finished with value: 0.6709235334597098 and parameters: {'n_estimators': 780, 'learning_rate': 0.1035529540768035, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 106}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:06:57,394] Trial 141 finished with value: 0.6741065233485851 and parameters: {'n_estimators': 633, 'learning_rate': 0.08706021115172378, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 319}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:07:03,671] Trial 142 finished with value: 0.67441623255645 and parameters: {'n_estimators': 620, 'learning_rate': 0.0819139917816466, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 380}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:07:10,613] Trial 143 finished with value: 0.6743638574062679 and parameters: {'n_estimators': 546, 'learning_rate': 0.08534484804271165, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 214}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:07:17,066] Trial 144 finished with value: 0.6749928987778739 and parameters: {'n_estimators': 595, 'learning_rate': 0.09305756929583146, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 346}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:07:23,702] Trial 145 finished with value: 0.6778157683432758 and parameters: {'n_estimators': 682, 'learning_rate': 0.09837194797368046, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 301}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:07:30,692] Trial 146 finished with value: 0.6733337903176981 and parameters: {'n_estimators': 504, 'learning_rate': 0.07458564315729656, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 685}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:07:36,582] Trial 147 finished with value: 0.6721854098054856 and parameters: {'n_estimators': 887, 'learning_rate': 0.08959075759483387, 'max_depth': 10, 'max_bin': 289, 'num_leaves': 62}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:07:44,928] Trial 148 finished with value: 0.6715822825919346 and parameters: {'n_estimators': 856, 'learning_rate': 0.07944315179762759, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 304}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:07:50,033] Trial 149 finished with value: 0.6651616395660742 and parameters: {'n_estimators': 571, 'learning_rate': 0.10983031766884904, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 83}. Best is trial 38 with value: 0.6820079319553323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 852\n",
      "\t\tlearning_rate: 0.10340146316890783\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 285\n",
      "\t\tnum_leaves: 84\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.682897    0.713345    0.691267\n",
      "1                    TP  396.000000  399.000000  365.000000\n",
      "2                    TN  341.000000  341.000000  360.000000\n",
      "3                    FP   85.000000   81.000000   92.000000\n",
      "4                    FN   77.000000   78.000000   82.000000\n",
      "5              Accuracy    0.819800    0.823137    0.806452\n",
      "6             Precision    0.823285    0.831250    0.798687\n",
      "7           Sensitivity    0.837209    0.836478    0.816555\n",
      "8           Specificity    0.800500    0.808100    0.796500\n",
      "9              F1 score    0.830189    0.833856    0.807522\n",
      "10  F1 score (weighted)    0.819701    0.823099    0.806440\n",
      "11     F1 score (macro)    0.819123    0.822398    0.806446\n",
      "12    Balanced Accuracy    0.818839    0.822267    0.806507\n",
      "13                  MCC    0.638376    0.644814    0.613091\n",
      "14                  NPV    0.815800    0.813800    0.814500\n",
      "15              ROC_AUC    0.818839    0.822267    0.806507\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_2_cat = np.where((y_pred_lgbm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:07:58,707] Trial 150 finished with value: 0.6646882089921314 and parameters: {'n_estimators': 621, 'learning_rate': 0.10163416893485479, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 328}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:08:04,209] Trial 151 finished with value: 0.666202342387118 and parameters: {'n_estimators': 690, 'learning_rate': 0.09807914371377899, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 283}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:08:10,039] Trial 152 finished with value: 0.6700641644320665 and parameters: {'n_estimators': 674, 'learning_rate': 0.09537284853976499, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 303}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:08:15,612] Trial 153 finished with value: 0.6646977188173538 and parameters: {'n_estimators': 660, 'learning_rate': 0.10701486474853303, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 263}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:08:20,129] Trial 154 finished with value: 0.6652774223939328 and parameters: {'n_estimators': 815, 'learning_rate': 0.11344556021624942, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 97}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:08:25,593] Trial 155 finished with value: 0.669280113566691 and parameters: {'n_estimators': 646, 'learning_rate': 0.10094244596849947, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 127}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:08:33,293] Trial 156 finished with value: 0.6648833312182226 and parameters: {'n_estimators': 742, 'learning_rate': 0.08793302006518879, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 317}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:08:38,164] Trial 157 finished with value: 0.6636071040666527 and parameters: {'n_estimators': 709, 'learning_rate': 0.09530051508876994, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 146}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:08:45,133] Trial 158 finished with value: 0.6676825084211625 and parameters: {'n_estimators': 589, 'learning_rate': 0.06566749536194624, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 330}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:08:50,055] Trial 159 finished with value: 0.6637999271242006 and parameters: {'n_estimators': 753, 'learning_rate': 0.08402556344538958, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 109}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:08:55,413] Trial 160 finished with value: 0.6603775367795309 and parameters: {'n_estimators': 642, 'learning_rate': 0.10511872807788128, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 293}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:00,518] Trial 161 finished with value: 0.6664933494355928 and parameters: {'n_estimators': 465, 'learning_rate': 0.10840091929218083, 'max_depth': 12, 'max_bin': 189, 'num_leaves': 285}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:06,735] Trial 162 finished with value: 0.6702042543533479 and parameters: {'n_estimators': 647, 'learning_rate': 0.11832335071659819, 'max_depth': 12, 'max_bin': 193, 'num_leaves': 276}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:11,503] Trial 163 finished with value: 0.6665819610454939 and parameters: {'n_estimators': 615, 'learning_rate': 0.11212118064665261, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 255}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:17,281] Trial 164 finished with value: 0.669472338661609 and parameters: {'n_estimators': 663, 'learning_rate': 0.09208612385243964, 'max_depth': 12, 'max_bin': 179, 'num_leaves': 230}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:22,134] Trial 165 finished with value: 0.6677006552088232 and parameters: {'n_estimators': 680, 'learning_rate': 0.10546141336339253, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 598}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:26,626] Trial 166 finished with value: 0.6644609499367774 and parameters: {'n_estimators': 696, 'learning_rate': 0.09990563156623145, 'max_depth': 9, 'max_bin': 216, 'num_leaves': 85}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:31,172] Trial 167 finished with value: 0.660287301410493 and parameters: {'n_estimators': 631, 'learning_rate': 0.1146929569676557, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 269}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:37,639] Trial 168 finished with value: 0.668275829902359 and parameters: {'n_estimators': 838, 'learning_rate': 0.07918595599244249, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 304}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:41,360] Trial 169 finished with value: 0.629765213592646 and parameters: {'n_estimators': 871, 'learning_rate': 0.0973744658593457, 'max_depth': 3, 'max_bin': 205, 'num_leaves': 247}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:46,307] Trial 170 finished with value: 0.6635239400311719 and parameters: {'n_estimators': 724, 'learning_rate': 0.12585242336466612, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 191}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:51,425] Trial 171 finished with value: 0.6669554584657013 and parameters: {'n_estimators': 853, 'learning_rate': 0.108236721648237, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 49}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:09:56,941] Trial 172 finished with value: 0.666973426600616 and parameters: {'n_estimators': 800, 'learning_rate': 0.10300379002821036, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 90}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:01,600] Trial 173 finished with value: 0.6626956578251034 and parameters: {'n_estimators': 334, 'learning_rate': 0.11999425401577046, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 123}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:05,970] Trial 174 finished with value: 0.6622911912681357 and parameters: {'n_estimators': 889, 'learning_rate': 0.13440817555370096, 'max_depth': 10, 'max_bin': 293, 'num_leaves': 72}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:10,398] Trial 175 finished with value: 0.6617582336086649 and parameters: {'n_estimators': 835, 'learning_rate': 0.11029697256918555, 'max_depth': 8, 'max_bin': 211, 'num_leaves': 112}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:16,064] Trial 176 finished with value: 0.6649998977695815 and parameters: {'n_estimators': 602, 'learning_rate': 0.08995208438265928, 'max_depth': 10, 'max_bin': 171, 'num_leaves': 165}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:21,191] Trial 177 finished with value: 0.6648928476020907 and parameters: {'n_estimators': 899, 'learning_rate': 0.10642610449414269, 'max_depth': 12, 'max_bin': 185, 'num_leaves': 98}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:25,373] Trial 178 finished with value: 0.6585092442350142 and parameters: {'n_estimators': 826, 'learning_rate': 0.17349832698561393, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 132}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:32,530] Trial 179 finished with value: 0.6662805868483458 and parameters: {'n_estimators': 854, 'learning_rate': 0.09875562860110693, 'max_depth': 12, 'max_bin': 298, 'num_leaves': 355}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:38,429] Trial 180 finished with value: 0.6665600705380959 and parameters: {'n_estimators': 775, 'learning_rate': 0.08588626491870319, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 143}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:43,433] Trial 181 finished with value: 0.6668488151020795 and parameters: {'n_estimators': 868, 'learning_rate': 0.10188617543808312, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 65}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:49,084] Trial 182 finished with value: 0.6653348580236359 and parameters: {'n_estimators': 880, 'learning_rate': 0.09259883214529836, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 32}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:54,042] Trial 183 finished with value: 0.6677545183042289 and parameters: {'n_estimators': 899, 'learning_rate': 0.11369791792804565, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 84}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:10:59,522] Trial 184 finished with value: 0.6663573387139128 and parameters: {'n_estimators': 655, 'learning_rate': 0.0956536394696407, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 57}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:04,635] Trial 185 finished with value: 0.6669396960728302 and parameters: {'n_estimators': 812, 'learning_rate': 0.10925166325572287, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 116}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:09,276] Trial 186 finished with value: 0.667198923477997 and parameters: {'n_estimators': 861, 'learning_rate': 0.10464921894636044, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 95}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:14,203] Trial 187 finished with value: 0.6619920623287443 and parameters: {'n_estimators': 624, 'learning_rate': 0.10059799301822266, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 45}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:19,894] Trial 188 finished with value: 0.6662451223759314 and parameters: {'n_estimators': 879, 'learning_rate': 0.0817725000200646, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 72}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:24,865] Trial 189 finished with value: 0.6670359300691422 and parameters: {'n_estimators': 842, 'learning_rate': 0.11781222893453544, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 315}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:29,057] Trial 190 finished with value: 0.664708448468984 and parameters: {'n_estimators': 680, 'learning_rate': 0.13820594741855857, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 296}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:33,381] Trial 191 finished with value: 0.6625772212324929 and parameters: {'n_estimators': 861, 'learning_rate': 0.1299883681088568, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 68}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:37,376] Trial 192 finished with value: 0.6588680854022753 and parameters: {'n_estimators': 825, 'learning_rate': 0.12270755389349014, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 106}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:43,006] Trial 193 finished with value: 0.6688884976518047 and parameters: {'n_estimators': 877, 'learning_rate': 0.08721442530299764, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 80}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:47,886] Trial 194 finished with value: 0.6678792725941964 and parameters: {'n_estimators': 899, 'learning_rate': 0.11533826611000098, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 56}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:54,312] Trial 195 finished with value: 0.661343883867114 and parameters: {'n_estimators': 843, 'learning_rate': 0.14587847310632507, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 269}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:11:58,395] Trial 196 finished with value: 0.6629415811813855 and parameters: {'n_estimators': 791, 'learning_rate': 0.12795208226973123, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 134}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:12:04,168] Trial 197 finished with value: 0.6654546332307409 and parameters: {'n_estimators': 615, 'learning_rate': 0.09409900836430642, 'max_depth': 10, 'max_bin': 286, 'num_leaves': 152}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:12:09,233] Trial 198 finished with value: 0.6681219931094341 and parameters: {'n_estimators': 651, 'learning_rate': 0.10978838454913094, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 96}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:12:15,707] Trial 199 finished with value: 0.670708097220098 and parameters: {'n_estimators': 699, 'learning_rate': 0.07634153899236358, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 117}. Best is trial 38 with value: 0.6820079319553323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 852\n",
      "\t\tlearning_rate: 0.10340146316890783\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 285\n",
      "\t\tnum_leaves: 84\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.682897    0.713345    0.691267    0.702469\n",
      "1                    TP  396.000000  399.000000  365.000000  396.000000\n",
      "2                    TN  341.000000  341.000000  360.000000  355.000000\n",
      "3                    FP   85.000000   81.000000   92.000000   65.000000\n",
      "4                    FN   77.000000   78.000000   82.000000   83.000000\n",
      "5              Accuracy    0.819800    0.823137    0.806452    0.835373\n",
      "6             Precision    0.823285    0.831250    0.798687    0.859002\n",
      "7           Sensitivity    0.837209    0.836478    0.816555    0.826722\n",
      "8           Specificity    0.800500    0.808100    0.796500    0.845200\n",
      "9              F1 score    0.830189    0.833856    0.807522    0.842553\n",
      "10  F1 score (weighted)    0.819701    0.823099    0.806440    0.835523\n",
      "11     F1 score (macro)    0.819123    0.822398    0.806446    0.835030\n",
      "12    Balanced Accuracy    0.818839    0.822267    0.806507    0.835980\n",
      "13                  MCC    0.638376    0.644814    0.613091    0.670731\n",
      "14                  NPV    0.815800    0.813800    0.814500    0.810500\n",
      "15              ROC_AUC    0.818839    0.822267    0.806507    0.835980\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_3_cat = np.where((y_pred_lgbm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:12:21,103] Trial 200 finished with value: 0.66141770811048 and parameters: {'n_estimators': 722, 'learning_rate': 0.14268459862976174, 'max_depth': 7, 'max_bin': 275, 'num_leaves': 332}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:12:29,225] Trial 201 finished with value: 0.6724264356990179 and parameters: {'n_estimators': 739, 'learning_rate': 0.09208717419750354, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 444}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:12:35,445] Trial 202 finished with value: 0.66476125379648 and parameters: {'n_estimators': 749, 'learning_rate': 0.09774291261831734, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 408}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:12:41,449] Trial 203 finished with value: 0.666031844749515 and parameters: {'n_estimators': 709, 'learning_rate': 0.08869561508873523, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 540}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:12:46,707] Trial 204 finished with value: 0.6648232110426743 and parameters: {'n_estimators': 670, 'learning_rate': 0.10427420855215341, 'max_depth': 9, 'max_bin': 231, 'num_leaves': 466}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:12:55,010] Trial 205 finished with value: 0.669371560422787 and parameters: {'n_estimators': 858, 'learning_rate': 0.082762584050146, 'max_depth': 10, 'max_bin': 223, 'num_leaves': 282}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:12:59,880] Trial 206 finished with value: 0.6629741055841482 and parameters: {'n_estimators': 591, 'learning_rate': 0.1330907059464753, 'max_depth': 11, 'max_bin': 193, 'num_leaves': 381}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:13:06,760] Trial 207 finished with value: 0.6743188357292454 and parameters: {'n_estimators': 726, 'learning_rate': 0.1000723160829352, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 508}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:13:11,735] Trial 208 finished with value: 0.6597950538243251 and parameters: {'n_estimators': 637, 'learning_rate': 0.10676320319410196, 'max_depth': 5, 'max_bin': 211, 'num_leaves': 75}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:13:17,715] Trial 209 finished with value: 0.6664550684327674 and parameters: {'n_estimators': 688, 'learning_rate': 0.09026807754666655, 'max_depth': 10, 'max_bin': 240, 'num_leaves': 444}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:13:24,383] Trial 210 finished with value: 0.6740432967630977 and parameters: {'n_estimators': 759, 'learning_rate': 0.09628747018553857, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 401}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:13:31,799] Trial 211 finished with value: 0.6712008161402089 and parameters: {'n_estimators': 825, 'learning_rate': 0.14887138375345088, 'max_depth': 11, 'max_bin': 185, 'num_leaves': 315}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:13:37,453] Trial 212 finished with value: 0.671503186988764 and parameters: {'n_estimators': 807, 'learning_rate': 0.14872552460294844, 'max_depth': 11, 'max_bin': 186, 'num_leaves': 321}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:13:44,491] Trial 213 finished with value: 0.6729642719286878 and parameters: {'n_estimators': 782, 'learning_rate': 0.0851218553307854, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 301}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:13:49,964] Trial 214 finished with value: 0.6689128347944029 and parameters: {'n_estimators': 388, 'learning_rate': 0.11182559188050283, 'max_depth': 11, 'max_bin': 179, 'num_leaves': 293}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:13:55,357] Trial 215 finished with value: 0.6693126375492772 and parameters: {'n_estimators': 839, 'learning_rate': 0.15727567740321693, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 354}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:14:02,084] Trial 216 finished with value: 0.6718583853189313 and parameters: {'n_estimators': 878, 'learning_rate': 0.10200337442079398, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 748}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:14:07,853] Trial 217 finished with value: 0.6700266554828275 and parameters: {'n_estimators': 813, 'learning_rate': 0.11624648533751261, 'max_depth': 10, 'max_bin': 190, 'num_leaves': 91}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:14:14,996] Trial 218 finished with value: 0.6737142164533697 and parameters: {'n_estimators': 854, 'learning_rate': 0.07868991816928762, 'max_depth': 12, 'max_bin': 198, 'num_leaves': 106}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:14:21,943] Trial 219 finished with value: 0.6699398033156316 and parameters: {'n_estimators': 869, 'learning_rate': 0.13829333870219973, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 423}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:14:31,885] Trial 220 finished with value: 0.6537122951255141 and parameters: {'n_estimators': 608, 'learning_rate': 0.0209355767977195, 'max_depth': 9, 'max_bin': 181, 'num_leaves': 279}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:14:38,558] Trial 221 finished with value: 0.6706747296446343 and parameters: {'n_estimators': 634, 'learning_rate': 0.0936350705597836, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 305}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:14:45,012] Trial 222 finished with value: 0.6639001772599813 and parameters: {'n_estimators': 550, 'learning_rate': 0.09725486295059405, 'max_depth': 10, 'max_bin': 233, 'num_leaves': 255}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:14:52,091] Trial 223 finished with value: 0.6616976651054791 and parameters: {'n_estimators': 575, 'learning_rate': 0.1059834850890623, 'max_depth': 8, 'max_bin': 207, 'num_leaves': 300}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:14:58,140] Trial 224 finished with value: 0.6671998138989395 and parameters: {'n_estimators': 661, 'learning_rate': 0.08889163696230878, 'max_depth': 10, 'max_bin': 245, 'num_leaves': 282}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:15:03,300] Trial 225 finished with value: 0.6673353608921168 and parameters: {'n_estimators': 276, 'learning_rate': 0.10032103972891558, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 338}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:15:06,335] Trial 226 finished with value: 0.6562999977496631 and parameters: {'n_estimators': 114, 'learning_rate': 0.12324212600767172, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 125}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:15:12,924] Trial 227 finished with value: 0.6694889776405646 and parameters: {'n_estimators': 838, 'learning_rate': 0.09255765964077044, 'max_depth': 10, 'max_bin': 261, 'num_leaves': 324}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:15:20,116] Trial 228 finished with value: 0.6760412639933161 and parameters: {'n_estimators': 617, 'learning_rate': 0.08472642415379311, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 689}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:15:29,306] Trial 229 finished with value: 0.6720648093338573 and parameters: {'n_estimators': 618, 'learning_rate': 0.08251814696323179, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 679}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:15:36,987] Trial 230 finished with value: 0.6753805198698573 and parameters: {'n_estimators': 636, 'learning_rate': 0.07319898985021864, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 676}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:15:43,709] Trial 231 finished with value: 0.6774090877696068 and parameters: {'n_estimators': 593, 'learning_rate': 0.0855415948733452, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 74}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:15:51,777] Trial 232 finished with value: 0.6754368930354229 and parameters: {'n_estimators': 604, 'learning_rate': 0.08715948443814228, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 627}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:15:58,460] Trial 233 finished with value: 0.6707631577114475 and parameters: {'n_estimators': 586, 'learning_rate': 0.08474847665875296, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 703}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:16:07,661] Trial 234 finished with value: 0.6720577487729622 and parameters: {'n_estimators': 617, 'learning_rate': 0.08241942524192362, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 74}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:16:14,611] Trial 235 finished with value: 0.6742497044228639 and parameters: {'n_estimators': 601, 'learning_rate': 0.09084510661156187, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 96}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:16:21,952] Trial 236 finished with value: 0.6727608553045716 and parameters: {'n_estimators': 656, 'learning_rate': 0.07857934639888309, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 723}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:16:27,133] Trial 237 finished with value: 0.6716135920336019 and parameters: {'n_estimators': 629, 'learning_rate': 0.10926802562811472, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 83}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:16:33,841] Trial 238 finished with value: 0.6708763265490072 and parameters: {'n_estimators': 887, 'learning_rate': 0.08724947917276063, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 67}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:16:40,147] Trial 239 finished with value: 0.6718659810586282 and parameters: {'n_estimators': 648, 'learning_rate': 0.10435707174398694, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 581}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:16:45,368] Trial 240 finished with value: 0.6618088471088232 and parameters: {'n_estimators': 797, 'learning_rate': 0.139857497917716, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 51}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:16:53,017] Trial 241 finished with value: 0.6739379482609265 and parameters: {'n_estimators': 505, 'learning_rate': 0.09529217339148442, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 104}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:16:59,089] Trial 242 finished with value: 0.6681904589266641 and parameters: {'n_estimators': 572, 'learning_rate': 0.09809660753016021, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 312}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:17:03,325] Trial 243 finished with value: 0.6602860798623625 and parameters: {'n_estimators': 476, 'learning_rate': 0.19538740040014113, 'max_depth': 10, 'max_bin': 246, 'num_leaves': 178}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:17:12,874] Trial 244 finished with value: 0.6711509678413996 and parameters: {'n_estimators': 711, 'learning_rate': 0.06882679936414327, 'max_depth': 10, 'max_bin': 242, 'num_leaves': 368}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:17:19,444] Trial 245 finished with value: 0.6687652951595852 and parameters: {'n_estimators': 823, 'learning_rate': 0.09069848014976409, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 220}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:17:27,891] Trial 246 finished with value: 0.6768228487215474 and parameters: {'n_estimators': 849, 'learning_rate': 0.07527854318263344, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 730}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:17:38,135] Trial 247 finished with value: 0.677526284907927 and parameters: {'n_estimators': 847, 'learning_rate': 0.07466932271611469, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 719}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:17:45,594] Trial 248 finished with value: 0.6715130731575356 and parameters: {'n_estimators': 850, 'learning_rate': 0.07118323409821402, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 727}. Best is trial 38 with value: 0.6820079319553323.\n",
      "[I 2023-12-20 00:17:54,019] Trial 249 finished with value: 0.6748134726201537 and parameters: {'n_estimators': 861, 'learning_rate': 0.0679459586260755, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 737}. Best is trial 38 with value: 0.6820079319553323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 852\n",
      "\t\tlearning_rate: 0.10340146316890783\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 285\n",
      "\t\tnum_leaves: 84\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.682897    0.713345    0.691267    0.702469   \n",
      "1                    TP  396.000000  399.000000  365.000000  396.000000   \n",
      "2                    TN  341.000000  341.000000  360.000000  355.000000   \n",
      "3                    FP   85.000000   81.000000   92.000000   65.000000   \n",
      "4                    FN   77.000000   78.000000   82.000000   83.000000   \n",
      "5              Accuracy    0.819800    0.823137    0.806452    0.835373   \n",
      "6             Precision    0.823285    0.831250    0.798687    0.859002   \n",
      "7           Sensitivity    0.837209    0.836478    0.816555    0.826722   \n",
      "8           Specificity    0.800500    0.808100    0.796500    0.845200   \n",
      "9              F1 score    0.830189    0.833856    0.807522    0.842553   \n",
      "10  F1 score (weighted)    0.819701    0.823099    0.806440    0.835523   \n",
      "11     F1 score (macro)    0.819123    0.822398    0.806446    0.835030   \n",
      "12    Balanced Accuracy    0.818839    0.822267    0.806507    0.835980   \n",
      "13                  MCC    0.638376    0.644814    0.613091    0.670731   \n",
      "14                  NPV    0.815800    0.813800    0.814500    0.810500   \n",
      "15              ROC_AUC    0.818839    0.822267    0.806507    0.835980   \n",
      "\n",
      "          Set4  \n",
      "0     0.689476  \n",
      "1   400.000000  \n",
      "2   347.000000  \n",
      "3    76.000000  \n",
      "4    76.000000  \n",
      "5     0.830923  \n",
      "6     0.840336  \n",
      "7     0.840336  \n",
      "8     0.820300  \n",
      "9     0.840336  \n",
      "10    0.830923  \n",
      "11    0.830334  \n",
      "12    0.830334  \n",
      "13    0.660667  \n",
      "14    0.820300  \n",
      "15    0.830334  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_4_cat = np.where((y_pred_lgbm_4 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:18:05,534] Trial 250 finished with value: 0.6915821612772735 and parameters: {'n_estimators': 837, 'learning_rate': 0.07572558613433532, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 746}. Best is trial 250 with value: 0.6915821612772735.\n",
      "[I 2023-12-20 00:18:12,754] Trial 251 finished with value: 0.6899184596807915 and parameters: {'n_estimators': 835, 'learning_rate': 0.07886436715071973, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 747}. Best is trial 250 with value: 0.6915821612772735.\n",
      "[I 2023-12-20 00:18:21,236] Trial 252 finished with value: 0.692115171644083 and parameters: {'n_estimators': 843, 'learning_rate': 0.07425577915348806, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 736}. Best is trial 252 with value: 0.692115171644083.\n",
      "[I 2023-12-20 00:18:29,670] Trial 253 finished with value: 0.6909312654597424 and parameters: {'n_estimators': 823, 'learning_rate': 0.07450786152262999, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 747}. Best is trial 252 with value: 0.692115171644083.\n",
      "[I 2023-12-20 00:18:40,271] Trial 254 finished with value: 0.6904788373750008 and parameters: {'n_estimators': 830, 'learning_rate': 0.0735796280453504, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 717}. Best is trial 252 with value: 0.692115171644083.\n",
      "[I 2023-12-20 00:18:48,392] Trial 255 finished with value: 0.6939320470944 and parameters: {'n_estimators': 830, 'learning_rate': 0.07476365020131102, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 742}. Best is trial 255 with value: 0.6939320470944.\n",
      "[I 2023-12-20 00:18:57,062] Trial 256 finished with value: 0.6903081543032152 and parameters: {'n_estimators': 826, 'learning_rate': 0.07517285212972113, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 745}. Best is trial 255 with value: 0.6939320470944.\n",
      "[I 2023-12-20 00:19:05,637] Trial 257 finished with value: 0.6903435858271221 and parameters: {'n_estimators': 815, 'learning_rate': 0.0735718713279337, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 744}. Best is trial 255 with value: 0.6939320470944.\n",
      "[I 2023-12-20 00:19:14,217] Trial 258 finished with value: 0.6908840691730219 and parameters: {'n_estimators': 822, 'learning_rate': 0.07370028284424007, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 744}. Best is trial 255 with value: 0.6939320470944.\n",
      "[I 2023-12-20 00:19:25,100] Trial 259 finished with value: 0.6953738217912808 and parameters: {'n_estimators': 807, 'learning_rate': 0.07267778442617548, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:19:33,811] Trial 260 finished with value: 0.6920942235503407 and parameters: {'n_estimators': 810, 'learning_rate': 0.07279045276735502, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 742}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:19:42,879] Trial 261 finished with value: 0.693280422583604 and parameters: {'n_estimators': 808, 'learning_rate': 0.07155547022989904, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 749}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:19:51,888] Trial 262 finished with value: 0.6888385227438125 and parameters: {'n_estimators': 808, 'learning_rate': 0.07150727398307147, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 744}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:20:01,318] Trial 263 finished with value: 0.6895657707266741 and parameters: {'n_estimators': 794, 'learning_rate': 0.07071673783311405, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 749}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:20:09,901] Trial 264 finished with value: 0.6924486125410457 and parameters: {'n_estimators': 808, 'learning_rate': 0.07157472835981402, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 746}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:20:17,933] Trial 265 finished with value: 0.6902649359551944 and parameters: {'n_estimators': 803, 'learning_rate': 0.06516669095727952, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 749}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:20:29,695] Trial 266 finished with value: 0.6935105266186561 and parameters: {'n_estimators': 800, 'learning_rate': 0.06436424663265217, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 746}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:20:39,093] Trial 267 finished with value: 0.6895341113296635 and parameters: {'n_estimators': 806, 'learning_rate': 0.06265651210871379, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 748}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:20:48,057] Trial 268 finished with value: 0.6898328424793398 and parameters: {'n_estimators': 804, 'learning_rate': 0.06328727933932468, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:20:57,084] Trial 269 finished with value: 0.6909212214528322 and parameters: {'n_estimators': 801, 'learning_rate': 0.06402439612210076, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 740}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:21:06,420] Trial 270 finished with value: 0.6914517286747932 and parameters: {'n_estimators': 803, 'learning_rate': 0.06283758476804562, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 749}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:21:15,158] Trial 271 finished with value: 0.6883614506836372 and parameters: {'n_estimators': 805, 'learning_rate': 0.0642242887631001, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 748}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:21:26,437] Trial 272 finished with value: 0.691264508173318 and parameters: {'n_estimators': 802, 'learning_rate': 0.06399652870902951, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 749}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:21:35,870] Trial 273 finished with value: 0.6917819496909902 and parameters: {'n_estimators': 798, 'learning_rate': 0.06534009823449465, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:21:46,329] Trial 274 finished with value: 0.692828930182547 and parameters: {'n_estimators': 795, 'learning_rate': 0.060883447310465966, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:21:58,203] Trial 275 finished with value: 0.6919807681866692 and parameters: {'n_estimators': 793, 'learning_rate': 0.06266068719761594, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:22:07,539] Trial 276 finished with value: 0.6918258556556983 and parameters: {'n_estimators': 790, 'learning_rate': 0.061291386194217035, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 749}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:22:16,376] Trial 277 finished with value: 0.6917311832310498 and parameters: {'n_estimators': 786, 'learning_rate': 0.06178590075991579, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 749}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:22:25,512] Trial 278 finished with value: 0.690339733669965 and parameters: {'n_estimators': 785, 'learning_rate': 0.059004091283954375, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 734}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:22:36,731] Trial 279 finished with value: 0.6913851180696537 and parameters: {'n_estimators': 780, 'learning_rate': 0.058418110855650324, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 733}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:22:46,108] Trial 280 finished with value: 0.6920085164312529 and parameters: {'n_estimators': 786, 'learning_rate': 0.05798103261730283, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 709}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:22:55,576] Trial 281 finished with value: 0.690697545239987 and parameters: {'n_estimators': 767, 'learning_rate': 0.058795624652732775, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 704}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:23:05,545] Trial 282 finished with value: 0.690814071929811 and parameters: {'n_estimators': 776, 'learning_rate': 0.05859411846025514, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 712}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:23:14,774] Trial 283 finished with value: 0.687834662314071 and parameters: {'n_estimators': 774, 'learning_rate': 0.05965951278929279, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 704}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:23:24,263] Trial 284 finished with value: 0.6903824587849184 and parameters: {'n_estimators': 777, 'learning_rate': 0.05614181662608984, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 727}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:23:34,068] Trial 285 finished with value: 0.6923145662019995 and parameters: {'n_estimators': 780, 'learning_rate': 0.054795250763514436, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 715}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:23:43,842] Trial 286 finished with value: 0.6901632810922719 and parameters: {'n_estimators': 769, 'learning_rate': 0.055427712212144195, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 715}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:23:52,807] Trial 287 finished with value: 0.687986640132662 and parameters: {'n_estimators': 782, 'learning_rate': 0.05475424882054463, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 725}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:24:02,495] Trial 288 finished with value: 0.6904277433988731 and parameters: {'n_estimators': 763, 'learning_rate': 0.06135412730118246, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 704}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:24:13,506] Trial 289 finished with value: 0.6889220668150376 and parameters: {'n_estimators': 765, 'learning_rate': 0.06016452735715947, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 698}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:24:22,970] Trial 290 finished with value: 0.688768039036983 and parameters: {'n_estimators': 789, 'learning_rate': 0.0665540094336832, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 714}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:24:33,182] Trial 291 finished with value: 0.6936525959380225 and parameters: {'n_estimators': 788, 'learning_rate': 0.06131469746500048, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 733}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:24:45,113] Trial 292 finished with value: 0.6899310795518129 and parameters: {'n_estimators': 786, 'learning_rate': 0.05302391140488726, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 731}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:24:53,985] Trial 293 finished with value: 0.6918416423065925 and parameters: {'n_estimators': 792, 'learning_rate': 0.06542382947825462, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 728}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:25:03,203] Trial 294 finished with value: 0.6911837584502957 and parameters: {'n_estimators': 795, 'learning_rate': 0.06626185236967382, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 732}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:25:12,371] Trial 295 finished with value: 0.6917585435665122 and parameters: {'n_estimators': 794, 'learning_rate': 0.06534042982114364, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 732}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:25:23,240] Trial 296 finished with value: 0.6912284565008717 and parameters: {'n_estimators': 793, 'learning_rate': 0.06748423640414684, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 730}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:25:32,495] Trial 297 finished with value: 0.6926109812156472 and parameters: {'n_estimators': 794, 'learning_rate': 0.0671821997812045, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 731}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:25:43,379] Trial 298 finished with value: 0.6886842125343201 and parameters: {'n_estimators': 788, 'learning_rate': 0.06662607783263158, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 728}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:25:53,123] Trial 299 finished with value: 0.6913501488464335 and parameters: {'n_estimators': 795, 'learning_rate': 0.06840409212283033, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 731}. Best is trial 259 with value: 0.6953738217912808.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6954\n",
      "\tBest params:\n",
      "\t\tn_estimators: 807\n",
      "\t\tlearning_rate: 0.07267778442617548\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 233\n",
      "\t\tnum_leaves: 750\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.682897    0.713345    0.691267    0.702469   \n",
      "1                    TP  396.000000  399.000000  365.000000  396.000000   \n",
      "2                    TN  341.000000  341.000000  360.000000  355.000000   \n",
      "3                    FP   85.000000   81.000000   92.000000   65.000000   \n",
      "4                    FN   77.000000   78.000000   82.000000   83.000000   \n",
      "5              Accuracy    0.819800    0.823137    0.806452    0.835373   \n",
      "6             Precision    0.823285    0.831250    0.798687    0.859002   \n",
      "7           Sensitivity    0.837209    0.836478    0.816555    0.826722   \n",
      "8           Specificity    0.800500    0.808100    0.796500    0.845200   \n",
      "9              F1 score    0.830189    0.833856    0.807522    0.842553   \n",
      "10  F1 score (weighted)    0.819701    0.823099    0.806440    0.835523   \n",
      "11     F1 score (macro)    0.819123    0.822398    0.806446    0.835030   \n",
      "12    Balanced Accuracy    0.818839    0.822267    0.806507    0.835980   \n",
      "13                  MCC    0.638376    0.644814    0.613091    0.670731   \n",
      "14                  NPV    0.815800    0.813800    0.814500    0.810500   \n",
      "15              ROC_AUC    0.818839    0.822267    0.806507    0.835980   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.689476    0.671779  \n",
      "1   400.000000  387.000000  \n",
      "2   347.000000  359.000000  \n",
      "3    76.000000   79.000000  \n",
      "4    76.000000   74.000000  \n",
      "5     0.830923    0.829811  \n",
      "6     0.840336    0.830472  \n",
      "7     0.840336    0.839479  \n",
      "8     0.820300    0.819600  \n",
      "9     0.840336    0.834951  \n",
      "10    0.830923    0.829781  \n",
      "11    0.830334    0.829646  \n",
      "12    0.830334    0.829557  \n",
      "13    0.660667    0.659343  \n",
      "14    0.820300    0.829100  \n",
      "15    0.830334    0.829557  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_5_cat = np.where((y_pred_lgbm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:26:01,876] Trial 300 finished with value: 0.6800346317604589 and parameters: {'n_estimators': 791, 'learning_rate': 0.06810821775617469, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 728}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:26:10,075] Trial 301 finished with value: 0.6792345068335607 and parameters: {'n_estimators': 799, 'learning_rate': 0.0626061097862957, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 729}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:26:18,460] Trial 302 finished with value: 0.6799870754807207 and parameters: {'n_estimators': 754, 'learning_rate': 0.06767659253037404, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 732}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:26:28,560] Trial 303 finished with value: 0.6792529752304247 and parameters: {'n_estimators': 813, 'learning_rate': 0.06355857020791808, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 715}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:26:38,443] Trial 304 finished with value: 0.6833650863808253 and parameters: {'n_estimators': 787, 'learning_rate': 0.05335851363748315, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 732}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:26:46,898] Trial 305 finished with value: 0.6801533987907061 and parameters: {'n_estimators': 773, 'learning_rate': 0.06988886555091936, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 720}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:26:55,621] Trial 306 finished with value: 0.681737744549673 and parameters: {'n_estimators': 798, 'learning_rate': 0.06683346131893296, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 693}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:27:04,421] Trial 307 finished with value: 0.6822459830259796 and parameters: {'n_estimators': 813, 'learning_rate': 0.06134013531416367, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 733}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:27:16,954] Trial 308 finished with value: 0.6849198438094614 and parameters: {'n_estimators': 792, 'learning_rate': 0.05197831908244235, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 735}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:27:24,510] Trial 309 finished with value: 0.6791576016399961 and parameters: {'n_estimators': 759, 'learning_rate': 0.06467311864948558, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 715}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:27:31,667] Trial 310 finished with value: 0.6751798205088224 and parameters: {'n_estimators': 815, 'learning_rate': 0.06951096937586881, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 731}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:27:44,265] Trial 311 finished with value: 0.6818153659892146 and parameters: {'n_estimators': 779, 'learning_rate': 0.057226040675605244, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:27:52,935] Trial 312 finished with value: 0.6811249616288032 and parameters: {'n_estimators': 797, 'learning_rate': 0.06152987495232376, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 719}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:28:00,251] Trial 313 finished with value: 0.673926418728046 and parameters: {'n_estimators': 813, 'learning_rate': 0.06932765961971406, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 735}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:28:10,343] Trial 314 finished with value: 0.6777873754294117 and parameters: {'n_estimators': 772, 'learning_rate': 0.0651918823094763, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:28:19,924] Trial 315 finished with value: 0.6811482909644552 and parameters: {'n_estimators': 751, 'learning_rate': 0.048309885267887265, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:28:30,515] Trial 316 finished with value: 0.6817106946662896 and parameters: {'n_estimators': 794, 'learning_rate': 0.05885453346679448, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 704}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:28:37,626] Trial 317 finished with value: 0.6740273531864049 and parameters: {'n_estimators': 816, 'learning_rate': 0.06954151314722905, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 719}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:28:46,358] Trial 318 finished with value: 0.6780932862680402 and parameters: {'n_estimators': 781, 'learning_rate': 0.06253669846682743, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 737}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:28:55,898] Trial 319 finished with value: 0.6793951975431523 and parameters: {'n_estimators': 823, 'learning_rate': 0.05761596066106954, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 721}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:29:04,508] Trial 320 finished with value: 0.6811423134879171 and parameters: {'n_estimators': 801, 'learning_rate': 0.06618909827507849, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:29:14,361] Trial 321 finished with value: 0.6802779665480992 and parameters: {'n_estimators': 763, 'learning_rate': 0.07036209901892873, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 699}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:29:23,214] Trial 322 finished with value: 0.6793511551717416 and parameters: {'n_estimators': 830, 'learning_rate': 0.06490484309157492, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 737}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:29:33,977] Trial 323 finished with value: 0.6817303592485582 and parameters: {'n_estimators': 783, 'learning_rate': 0.05965170739704993, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 722}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:29:46,687] Trial 324 finished with value: 0.6849720192910084 and parameters: {'n_estimators': 809, 'learning_rate': 0.05199727730995205, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:29:56,311] Trial 325 finished with value: 0.6864678682836319 and parameters: {'n_estimators': 802, 'learning_rate': 0.06115781265926551, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 707}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:30:06,663] Trial 326 finished with value: 0.6788216578694828 and parameters: {'n_estimators': 768, 'learning_rate': 0.07082425464810678, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:30:15,004] Trial 327 finished with value: 0.6818558609432103 and parameters: {'n_estimators': 788, 'learning_rate': 0.06662398920874549, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 724}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:30:24,285] Trial 328 finished with value: 0.682452816263458 and parameters: {'n_estimators': 820, 'learning_rate': 0.04937251857394647, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 679}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:30:36,116] Trial 329 finished with value: 0.6810543181311708 and parameters: {'n_estimators': 831, 'learning_rate': 0.057214633213383784, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:30:44,300] Trial 330 finished with value: 0.6822609265959066 and parameters: {'n_estimators': 752, 'learning_rate': 0.06235163250809449, 'max_depth': 12, 'max_bin': 152, 'num_leaves': 714}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:30:52,307] Trial 331 finished with value: 0.6798535529385629 and parameters: {'n_estimators': 798, 'learning_rate': 0.07206598503567434, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 693}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:31:01,387] Trial 332 finished with value: 0.6800695391361358 and parameters: {'n_estimators': 777, 'learning_rate': 0.055608366570948614, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:31:12,951] Trial 333 finished with value: 0.6810677943703756 and parameters: {'n_estimators': 809, 'learning_rate': 0.06653989466357192, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:31:21,279] Trial 334 finished with value: 0.6771514173371621 and parameters: {'n_estimators': 793, 'learning_rate': 0.063269817326054, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 724}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:31:30,316] Trial 335 finished with value: 0.6796475636928212 and parameters: {'n_estimators': 831, 'learning_rate': 0.06869781158626147, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 710}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:31:39,888] Trial 336 finished with value: 0.6820744156955125 and parameters: {'n_estimators': 772, 'learning_rate': 0.056076239563454026, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 735}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:31:45,409] Trial 337 finished with value: 0.670896415915456 and parameters: {'n_estimators': 817, 'learning_rate': 0.07577033933226399, 'max_depth': 6, 'max_bin': 235, 'num_leaves': 724}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:31:53,321] Trial 338 finished with value: 0.6784049602800521 and parameters: {'n_estimators': 739, 'learning_rate': 0.07215227678736152, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 749}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:32:02,810] Trial 339 finished with value: 0.6820819588428473 and parameters: {'n_estimators': 787, 'learning_rate': 0.0633794459267817, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 734}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:32:13,377] Trial 340 finished with value: 0.6833717782452109 and parameters: {'n_estimators': 806, 'learning_rate': 0.06000325657595095, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:32:20,487] Trial 341 finished with value: 0.6543408918751104 and parameters: {'n_estimators': 835, 'learning_rate': 0.06768050578747621, 'max_depth': 4, 'max_bin': 235, 'num_leaves': 710}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:32:30,168] Trial 342 finished with value: 0.6772424655453858 and parameters: {'n_estimators': 759, 'learning_rate': 0.05017944346857494, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 721}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:32:42,372] Trial 343 finished with value: 0.6839911330010584 and parameters: {'n_estimators': 782, 'learning_rate': 0.04465920116927517, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 735}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:32:52,119] Trial 344 finished with value: 0.6803642502482397 and parameters: {'n_estimators': 802, 'learning_rate': 0.05508731120589482, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 691}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:32:58,879] Trial 345 finished with value: 0.677506500104939 and parameters: {'n_estimators': 813, 'learning_rate': 0.07210390317501544, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:33:08,311] Trial 346 finished with value: 0.6819070487153617 and parameters: {'n_estimators': 770, 'learning_rate': 0.061589700383216595, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 722}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:33:17,695] Trial 347 finished with value: 0.6805839636072053 and parameters: {'n_estimators': 791, 'learning_rate': 0.06732690155709511, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 707}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:33:27,109] Trial 348 finished with value: 0.6813191810689568 and parameters: {'n_estimators': 832, 'learning_rate': 0.05850888870698083, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:33:34,205] Trial 349 finished with value: 0.6785655110878437 and parameters: {'n_estimators': 747, 'learning_rate': 0.07575250088422575, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 735}. Best is trial 259 with value: 0.6953738217912808.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.695374\n",
      "\tBest params:\n",
      "\t\tn_estimators: 807\n",
      "\t\tlearning_rate: 0.07267778442617548\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 233\n",
      "\t\tnum_leaves: 750\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.682897    0.713345    0.691267    0.702469   \n",
      "1                    TP  396.000000  399.000000  365.000000  396.000000   \n",
      "2                    TN  341.000000  341.000000  360.000000  355.000000   \n",
      "3                    FP   85.000000   81.000000   92.000000   65.000000   \n",
      "4                    FN   77.000000   78.000000   82.000000   83.000000   \n",
      "5              Accuracy    0.819800    0.823137    0.806452    0.835373   \n",
      "6             Precision    0.823285    0.831250    0.798687    0.859002   \n",
      "7           Sensitivity    0.837209    0.836478    0.816555    0.826722   \n",
      "8           Specificity    0.800500    0.808100    0.796500    0.845200   \n",
      "9              F1 score    0.830189    0.833856    0.807522    0.842553   \n",
      "10  F1 score (weighted)    0.819701    0.823099    0.806440    0.835523   \n",
      "11     F1 score (macro)    0.819123    0.822398    0.806446    0.835030   \n",
      "12    Balanced Accuracy    0.818839    0.822267    0.806507    0.835980   \n",
      "13                  MCC    0.638376    0.644814    0.613091    0.670731   \n",
      "14                  NPV    0.815800    0.813800    0.814500    0.810500   \n",
      "15              ROC_AUC    0.818839    0.822267    0.806507    0.835980   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.689476    0.671779    0.691702  \n",
      "1   400.000000  387.000000  404.000000  \n",
      "2   347.000000  359.000000  344.000000  \n",
      "3    76.000000   79.000000   85.000000  \n",
      "4    76.000000   74.000000   66.000000  \n",
      "5     0.830923    0.829811    0.832036  \n",
      "6     0.840336    0.830472    0.826176  \n",
      "7     0.840336    0.839479    0.859574  \n",
      "8     0.820300    0.819600    0.801900  \n",
      "9     0.840336    0.834951    0.842544  \n",
      "10    0.830923    0.829781    0.831798  \n",
      "11    0.830334    0.829646    0.831284  \n",
      "12    0.830334    0.829557    0.830720  \n",
      "13    0.660667    0.659343    0.663317  \n",
      "14    0.820300    0.829100    0.839000  \n",
      "15    0.830334    0.829557    0.830720  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_6_cat = np.where((y_pred_lgbm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:33:44,750] Trial 350 finished with value: 0.6781817593221979 and parameters: {'n_estimators': 813, 'learning_rate': 0.06486578351272239, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 723}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:33:52,118] Trial 351 finished with value: 0.676559031152558 and parameters: {'n_estimators': 789, 'learning_rate': 0.07049819049008245, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 737}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:33:59,825] Trial 352 finished with value: 0.6767377627781233 and parameters: {'n_estimators': 771, 'learning_rate': 0.07773317981267978, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 709}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:34:08,794] Trial 353 finished with value: 0.6773617533986874 and parameters: {'n_estimators': 823, 'learning_rate': 0.056285812123316195, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 737}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:34:18,426] Trial 354 finished with value: 0.6778777350768469 and parameters: {'n_estimators': 799, 'learning_rate': 0.06441832402494167, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 749}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:34:26,765] Trial 355 finished with value: 0.6780484250447163 and parameters: {'n_estimators': 782, 'learning_rate': 0.053008868363302306, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 669}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:34:35,122] Trial 356 finished with value: 0.6771294138856634 and parameters: {'n_estimators': 837, 'learning_rate': 0.06069959056159906, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 692}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:34:42,278] Trial 357 finished with value: 0.6721892146932069 and parameters: {'n_estimators': 805, 'learning_rate': 0.06921246465133295, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 722}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:34:51,069] Trial 358 finished with value: 0.6799686170968434 and parameters: {'n_estimators': 822, 'learning_rate': 0.07313548313410421, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:34:59,138] Trial 359 finished with value: 0.6794389275169055 and parameters: {'n_estimators': 760, 'learning_rate': 0.0664764124725954, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 725}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:35:09,406] Trial 360 finished with value: 0.6796664356829277 and parameters: {'n_estimators': 792, 'learning_rate': 0.04784056973709345, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 705}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:35:17,477] Trial 361 finished with value: 0.6763571756292185 and parameters: {'n_estimators': 812, 'learning_rate': 0.0591427186139738, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 737}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:35:27,276] Trial 362 finished with value: 0.6780439858497687 and parameters: {'n_estimators': 776, 'learning_rate': 0.06313255187500923, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:35:35,655] Trial 363 finished with value: 0.6750023033041368 and parameters: {'n_estimators': 842, 'learning_rate': 0.07761973033120043, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 718}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:35:45,064] Trial 364 finished with value: 0.6801629436034788 and parameters: {'n_estimators': 802, 'learning_rate': 0.05435487923208426, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 728}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:35:52,252] Trial 365 finished with value: 0.6774531955359514 and parameters: {'n_estimators': 825, 'learning_rate': 0.06989329200845365, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 735}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:36:00,232] Trial 366 finished with value: 0.676866386380175 and parameters: {'n_estimators': 745, 'learning_rate': 0.06611635165782583, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 711}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:36:10,298] Trial 367 finished with value: 0.6774758895666271 and parameters: {'n_estimators': 784, 'learning_rate': 0.06067822492200565, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:36:17,555] Trial 368 finished with value: 0.6753555553311734 and parameters: {'n_estimators': 762, 'learning_rate': 0.07300552900153957, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 696}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:36:26,794] Trial 369 finished with value: 0.6772951238093058 and parameters: {'n_estimators': 799, 'learning_rate': 0.05814091071437246, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:36:31,339] Trial 370 finished with value: 0.6632830391582016 and parameters: {'n_estimators': 185, 'learning_rate': 0.06871662201911631, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 720}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:36:41,722] Trial 371 finished with value: 0.6799992304397533 and parameters: {'n_estimators': 816, 'learning_rate': 0.06361542385790174, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:36:46,921] Trial 372 finished with value: 0.6654744916732465 and parameters: {'n_estimators': 775, 'learning_rate': 0.07498883419335571, 'max_depth': 5, 'max_bin': 236, 'num_leaves': 724}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:36:55,044] Trial 373 finished with value: 0.6772286260366667 and parameters: {'n_estimators': 841, 'learning_rate': 0.05027058293682113, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:37:01,664] Trial 374 finished with value: 0.6781534149247798 and parameters: {'n_estimators': 795, 'learning_rate': 0.07895379632416466, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 709}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:37:10,819] Trial 375 finished with value: 0.6809919571869362 and parameters: {'n_estimators': 808, 'learning_rate': 0.05552470882334162, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 735}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:37:22,150] Trial 376 finished with value: 0.679102472702209 and parameters: {'n_estimators': 781, 'learning_rate': 0.04511276271277649, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 725}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:37:33,977] Trial 377 finished with value: 0.6814556565424177 and parameters: {'n_estimators': 829, 'learning_rate': 0.03863682358396056, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 737}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:37:41,617] Trial 378 finished with value: 0.6786568088391798 and parameters: {'n_estimators': 758, 'learning_rate': 0.06638599653247122, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:37:48,787] Trial 379 finished with value: 0.6754157910671912 and parameters: {'n_estimators': 794, 'learning_rate': 0.07191164697613305, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 699}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:37:56,697] Trial 380 finished with value: 0.6765439724045285 and parameters: {'n_estimators': 820, 'learning_rate': 0.06169736208268733, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 717}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:38:04,580] Trial 381 finished with value: 0.679289607132729 and parameters: {'n_estimators': 768, 'learning_rate': 0.06733055403896636, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 737}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:38:12,089] Trial 382 finished with value: 0.6737884973518403 and parameters: {'n_estimators': 806, 'learning_rate': 0.07065595014167846, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:38:20,361] Trial 383 finished with value: 0.6796905829379986 and parameters: {'n_estimators': 841, 'learning_rate': 0.06275413008573692, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 725}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:38:27,570] Trial 384 finished with value: 0.6741648752614925 and parameters: {'n_estimators': 739, 'learning_rate': 0.07592291224028859, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 711}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:38:38,469] Trial 385 finished with value: 0.6795153652476902 and parameters: {'n_estimators': 782, 'learning_rate': 0.05876823623834017, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 738}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:38:45,837] Trial 386 finished with value: 0.6764591883554252 and parameters: {'n_estimators': 819, 'learning_rate': 0.06542609836119569, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 684}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:38:54,780] Trial 387 finished with value: 0.6776663331862074 and parameters: {'n_estimators': 797, 'learning_rate': 0.05279384073517643, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 726}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:39:02,337] Trial 388 finished with value: 0.6777707991371364 and parameters: {'n_estimators': 850, 'learning_rate': 0.07179016932814422, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:39:12,216] Trial 389 finished with value: 0.6776704406393386 and parameters: {'n_estimators': 783, 'learning_rate': 0.05741749225774722, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 703}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:39:18,311] Trial 390 finished with value: 0.6748647067845861 and parameters: {'n_estimators': 297, 'learning_rate': 0.06892375569448617, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:39:27,772] Trial 391 finished with value: 0.6739954410096433 and parameters: {'n_estimators': 816, 'learning_rate': 0.0621192015393098, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 721}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:39:34,148] Trial 392 finished with value: 0.6737764628783867 and parameters: {'n_estimators': 757, 'learning_rate': 0.0757428904302001, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:39:41,650] Trial 393 finished with value: 0.6769124998816701 and parameters: {'n_estimators': 799, 'learning_rate': 0.06496781495070153, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 714}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:39:47,866] Trial 394 finished with value: 0.673510195941212 and parameters: {'n_estimators': 829, 'learning_rate': 0.07991169682062149, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 728}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:39:56,547] Trial 395 finished with value: 0.6818700763540069 and parameters: {'n_estimators': 770, 'learning_rate': 0.0600655794141145, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 739}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:40:03,796] Trial 396 finished with value: 0.6765362740557697 and parameters: {'n_estimators': 785, 'learning_rate': 0.06837395635309958, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 704}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:40:13,319] Trial 397 finished with value: 0.680544556908895 and parameters: {'n_estimators': 809, 'learning_rate': 0.07309640073602555, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:40:22,176] Trial 398 finished with value: 0.6797652740420993 and parameters: {'n_estimators': 838, 'learning_rate': 0.053357436242345, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 727}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:40:30,286] Trial 399 finished with value: 0.6769761191248785 and parameters: {'n_estimators': 798, 'learning_rate': 0.06424043282978177, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6953738\n",
      "\tBest params:\n",
      "\t\tn_estimators: 807\n",
      "\t\tlearning_rate: 0.07267778442617548\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 233\n",
      "\t\tnum_leaves: 750\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.682897    0.713345    0.691267    0.702469   \n",
      "1                    TP  396.000000  399.000000  365.000000  396.000000   \n",
      "2                    TN  341.000000  341.000000  360.000000  355.000000   \n",
      "3                    FP   85.000000   81.000000   92.000000   65.000000   \n",
      "4                    FN   77.000000   78.000000   82.000000   83.000000   \n",
      "5              Accuracy    0.819800    0.823137    0.806452    0.835373   \n",
      "6             Precision    0.823285    0.831250    0.798687    0.859002   \n",
      "7           Sensitivity    0.837209    0.836478    0.816555    0.826722   \n",
      "8           Specificity    0.800500    0.808100    0.796500    0.845200   \n",
      "9              F1 score    0.830189    0.833856    0.807522    0.842553   \n",
      "10  F1 score (weighted)    0.819701    0.823099    0.806440    0.835523   \n",
      "11     F1 score (macro)    0.819123    0.822398    0.806446    0.835030   \n",
      "12    Balanced Accuracy    0.818839    0.822267    0.806507    0.835980   \n",
      "13                  MCC    0.638376    0.644814    0.613091    0.670731   \n",
      "14                  NPV    0.815800    0.813800    0.814500    0.810500   \n",
      "15              ROC_AUC    0.818839    0.822267    0.806507    0.835980   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.689476    0.671779    0.691702    0.681573  \n",
      "1   400.000000  387.000000  404.000000  402.000000  \n",
      "2   347.000000  359.000000  344.000000  340.000000  \n",
      "3    76.000000   79.000000   85.000000   73.000000  \n",
      "4    76.000000   74.000000   66.000000   84.000000  \n",
      "5     0.830923    0.829811    0.832036    0.825362  \n",
      "6     0.840336    0.830472    0.826176    0.846316  \n",
      "7     0.840336    0.839479    0.859574    0.827160  \n",
      "8     0.820300    0.819600    0.801900    0.823200  \n",
      "9     0.840336    0.834951    0.842544    0.836629  \n",
      "10    0.830923    0.829781    0.831798    0.825510  \n",
      "11    0.830334    0.829646    0.831284    0.824527  \n",
      "12    0.830334    0.829557    0.830720    0.825203  \n",
      "13    0.660667    0.659343    0.663317    0.649303  \n",
      "14    0.820300    0.829100    0.839000    0.801900  \n",
      "15    0.830334    0.829557    0.830720    0.825203  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_7_cat = np.where((y_pred_lgbm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:40:40,438] Trial 400 finished with value: 0.6860328669649192 and parameters: {'n_estimators': 763, 'learning_rate': 0.05702457384269754, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 714}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:40:47,668] Trial 401 finished with value: 0.6849724051105442 and parameters: {'n_estimators': 821, 'learning_rate': 0.07861118828913284, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:40:55,034] Trial 402 finished with value: 0.6857641941902645 and parameters: {'n_estimators': 405, 'learning_rate': 0.06745939894150002, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 689}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:41:04,437] Trial 403 finished with value: 0.6872417530481826 and parameters: {'n_estimators': 785, 'learning_rate': 0.07204147088783966, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 723}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:41:12,593] Trial 404 finished with value: 0.6817577423329715 and parameters: {'n_estimators': 855, 'learning_rate': 0.06218508545021448, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 667}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:41:22,776] Trial 405 finished with value: 0.6860567602162002 and parameters: {'n_estimators': 744, 'learning_rate': 0.06051702448463102, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 735}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:41:34,052] Trial 406 finished with value: 0.6877374113802306 and parameters: {'n_estimators': 806, 'learning_rate': 0.0695654489662049, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 711}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:41:43,446] Trial 407 finished with value: 0.6847145165143739 and parameters: {'n_estimators': 776, 'learning_rate': 0.056104543831568134, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:41:52,676] Trial 408 finished with value: 0.6840955443225989 and parameters: {'n_estimators': 825, 'learning_rate': 0.07564921776566562, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:42:01,996] Trial 409 finished with value: 0.6855981030236485 and parameters: {'n_estimators': 789, 'learning_rate': 0.06573146114523482, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 723}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:42:13,119] Trial 410 finished with value: 0.6867832528703631 and parameters: {'n_estimators': 807, 'learning_rate': 0.050430870102276024, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 698}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:42:21,548] Trial 411 finished with value: 0.6827717242808075 and parameters: {'n_estimators': 848, 'learning_rate': 0.059412807653321166, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 739}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:42:30,327] Trial 412 finished with value: 0.6831746997678007 and parameters: {'n_estimators': 771, 'learning_rate': 0.0652640525605899, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:42:38,721] Trial 413 finished with value: 0.6822945755671503 and parameters: {'n_estimators': 801, 'learning_rate': 0.08036306342097818, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 716}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:42:47,066] Trial 414 finished with value: 0.6857317525639212 and parameters: {'n_estimators': 831, 'learning_rate': 0.07167407667505962, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 727}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:42:57,383] Trial 415 finished with value: 0.681218922749189 and parameters: {'n_estimators': 757, 'learning_rate': 0.041327108389577934, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 735}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:43:07,506] Trial 416 finished with value: 0.6852501173102461 and parameters: {'n_estimators': 789, 'learning_rate': 0.053760386824135635, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 644}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:43:14,684] Trial 417 finished with value: 0.678649814636325 and parameters: {'n_estimators': 812, 'learning_rate': 0.06890850083902472, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 711}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:43:25,102] Trial 418 finished with value: 0.6840566236073296 and parameters: {'n_estimators': 778, 'learning_rate': 0.06277349886184591, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:43:32,220] Trial 419 finished with value: 0.6813719397571808 and parameters: {'n_estimators': 836, 'learning_rate': 0.07564836500784222, 'max_depth': 7, 'max_bin': 233, 'num_leaves': 724}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:43:37,078] Trial 420 finished with value: 0.6757103469524643 and parameters: {'n_estimators': 818, 'learning_rate': 0.16074471285752087, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:43:47,163] Trial 421 finished with value: 0.6849388853475766 and parameters: {'n_estimators': 794, 'learning_rate': 0.05721741022191457, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 701}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:43:56,605] Trial 422 finished with value: 0.6892993110612053 and parameters: {'n_estimators': 808, 'learning_rate': 0.06612077733588563, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:44:04,011] Trial 423 finished with value: 0.6833294745769364 and parameters: {'n_estimators': 771, 'learning_rate': 0.07311191342104055, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 725}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:44:14,621] Trial 424 finished with value: 0.6860029541701042 and parameters: {'n_estimators': 733, 'learning_rate': 0.0611260857933892, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:44:22,329] Trial 425 finished with value: 0.6815524457353191 and parameters: {'n_estimators': 793, 'learning_rate': 0.06880434496674129, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 685}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:44:30,950] Trial 426 finished with value: 0.6860781947463225 and parameters: {'n_estimators': 752, 'learning_rate': 0.06360192698234332, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 716}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:44:41,130] Trial 427 finished with value: 0.6737935664877837 and parameters: {'n_estimators': 825, 'learning_rate': 0.046013305442512406, 'max_depth': 6, 'max_bin': 235, 'num_leaves': 737}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:44:52,241] Trial 428 finished with value: 0.6865899956077193 and parameters: {'n_estimators': 848, 'learning_rate': 0.04909964402615517, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:45:10,397] Trial 429 finished with value: 0.6463878238750278 and parameters: {'n_estimators': 778, 'learning_rate': 0.0071571102162251155, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 725}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:45:17,543] Trial 430 finished with value: 0.6829935952248442 and parameters: {'n_estimators': 810, 'learning_rate': 0.07956824661536309, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 701}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:45:28,513] Trial 431 finished with value: 0.6846289277650645 and parameters: {'n_estimators': 792, 'learning_rate': 0.05776597056250312, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 738}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:45:35,329] Trial 432 finished with value: 0.6851274661963412 and parameters: {'n_estimators': 830, 'learning_rate': 0.07066766989275536, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 712}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:45:47,596] Trial 433 finished with value: 0.6862738447040275 and parameters: {'n_estimators': 863, 'learning_rate': 0.053386921318856584, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 729}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:45:55,798] Trial 434 finished with value: 0.6834714754635491 and parameters: {'n_estimators': 757, 'learning_rate': 0.0758877569483354, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:46:06,306] Trial 435 finished with value: 0.6879595555716924 and parameters: {'n_estimators': 800, 'learning_rate': 0.06628847376256725, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 722}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:46:11,084] Trial 436 finished with value: 0.5899192082689432 and parameters: {'n_estimators': 818, 'learning_rate': 0.03261921369768893, 'max_depth': 3, 'max_bin': 232, 'num_leaves': 738}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:46:19,775] Trial 437 finished with value: 0.6857753643319855 and parameters: {'n_estimators': 770, 'learning_rate': 0.061234030176596795, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 711}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:46:25,513] Trial 438 finished with value: 0.6487169249808076 and parameters: {'n_estimators': 790, 'learning_rate': 0.05668253506111999, 'max_depth': 4, 'max_bin': 234, 'num_leaves': 736}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:46:33,620] Trial 439 finished with value: 0.684714734786777 and parameters: {'n_estimators': 807, 'learning_rate': 0.07258453170513915, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 725}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:46:42,772] Trial 440 finished with value: 0.6853847015298594 and parameters: {'n_estimators': 847, 'learning_rate': 0.06414422386661742, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 695}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:46:50,952] Trial 441 finished with value: 0.6852271786831847 and parameters: {'n_estimators': 782, 'learning_rate': 0.0685382758359096, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 739}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:47:04,436] Trial 442 finished with value: 0.6854386054350365 and parameters: {'n_estimators': 827, 'learning_rate': 0.050637668477319785, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 714}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:47:14,689] Trial 443 finished with value: 0.6854455577485541 and parameters: {'n_estimators': 810, 'learning_rate': 0.0601022620661723, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 750}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:47:19,481] Trial 444 finished with value: 0.6759514103000024 and parameters: {'n_estimators': 765, 'learning_rate': 0.1895237280116751, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 727}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:47:26,766] Trial 445 finished with value: 0.6840025901267971 and parameters: {'n_estimators': 838, 'learning_rate': 0.07611968468425649, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 737}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:47:37,255] Trial 446 finished with value: 0.6839554141805022 and parameters: {'n_estimators': 795, 'learning_rate': 0.06477708727626348, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 698}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:47:44,587] Trial 447 finished with value: 0.6861826093851378 and parameters: {'n_estimators': 744, 'learning_rate': 0.08026893010260945, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 716}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:47:53,873] Trial 448 finished with value: 0.6834352652127146 and parameters: {'n_estimators': 779, 'learning_rate': 0.05416569894557769, 'max_depth': 11, 'max_bin': 239, 'num_leaves': 737}. Best is trial 259 with value: 0.6953738217912808.\n",
      "[I 2023-12-20 00:48:01,558] Trial 449 finished with value: 0.6849484495950884 and parameters: {'n_estimators': 816, 'learning_rate': 0.06949477511955453, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 725}. Best is trial 259 with value: 0.6953738217912808.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.69537382\n",
      "\tBest params:\n",
      "\t\tn_estimators: 807\n",
      "\t\tlearning_rate: 0.07267778442617548\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 233\n",
      "\t\tnum_leaves: 750\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.682897    0.713345    0.691267    0.702469   \n",
      "1                    TP  396.000000  399.000000  365.000000  396.000000   \n",
      "2                    TN  341.000000  341.000000  360.000000  355.000000   \n",
      "3                    FP   85.000000   81.000000   92.000000   65.000000   \n",
      "4                    FN   77.000000   78.000000   82.000000   83.000000   \n",
      "5              Accuracy    0.819800    0.823137    0.806452    0.835373   \n",
      "6             Precision    0.823285    0.831250    0.798687    0.859002   \n",
      "7           Sensitivity    0.837209    0.836478    0.816555    0.826722   \n",
      "8           Specificity    0.800500    0.808100    0.796500    0.845200   \n",
      "9              F1 score    0.830189    0.833856    0.807522    0.842553   \n",
      "10  F1 score (weighted)    0.819701    0.823099    0.806440    0.835523   \n",
      "11     F1 score (macro)    0.819123    0.822398    0.806446    0.835030   \n",
      "12    Balanced Accuracy    0.818839    0.822267    0.806507    0.835980   \n",
      "13                  MCC    0.638376    0.644814    0.613091    0.670731   \n",
      "14                  NPV    0.815800    0.813800    0.814500    0.810500   \n",
      "15              ROC_AUC    0.818839    0.822267    0.806507    0.835980   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.689476    0.671779    0.691702    0.681573    0.697324  \n",
      "1   400.000000  387.000000  404.000000  402.000000  400.000000  \n",
      "2   347.000000  359.000000  344.000000  340.000000  344.000000  \n",
      "3    76.000000   79.000000   85.000000   73.000000   82.000000  \n",
      "4    76.000000   74.000000   66.000000   84.000000   73.000000  \n",
      "5     0.830923    0.829811    0.832036    0.825362    0.827586  \n",
      "6     0.840336    0.830472    0.826176    0.846316    0.829876  \n",
      "7     0.840336    0.839479    0.859574    0.827160    0.845666  \n",
      "8     0.820300    0.819600    0.801900    0.823200    0.807500  \n",
      "9     0.840336    0.834951    0.842544    0.836629    0.837696  \n",
      "10    0.830923    0.829781    0.831798    0.825510    0.827478  \n",
      "11    0.830334    0.829646    0.831284    0.824527    0.826915  \n",
      "12    0.830334    0.829557    0.830720    0.825203    0.826589  \n",
      "13    0.660667    0.659343    0.663317    0.649303    0.653996  \n",
      "14    0.820300    0.829100    0.839000    0.801900    0.824900  \n",
      "15    0.830334    0.829557    0.830720    0.825203    0.826589  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_8_cat = np.where((y_pred_lgbm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:48:11,758] Trial 450 finished with value: 0.6989217749085941 and parameters: {'n_estimators': 794, 'learning_rate': 0.06027482670548841, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 739}. Best is trial 450 with value: 0.6989217749085941.\n",
      "[I 2023-12-20 00:48:16,923] Trial 451 finished with value: 0.6808794107038202 and parameters: {'n_estimators': 217, 'learning_rate': 0.056586217939849616, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 743}. Best is trial 450 with value: 0.6989217749085941.\n",
      "[I 2023-12-20 00:48:25,513] Trial 452 finished with value: 0.6930069120265261 and parameters: {'n_estimators': 363, 'learning_rate': 0.060174209146528816, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 749}. Best is trial 450 with value: 0.6989217749085941.\n",
      "[I 2023-12-20 00:48:35,600] Trial 453 finished with value: 0.6991359848793074 and parameters: {'n_estimators': 831, 'learning_rate': 0.05969366316111392, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 750}. Best is trial 453 with value: 0.6991359848793074.\n",
      "[I 2023-12-20 00:48:48,010] Trial 454 finished with value: 0.6955194980083782 and parameters: {'n_estimators': 865, 'learning_rate': 0.054242457484172876, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 750}. Best is trial 453 with value: 0.6991359848793074.\n",
      "[I 2023-12-20 00:48:58,263] Trial 455 finished with value: 0.6957449279574848 and parameters: {'n_estimators': 870, 'learning_rate': 0.04865054992819936, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 749}. Best is trial 453 with value: 0.6991359848793074.\n",
      "[I 2023-12-20 00:49:05,360] Trial 456 finished with value: 0.6841605495281853 and parameters: {'n_estimators': 349, 'learning_rate': 0.045773940081854037, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 750}. Best is trial 453 with value: 0.6991359848793074.\n",
      "[I 2023-12-20 00:49:16,869] Trial 457 finished with value: 0.6995361935900264 and parameters: {'n_estimators': 880, 'learning_rate': 0.04993251255384064, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 750}. Best is trial 457 with value: 0.6995361935900264.\n",
      "[I 2023-12-20 00:49:27,975] Trial 458 finished with value: 0.6968110643836234 and parameters: {'n_estimators': 878, 'learning_rate': 0.047251413137425834, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 749}. Best is trial 457 with value: 0.6995361935900264.\n",
      "[I 2023-12-20 00:49:39,681] Trial 459 finished with value: 0.6996663012407282 and parameters: {'n_estimators': 878, 'learning_rate': 0.047374512319916406, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 749}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:49:52,462] Trial 460 finished with value: 0.6967385917017925 and parameters: {'n_estimators': 873, 'learning_rate': 0.04740979861499282, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 741}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:50:03,538] Trial 461 finished with value: 0.6939666352312737 and parameters: {'n_estimators': 886, 'learning_rate': 0.044457912369081386, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 749}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:50:12,141] Trial 462 finished with value: 0.6860686728100103 and parameters: {'n_estimators': 878, 'learning_rate': 0.04310702970649676, 'max_depth': 8, 'max_bin': 251, 'num_leaves': 737}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:50:24,709] Trial 463 finished with value: 0.6941127099716076 and parameters: {'n_estimators': 882, 'learning_rate': 0.04714367036247636, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 750}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:50:37,392] Trial 464 finished with value: 0.6970820374875225 and parameters: {'n_estimators': 889, 'learning_rate': 0.04058545725814781, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 750}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:50:49,428] Trial 465 finished with value: 0.690700950704313 and parameters: {'n_estimators': 887, 'learning_rate': 0.03699698662049059, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 749}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:51:01,125] Trial 466 finished with value: 0.6937156423925873 and parameters: {'n_estimators': 888, 'learning_rate': 0.04253506409402313, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 749}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:51:13,366] Trial 467 finished with value: 0.6974809474049615 and parameters: {'n_estimators': 898, 'learning_rate': 0.0422857986960893, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 739}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:51:26,590] Trial 468 finished with value: 0.6978841676963132 and parameters: {'n_estimators': 899, 'learning_rate': 0.042956249798766485, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 738}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:51:39,848] Trial 469 finished with value: 0.6946322202148214 and parameters: {'n_estimators': 899, 'learning_rate': 0.0406235537864674, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 749}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:51:50,941] Trial 470 finished with value: 0.6933301022238336 and parameters: {'n_estimators': 895, 'learning_rate': 0.04166267203244876, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 736}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:52:02,657] Trial 471 finished with value: 0.6944215023057827 and parameters: {'n_estimators': 895, 'learning_rate': 0.041100531482505975, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 738}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:52:13,188] Trial 472 finished with value: 0.6932184959964157 and parameters: {'n_estimators': 896, 'learning_rate': 0.04115917852904814, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 750}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:52:25,500] Trial 473 finished with value: 0.6961376729492503 and parameters: {'n_estimators': 897, 'learning_rate': 0.0423648624339998, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 750}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:52:36,456] Trial 474 finished with value: 0.6946943341450337 and parameters: {'n_estimators': 887, 'learning_rate': 0.04048724286950276, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 750}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:52:49,551] Trial 475 finished with value: 0.6911633450671817 and parameters: {'n_estimators': 900, 'learning_rate': 0.03791210772452612, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 738}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:52:59,906] Trial 476 finished with value: 0.6926844209073375 and parameters: {'n_estimators': 886, 'learning_rate': 0.04264491521224931, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 750}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:53:10,172] Trial 477 finished with value: 0.6906707568025198 and parameters: {'n_estimators': 899, 'learning_rate': 0.04417786474749951, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 736}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:53:22,050] Trial 478 finished with value: 0.6980380304773403 and parameters: {'n_estimators': 882, 'learning_rate': 0.04092960836628519, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 750}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:53:36,618] Trial 479 finished with value: 0.6916135347910503 and parameters: {'n_estimators': 875, 'learning_rate': 0.03662725603451466, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 750}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:53:48,885] Trial 480 finished with value: 0.6952276527682058 and parameters: {'n_estimators': 876, 'learning_rate': 0.040641031637928066, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 723}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:54:01,325] Trial 481 finished with value: 0.6943722897679836 and parameters: {'n_estimators': 878, 'learning_rate': 0.039896264424140515, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 555}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:54:13,127] Trial 482 finished with value: 0.6923576092059749 and parameters: {'n_estimators': 872, 'learning_rate': 0.03545172876129618, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 535}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:54:25,126] Trial 483 finished with value: 0.6923804119897831 and parameters: {'n_estimators': 879, 'learning_rate': 0.046347049991658285, 'max_depth': 11, 'max_bin': 257, 'num_leaves': 722}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:54:35,983] Trial 484 finished with value: 0.6929774535665617 and parameters: {'n_estimators': 872, 'learning_rate': 0.04052044192165604, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 725}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:54:48,221] Trial 485 finished with value: 0.6882951966522264 and parameters: {'n_estimators': 883, 'learning_rate': 0.033118882457397206, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 736}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:54:59,500] Trial 486 finished with value: 0.6930066864178236 and parameters: {'n_estimators': 869, 'learning_rate': 0.04012313549203577, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 650}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:55:11,648] Trial 487 finished with value: 0.6934230988541941 and parameters: {'n_estimators': 898, 'learning_rate': 0.04644717834648968, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 554}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:55:23,769] Trial 488 finished with value: 0.6952339827543458 and parameters: {'n_estimators': 882, 'learning_rate': 0.04446860243323631, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 511}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:55:37,953] Trial 489 finished with value: 0.6993740569344095 and parameters: {'n_estimators': 885, 'learning_rate': 0.04395074793471623, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 721}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:55:48,150] Trial 490 finished with value: 0.6908796150168598 and parameters: {'n_estimators': 883, 'learning_rate': 0.04246683183961789, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 447}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:55:59,425] Trial 491 finished with value: 0.6947482935739222 and parameters: {'n_estimators': 867, 'learning_rate': 0.04665778836498053, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 592}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:56:11,153] Trial 492 finished with value: 0.6957634015391084 and parameters: {'n_estimators': 864, 'learning_rate': 0.047017878774408994, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 516}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:56:21,717] Trial 493 finished with value: 0.6964478434961509 and parameters: {'n_estimators': 864, 'learning_rate': 0.04691579276025193, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 600}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:56:33,438] Trial 494 finished with value: 0.6923961323891465 and parameters: {'n_estimators': 869, 'learning_rate': 0.047668826844679255, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 498}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:56:45,597] Trial 495 finished with value: 0.6914906027040915 and parameters: {'n_estimators': 864, 'learning_rate': 0.034144479434671714, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 528}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:56:56,787] Trial 496 finished with value: 0.6932024694655761 and parameters: {'n_estimators': 900, 'learning_rate': 0.03886111374114448, 'max_depth': 11, 'max_bin': 257, 'num_leaves': 513}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:57:07,651] Trial 497 finished with value: 0.6911873717688095 and parameters: {'n_estimators': 867, 'learning_rate': 0.04662663445737202, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 598}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:57:19,254] Trial 498 finished with value: 0.6924642439956996 and parameters: {'n_estimators': 866, 'learning_rate': 0.03828276142394833, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 582}. Best is trial 459 with value: 0.6996663012407282.\n",
      "[I 2023-12-20 00:57:30,232] Trial 499 finished with value: 0.6942798292411385 and parameters: {'n_estimators': 881, 'learning_rate': 0.04753671893205566, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 624}. Best is trial 459 with value: 0.6996663012407282.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.699666301\n",
      "\tBest params:\n",
      "\t\tn_estimators: 878\n",
      "\t\tlearning_rate: 0.047374512319916406\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 250\n",
      "\t\tnum_leaves: 749\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.682897    0.713345    0.691267    0.702469   \n",
      "1                    TP  396.000000  399.000000  365.000000  396.000000   \n",
      "2                    TN  341.000000  341.000000  360.000000  355.000000   \n",
      "3                    FP   85.000000   81.000000   92.000000   65.000000   \n",
      "4                    FN   77.000000   78.000000   82.000000   83.000000   \n",
      "5              Accuracy    0.819800    0.823137    0.806452    0.835373   \n",
      "6             Precision    0.823285    0.831250    0.798687    0.859002   \n",
      "7           Sensitivity    0.837209    0.836478    0.816555    0.826722   \n",
      "8           Specificity    0.800500    0.808100    0.796500    0.845200   \n",
      "9              F1 score    0.830189    0.833856    0.807522    0.842553   \n",
      "10  F1 score (weighted)    0.819701    0.823099    0.806440    0.835523   \n",
      "11     F1 score (macro)    0.819123    0.822398    0.806446    0.835030   \n",
      "12    Balanced Accuracy    0.818839    0.822267    0.806507    0.835980   \n",
      "13                  MCC    0.638376    0.644814    0.613091    0.670731   \n",
      "14                  NPV    0.815800    0.813800    0.814500    0.810500   \n",
      "15              ROC_AUC    0.818839    0.822267    0.806507    0.835980   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.689476    0.671779    0.691702    0.681573    0.697324    0.677127  \n",
      "1   400.000000  387.000000  404.000000  402.000000  400.000000  385.000000  \n",
      "2   347.000000  359.000000  344.000000  340.000000  344.000000  346.000000  \n",
      "3    76.000000   79.000000   85.000000   73.000000   82.000000   97.000000  \n",
      "4    76.000000   74.000000   66.000000   84.000000   73.000000   71.000000  \n",
      "5     0.830923    0.829811    0.832036    0.825362    0.827586    0.813126  \n",
      "6     0.840336    0.830472    0.826176    0.846316    0.829876    0.798755  \n",
      "7     0.840336    0.839479    0.859574    0.827160    0.845666    0.844298  \n",
      "8     0.820300    0.819600    0.801900    0.823200    0.807500    0.781000  \n",
      "9     0.840336    0.834951    0.842544    0.836629    0.837696    0.820896  \n",
      "10    0.830923    0.829781    0.831798    0.825510    0.827478    0.812891  \n",
      "11    0.830334    0.829646    0.831284    0.824527    0.826915    0.812773  \n",
      "12    0.830334    0.829557    0.830720    0.825203    0.826589    0.812668  \n",
      "13    0.660667    0.659343    0.663317    0.649303    0.653996    0.626912  \n",
      "14    0.820300    0.829100    0.839000    0.801900    0.824900    0.829700  \n",
      "15    0.830334    0.829557    0.830720    0.825203    0.826589    0.812668  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_9_cat = np.where((y_pred_lgbm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                           np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.703916     0.017752\n",
      "1                    TP       196.900000     7.489993\n",
      "2                    TN       175.300000     5.888784\n",
      "3                    FP        37.800000     2.699794\n",
      "4                    FN        39.200000     5.138093\n",
      "5              Accuracy         0.828582     0.011190\n",
      "6             Precision         0.838772     0.013335\n",
      "7           Sensitivity         0.834034     0.020667\n",
      "8           Specificity         0.822730     0.009237\n",
      "9              F1 score         0.836254     0.012988\n",
      "10  F1 score (weighted)         0.828623     0.011130\n",
      "11     F1 score (macro)         0.828071     0.011107\n",
      "12    Balanced Accuracy         0.828375     0.010894\n",
      "13                  MCC         0.656472     0.022261\n",
      "14                  NPV         0.817440     0.021849\n",
      "15              ROC_AUC         0.828375     0.010894\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.682897</td>\n",
       "      <td>0.713345</td>\n",
       "      <td>0.691267</td>\n",
       "      <td>0.702469</td>\n",
       "      <td>0.689476</td>\n",
       "      <td>0.671779</td>\n",
       "      <td>0.691702</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.697324</td>\n",
       "      <td>0.677127</td>\n",
       "      <td>0.689896</td>\n",
       "      <td>0.012374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>393.400000</td>\n",
       "      <td>11.720827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>347.700000</td>\n",
       "      <td>7.543209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>81.500000</td>\n",
       "      <td>9.168182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>76.400000</td>\n",
       "      <td>5.680376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.819800</td>\n",
       "      <td>0.823137</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.835373</td>\n",
       "      <td>0.830923</td>\n",
       "      <td>0.829811</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.825362</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.813126</td>\n",
       "      <td>0.824360</td>\n",
       "      <td>0.009028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.823285</td>\n",
       "      <td>0.831250</td>\n",
       "      <td>0.798687</td>\n",
       "      <td>0.859002</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.830472</td>\n",
       "      <td>0.826176</td>\n",
       "      <td>0.846316</td>\n",
       "      <td>0.829876</td>\n",
       "      <td>0.798755</td>\n",
       "      <td>0.828415</td>\n",
       "      <td>0.018878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.836478</td>\n",
       "      <td>0.816555</td>\n",
       "      <td>0.826722</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.839479</td>\n",
       "      <td>0.859574</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.845666</td>\n",
       "      <td>0.844298</td>\n",
       "      <td>0.837348</td>\n",
       "      <td>0.011901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.808100</td>\n",
       "      <td>0.796500</td>\n",
       "      <td>0.845200</td>\n",
       "      <td>0.820300</td>\n",
       "      <td>0.819600</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.807500</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.810380</td>\n",
       "      <td>0.017635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.833856</td>\n",
       "      <td>0.807522</td>\n",
       "      <td>0.842553</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.834951</td>\n",
       "      <td>0.842544</td>\n",
       "      <td>0.836629</td>\n",
       "      <td>0.837696</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.832717</td>\n",
       "      <td>0.010948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.819701</td>\n",
       "      <td>0.823099</td>\n",
       "      <td>0.806440</td>\n",
       "      <td>0.835523</td>\n",
       "      <td>0.830923</td>\n",
       "      <td>0.829781</td>\n",
       "      <td>0.831798</td>\n",
       "      <td>0.825510</td>\n",
       "      <td>0.827478</td>\n",
       "      <td>0.812891</td>\n",
       "      <td>0.824314</td>\n",
       "      <td>0.009064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.819123</td>\n",
       "      <td>0.822398</td>\n",
       "      <td>0.806446</td>\n",
       "      <td>0.835030</td>\n",
       "      <td>0.830334</td>\n",
       "      <td>0.829646</td>\n",
       "      <td>0.831284</td>\n",
       "      <td>0.824527</td>\n",
       "      <td>0.826915</td>\n",
       "      <td>0.812773</td>\n",
       "      <td>0.823847</td>\n",
       "      <td>0.008918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.818839</td>\n",
       "      <td>0.822267</td>\n",
       "      <td>0.806507</td>\n",
       "      <td>0.835980</td>\n",
       "      <td>0.830334</td>\n",
       "      <td>0.829557</td>\n",
       "      <td>0.830720</td>\n",
       "      <td>0.825203</td>\n",
       "      <td>0.826589</td>\n",
       "      <td>0.812668</td>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.009016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.638376</td>\n",
       "      <td>0.644814</td>\n",
       "      <td>0.613091</td>\n",
       "      <td>0.670731</td>\n",
       "      <td>0.660667</td>\n",
       "      <td>0.659343</td>\n",
       "      <td>0.663317</td>\n",
       "      <td>0.649303</td>\n",
       "      <td>0.653996</td>\n",
       "      <td>0.626912</td>\n",
       "      <td>0.648055</td>\n",
       "      <td>0.017777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.814500</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.820300</td>\n",
       "      <td>0.829100</td>\n",
       "      <td>0.839000</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.824900</td>\n",
       "      <td>0.829700</td>\n",
       "      <td>0.819950</td>\n",
       "      <td>0.010884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.818839</td>\n",
       "      <td>0.822267</td>\n",
       "      <td>0.806507</td>\n",
       "      <td>0.835980</td>\n",
       "      <td>0.830334</td>\n",
       "      <td>0.829557</td>\n",
       "      <td>0.830720</td>\n",
       "      <td>0.825203</td>\n",
       "      <td>0.826589</td>\n",
       "      <td>0.812668</td>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.009016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.682897    0.713345    0.691267    0.702469   \n",
       "1                    TP  396.000000  399.000000  365.000000  396.000000   \n",
       "2                    TN  341.000000  341.000000  360.000000  355.000000   \n",
       "3                    FP   85.000000   81.000000   92.000000   65.000000   \n",
       "4                    FN   77.000000   78.000000   82.000000   83.000000   \n",
       "5              Accuracy    0.819800    0.823137    0.806452    0.835373   \n",
       "6             Precision    0.823285    0.831250    0.798687    0.859002   \n",
       "7           Sensitivity    0.837209    0.836478    0.816555    0.826722   \n",
       "8           Specificity    0.800500    0.808100    0.796500    0.845200   \n",
       "9              F1 score    0.830189    0.833856    0.807522    0.842553   \n",
       "10  F1 score (weighted)    0.819701    0.823099    0.806440    0.835523   \n",
       "11     F1 score (macro)    0.819123    0.822398    0.806446    0.835030   \n",
       "12    Balanced Accuracy    0.818839    0.822267    0.806507    0.835980   \n",
       "13                  MCC    0.638376    0.644814    0.613091    0.670731   \n",
       "14                  NPV    0.815800    0.813800    0.814500    0.810500   \n",
       "15              ROC_AUC    0.818839    0.822267    0.806507    0.835980   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.689476    0.671779    0.691702    0.681573    0.697324    0.677127   \n",
       "1   400.000000  387.000000  404.000000  402.000000  400.000000  385.000000   \n",
       "2   347.000000  359.000000  344.000000  340.000000  344.000000  346.000000   \n",
       "3    76.000000   79.000000   85.000000   73.000000   82.000000   97.000000   \n",
       "4    76.000000   74.000000   66.000000   84.000000   73.000000   71.000000   \n",
       "5     0.830923    0.829811    0.832036    0.825362    0.827586    0.813126   \n",
       "6     0.840336    0.830472    0.826176    0.846316    0.829876    0.798755   \n",
       "7     0.840336    0.839479    0.859574    0.827160    0.845666    0.844298   \n",
       "8     0.820300    0.819600    0.801900    0.823200    0.807500    0.781000   \n",
       "9     0.840336    0.834951    0.842544    0.836629    0.837696    0.820896   \n",
       "10    0.830923    0.829781    0.831798    0.825510    0.827478    0.812891   \n",
       "11    0.830334    0.829646    0.831284    0.824527    0.826915    0.812773   \n",
       "12    0.830334    0.829557    0.830720    0.825203    0.826589    0.812668   \n",
       "13    0.660667    0.659343    0.663317    0.649303    0.653996    0.626912   \n",
       "14    0.820300    0.829100    0.839000    0.801900    0.824900    0.829700   \n",
       "15    0.830334    0.829557    0.830720    0.825203    0.826589    0.812668   \n",
       "\n",
       "           ave        std  \n",
       "0     0.689896   0.012374  \n",
       "1   393.400000  11.720827  \n",
       "2   347.700000   7.543209  \n",
       "3    81.500000   9.168182  \n",
       "4    76.400000   5.680376  \n",
       "5     0.824360   0.009028  \n",
       "6     0.828415   0.018878  \n",
       "7     0.837348   0.011901  \n",
       "8     0.810380   0.017635  \n",
       "9     0.832717   0.010948  \n",
       "10    0.824314   0.009064  \n",
       "11    0.823847   0.008918  \n",
       "12    0.823866   0.009016  \n",
       "13    0.648055   0.017777  \n",
       "14    0.819950   0.010884  \n",
       "15    0.823866   0.009016  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_lgbm0</th>\n",
       "      <th>y_pred_lgbm1</th>\n",
       "      <th>y_pred_lgbm2</th>\n",
       "      <th>y_pred_lgbm3</th>\n",
       "      <th>y_pred_lgbm4</th>\n",
       "      <th>y_pred_lgbm_ave</th>\n",
       "      <th>y_pred_lgbm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3921050</td>\n",
       "      <td>0</td>\n",
       "      <td>6.17</td>\n",
       "      <td>5.726472</td>\n",
       "      <td>5.632965</td>\n",
       "      <td>5.734820</td>\n",
       "      <td>5.654171</td>\n",
       "      <td>5.775805</td>\n",
       "      <td>5.782372</td>\n",
       "      <td>0.180020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL270476</td>\n",
       "      <td>1</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.771562</td>\n",
       "      <td>6.751000</td>\n",
       "      <td>6.578990</td>\n",
       "      <td>6.491464</td>\n",
       "      <td>6.731084</td>\n",
       "      <td>6.687350</td>\n",
       "      <td>0.112442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3664128</td>\n",
       "      <td>2</td>\n",
       "      <td>7.62</td>\n",
       "      <td>6.877704</td>\n",
       "      <td>7.088313</td>\n",
       "      <td>7.232468</td>\n",
       "      <td>7.116716</td>\n",
       "      <td>7.187726</td>\n",
       "      <td>7.187154</td>\n",
       "      <td>0.223569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4456250</td>\n",
       "      <td>3</td>\n",
       "      <td>5.26</td>\n",
       "      <td>5.826164</td>\n",
       "      <td>6.104230</td>\n",
       "      <td>6.005490</td>\n",
       "      <td>6.067481</td>\n",
       "      <td>5.948684</td>\n",
       "      <td>5.868675</td>\n",
       "      <td>0.286479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2408818</td>\n",
       "      <td>4</td>\n",
       "      <td>6.32</td>\n",
       "      <td>6.328565</td>\n",
       "      <td>6.105591</td>\n",
       "      <td>6.304581</td>\n",
       "      <td>6.214122</td>\n",
       "      <td>6.144022</td>\n",
       "      <td>6.236147</td>\n",
       "      <td>0.087819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4250302</td>\n",
       "      <td>4487</td>\n",
       "      <td>10.25</td>\n",
       "      <td>9.175185</td>\n",
       "      <td>9.235402</td>\n",
       "      <td>8.946572</td>\n",
       "      <td>9.674591</td>\n",
       "      <td>9.377685</td>\n",
       "      <td>9.443239</td>\n",
       "      <td>0.422376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL483893</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.954971</td>\n",
       "      <td>7.801202</td>\n",
       "      <td>7.716017</td>\n",
       "      <td>7.818377</td>\n",
       "      <td>7.769491</td>\n",
       "      <td>7.815009</td>\n",
       "      <td>0.072901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL3655914</td>\n",
       "      <td>4489</td>\n",
       "      <td>6.69</td>\n",
       "      <td>6.485789</td>\n",
       "      <td>6.523522</td>\n",
       "      <td>6.690245</td>\n",
       "      <td>6.561860</td>\n",
       "      <td>6.600607</td>\n",
       "      <td>6.592004</td>\n",
       "      <td>0.077684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL467876</td>\n",
       "      <td>4490</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.367752</td>\n",
       "      <td>7.201621</td>\n",
       "      <td>7.246571</td>\n",
       "      <td>7.417990</td>\n",
       "      <td>7.354891</td>\n",
       "      <td>7.331471</td>\n",
       "      <td>0.079703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4458544</td>\n",
       "      <td>4491</td>\n",
       "      <td>7.37</td>\n",
       "      <td>7.128928</td>\n",
       "      <td>7.379564</td>\n",
       "      <td>7.412878</td>\n",
       "      <td>7.416619</td>\n",
       "      <td>6.431057</td>\n",
       "      <td>7.189841</td>\n",
       "      <td>0.353339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_lgbm0  y_pred_lgbm1  \\\n",
       "0         CHEMBL3921050            0     6.17      5.726472      5.632965   \n",
       "1          CHEMBL270476            1     6.80      6.771562      6.751000   \n",
       "2         CHEMBL3664128            2     7.62      6.877704      7.088313   \n",
       "3         CHEMBL4456250            3     5.26      5.826164      6.104230   \n",
       "4         CHEMBL2408818            4     6.32      6.328565      6.105591   \n",
       "...                 ...          ...      ...           ...           ...   \n",
       "4487      CHEMBL4250302         4487    10.25      9.175185      9.235402   \n",
       "4488       CHEMBL483893         4488     7.83      7.954971      7.801202   \n",
       "4489      CHEMBL3655914         4489     6.69      6.485789      6.523522   \n",
       "4490       CHEMBL467876         4490     7.40      7.367752      7.201621   \n",
       "4491      CHEMBL4458544         4491     7.37      7.128928      7.379564   \n",
       "\n",
       "      y_pred_lgbm2  y_pred_lgbm3  y_pred_lgbm4  y_pred_lgbm_ave  \\\n",
       "0         5.734820      5.654171      5.775805         5.782372   \n",
       "1         6.578990      6.491464      6.731084         6.687350   \n",
       "2         7.232468      7.116716      7.187726         7.187154   \n",
       "3         6.005490      6.067481      5.948684         5.868675   \n",
       "4         6.304581      6.214122      6.144022         6.236147   \n",
       "...            ...           ...           ...              ...   \n",
       "4487      8.946572      9.674591      9.377685         9.443239   \n",
       "4488      7.716017      7.818377      7.769491         7.815009   \n",
       "4489      6.690245      6.561860      6.600607         6.592004   \n",
       "4490      7.246571      7.417990      7.354891         7.331471   \n",
       "4491      7.412878      7.416619      6.431057         7.189841   \n",
       "\n",
       "      y_pred_lgbm_std  \n",
       "0            0.180020  \n",
       "1            0.112442  \n",
       "2            0.223569  \n",
       "3            0.286479  \n",
       "4            0.087819  \n",
       "...               ...  \n",
       "4487         0.422376  \n",
       "4488         0.072901  \n",
       "4489         0.077684  \n",
       "4490         0.079703  \n",
       "4491         0.353339  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where((y_pred_optimized_lgbm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "lgbm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "537f1446-e8d9-4b66-8ee1-3b983d42520c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.702511</td>\n",
       "      <td>0.024708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.829963</td>\n",
       "      <td>0.016795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.841218</td>\n",
       "      <td>0.023278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.834114</td>\n",
       "      <td>0.026497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.825432</td>\n",
       "      <td>0.026409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.837296</td>\n",
       "      <td>0.017856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.829968</td>\n",
       "      <td>0.016788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.829349</td>\n",
       "      <td>0.016755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.829772</td>\n",
       "      <td>0.016725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.659510</td>\n",
       "      <td>0.033481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.818258</td>\n",
       "      <td>0.025922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.829772</td>\n",
       "      <td>0.016725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.702511     0.024708\n",
       "1              Accuracy         0.829963     0.016795\n",
       "2             Precision         0.841218     0.023278\n",
       "3           Sensitivity         0.834114     0.026497\n",
       "4           Specificity         0.825432     0.026409\n",
       "5              F1 score         0.837296     0.017856\n",
       "6   F1 score (weighted)         0.829968     0.016788\n",
       "7      F1 score (macro)         0.829349     0.016755\n",
       "8     Balanced Accuracy         0.829772     0.016725\n",
       "9                   MCC         0.659510     0.033481\n",
       "10                  NPV         0.818258     0.025922\n",
       "11              ROC_AUC         0.829772     0.016725"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.6713 with a standard deviation of 0.0188\n",
      "LightGBM optimized model r2_score 0.7067 with a standard deviation of 0.0198\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm.joblib']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"OUTPUT/lgbm_reg.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm.joblib\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.656270     0.028237\n",
      "1                    TP       193.300000     7.528465\n",
      "2                    TN       171.000000     8.705043\n",
      "3                    FP        42.100000     6.026792\n",
      "4                    FN        42.800000     3.994441\n",
      "5              Accuracy         0.810997     0.016811\n",
      "6             Precision         0.821356     0.023250\n",
      "7           Sensitivity         0.818689     0.016089\n",
      "8           Specificity         0.802400     0.027562\n",
      "9              F1 score         0.819863     0.015956\n",
      "10  F1 score (weighted)         0.810998     0.016848\n",
      "11     F1 score (macro)         0.810351     0.016872\n",
      "12    Balanced Accuracy         0.810546     0.017039\n",
      "13                  MCC         0.621054     0.033807\n",
      "14                  NPV         0.799650     0.018890\n",
      "15              ROC_AUC         0.810546     0.017039\n",
      "CPU times: user 2h 5min 54s, sys: 8.15 s, total: 2h 6min 2s\n",
      "Wall time: 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=16, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=16, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 01:06:16,109] A new study created in memory with name: XGBRegressor\n",
      "[I 2023-12-20 01:06:36,142] Trial 0 finished with value: 0.6803221030245348 and parameters: {'n_estimators': 503, 'eta': 0.07343108108415618, 'max_depth': 10, 'alpha': 0.9039, 'lambda': 17.3866904875639, 'max_bin': 436}. Best is trial 0 with value: 0.6803221030245348.\n",
      "[I 2023-12-20 01:06:45,701] Trial 1 finished with value: 0.6672363717423804 and parameters: {'n_estimators': 375, 'eta': 0.08819308254605975, 'max_depth': 7, 'alpha': 0.6915, 'lambda': 1.368252527236697, 'max_bin': 281}. Best is trial 0 with value: 0.6803221030245348.\n",
      "[I 2023-12-20 01:07:14,766] Trial 2 finished with value: 0.6130601160679923 and parameters: {'n_estimators': 781, 'eta': 0.006575299427652851, 'max_depth': 9, 'alpha': 0.2223, 'lambda': 9.097592367958981, 'max_bin': 358}. Best is trial 0 with value: 0.6803221030245348.\n",
      "[I 2023-12-20 01:07:29,569] Trial 3 finished with value: 0.6797737360652987 and parameters: {'n_estimators': 513, 'eta': 0.09139629924961294, 'max_depth': 7, 'alpha': 0.8573000000000001, 'lambda': 16.588434345942915, 'max_bin': 268}. Best is trial 0 with value: 0.6803221030245348.\n",
      "[I 2023-12-20 01:07:45,159] Trial 4 finished with value: 0.6504377983171977 and parameters: {'n_estimators': 719, 'eta': 0.027830162301959824, 'max_depth': 5, 'alpha': 0.33240000000000003, 'lambda': 5.08966408166677, 'max_bin': 357}. Best is trial 0 with value: 0.6803221030245348.\n",
      "[I 2023-12-20 01:07:46,475] Trial 5 finished with value: -5.660268777373959 and parameters: {'n_estimators': 123, 'eta': 0.006713281316274244, 'max_depth': 8, 'alpha': 0.5784, 'lambda': 16.063635734559192, 'max_bin': 479}. Best is trial 0 with value: 0.6803221030245348.\n",
      "[I 2023-12-20 01:08:07,516] Trial 6 finished with value: 0.6786365168712313 and parameters: {'n_estimators': 746, 'eta': 0.08320587350620975, 'max_depth': 11, 'alpha': 0.2927, 'lambda': 30.634109842502095, 'max_bin': 476}. Best is trial 0 with value: 0.6803221030245348.\n",
      "[I 2023-12-20 01:08:22,123] Trial 7 finished with value: 0.6690479599610698 and parameters: {'n_estimators': 798, 'eta': 0.08608703229561833, 'max_depth': 5, 'alpha': 0.18430000000000002, 'lambda': 38.624538691124016, 'max_bin': 411}. Best is trial 0 with value: 0.6803221030245348.\n",
      "[I 2023-12-20 01:08:57,418] Trial 8 finished with value: 0.681821435323057 and parameters: {'n_estimators': 780, 'eta': 0.03173457517036925, 'max_depth': 11, 'alpha': 0.5082, 'lambda': 21.56242565379783, 'max_bin': 275}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:08:58,800] Trial 9 finished with value: -23.267829927486236 and parameters: {'n_estimators': 145, 'eta': 0.0008163381630860716, 'max_depth': 8, 'alpha': 0.3014, 'lambda': 38.25736409521501, 'max_bin': 393}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:09:29,268] Trial 10 finished with value: 0.6814880963652366 and parameters: {'n_estimators': 638, 'eta': 0.051811359803460154, 'max_depth': 12, 'alpha': 0.057800000000000004, 'lambda': 25.706201628418892, 'max_bin': 309}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:10:01,012] Trial 11 finished with value: 0.6800848535756361 and parameters: {'n_estimators': 612, 'eta': 0.0434525902921987, 'max_depth': 12, 'alpha': 0.030500000000000003, 'lambda': 25.365089645991112, 'max_bin': 314}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:10:26,800] Trial 12 finished with value: 0.6813244392914842 and parameters: {'n_estimators': 875, 'eta': 0.055284532279190626, 'max_depth': 12, 'alpha': 0.49620000000000003, 'lambda': 24.507194896728787, 'max_bin': 313}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:10:57,151] Trial 13 finished with value: 0.6803461725789106 and parameters: {'n_estimators': 642, 'eta': 0.03646005296557619, 'max_depth': 11, 'alpha': 0.006500000000000001, 'lambda': 23.605819385574513, 'max_bin': 252}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:11:12,737] Trial 14 finished with value: 0.6733116173721772 and parameters: {'n_estimators': 358, 'eta': 0.05690247663110156, 'max_depth': 10, 'alpha': 0.4877, 'lambda': 28.703048927379402, 'max_bin': 315}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:11:45,412] Trial 15 finished with value: 0.680411931656493 and parameters: {'n_estimators': 615, 'eta': 0.024956787330298255, 'max_depth': 12, 'alpha': 0.7082, 'lambda': 19.938991175477828, 'max_bin': 293}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:12:06,839] Trial 16 finished with value: 0.6779784105948015 and parameters: {'n_estimators': 897, 'eta': 0.06501283557818305, 'max_depth': 11, 'alpha': 0.4383, 'lambda': 13.218893004707194, 'max_bin': 339}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:12:24,091] Trial 17 finished with value: 0.6737039599918508 and parameters: {'n_estimators': 390, 'eta': 0.045673938462297475, 'max_depth': 10, 'alpha': 0.6611, 'lambda': 32.95575850446015, 'max_bin': 256}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:12:41,408] Trial 18 finished with value: 0.6802527208401039 and parameters: {'n_estimators': 663, 'eta': 0.09861447320694589, 'max_depth': 9, 'alpha': 0.9875, 'lambda': 21.260252693665805, 'max_bin': 295}. Best is trial 8 with value: 0.681821435323057.\n",
      "[I 2023-12-20 01:13:07,204] Trial 19 finished with value: 0.6823230817669399 and parameters: {'n_estimators': 554, 'eta': 0.0654964938011603, 'max_depth': 11, 'alpha': 0.1044, 'lambda': 28.200910035945892, 'max_bin': 332}. Best is trial 19 with value: 0.6823230817669399.\n",
      "[I 2023-12-20 01:13:20,959] Trial 20 finished with value: 0.6763791953273902 and parameters: {'n_estimators': 263, 'eta': 0.07047724187501077, 'max_depth': 11, 'alpha': 0.3996, 'lambda': 33.55048840184923, 'max_bin': 340}. Best is trial 19 with value: 0.6823230817669399.\n",
      "[I 2023-12-20 01:13:49,352] Trial 21 finished with value: 0.6773612023118867 and parameters: {'n_estimators': 558, 'eta': 0.058109730927506445, 'max_depth': 12, 'alpha': 0.1431, 'lambda': 27.093287173944734, 'max_bin': 335}. Best is trial 19 with value: 0.6823230817669399.\n",
      "[I 2023-12-20 01:14:08,474] Trial 22 finished with value: 0.680730469736057 and parameters: {'n_estimators': 435, 'eta': 0.04949021974930627, 'max_depth': 10, 'alpha': 0.11470000000000001, 'lambda': 22.012326157924853, 'max_bin': 283}. Best is trial 19 with value: 0.6823230817669399.\n",
      "[I 2023-12-20 01:14:43,851] Trial 23 finished with value: 0.6828722337659399 and parameters: {'n_estimators': 712, 'eta': 0.0388946966243672, 'max_depth': 11, 'alpha': 0.0745, 'lambda': 26.025935156819287, 'max_bin': 380}. Best is trial 23 with value: 0.6828722337659399.\n",
      "[I 2023-12-20 01:15:23,740] Trial 24 finished with value: 0.6833956984889229 and parameters: {'n_estimators': 816, 'eta': 0.034744885712228134, 'max_depth': 11, 'alpha': 0.2238, 'lambda': 28.815921935411563, 'max_bin': 383}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:15:55,984] Trial 25 finished with value: 0.681412931850591 and parameters: {'n_estimators': 850, 'eta': 0.038364180187773524, 'max_depth': 9, 'alpha': 0.21150000000000002, 'lambda': 29.08234146510596, 'max_bin': 388}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:16:28,083] Trial 26 finished with value: 0.6756335018327598 and parameters: {'n_estimators': 708, 'eta': 0.02551828485183911, 'max_depth': 10, 'alpha': 0.084, 'lambda': 31.43573996519407, 'max_bin': 427}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:16:57,357] Trial 27 finished with value: 0.6807638710095582 and parameters: {'n_estimators': 535, 'eta': 0.041574342406831735, 'max_depth': 11, 'alpha': 0.2495, 'lambda': 27.433031244731307, 'max_bin': 375}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:17:17,993] Trial 28 finished with value: 0.6764091084254144 and parameters: {'n_estimators': 823, 'eta': 0.06561063391291419, 'max_depth': 6, 'alpha': 0.36960000000000004, 'lambda': 35.004441520181146, 'max_bin': 455}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:17:37,798] Trial 29 finished with value: 0.6813918534564806 and parameters: {'n_estimators': 459, 'eta': 0.07725506243729213, 'max_depth': 10, 'alpha': 0.14800000000000002, 'lambda': 28.99825230919943, 'max_bin': 415}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:18:07,059] Trial 30 finished with value: 0.6818916206909164 and parameters: {'n_estimators': 704, 'eta': 0.046649751102514465, 'max_depth': 9, 'alpha': 0.0044, 'lambda': 35.79101998355153, 'max_bin': 380}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:18:34,778] Trial 31 finished with value: 0.6793835305986962 and parameters: {'n_estimators': 703, 'eta': 0.03472274747619382, 'max_depth': 9, 'alpha': 0.0054, 'lambda': 35.29070716308204, 'max_bin': 373}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:19:00,312] Trial 32 finished with value: 0.6800154190216123 and parameters: {'n_estimators': 570, 'eta': 0.06004658326450749, 'max_depth': 10, 'alpha': 0.08900000000000001, 'lambda': 31.17372134602395, 'max_bin': 399}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:19:24,610] Trial 33 finished with value: 0.6803573920828092 and parameters: {'n_estimators': 759, 'eta': 0.04825765411439202, 'max_depth': 8, 'alpha': 0.1532, 'lambda': 36.37881572864771, 'max_bin': 359}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:19:45,046] Trial 34 finished with value: 0.6706529032170587 and parameters: {'n_estimators': 686, 'eta': 0.04522920198566036, 'max_depth': 7, 'alpha': 0.0678, 'lambda': 33.06492050869527, 'max_bin': 360}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:20:17,318] Trial 35 finished with value: 0.6829273791162773 and parameters: {'n_estimators': 822, 'eta': 0.05092064124087136, 'max_depth': 11, 'alpha': 0.24830000000000002, 'lambda': 27.43437068910155, 'max_bin': 386}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:20:45,392] Trial 36 finished with value: 0.6817559273703815 and parameters: {'n_estimators': 839, 'eta': 0.06447283080349142, 'max_depth': 11, 'alpha': 0.24760000000000001, 'lambda': 26.232002952955433, 'max_bin': 451}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:21:18,596] Trial 37 finished with value: 0.6813254267142536 and parameters: {'n_estimators': 805, 'eta': 0.0533833523728077, 'max_depth': 12, 'alpha': 0.27640000000000003, 'lambda': 23.72193038776289, 'max_bin': 406}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:21:44,841] Trial 38 finished with value: 0.6787468662207664 and parameters: {'n_estimators': 506, 'eta': 0.07442699551900252, 'max_depth': 11, 'alpha': 0.339, 'lambda': 29.719090882012107, 'max_bin': 425}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:22:20,056] Trial 39 finished with value: 0.6739663119036482 and parameters: {'n_estimators': 748, 'eta': 0.019424169062301322, 'max_depth': 10, 'alpha': 0.17900000000000002, 'lambda': 26.954889663340737, 'max_bin': 351}. Best is trial 24 with value: 0.6833956984889229.\n",
      "[I 2023-12-20 01:22:56,417] Trial 40 finished with value: 0.6840639006627747 and parameters: {'n_estimators': 863, 'eta': 0.04049106965479992, 'max_depth': 11, 'alpha': 0.2132, 'lambda': 23.71448227647675, 'max_bin': 441}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:23:29,383] Trial 41 finished with value: 0.6796997669316804 and parameters: {'n_estimators': 898, 'eta': 0.040136405611129425, 'max_depth': 11, 'alpha': 0.2235, 'lambda': 18.87553123768215, 'max_bin': 500}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:24:09,504] Trial 42 finished with value: 0.6823265139389136 and parameters: {'n_estimators': 786, 'eta': 0.03151646574352865, 'max_depth': 11, 'alpha': 0.1148, 'lambda': 23.488099205336628, 'max_bin': 442}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:24:50,493] Trial 43 finished with value: 0.683555462587243 and parameters: {'n_estimators': 790, 'eta': 0.03018402721374619, 'max_depth': 12, 'alpha': 0.1955, 'lambda': 23.776423108764465, 'max_bin': 453}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:25:28,693] Trial 44 finished with value: 0.6810575371836192 and parameters: {'n_estimators': 837, 'eta': 0.0404262002809013, 'max_depth': 12, 'alpha': 0.3279, 'lambda': 25.171149163935954, 'max_bin': 474}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:26:08,902] Trial 45 finished with value: 0.6817865024827041 and parameters: {'n_estimators': 755, 'eta': 0.03301767153049388, 'max_depth': 12, 'alpha': 0.1998, 'lambda': 22.82679667267906, 'max_bin': 458}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:26:59,512] Trial 46 finished with value: 0.6820981383755907 and parameters: {'n_estimators': 863, 'eta': 0.02109785626844274, 'max_depth': 12, 'alpha': 0.2797, 'lambda': 25.386023056239765, 'max_bin': 433}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:27:40,780] Trial 47 finished with value: 0.6813326293499518 and parameters: {'n_estimators': 803, 'eta': 0.029727894780211168, 'max_depth': 12, 'alpha': 0.3815, 'lambda': 18.441807485830992, 'max_bin': 467}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:28:15,081] Trial 48 finished with value: 0.6832777452334826 and parameters: {'n_estimators': 776, 'eta': 0.03438758184891999, 'max_depth': 10, 'alpha': 0.1825, 'lambda': 22.22353521512879, 'max_bin': 492}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:28:50,058] Trial 49 finished with value: 0.6837171044362446 and parameters: {'n_estimators': 881, 'eta': 0.035912791291030005, 'max_depth': 10, 'alpha': 0.4328, 'lambda': 19.958033315289345, 'max_bin': 495}. Best is trial 40 with value: 0.6840639006627747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6841\n",
      "\tBest params:\n",
      "\t\tn_estimators: 863\n",
      "\t\teta: 0.04049106965479992\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.2132\n",
      "\t\tlambda: 23.71448227647675\n",
      "\t\tmax_bin: 441\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.696313\n",
      "1                    TP  401.000000\n",
      "2                    TN  350.000000\n",
      "3                    FP   76.000000\n",
      "4                    FN   72.000000\n",
      "5              Accuracy    0.835373\n",
      "6             Precision    0.840671\n",
      "7           Sensitivity    0.847780\n",
      "8           Specificity    0.821600\n",
      "9              F1 score    0.844211\n",
      "10  F1 score (weighted)    0.835331\n",
      "11     F1 score (macro)    0.834841\n",
      "12    Balanced Accuracy    0.834688\n",
      "13                  MCC    0.669715\n",
      "14                  NPV    0.829400\n",
      "15              ROC_AUC    0.834688\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "y_pred_xgb_0_cat = np.where((y_pred_xgb_0 >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 01:29:33,215] Trial 50 finished with value: 0.6774787370346338 and parameters: {'n_estimators': 873, 'eta': 0.03545715373766487, 'max_depth': 10, 'alpha': 0.444, 'lambda': 20.462630079096844, 'max_bin': 495}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:29:35,041] Trial 51 finished with value: 0.13643817548010848 and parameters: {'n_estimators': 57, 'eta': 0.04392110972781907, 'max_depth': 11, 'alpha': 0.5729000000000001, 'lambda': 22.208057177611668, 'max_bin': 490}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:30:10,027] Trial 52 finished with value: 0.6759205036866187 and parameters: {'n_estimators': 778, 'eta': 0.02768647321810188, 'max_depth': 10, 'alpha': 0.3245, 'lambda': 15.62441025981625, 'max_bin': 481}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:30:45,089] Trial 53 finished with value: 0.677087080045697 and parameters: {'n_estimators': 807, 'eta': 0.05029512373642307, 'max_depth': 11, 'alpha': 0.8082, 'lambda': 24.30052348005917, 'max_bin': 486}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:31:21,307] Trial 54 finished with value: 0.675103411096997 and parameters: {'n_estimators': 900, 'eta': 0.036074521350701154, 'max_depth': 10, 'alpha': 0.1764, 'lambda': 20.336168952197557, 'max_bin': 472}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:32:04,130] Trial 55 finished with value: 0.6773377245655066 and parameters: {'n_estimators': 857, 'eta': 0.04253396279500935, 'max_depth': 12, 'alpha': 0.2641, 'lambda': 22.17302658131289, 'max_bin': 442}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:32:41,585] Trial 56 finished with value: 0.6762507126736657 and parameters: {'n_estimators': 737, 'eta': 0.03204648377157662, 'max_depth': 11, 'alpha': 0.23, 'lambda': 24.072268924376065, 'max_bin': 417}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:33:10,537] Trial 57 finished with value: 0.6788704600461486 and parameters: {'n_estimators': 820, 'eta': 0.05299566109312357, 'max_depth': 9, 'alpha': 0.5616, 'lambda': 27.408833038031574, 'max_bin': 466}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:33:42,044] Trial 58 finished with value: 0.6763912310469207 and parameters: {'n_estimators': 780, 'eta': 0.03784366660311991, 'max_depth': 10, 'alpha': 0.6303000000000001, 'lambda': 17.441793327320582, 'max_bin': 482}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:34:16,838] Trial 59 finished with value: 0.6762124731355368 and parameters: {'n_estimators': 867, 'eta': 0.047184005082045205, 'max_depth': 12, 'alpha': 0.4141, 'lambda': 21.195812909947215, 'max_bin': 444}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:34:50,435] Trial 60 finished with value: 0.6712435625384864 and parameters: {'n_estimators': 656, 'eta': 0.029741069223571857, 'max_depth': 11, 'alpha': 0.29660000000000003, 'lambda': 22.755622265139426, 'max_bin': 462}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:35:29,261] Trial 61 finished with value: 0.6779843672046295 and parameters: {'n_estimators': 833, 'eta': 0.03843012830701903, 'max_depth': 11, 'alpha': 0.13770000000000002, 'lambda': 25.5949087276707, 'max_bin': 392}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:36:04,921] Trial 62 finished with value: 0.6790324202004923 and parameters: {'n_estimators': 741, 'eta': 0.04155865778969284, 'max_depth': 11, 'alpha': 0.0597, 'lambda': 27.988350172656713, 'max_bin': 383}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:36:44,783] Trial 63 finished with value: 0.6754021844363602 and parameters: {'n_estimators': 765, 'eta': 0.034673693089549165, 'max_depth': 11, 'alpha': 0.1888, 'lambda': 26.46459787619726, 'max_bin': 400}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:36:59,070] Trial 64 finished with value: 0.6512180832550581 and parameters: {'n_estimators': 294, 'eta': 0.037978729412278735, 'max_depth': 10, 'alpha': 0.051300000000000005, 'lambda': 30.201521113051577, 'max_bin': 350}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:37:33,786] Trial 65 finished with value: 0.6754837044922126 and parameters: {'n_estimators': 678, 'eta': 0.05040587800934687, 'max_depth': 12, 'alpha': 0.12050000000000001, 'lambda': 28.267237210325835, 'max_bin': 372}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:38:00,633] Trial 66 finished with value: 0.6751359725700625 and parameters: {'n_estimators': 619, 'eta': 0.04466670355717229, 'max_depth': 10, 'alpha': 0.3558, 'lambda': 25.106111969278235, 'max_bin': 423}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:38:30,915] Trial 67 finished with value: 0.6680763937931355 and parameters: {'n_estimators': 879, 'eta': 0.025716122576972804, 'max_depth': 8, 'alpha': 0.1651, 'lambda': 21.38175582995558, 'max_bin': 364}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:39:01,240] Trial 68 finished with value: 0.6728745563559115 and parameters: {'n_estimators': 727, 'eta': 0.03322336666843436, 'max_depth': 9, 'alpha': 0.2182, 'lambda': 24.401181293529415, 'max_bin': 403}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:39:18,212] Trial 69 finished with value: 0.6501408862340161 and parameters: {'n_estimators': 839, 'eta': 0.04001756510205261, 'max_depth': 5, 'alpha': 0.7428, 'lambda': 19.480971813841414, 'max_bin': 324}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:39:55,502] Trial 70 finished with value: 0.6750856182854689 and parameters: {'n_estimators': 803, 'eta': 0.048020021307368925, 'max_depth': 11, 'alpha': 0.30920000000000003, 'lambda': 26.236449117333176, 'max_bin': 434}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:40:36,079] Trial 71 finished with value: 0.6747505406840253 and parameters: {'n_estimators': 796, 'eta': 0.030788426391286396, 'max_depth': 11, 'alpha': 0.11120000000000001, 'lambda': 24.116487289036662, 'max_bin': 449}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:41:15,097] Trial 72 finished with value: 0.6746846890296265 and parameters: {'n_estimators': 779, 'eta': 0.032860098906936856, 'max_depth': 11, 'alpha': 0.08940000000000001, 'lambda': 23.467065041462824, 'max_bin': 500}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:41:48,161] Trial 73 finished with value: 0.6755757991201652 and parameters: {'n_estimators': 711, 'eta': 0.04299685421782588, 'max_depth': 11, 'alpha': 0.038900000000000004, 'lambda': 23.034646931988146, 'max_bin': 408}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:42:30,940] Trial 74 finished with value: 0.6755230206310916 and parameters: {'n_estimators': 826, 'eta': 0.03654692938455962, 'max_depth': 12, 'alpha': 0.2416, 'lambda': 28.875446061941627, 'max_bin': 489}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:43:12,654] Trial 75 finished with value: 0.6742090925026432 and parameters: {'n_estimators': 876, 'eta': 0.02348469934483806, 'max_depth': 10, 'alpha': 0.2023, 'lambda': 26.7834410754768, 'max_bin': 387}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:43:58,377] Trial 76 finished with value: 0.6760307238206196 and parameters: {'n_estimators': 854, 'eta': 0.029424795512251405, 'max_depth': 12, 'alpha': 0.1283, 'lambda': 21.263004724327047, 'max_bin': 441}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:44:36,642] Trial 77 finished with value: 0.6731021200718648 and parameters: {'n_estimators': 767, 'eta': 0.027881553326897193, 'max_depth': 11, 'alpha': 0.1521, 'lambda': 25.45349468648723, 'max_bin': 476}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:45:04,602] Trial 78 finished with value: 0.6771499862548912 and parameters: {'n_estimators': 689, 'eta': 0.04553280931858901, 'max_depth': 10, 'alpha': 0.5367000000000001, 'lambda': 23.034914452790392, 'max_bin': 416}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:45:39,666] Trial 79 finished with value: 0.6752246614656301 and parameters: {'n_estimators': 726, 'eta': 0.034484329218448496, 'max_depth': 11, 'alpha': 0.2566, 'lambda': 27.28315546807362, 'max_bin': 365}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:46:22,730] Trial 80 finished with value: 0.676259187354257 and parameters: {'n_estimators': 792, 'eta': 0.039175385120356926, 'max_depth': 12, 'alpha': 0.0892, 'lambda': 30.073624183379746, 'max_bin': 456}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:46:48,317] Trial 81 finished with value: 0.6670514894100315 and parameters: {'n_estimators': 472, 'eta': 0.03153981236271334, 'max_depth': 11, 'alpha': 0.0983, 'lambda': 28.035873363788163, 'max_bin': 396}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:47:17,685] Trial 82 finished with value: 0.6744110097665834 and parameters: {'n_estimators': 599, 'eta': 0.05609228993548758, 'max_depth': 11, 'alpha': 0.024300000000000002, 'lambda': 31.590608842759217, 'max_bin': 430}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:47:36,640] Trial 83 finished with value: 0.6671005605017087 and parameters: {'n_estimators': 396, 'eta': 0.04077910740837731, 'max_depth': 10, 'alpha': 0.1829, 'lambda': 24.59771056855933, 'max_bin': 348}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:48:15,073] Trial 84 finished with value: 0.6766108028835398 and parameters: {'n_estimators': 821, 'eta': 0.03702962194266897, 'max_depth': 11, 'alpha': 0.1615, 'lambda': 25.97317325626955, 'max_bin': 306}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:48:44,899] Trial 85 finished with value: 0.6776235031843199 and parameters: {'n_estimators': 563, 'eta': 0.04266798209729211, 'max_depth': 11, 'alpha': 0.0697, 'lambda': 28.79620521089014, 'max_bin': 326}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:49:30,457] Trial 86 finished with value: 0.6784838027273314 and parameters: {'n_estimators': 886, 'eta': 0.034101376451836043, 'max_depth': 12, 'alpha': 0.4616, 'lambda': 23.543860910675395, 'max_bin': 379}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:49:58,869] Trial 87 finished with value: 0.6767703537883916 and parameters: {'n_estimators': 847, 'eta': 0.051364625525870906, 'max_depth': 10, 'alpha': 0.1247, 'lambda': 22.413731649119775, 'max_bin': 369}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:50:26,444] Trial 88 finished with value: 0.675349409688175 and parameters: {'n_estimators': 757, 'eta': 0.04690439302687382, 'max_depth': 9, 'alpha': 0.2076, 'lambda': 20.160534986702814, 'max_bin': 493}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:50:43,586] Trial 89 finished with value: 0.6520703694149763 and parameters: {'n_estimators': 532, 'eta': 0.03579065815789345, 'max_depth': 7, 'alpha': 0.2306, 'lambda': 25.095888602377617, 'max_bin': 466}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:51:15,794] Trial 90 finished with value: 0.6759207669718855 and parameters: {'n_estimators': 814, 'eta': 0.05997406740152691, 'max_depth': 11, 'alpha': 0.029900000000000003, 'lambda': 26.63568911075403, 'max_bin': 449}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:52:05,798] Trial 91 finished with value: 0.6722258629851632 and parameters: {'n_estimators': 859, 'eta': 0.018899078459604944, 'max_depth': 12, 'alpha': 0.28150000000000003, 'lambda': 27.7607822001079, 'max_bin': 429}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:52:48,060] Trial 92 finished with value: 0.6754367287034153 and parameters: {'n_estimators': 900, 'eta': 0.03162288690907332, 'max_depth': 12, 'alpha': 0.2862, 'lambda': 25.824239653429686, 'max_bin': 436}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:53:28,135] Trial 93 finished with value: 0.6774538077207579 and parameters: {'n_estimators': 784, 'eta': 0.037906355236058474, 'max_depth': 12, 'alpha': 0.2601, 'lambda': 29.612885313222915, 'max_bin': 484}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:54:17,355] Trial 94 finished with value: 0.6757910637101021 and parameters: {'n_estimators': 862, 'eta': 0.021845009435147614, 'max_depth': 12, 'alpha': 0.14800000000000002, 'lambda': 21.897634207751114, 'max_bin': 471}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:54:59,146] Trial 95 finished with value: 0.6737174581590715 and parameters: {'n_estimators': 830, 'eta': 0.027522999615855682, 'max_depth': 11, 'alpha': 0.18180000000000002, 'lambda': 24.323592813730517, 'max_bin': 413}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:55:36,169] Trial 96 finished with value: 0.6807069849374183 and parameters: {'n_estimators': 741, 'eta': 0.04088239870067957, 'max_depth': 11, 'alpha': 0.35150000000000003, 'lambda': 25.095260085072145, 'max_bin': 385}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:55:45,437] Trial 97 finished with value: 0.6541729901635003 and parameters: {'n_estimators': 177, 'eta': 0.054360537391294664, 'max_depth': 12, 'alpha': 0.2184, 'lambda': 20.974474666542843, 'max_bin': 460}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:56:15,720] Trial 98 finished with value: 0.6756282644730807 and parameters: {'n_estimators': 849, 'eta': 0.04828929900287478, 'max_depth': 10, 'alpha': 0.3111, 'lambda': 27.080792406657416, 'max_bin': 422}. Best is trial 40 with value: 0.6840639006627747.\n",
      "[I 2023-12-20 01:56:52,191] Trial 99 finished with value: 0.6737410586102586 and parameters: {'n_estimators': 807, 'eta': 0.044868868931081246, 'max_depth': 11, 'alpha': 0.112, 'lambda': 23.41301990953294, 'max_bin': 377}. Best is trial 40 with value: 0.6840639006627747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6841\n",
      "\tBest params:\n",
      "\t\tn_estimators: 863\n",
      "\t\teta: 0.04049106965479992\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.2132\n",
      "\t\tlambda: 23.71448227647675\n",
      "\t\tmax_bin: 441\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.696313    0.701416\n",
      "1                    TP  401.000000  403.000000\n",
      "2                    TN  350.000000  338.000000\n",
      "3                    FP   76.000000   84.000000\n",
      "4                    FN   72.000000   74.000000\n",
      "5              Accuracy    0.835373    0.824249\n",
      "6             Precision    0.840671    0.827515\n",
      "7           Sensitivity    0.847780    0.844864\n",
      "8           Specificity    0.821600    0.800900\n",
      "9              F1 score    0.844211    0.836100\n",
      "10  F1 score (weighted)    0.835331    0.824107\n",
      "11     F1 score (macro)    0.834841    0.823326\n",
      "12    Balanced Accuracy    0.834688    0.822906\n",
      "13                  MCC    0.669715    0.646857\n",
      "14                  NPV    0.829400    0.820400\n",
      "15              ROC_AUC    0.834688    0.822906\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_1_cat = np.where((y_pred_xgb_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 01:57:35,282] Trial 100 finished with value: 0.6855349795455682 and parameters: {'n_estimators': 885, 'eta': 0.03569250329675222, 'max_depth': 12, 'alpha': 0.9468000000000001, 'lambda': 22.45991768300684, 'max_bin': 393}. Best is trial 100 with value: 0.6855349795455682.\n",
      "[I 2023-12-20 01:58:20,584] Trial 101 finished with value: 0.6845586573462541 and parameters: {'n_estimators': 886, 'eta': 0.033710308869760015, 'max_depth': 12, 'alpha': 0.8669, 'lambda': 22.319655762587413, 'max_bin': 393}. Best is trial 100 with value: 0.6855349795455682.\n",
      "[I 2023-12-20 01:59:05,734] Trial 102 finished with value: 0.6827618595522782 and parameters: {'n_estimators': 885, 'eta': 0.033877525230696044, 'max_depth': 12, 'alpha': 0.6127, 'lambda': 22.25076630864379, 'max_bin': 395}. Best is trial 100 with value: 0.6855349795455682.\n",
      "[I 2023-12-20 01:59:49,278] Trial 103 finished with value: 0.6854285760407307 and parameters: {'n_estimators': 884, 'eta': 0.03384607387578621, 'max_depth': 12, 'alpha': 0.9401, 'lambda': 22.047955035779005, 'max_bin': 399}. Best is trial 100 with value: 0.6855349795455682.\n",
      "[I 2023-12-20 02:00:29,400] Trial 104 finished with value: 0.6845561357961885 and parameters: {'n_estimators': 892, 'eta': 0.034504213024996455, 'max_depth': 12, 'alpha': 0.9663, 'lambda': 22.02248910183947, 'max_bin': 392}. Best is trial 100 with value: 0.6855349795455682.\n",
      "[I 2023-12-20 02:01:11,383] Trial 105 finished with value: 0.6857877164945942 and parameters: {'n_estimators': 885, 'eta': 0.036087465994223855, 'max_depth': 12, 'alpha': 0.937, 'lambda': 20.744104034782247, 'max_bin': 389}. Best is trial 105 with value: 0.6857877164945942.\n",
      "[I 2023-12-20 02:01:55,862] Trial 106 finished with value: 0.6856471555605037 and parameters: {'n_estimators': 885, 'eta': 0.036761935498156074, 'max_depth': 12, 'alpha': 0.9587, 'lambda': 19.623130278454884, 'max_bin': 388}. Best is trial 105 with value: 0.6857877164945942.\n",
      "[I 2023-12-20 02:02:37,996] Trial 107 finished with value: 0.6870735490021334 and parameters: {'n_estimators': 886, 'eta': 0.03609118565096932, 'max_depth': 12, 'alpha': 0.9590000000000001, 'lambda': 19.094219659510067, 'max_bin': 391}. Best is trial 107 with value: 0.6870735490021334.\n",
      "[I 2023-12-20 02:03:19,060] Trial 108 finished with value: 0.6881605430568823 and parameters: {'n_estimators': 886, 'eta': 0.03673374881888599, 'max_depth': 12, 'alpha': 0.9622, 'lambda': 20.078442715825958, 'max_bin': 390}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:04:00,004] Trial 109 finished with value: 0.6878501249051462 and parameters: {'n_estimators': 885, 'eta': 0.03698040868245675, 'max_depth': 12, 'alpha': 0.9646, 'lambda': 19.235718558507134, 'max_bin': 391}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:04:39,634] Trial 110 finished with value: 0.6836116156894609 and parameters: {'n_estimators': 884, 'eta': 0.03890968812833466, 'max_depth': 12, 'alpha': 0.9654, 'lambda': 18.597821678286213, 'max_bin': 390}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:05:16,221] Trial 111 finished with value: 0.6849562549214367 and parameters: {'n_estimators': 886, 'eta': 0.039535388001525305, 'max_depth': 12, 'alpha': 0.9504, 'lambda': 18.846635718203125, 'max_bin': 391}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:05:58,089] Trial 112 finished with value: 0.6868404695519705 and parameters: {'n_estimators': 900, 'eta': 0.03633477988711579, 'max_depth': 12, 'alpha': 0.9218000000000001, 'lambda': 19.3096329765568, 'max_bin': 401}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:06:38,547] Trial 113 finished with value: 0.6859407501630569 and parameters: {'n_estimators': 893, 'eta': 0.03720631762805262, 'max_depth': 12, 'alpha': 0.9186000000000001, 'lambda': 19.599239022478947, 'max_bin': 401}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:07:19,418] Trial 114 finished with value: 0.685521770896378 and parameters: {'n_estimators': 896, 'eta': 0.03675596444843024, 'max_depth': 12, 'alpha': 0.9084000000000001, 'lambda': 19.310079446189775, 'max_bin': 403}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:07:59,328] Trial 115 finished with value: 0.6843984096772038 and parameters: {'n_estimators': 900, 'eta': 0.03725547745685877, 'max_depth': 12, 'alpha': 0.8987, 'lambda': 19.40660980382008, 'max_bin': 403}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:08:37,274] Trial 116 finished with value: 0.6852783408914471 and parameters: {'n_estimators': 873, 'eta': 0.039316253116032526, 'max_depth': 12, 'alpha': 0.926, 'lambda': 18.086156876679162, 'max_bin': 407}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:09:12,155] Trial 117 finished with value: 0.6870099691316855 and parameters: {'n_estimators': 868, 'eta': 0.04224486518182595, 'max_depth': 12, 'alpha': 0.9323, 'lambda': 17.770726206380537, 'max_bin': 408}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:09:51,625] Trial 118 finished with value: 0.684295253348107 and parameters: {'n_estimators': 868, 'eta': 0.04388910461374539, 'max_depth': 12, 'alpha': 0.9286000000000001, 'lambda': 17.643538835357127, 'max_bin': 406}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:10:28,939] Trial 119 finished with value: 0.6857506444409199 and parameters: {'n_estimators': 845, 'eta': 0.037633744011138254, 'max_depth': 12, 'alpha': 0.9985, 'lambda': 16.221269085042053, 'max_bin': 398}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:11:04,031] Trial 120 finished with value: 0.6847801127481501 and parameters: {'n_estimators': 850, 'eta': 0.041655304542703345, 'max_depth': 12, 'alpha': 0.9955, 'lambda': 17.06839538964656, 'max_bin': 399}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:11:45,298] Trial 121 finished with value: 0.6842235094578458 and parameters: {'n_estimators': 871, 'eta': 0.035894537189343656, 'max_depth': 12, 'alpha': 0.9029, 'lambda': 18.07816627263123, 'max_bin': 410}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:12:25,353] Trial 122 finished with value: 0.6863477417904322 and parameters: {'n_estimators': 840, 'eta': 0.03730326314623963, 'max_depth': 12, 'alpha': 0.8725, 'lambda': 16.52463564815698, 'max_bin': 400}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:13:05,977] Trial 123 finished with value: 0.6842930351981817 and parameters: {'n_estimators': 837, 'eta': 0.036897082361022196, 'max_depth': 12, 'alpha': 0.8735, 'lambda': 16.387498148648614, 'max_bin': 399}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:13:42,312] Trial 124 finished with value: 0.6823746012720119 and parameters: {'n_estimators': 842, 'eta': 0.04265843038723478, 'max_depth': 12, 'alpha': 0.9314, 'lambda': 19.40654244179678, 'max_bin': 419}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:14:20,650] Trial 125 finished with value: 0.6871736432706396 and parameters: {'n_estimators': 899, 'eta': 0.038056795143904185, 'max_depth': 12, 'alpha': 0.8238000000000001, 'lambda': 15.360416600476725, 'max_bin': 381}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:14:55,765] Trial 126 finished with value: 0.6838504198358184 and parameters: {'n_estimators': 899, 'eta': 0.041140400714744604, 'max_depth': 12, 'alpha': 0.7909, 'lambda': 15.889211833846208, 'max_bin': 373}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:15:16,379] Trial 127 finished with value: 0.6679849708540802 and parameters: {'n_estimators': 857, 'eta': 0.03689366581038036, 'max_depth': 6, 'alpha': 0.9767, 'lambda': 15.102506085815818, 'max_bin': 380}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:15:56,085] Trial 128 finished with value: 0.6860276524598932 and parameters: {'n_estimators': 867, 'eta': 0.03868553128465798, 'max_depth': 12, 'alpha': 0.8407, 'lambda': 16.87682376882276, 'max_bin': 385}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:16:27,611] Trial 129 finished with value: 0.6864809389044573 and parameters: {'n_estimators': 866, 'eta': 0.04413095137960157, 'max_depth': 12, 'alpha': 0.8502000000000001, 'lambda': 17.042121798843276, 'max_bin': 386}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:17:00,355] Trial 130 finished with value: 0.6860507575890562 and parameters: {'n_estimators': 831, 'eta': 0.04540906438023487, 'max_depth': 12, 'alpha': 0.8351000000000001, 'lambda': 16.74310250670466, 'max_bin': 388}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:17:32,406] Trial 131 finished with value: 0.6822509955349159 and parameters: {'n_estimators': 833, 'eta': 0.043695955641365754, 'max_depth': 12, 'alpha': 0.8306, 'lambda': 16.93578530279485, 'max_bin': 384}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:18:05,483] Trial 132 finished with value: 0.6798830409187264 and parameters: {'n_estimators': 863, 'eta': 0.04656554105065738, 'max_depth': 12, 'alpha': 0.8436, 'lambda': 14.736881922880931, 'max_bin': 387}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:18:44,028] Trial 133 finished with value: 0.6859557858003512 and parameters: {'n_estimators': 847, 'eta': 0.039955993870022875, 'max_depth': 12, 'alpha': 0.7828, 'lambda': 16.821914449693907, 'max_bin': 369}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:19:19,846] Trial 134 finished with value: 0.6867246948796962 and parameters: {'n_estimators': 847, 'eta': 0.04179928524586191, 'max_depth': 12, 'alpha': 0.7762, 'lambda': 16.590259151925043, 'max_bin': 371}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:19:54,507] Trial 135 finished with value: 0.6836296811859889 and parameters: {'n_estimators': 868, 'eta': 0.044768155355320034, 'max_depth': 12, 'alpha': 0.7732, 'lambda': 17.281022098259147, 'max_bin': 370}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:20:27,518] Trial 136 finished with value: 0.6850679196794124 and parameters: {'n_estimators': 820, 'eta': 0.04200164029322414, 'max_depth': 12, 'alpha': 0.885, 'lambda': 16.670677184122187, 'max_bin': 365}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:21:06,433] Trial 137 finished with value: 0.6846535561458248 and parameters: {'n_estimators': 851, 'eta': 0.03969222099489129, 'max_depth': 12, 'alpha': 0.8115, 'lambda': 17.967948393622972, 'max_bin': 376}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:21:35,019] Trial 138 finished with value: 0.6769140199282271 and parameters: {'n_estimators': 827, 'eta': 0.04866027928801815, 'max_depth': 12, 'alpha': 0.8485, 'lambda': 14.647817051278823, 'max_bin': 382}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:22:08,004] Trial 139 finished with value: 0.6854465100916535 and parameters: {'n_estimators': 867, 'eta': 0.043118458152920695, 'max_depth': 12, 'alpha': 0.7203, 'lambda': 17.960442428982137, 'max_bin': 412}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:22:41,985] Trial 140 finished with value: 0.6837924661171624 and parameters: {'n_estimators': 813, 'eta': 0.04509846317478311, 'max_depth': 12, 'alpha': 0.7595000000000001, 'lambda': 20.634345288814163, 'max_bin': 355}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:23:15,842] Trial 141 finished with value: 0.6851443258352289 and parameters: {'n_estimators': 841, 'eta': 0.04163162189061735, 'max_depth': 12, 'alpha': 0.8190000000000001, 'lambda': 16.342470922701697, 'max_bin': 376}. Best is trial 108 with value: 0.6881605430568823.\n",
      "[I 2023-12-20 02:23:51,932] Trial 142 finished with value: 0.6882679854207645 and parameters: {'n_estimators': 854, 'eta': 0.03904978341331239, 'max_depth': 12, 'alpha': 0.9987, 'lambda': 15.500449545987768, 'max_bin': 396}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:24:25,210] Trial 143 finished with value: 0.686615781982185 and parameters: {'n_estimators': 868, 'eta': 0.03895005034220579, 'max_depth': 12, 'alpha': 0.8862, 'lambda': 13.9081283954141, 'max_bin': 382}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:25:01,551] Trial 144 finished with value: 0.6853159019278383 and parameters: {'n_estimators': 857, 'eta': 0.03908256437988876, 'max_depth': 12, 'alpha': 0.8841, 'lambda': 14.206723495775913, 'max_bin': 367}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:25:38,670] Trial 145 finished with value: 0.6850108991673828 and parameters: {'n_estimators': 865, 'eta': 0.04065494333801506, 'max_depth': 12, 'alpha': 0.9172, 'lambda': 15.386633702945659, 'max_bin': 383}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:26:09,146] Trial 146 finished with value: 0.680804601429575 and parameters: {'n_estimators': 834, 'eta': 0.046521886585650526, 'max_depth': 12, 'alpha': 0.8527, 'lambda': 13.710517294391622, 'max_bin': 403}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:26:43,398] Trial 147 finished with value: 0.6838634743041359 and parameters: {'n_estimators': 799, 'eta': 0.03898855836430651, 'max_depth': 12, 'alpha': 0.7956000000000001, 'lambda': 15.629881232448799, 'max_bin': 395}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:27:26,234] Trial 148 finished with value: 0.6847074569905385 and parameters: {'n_estimators': 900, 'eta': 0.03182603006846682, 'max_depth': 12, 'alpha': 0.8917, 'lambda': 12.43403804432053, 'max_bin': 361}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:28:00,512] Trial 149 finished with value: 0.6869110207958664 and parameters: {'n_estimators': 875, 'eta': 0.04249583725480857, 'max_depth': 12, 'alpha': 0.8361000000000001, 'lambda': 18.654136714061803, 'max_bin': 372}. Best is trial 142 with value: 0.6882679854207645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6883\n",
      "\tBest params:\n",
      "\t\tn_estimators: 854\n",
      "\t\teta: 0.03904978341331239\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9987\n",
      "\t\tlambda: 15.500449545987768\n",
      "\t\tmax_bin: 396\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.696313    0.701416    0.713121\n",
      "1                    TP  401.000000  403.000000  373.000000\n",
      "2                    TN  350.000000  338.000000  372.000000\n",
      "3                    FP   76.000000   84.000000   80.000000\n",
      "4                    FN   72.000000   74.000000   74.000000\n",
      "5              Accuracy    0.835373    0.824249    0.828699\n",
      "6             Precision    0.840671    0.827515    0.823400\n",
      "7           Sensitivity    0.847780    0.844864    0.834452\n",
      "8           Specificity    0.821600    0.800900    0.823000\n",
      "9              F1 score    0.844211    0.836100    0.828889\n",
      "10  F1 score (weighted)    0.835331    0.824107    0.828697\n",
      "11     F1 score (macro)    0.834841    0.823326    0.828698\n",
      "12    Balanced Accuracy    0.834688    0.822906    0.828730\n",
      "13                  MCC    0.669715    0.646857    0.657471\n",
      "14                  NPV    0.829400    0.820400    0.834100\n",
      "15              ROC_AUC    0.834688    0.822906    0.828730\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_2_cat = np.where((y_pred_xgb_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 02:28:39,360] Trial 150 finished with value: 0.6814516435107054 and parameters: {'n_estimators': 872, 'eta': 0.043961001448516654, 'max_depth': 12, 'alpha': 0.8313, 'lambda': 17.105633495783476, 'max_bin': 379}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:29:12,507] Trial 151 finished with value: 0.6794873967210816 and parameters: {'n_estimators': 852, 'eta': 0.04198618637452359, 'max_depth': 12, 'alpha': 0.8654000000000001, 'lambda': 18.691019309881764, 'max_bin': 371}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:29:47,788] Trial 152 finished with value: 0.6794471505158292 and parameters: {'n_estimators': 875, 'eta': 0.03968023286574057, 'max_depth': 12, 'alpha': 0.9793000000000001, 'lambda': 17.661091810757792, 'max_bin': 386}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:30:18,786] Trial 153 finished with value: 0.6792961505428569 and parameters: {'n_estimators': 822, 'eta': 0.04936107447812272, 'max_depth': 12, 'alpha': 0.8746, 'lambda': 18.69419040172897, 'max_bin': 356}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:30:58,120] Trial 154 finished with value: 0.6782300889290751 and parameters: {'n_estimators': 900, 'eta': 0.03823250373067641, 'max_depth': 12, 'alpha': 0.7828, 'lambda': 16.234729157648392, 'max_bin': 374}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:31:32,544] Trial 155 finished with value: 0.680169572217881 and parameters: {'n_estimators': 846, 'eta': 0.045810491592539186, 'max_depth': 12, 'alpha': 0.9124, 'lambda': 17.03330465800901, 'max_bin': 395}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:32:06,690] Trial 156 finished with value: 0.6815739656144599 and parameters: {'n_estimators': 875, 'eta': 0.04350709519518163, 'max_depth': 12, 'alpha': 0.8312, 'lambda': 15.46048592746602, 'max_bin': 380}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:32:44,678] Trial 157 finished with value: 0.6776172217493686 and parameters: {'n_estimators': 860, 'eta': 0.0407063938064851, 'max_depth': 12, 'alpha': 0.7571, 'lambda': 20.008929435902598, 'max_bin': 389}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:33:25,344] Trial 158 finished with value: 0.6784493337574885 and parameters: {'n_estimators': 834, 'eta': 0.03254327535428494, 'max_depth': 12, 'alpha': 0.9537, 'lambda': 17.8398640619524, 'max_bin': 406}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:34:05,755] Trial 159 finished with value: 0.6784695237539806 and parameters: {'n_estimators': 809, 'eta': 0.0353017691896044, 'max_depth': 12, 'alpha': 0.8541000000000001, 'lambda': 18.816391713203508, 'max_bin': 384}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:34:44,424] Trial 160 finished with value: 0.6788730453785101 and parameters: {'n_estimators': 875, 'eta': 0.03837940302564936, 'max_depth': 12, 'alpha': 0.6897, 'lambda': 16.781258578649926, 'max_bin': 362}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:35:24,235] Trial 161 finished with value: 0.6789093224274227 and parameters: {'n_estimators': 881, 'eta': 0.035300001952649755, 'max_depth': 12, 'alpha': 0.9409000000000001, 'lambda': 20.58222879407251, 'max_bin': 391}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:36:00,421] Trial 162 finished with value: 0.6786083331865707 and parameters: {'n_estimators': 899, 'eta': 0.04089875558788133, 'max_depth': 12, 'alpha': 0.9841000000000001, 'lambda': 19.557684295861446, 'max_bin': 399}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:36:38,292] Trial 163 finished with value: 0.6759784664180735 and parameters: {'n_estimators': 854, 'eta': 0.037985664943895846, 'max_depth': 12, 'alpha': 0.9174, 'lambda': 18.4926743470192, 'max_bin': 389}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:37:10,547] Trial 164 finished with value: 0.6819538764597486 and parameters: {'n_estimators': 886, 'eta': 0.042699294664700134, 'max_depth': 12, 'alpha': 0.8018000000000001, 'lambda': 16.10314994602166, 'max_bin': 377}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:37:50,672] Trial 165 finished with value: 0.6792862383433557 and parameters: {'n_estimators': 842, 'eta': 0.03336034500207032, 'max_depth': 12, 'alpha': 0.9719000000000001, 'lambda': 20.31674340262111, 'max_bin': 412}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:38:31,963] Trial 166 finished with value: 0.6807115551558977 and parameters: {'n_estimators': 865, 'eta': 0.035906804811778346, 'max_depth': 12, 'alpha': 0.8959, 'lambda': 17.477805082473527, 'max_bin': 395}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:39:04,078] Trial 167 finished with value: 0.6821358396398789 and parameters: {'n_estimators': 881, 'eta': 0.046760548952753694, 'max_depth': 12, 'alpha': 0.9383, 'lambda': 14.969925817213232, 'max_bin': 404}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:39:41,977] Trial 168 finished with value: 0.6753835509872108 and parameters: {'n_estimators': 825, 'eta': 0.039866567341834365, 'max_depth': 12, 'alpha': 0.8737, 'lambda': 19.186987658509196, 'max_bin': 372}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:39:59,846] Trial 169 finished with value: 0.6732586032436643 and parameters: {'n_estimators': 334, 'eta': 0.03752310597855481, 'max_depth': 12, 'alpha': 0.8365, 'lambda': 12.94931808992354, 'max_bin': 384}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:40:37,536] Trial 170 finished with value: 0.6800674339403691 and parameters: {'n_estimators': 853, 'eta': 0.04478645088181552, 'max_depth': 12, 'alpha': 0.9067000000000001, 'lambda': 18.356285645155303, 'max_bin': 368}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:41:15,797] Trial 171 finished with value: 0.6784577808141534 and parameters: {'n_estimators': 840, 'eta': 0.03720354386639563, 'max_depth': 12, 'alpha': 0.9956, 'lambda': 16.224677332047513, 'max_bin': 398}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:41:49,152] Trial 172 finished with value: 0.6823336546130665 and parameters: {'n_estimators': 879, 'eta': 0.04187933161405279, 'max_depth': 12, 'alpha': 0.9573, 'lambda': 16.693108337447725, 'max_bin': 401}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:42:25,217] Trial 173 finished with value: 0.6797135777823364 and parameters: {'n_estimators': 853, 'eta': 0.03860357155853798, 'max_depth': 12, 'alpha': 0.9922000000000001, 'lambda': 15.656104504762343, 'max_bin': 388}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:43:03,579] Trial 174 finished with value: 0.6806780534656115 and parameters: {'n_estimators': 898, 'eta': 0.03529984020556326, 'max_depth': 12, 'alpha': 0.9981000000000001, 'lambda': 14.239912946378817, 'max_bin': 395}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:43:44,261] Trial 175 finished with value: 0.6799319567835469 and parameters: {'n_estimators': 870, 'eta': 0.03268222476836222, 'max_depth': 12, 'alpha': 0.9273, 'lambda': 17.35718299623657, 'max_bin': 381}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:44:27,536] Trial 176 finished with value: 0.6799238502762266 and parameters: {'n_estimators': 844, 'eta': 0.029920427729336407, 'max_depth': 12, 'alpha': 0.9704, 'lambda': 18.041323008693336, 'max_bin': 391}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:45:07,972] Trial 177 finished with value: 0.6777741540115534 and parameters: {'n_estimators': 816, 'eta': 0.040323471145491896, 'max_depth': 12, 'alpha': 0.8583000000000001, 'lambda': 21.086606888159828, 'max_bin': 411}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:45:43,393] Trial 178 finished with value: 0.6811069715009038 and parameters: {'n_estimators': 863, 'eta': 0.04326807721496961, 'max_depth': 12, 'alpha': 0.8166, 'lambda': 20.050373879041373, 'max_bin': 266}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:46:19,575] Trial 179 finished with value: 0.6796997813559886 and parameters: {'n_estimators': 884, 'eta': 0.03728334096848411, 'max_depth': 12, 'alpha': 0.9520000000000001, 'lambda': 16.055363058426856, 'max_bin': 398}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:46:56,928] Trial 180 finished with value: 0.6792998708437316 and parameters: {'n_estimators': 794, 'eta': 0.034213218526946275, 'max_depth': 12, 'alpha': 0.8841, 'lambda': 19.061836345410267, 'max_bin': 408}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:47:34,141] Trial 181 finished with value: 0.678465833339347 and parameters: {'n_estimators': 899, 'eta': 0.03655600747585393, 'max_depth': 12, 'alpha': 0.9632000000000001, 'lambda': 19.865790052996527, 'max_bin': 386}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:48:10,419] Trial 182 finished with value: 0.6794361022717033 and parameters: {'n_estimators': 878, 'eta': 0.03905773411937274, 'max_depth': 12, 'alpha': 0.9363, 'lambda': 14.959374865874263, 'max_bin': 389}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:48:47,318] Trial 183 finished with value: 0.6802096459090817 and parameters: {'n_estimators': 856, 'eta': 0.041236102052504206, 'max_depth': 12, 'alpha': 0.9203, 'lambda': 17.311589669828244, 'max_bin': 379}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:49:27,430] Trial 184 finished with value: 0.6803153517525982 and parameters: {'n_estimators': 835, 'eta': 0.03608870234860993, 'max_depth': 12, 'alpha': 0.9709000000000001, 'lambda': 19.5167199271846, 'max_bin': 393}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:50:07,419] Trial 185 finished with value: 0.6802145988463244 and parameters: {'n_estimators': 885, 'eta': 0.038655339973726624, 'max_depth': 12, 'alpha': 0.9012, 'lambda': 17.93388462009172, 'max_bin': 399}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:50:48,101] Trial 186 finished with value: 0.6812439310554355 and parameters: {'n_estimators': 869, 'eta': 0.03287125014872657, 'max_depth': 12, 'alpha': 0.9592, 'lambda': 16.565467877006913, 'max_bin': 375}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:50:59,363] Trial 187 finished with value: 0.6529537081946204 and parameters: {'n_estimators': 205, 'eta': 0.04208253424555438, 'max_depth': 12, 'alpha': 0.8470000000000001, 'lambda': 20.97300996621285, 'max_bin': 384}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:51:41,416] Trial 188 finished with value: 0.6803153786521292 and parameters: {'n_estimators': 899, 'eta': 0.03478171092886913, 'max_depth': 12, 'alpha': 0.8099000000000001, 'lambda': 18.52609037219268, 'max_bin': 388}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:52:05,437] Trial 189 finished with value: 0.673840680693683 and parameters: {'n_estimators': 425, 'eta': 0.030853032221186228, 'max_depth': 12, 'alpha': 0.9406, 'lambda': 19.17318443295029, 'max_bin': 418}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:52:42,443] Trial 190 finished with value: 0.678965966850184 and parameters: {'n_estimators': 848, 'eta': 0.03673744519990158, 'max_depth': 12, 'alpha': 0.8683000000000001, 'lambda': 13.5394200024511, 'max_bin': 402}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:53:23,086] Trial 191 finished with value: 0.6800369844305433 and parameters: {'n_estimators': 887, 'eta': 0.04031726876710505, 'max_depth': 12, 'alpha': 0.9491, 'lambda': 21.574384844487785, 'max_bin': 394}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:54:03,999] Trial 192 finished with value: 0.6799239617514259 and parameters: {'n_estimators': 869, 'eta': 0.035494537129892305, 'max_depth': 12, 'alpha': 0.9978, 'lambda': 15.45215306973201, 'max_bin': 394}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:54:40,610] Trial 193 finished with value: 0.6783260537670405 and parameters: {'n_estimators': 884, 'eta': 0.03767884368430918, 'max_depth': 12, 'alpha': 0.9218000000000001, 'lambda': 19.85170256426985, 'max_bin': 381}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:55:18,840] Trial 194 finished with value: 0.6795580231206888 and parameters: {'n_estimators': 862, 'eta': 0.04426030242973002, 'max_depth': 12, 'alpha': 0.9767, 'lambda': 17.003274996467965, 'max_bin': 406}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:55:58,032] Trial 195 finished with value: 0.6782046910438104 and parameters: {'n_estimators': 828, 'eta': 0.03956431779104298, 'max_depth': 12, 'alpha': 0.7764000000000001, 'lambda': 20.70689643125133, 'max_bin': 390}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:56:25,903] Trial 196 finished with value: 0.6791641783722024 and parameters: {'n_estimators': 882, 'eta': 0.035023699046819325, 'max_depth': 8, 'alpha': 0.8868, 'lambda': 14.296324046135833, 'max_bin': 369}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:57:04,566] Trial 197 finished with value: 0.6802555029947704 and parameters: {'n_estimators': 850, 'eta': 0.03792224561018441, 'max_depth': 12, 'alpha': 0.9021, 'lambda': 18.1020648228088, 'max_bin': 385}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:57:33,036] Trial 198 finished with value: 0.6799578677747682 and parameters: {'n_estimators': 866, 'eta': 0.04789579553388752, 'max_depth': 12, 'alpha': 0.9407000000000001, 'lambda': 16.007838913174194, 'max_bin': 375}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:58:16,426] Trial 199 finished with value: 0.6817385281814891 and parameters: {'n_estimators': 887, 'eta': 0.03235340073609628, 'max_depth': 12, 'alpha': 0.9487000000000001, 'lambda': 17.432698868082884, 'max_bin': 397}. Best is trial 142 with value: 0.6882679854207645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6883\n",
      "\tBest params:\n",
      "\t\tn_estimators: 854\n",
      "\t\teta: 0.03904978341331239\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9987\n",
      "\t\tlambda: 15.500449545987768\n",
      "\t\tmax_bin: 396\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.696313    0.701416    0.713121    0.730474\n",
      "1                    TP  401.000000  403.000000  373.000000  410.000000\n",
      "2                    TN  350.000000  338.000000  372.000000  349.000000\n",
      "3                    FP   76.000000   84.000000   80.000000   71.000000\n",
      "4                    FN   72.000000   74.000000   74.000000   69.000000\n",
      "5              Accuracy    0.835373    0.824249    0.828699    0.844271\n",
      "6             Precision    0.840671    0.827515    0.823400    0.852391\n",
      "7           Sensitivity    0.847780    0.844864    0.834452    0.855950\n",
      "8           Specificity    0.821600    0.800900    0.823000    0.831000\n",
      "9              F1 score    0.844211    0.836100    0.828889    0.854167\n",
      "10  F1 score (weighted)    0.835331    0.824107    0.828697    0.844248\n",
      "11     F1 score (macro)    0.834841    0.823326    0.828698    0.843551\n",
      "12    Balanced Accuracy    0.834688    0.822906    0.828730    0.843451\n",
      "13                  MCC    0.669715    0.646857    0.657471    0.687111\n",
      "14                  NPV    0.829400    0.820400    0.834100    0.834900\n",
      "15              ROC_AUC    0.834688    0.822906    0.828730    0.843451\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_3_cat = np.where((y_pred_xgb_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 02:59:01,351] Trial 200 finished with value: 0.6822061068548834 and parameters: {'n_estimators': 835, 'eta': 0.042631285168769545, 'max_depth': 12, 'alpha': 0.9772000000000001, 'lambda': 18.94932100863378, 'max_bin': 403}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 02:59:47,256] Trial 201 finished with value: 0.6833164248383599 and parameters: {'n_estimators': 892, 'eta': 0.037192197089989054, 'max_depth': 12, 'alpha': 0.9143, 'lambda': 19.808772209924356, 'max_bin': 401}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:00:29,101] Trial 202 finished with value: 0.682138920798429 and parameters: {'n_estimators': 899, 'eta': 0.03636865267419925, 'max_depth': 12, 'alpha': 0.8230000000000001, 'lambda': 19.21477460460253, 'max_bin': 392}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:01:09,819] Trial 203 finished with value: 0.6805074084679033 and parameters: {'n_estimators': 873, 'eta': 0.03981340727188514, 'max_depth': 12, 'alpha': 0.8402000000000001, 'lambda': 20.325568811458016, 'max_bin': 406}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:01:56,471] Trial 204 finished with value: 0.6821946269566789 and parameters: {'n_estimators': 856, 'eta': 0.03338991073972969, 'max_depth': 12, 'alpha': 0.8951, 'lambda': 16.535036366644977, 'max_bin': 387}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:02:35,828] Trial 205 finished with value: 0.68016702776586 and parameters: {'n_estimators': 876, 'eta': 0.04115347469032484, 'max_depth': 12, 'alpha': 0.9232, 'lambda': 18.358242135241746, 'max_bin': 381}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:03:15,944] Trial 206 finished with value: 0.6825181888826591 and parameters: {'n_estimators': 886, 'eta': 0.03829675226811471, 'max_depth': 12, 'alpha': 0.8658, 'lambda': 21.29163653818892, 'max_bin': 397}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:03:59,318] Trial 207 finished with value: 0.681242172921863 and parameters: {'n_estimators': 852, 'eta': 0.03505597722716978, 'max_depth': 12, 'alpha': 0.9567, 'lambda': 14.952100195726954, 'max_bin': 391}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:04:34,863] Trial 208 finished with value: 0.6812516601950133 and parameters: {'n_estimators': 899, 'eta': 0.04532839482259646, 'max_depth': 12, 'alpha': 0.9097000000000001, 'lambda': 17.61500305869933, 'max_bin': 413}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:05:21,668] Trial 209 finished with value: 0.6830089034555774 and parameters: {'n_estimators': 900, 'eta': 0.036227392380159366, 'max_depth': 12, 'alpha': 0.7994, 'lambda': 19.49856085129891, 'max_bin': 402}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:06:02,494] Trial 210 finished with value: 0.6798517263411166 and parameters: {'n_estimators': 866, 'eta': 0.03995171533475726, 'max_depth': 12, 'alpha': 0.9758, 'lambda': 16.62443289519847, 'max_bin': 377}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:06:37,393] Trial 211 finished with value: 0.6784712168635668 and parameters: {'n_estimators': 871, 'eta': 0.04319096289240037, 'max_depth': 12, 'alpha': 0.7539, 'lambda': 18.36449142359554, 'max_bin': 411}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:07:18,016] Trial 212 finished with value: 0.6784569115539318 and parameters: {'n_estimators': 846, 'eta': 0.04301472439523494, 'max_depth': 12, 'alpha': 0.7814, 'lambda': 17.6633091741843, 'max_bin': 415}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:07:57,327] Trial 213 finished with value: 0.6794604725331779 and parameters: {'n_estimators': 873, 'eta': 0.03816400955313959, 'max_depth': 12, 'alpha': 0.8529, 'lambda': 15.714616875415016, 'max_bin': 408}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:08:35,854] Trial 214 finished with value: 0.6796446001580956 and parameters: {'n_estimators': 826, 'eta': 0.041047118806719615, 'max_depth': 12, 'alpha': 0.9995, 'lambda': 18.933681709631724, 'max_bin': 396}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:09:14,303] Trial 215 finished with value: 0.6790091300988388 and parameters: {'n_estimators': 861, 'eta': 0.04509383324822682, 'max_depth': 12, 'alpha': 0.7006, 'lambda': 20.307587595745435, 'max_bin': 387}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:09:58,550] Trial 216 finished with value: 0.6833868768541379 and parameters: {'n_estimators': 883, 'eta': 0.03439772675157553, 'max_depth': 12, 'alpha': 0.9364, 'lambda': 17.18759393022625, 'max_bin': 382}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:10:19,977] Trial 217 finished with value: 0.6660932948191649 and parameters: {'n_estimators': 844, 'eta': 0.03889127041315053, 'max_depth': 6, 'alpha': 0.7419, 'lambda': 18.214969439837898, 'max_bin': 402}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:11:00,317] Trial 218 finished with value: 0.6801707731550222 and parameters: {'n_estimators': 863, 'eta': 0.04206215801533022, 'max_depth': 12, 'alpha': 0.7366, 'lambda': 16.493101754414617, 'max_bin': 366}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:11:44,258] Trial 219 finished with value: 0.6815540702470592 and parameters: {'n_estimators': 879, 'eta': 0.03685865609931548, 'max_depth': 12, 'alpha': 0.8748, 'lambda': 15.602855591468908, 'max_bin': 392}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:12:22,229] Trial 220 finished with value: 0.6804519733028023 and parameters: {'n_estimators': 900, 'eta': 0.04017911161483642, 'max_depth': 12, 'alpha': 0.8305, 'lambda': 12.388604971276479, 'max_bin': 396}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:13:05,648] Trial 221 finished with value: 0.683389738527399 and parameters: {'n_estimators': 884, 'eta': 0.03348677220566085, 'max_depth': 12, 'alpha': 0.9568000000000001, 'lambda': 20.703814815540355, 'max_bin': 399}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:13:50,443] Trial 222 finished with value: 0.6848241843233178 and parameters: {'n_estimators': 861, 'eta': 0.031169331453525295, 'max_depth': 12, 'alpha': 0.929, 'lambda': 22.043478608266668, 'max_bin': 409}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:14:11,096] Trial 223 finished with value: 0.6763345717267973 and parameters: {'n_estimators': 886, 'eta': 0.09814757428941412, 'max_depth': 12, 'alpha': 0.9408000000000001, 'lambda': 21.54163016020907, 'max_bin': 389}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:14:53,484] Trial 224 finished with value: 0.6805264140954012 and parameters: {'n_estimators': 839, 'eta': 0.035574944630589994, 'max_depth': 12, 'alpha': 0.9772000000000001, 'lambda': 19.24793256428584, 'max_bin': 404}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:15:40,274] Trial 225 finished with value: 0.6803492885324095 and parameters: {'n_estimators': 873, 'eta': 0.03745558924410925, 'max_depth': 12, 'alpha': 0.8926000000000001, 'lambda': 22.63046559832601, 'max_bin': 385}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:16:22,196] Trial 226 finished with value: 0.6823035726616274 and parameters: {'n_estimators': 812, 'eta': 0.03410760660742292, 'max_depth': 12, 'alpha': 0.9126000000000001, 'lambda': 17.73280369245026, 'max_bin': 372}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:16:57,324] Trial 227 finished with value: 0.6834130085410222 and parameters: {'n_estimators': 855, 'eta': 0.05131461536100626, 'max_depth': 12, 'alpha': 0.8099000000000001, 'lambda': 19.89557752907984, 'max_bin': 420}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:17:20,207] Trial 228 finished with value: 0.6857914207503997 and parameters: {'n_estimators': 879, 'eta': 0.046259635233164266, 'max_depth': 7, 'alpha': 0.9450000000000001, 'lambda': 3.4113469096409936, 'max_bin': 393}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:17:46,870] Trial 229 finished with value: 0.6791923279107961 and parameters: {'n_estimators': 900, 'eta': 0.04754654355055569, 'max_depth': 12, 'alpha': 0.9628000000000001, 'lambda': 1.9646605866357252, 'max_bin': 392}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:18:16,825] Trial 230 finished with value: 0.6764636499914922 and parameters: {'n_estimators': 839, 'eta': 0.04554722763007935, 'max_depth': 12, 'alpha': 0.8531000000000001, 'lambda': 4.233651311839663, 'max_bin': 378}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:18:19,006] Trial 231 finished with value: 0.0561680934687467 and parameters: {'n_estimators': 54, 'eta': 0.04252864435423804, 'max_depth': 12, 'alpha': 0.9421, 'lambda': 11.894371747426561, 'max_bin': 397}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:18:54,796] Trial 232 finished with value: 0.6796511469569089 and parameters: {'n_estimators': 881, 'eta': 0.0388704015340837, 'max_depth': 12, 'alpha': 0.9223, 'lambda': 7.717090968446517, 'max_bin': 401}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:19:22,221] Trial 233 finished with value: 0.6818914537706628 and parameters: {'n_estimators': 863, 'eta': 0.044300061000524346, 'max_depth': 8, 'alpha': 0.7179, 'lambda': 16.218296419989475, 'max_bin': 392}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:19:47,471] Trial 234 finished with value: 0.6799087469182424 and parameters: {'n_estimators': 883, 'eta': 0.0495320901773585, 'max_depth': 7, 'alpha': 0.6686000000000001, 'lambda': 9.714712352386636, 'max_bin': 385}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:20:27,587] Trial 235 finished with value: 0.6803671009315859 and parameters: {'n_estimators': 871, 'eta': 0.03631838418285839, 'max_depth': 12, 'alpha': 0.9843000000000001, 'lambda': 14.694797405513423, 'max_bin': 397}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:21:06,754] Trial 236 finished with value: 0.6786443849291598 and parameters: {'n_estimators': 854, 'eta': 0.04079212737937251, 'max_depth': 12, 'alpha': 0.9515, 'lambda': 18.63142333431456, 'max_bin': 406}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:21:49,666] Trial 237 finished with value: 0.679897382156447 and parameters: {'n_estimators': 885, 'eta': 0.03850755649488033, 'max_depth': 12, 'alpha': 0.9077000000000001, 'lambda': 17.002643047461206, 'max_bin': 388}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:22:05,079] Trial 238 finished with value: 0.676761617082328 and parameters: {'n_estimators': 900, 'eta': 0.08047555687123635, 'max_depth': 5, 'alpha': 0.8821, 'lambda': 13.897748522613046, 'max_bin': 380}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:22:47,828] Trial 239 finished with value: 0.6840039228023777 and parameters: {'n_estimators': 869, 'eta': 0.03262757733079712, 'max_depth': 12, 'alpha': 0.9597, 'lambda': 20.87786213855473, 'max_bin': 394}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:23:08,704] Trial 240 finished with value: 0.6652842834005506 and parameters: {'n_estimators': 829, 'eta': 0.036117919692968965, 'max_depth': 6, 'alpha': 0.9349000000000001, 'lambda': 15.51277109508578, 'max_bin': 402}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:23:13,208] Trial 241 finished with value: 0.49666116796959203 and parameters: {'n_estimators': 87, 'eta': 0.03989693457640044, 'max_depth': 12, 'alpha': 0.8757, 'lambda': 14.398313132856446, 'max_bin': 361}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:23:55,245] Trial 242 finished with value: 0.6812676440472206 and parameters: {'n_estimators': 852, 'eta': 0.03814238797712439, 'max_depth': 12, 'alpha': 0.8357, 'lambda': 22.9029389514213, 'max_bin': 365}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:24:27,629] Trial 243 finished with value: 0.6810697289895993 and parameters: {'n_estimators': 864, 'eta': 0.04318564120308156, 'max_depth': 12, 'alpha': 0.8989, 'lambda': 13.063902090185355, 'max_bin': 370}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:25:08,474] Trial 244 finished with value: 0.6812428742803427 and parameters: {'n_estimators': 886, 'eta': 0.0464965194278441, 'max_depth': 12, 'alpha': 0.8829, 'lambda': 19.544210825892655, 'max_bin': 374}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:25:32,298] Trial 245 finished with value: 0.6716272819730169 and parameters: {'n_estimators': 848, 'eta': 0.03458074898300367, 'max_depth': 7, 'alpha': 0.9791000000000001, 'lambda': 13.940868010689892, 'max_bin': 383}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:25:40,002] Trial 246 finished with value: -6.826580358950404 and parameters: {'n_estimators': 873, 'eta': 0.0008481761153424083, 'max_depth': 12, 'alpha': 0.8550000000000001, 'lambda': 15.21724789047319, 'max_bin': 390}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:26:19,793] Trial 247 finished with value: 0.6821272882961337 and parameters: {'n_estimators': 885, 'eta': 0.04095805360825674, 'max_depth': 12, 'alpha': 0.9991000000000001, 'lambda': 17.911562061889576, 'max_bin': 397}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:26:53,601] Trial 248 finished with value: 0.6785765822742957 and parameters: {'n_estimators': 858, 'eta': 0.03752281878636831, 'max_depth': 12, 'alpha': 0.8218000000000001, 'lambda': 6.8651365338906585, 'max_bin': 413}. Best is trial 142 with value: 0.6882679854207645.\n",
      "[I 2023-12-20 03:27:29,528] Trial 249 finished with value: 0.6795471392550885 and parameters: {'n_estimators': 833, 'eta': 0.03921386994495687, 'max_depth': 12, 'alpha': 0.9184, 'lambda': 11.48861492891959, 'max_bin': 354}. Best is trial 142 with value: 0.6882679854207645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6883\n",
      "\tBest params:\n",
      "\t\tn_estimators: 854\n",
      "\t\teta: 0.03904978341331239\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9987\n",
      "\t\tlambda: 15.500449545987768\n",
      "\t\tmax_bin: 396\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.696313    0.701416    0.713121    0.730474   \n",
      "1                    TP  401.000000  403.000000  373.000000  410.000000   \n",
      "2                    TN  350.000000  338.000000  372.000000  349.000000   \n",
      "3                    FP   76.000000   84.000000   80.000000   71.000000   \n",
      "4                    FN   72.000000   74.000000   74.000000   69.000000   \n",
      "5              Accuracy    0.835373    0.824249    0.828699    0.844271   \n",
      "6             Precision    0.840671    0.827515    0.823400    0.852391   \n",
      "7           Sensitivity    0.847780    0.844864    0.834452    0.855950   \n",
      "8           Specificity    0.821600    0.800900    0.823000    0.831000   \n",
      "9              F1 score    0.844211    0.836100    0.828889    0.854167   \n",
      "10  F1 score (weighted)    0.835331    0.824107    0.828697    0.844248   \n",
      "11     F1 score (macro)    0.834841    0.823326    0.828698    0.843551   \n",
      "12    Balanced Accuracy    0.834688    0.822906    0.828730    0.843451   \n",
      "13                  MCC    0.669715    0.646857    0.657471    0.687111   \n",
      "14                  NPV    0.829400    0.820400    0.834100    0.834900   \n",
      "15              ROC_AUC    0.834688    0.822906    0.828730    0.843451   \n",
      "\n",
      "          Set4  \n",
      "0     0.712344  \n",
      "1   411.000000  \n",
      "2   344.000000  \n",
      "3    79.000000  \n",
      "4    65.000000  \n",
      "5     0.839822  \n",
      "6     0.838776  \n",
      "7     0.863445  \n",
      "8     0.813200  \n",
      "9     0.850932  \n",
      "10    0.839635  \n",
      "11    0.838927  \n",
      "12    0.838342  \n",
      "13    0.678266  \n",
      "14    0.841100  \n",
      "15    0.838342  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_4_cat = np.where((y_pred_xgb_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c6c1fb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_xgb_4_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 03:28:15,654] Trial 250 finished with value: 0.7035474082433079 and parameters: {'n_estimators': 871, 'eta': 0.04227685651475006, 'max_depth': 12, 'alpha': 0.9428000000000001, 'lambda': 17.08604578987236, 'max_bin': 378}. Best is trial 250 with value: 0.7035474082433079.\n",
      "[I 2023-12-20 03:28:58,579] Trial 251 finished with value: 0.7005489317141368 and parameters: {'n_estimators': 885, 'eta': 0.04401005123937145, 'max_depth': 12, 'alpha': 0.9576, 'lambda': 16.1749063196018, 'max_bin': 378}. Best is trial 250 with value: 0.7035474082433079.\n",
      "[I 2023-12-20 03:29:37,639] Trial 252 finished with value: 0.7011520100172944 and parameters: {'n_estimators': 900, 'eta': 0.044738222517564305, 'max_depth': 12, 'alpha': 0.9656, 'lambda': 16.94570192661109, 'max_bin': 378}. Best is trial 250 with value: 0.7035474082433079.\n",
      "[I 2023-12-20 03:30:18,756] Trial 253 finished with value: 0.7022190863803869 and parameters: {'n_estimators': 898, 'eta': 0.04690023212437378, 'max_depth': 12, 'alpha': 0.9533, 'lambda': 16.73918581353324, 'max_bin': 375}. Best is trial 250 with value: 0.7035474082433079.\n",
      "[I 2023-12-20 03:30:56,621] Trial 254 finished with value: 0.6995380297115247 and parameters: {'n_estimators': 899, 'eta': 0.048148081423655495, 'max_depth': 12, 'alpha': 0.9629000000000001, 'lambda': 16.65523137688786, 'max_bin': 376}. Best is trial 250 with value: 0.7035474082433079.\n",
      "[I 2023-12-20 03:31:30,592] Trial 255 finished with value: 0.7028124107976419 and parameters: {'n_estimators': 895, 'eta': 0.04910739854898709, 'max_depth': 12, 'alpha': 0.9661000000000001, 'lambda': 16.45913237193237, 'max_bin': 376}. Best is trial 250 with value: 0.7035474082433079.\n",
      "[I 2023-12-20 03:32:06,042] Trial 256 finished with value: 0.700498116911609 and parameters: {'n_estimators': 893, 'eta': 0.04945873527245936, 'max_depth': 12, 'alpha': 0.9720000000000001, 'lambda': 16.31797113693946, 'max_bin': 372}. Best is trial 250 with value: 0.7035474082433079.\n",
      "[I 2023-12-20 03:32:39,701] Trial 257 finished with value: 0.7009913430552999 and parameters: {'n_estimators': 895, 'eta': 0.049886404499440616, 'max_depth': 12, 'alpha': 0.9766, 'lambda': 16.95351813830112, 'max_bin': 373}. Best is trial 250 with value: 0.7035474082433079.\n",
      "[I 2023-12-20 03:33:13,651] Trial 258 finished with value: 0.7037704215076103 and parameters: {'n_estimators': 900, 'eta': 0.05000155475286823, 'max_depth': 9, 'alpha': 0.9705, 'lambda': 16.661726427048915, 'max_bin': 372}. Best is trial 258 with value: 0.7037704215076103.\n",
      "[I 2023-12-20 03:33:49,606] Trial 259 finished with value: 0.7035775423370432 and parameters: {'n_estimators': 897, 'eta': 0.05207131930408975, 'max_depth': 12, 'alpha': 0.9713, 'lambda': 16.79287934606781, 'max_bin': 372}. Best is trial 258 with value: 0.7037704215076103.\n",
      "[I 2023-12-20 03:34:24,515] Trial 260 finished with value: 0.7028151031130269 and parameters: {'n_estimators': 898, 'eta': 0.050243957986117874, 'max_depth': 9, 'alpha': 0.9674, 'lambda': 16.76409636402189, 'max_bin': 371}. Best is trial 258 with value: 0.7037704215076103.\n",
      "[I 2023-12-20 03:34:58,491] Trial 261 finished with value: 0.7053207988571539 and parameters: {'n_estimators': 899, 'eta': 0.051150950937353966, 'max_depth': 9, 'alpha': 0.9747, 'lambda': 16.330692005741184, 'max_bin': 374}. Best is trial 261 with value: 0.7053207988571539.\n",
      "[I 2023-12-20 03:35:30,748] Trial 262 finished with value: 0.7035323510918564 and parameters: {'n_estimators': 900, 'eta': 0.05212627956770395, 'max_depth': 8, 'alpha': 0.9803000000000001, 'lambda': 16.19916886967881, 'max_bin': 373}. Best is trial 261 with value: 0.7053207988571539.\n",
      "[I 2023-12-20 03:36:04,119] Trial 263 finished with value: 0.7085707603331093 and parameters: {'n_estimators': 900, 'eta': 0.052282964474475044, 'max_depth': 9, 'alpha': 0.9758, 'lambda': 15.852769374672869, 'max_bin': 372}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:36:36,974] Trial 264 finished with value: 0.7058428984226504 and parameters: {'n_estimators': 899, 'eta': 0.05168000644446776, 'max_depth': 9, 'alpha': 0.9762000000000001, 'lambda': 16.012948981133245, 'max_bin': 371}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:37:09,113] Trial 265 finished with value: 0.7067886596324396 and parameters: {'n_estimators': 896, 'eta': 0.0528882418833847, 'max_depth': 9, 'alpha': 0.9787, 'lambda': 16.03967481718064, 'max_bin': 371}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:37:45,265] Trial 266 finished with value: 0.7073321946055889 and parameters: {'n_estimators': 897, 'eta': 0.053017042219635484, 'max_depth': 9, 'alpha': 0.9813000000000001, 'lambda': 16.0405285043122, 'max_bin': 369}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:38:19,067] Trial 267 finished with value: 0.7052778563678883 and parameters: {'n_estimators': 896, 'eta': 0.05329919083563928, 'max_depth': 9, 'alpha': 0.9779, 'lambda': 15.819102969378154, 'max_bin': 366}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:38:52,653] Trial 268 finished with value: 0.7053486507019258 and parameters: {'n_estimators': 898, 'eta': 0.05258958416028072, 'max_depth': 9, 'alpha': 0.9779, 'lambda': 15.79333243606299, 'max_bin': 366}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:39:25,499] Trial 269 finished with value: 0.7054207297414276 and parameters: {'n_estimators': 899, 'eta': 0.05253515535108214, 'max_depth': 9, 'alpha': 0.9816, 'lambda': 15.798860237718133, 'max_bin': 362}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:39:57,401] Trial 270 finished with value: 0.7044046387653597 and parameters: {'n_estimators': 900, 'eta': 0.052656071345438524, 'max_depth': 9, 'alpha': 0.9796, 'lambda': 15.656739226869744, 'max_bin': 360}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:40:32,996] Trial 271 finished with value: 0.704540063436119 and parameters: {'n_estimators': 900, 'eta': 0.053306497425061716, 'max_depth': 9, 'alpha': 0.9815, 'lambda': 15.916839916126012, 'max_bin': 359}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:41:06,429] Trial 272 finished with value: 0.7059051872271842 and parameters: {'n_estimators': 899, 'eta': 0.05401683247025714, 'max_depth': 9, 'alpha': 0.9832000000000001, 'lambda': 15.6124870176888, 'max_bin': 359}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:41:40,115] Trial 273 finished with value: 0.7065233461587385 and parameters: {'n_estimators': 898, 'eta': 0.05253316260759288, 'max_depth': 9, 'alpha': 0.9823000000000001, 'lambda': 15.911759616268585, 'max_bin': 348}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:42:12,882] Trial 274 finished with value: 0.7067678342701796 and parameters: {'n_estimators': 900, 'eta': 0.05271184487599227, 'max_depth': 9, 'alpha': 0.9812000000000001, 'lambda': 15.474635726374443, 'max_bin': 348}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:42:45,866] Trial 275 finished with value: 0.7034038508760085 and parameters: {'n_estimators': 900, 'eta': 0.05304109437003656, 'max_depth': 9, 'alpha': 0.9820000000000001, 'lambda': 15.799360259548259, 'max_bin': 357}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:43:19,218] Trial 276 finished with value: 0.7049039422620581 and parameters: {'n_estimators': 900, 'eta': 0.05307719801436019, 'max_depth': 9, 'alpha': 0.9858, 'lambda': 15.712151944361544, 'max_bin': 343}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:43:55,044] Trial 277 finished with value: 0.7046276867227446 and parameters: {'n_estimators': 899, 'eta': 0.053296587216436535, 'max_depth': 9, 'alpha': 0.9860000000000001, 'lambda': 14.976403418492358, 'max_bin': 343}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:44:28,232] Trial 278 finished with value: 0.7057369329988866 and parameters: {'n_estimators': 900, 'eta': 0.05338636304192077, 'max_depth': 9, 'alpha': 0.9850000000000001, 'lambda': 15.186580565847143, 'max_bin': 344}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:45:00,776] Trial 279 finished with value: 0.7063073296908571 and parameters: {'n_estimators': 899, 'eta': 0.053558244097246605, 'max_depth': 9, 'alpha': 0.9874, 'lambda': 14.730354943514463, 'max_bin': 336}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:45:30,949] Trial 280 finished with value: 0.7048108255145309 and parameters: {'n_estimators': 900, 'eta': 0.0535512319971725, 'max_depth': 9, 'alpha': 0.9885, 'lambda': 14.926728978606677, 'max_bin': 342}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:46:05,413] Trial 281 finished with value: 0.7046145056865714 and parameters: {'n_estimators': 899, 'eta': 0.05310032339974135, 'max_depth': 9, 'alpha': 0.9988, 'lambda': 14.827118530789074, 'max_bin': 338}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:46:34,916] Trial 282 finished with value: 0.7070266706716833 and parameters: {'n_estimators': 900, 'eta': 0.05404015993589886, 'max_depth': 9, 'alpha': 0.9899, 'lambda': 15.069213898424895, 'max_bin': 343}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:47:08,799] Trial 283 finished with value: 0.7056478715640788 and parameters: {'n_estimators': 899, 'eta': 0.053379410275541994, 'max_depth': 9, 'alpha': 0.9905, 'lambda': 14.818577982754785, 'max_bin': 343}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:47:42,197] Trial 284 finished with value: 0.7025157703153201 and parameters: {'n_estimators': 900, 'eta': 0.05426978763027484, 'max_depth': 9, 'alpha': 0.9962000000000001, 'lambda': 14.765752059533026, 'max_bin': 343}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:48:14,527] Trial 285 finished with value: 0.7056173941867593 and parameters: {'n_estimators': 897, 'eta': 0.05576094236773923, 'max_depth': 9, 'alpha': 0.9982000000000001, 'lambda': 14.793267075405625, 'max_bin': 333}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:48:45,683] Trial 286 finished with value: 0.7062454346604351 and parameters: {'n_estimators': 900, 'eta': 0.055957292018465915, 'max_depth': 9, 'alpha': 0.9978, 'lambda': 15.038161978536738, 'max_bin': 336}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:49:17,785] Trial 287 finished with value: 0.7067415041280052 and parameters: {'n_estimators': 900, 'eta': 0.05554986706025378, 'max_depth': 9, 'alpha': 0.9994000000000001, 'lambda': 14.860150802723235, 'max_bin': 333}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:49:48,418] Trial 288 finished with value: 0.7054793647009632 and parameters: {'n_estimators': 900, 'eta': 0.05539046409123967, 'max_depth': 9, 'alpha': 0.9875, 'lambda': 14.640666483907674, 'max_bin': 335}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:50:18,935] Trial 289 finished with value: 0.7073521816044688 and parameters: {'n_estimators': 885, 'eta': 0.056085143324197784, 'max_depth': 9, 'alpha': 0.9987, 'lambda': 14.586856687373553, 'max_bin': 335}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:50:49,979] Trial 290 finished with value: 0.7030048363629887 and parameters: {'n_estimators': 881, 'eta': 0.056070370941335414, 'max_depth': 9, 'alpha': 0.9996, 'lambda': 14.903291465434988, 'max_bin': 334}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:51:22,169] Trial 291 finished with value: 0.7047583871381159 and parameters: {'n_estimators': 882, 'eta': 0.0549168517944627, 'max_depth': 9, 'alpha': 0.9924000000000001, 'lambda': 14.342324056481612, 'max_bin': 341}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:51:54,851] Trial 292 finished with value: 0.7012699201487524 and parameters: {'n_estimators': 882, 'eta': 0.055319808167614164, 'max_depth': 9, 'alpha': 0.996, 'lambda': 14.456572166562468, 'max_bin': 342}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:52:25,078] Trial 293 finished with value: 0.703606949474786 and parameters: {'n_estimators': 880, 'eta': 0.05705249735377032, 'max_depth': 9, 'alpha': 0.9911000000000001, 'lambda': 13.442882832901, 'max_bin': 328}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:52:59,097] Trial 294 finished with value: 0.7050850802441893 and parameters: {'n_estimators': 882, 'eta': 0.055226529064320566, 'max_depth': 9, 'alpha': 0.9962000000000001, 'lambda': 14.234014510537381, 'max_bin': 347}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:53:32,556] Trial 295 finished with value: 0.7030935229343005 and parameters: {'n_estimators': 881, 'eta': 0.05521740929710764, 'max_depth': 9, 'alpha': 0.9999, 'lambda': 13.96418112534204, 'max_bin': 347}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:54:03,389] Trial 296 finished with value: 0.7049920295185573 and parameters: {'n_estimators': 882, 'eta': 0.05771166100733841, 'max_depth': 9, 'alpha': 0.9991000000000001, 'lambda': 14.586261094197907, 'max_bin': 347}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:54:35,856] Trial 297 finished with value: 0.7036271560642758 and parameters: {'n_estimators': 880, 'eta': 0.058133684452256335, 'max_depth': 9, 'alpha': 0.9997, 'lambda': 14.081024642308368, 'max_bin': 330}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:55:05,564] Trial 298 finished with value: 0.7079795347811481 and parameters: {'n_estimators': 876, 'eta': 0.057615591740704776, 'max_depth': 9, 'alpha': 0.9801000000000001, 'lambda': 13.240464467680285, 'max_bin': 320}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:55:36,378] Trial 299 finished with value: 0.7054273070890634 and parameters: {'n_estimators': 881, 'eta': 0.05767789515623309, 'max_depth': 9, 'alpha': 0.9787, 'lambda': 13.287353896794567, 'max_bin': 349}. Best is trial 263 with value: 0.7085707603331093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7086\n",
      "\tBest params:\n",
      "\t\tn_estimators: 900\n",
      "\t\teta: 0.052282964474475044\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.9758\n",
      "\t\tlambda: 15.852769374672869\n",
      "\t\tmax_bin: 372\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.696313    0.701416    0.713121    0.730474   \n",
      "1                    TP  401.000000  403.000000  373.000000  410.000000   \n",
      "2                    TN  350.000000  338.000000  372.000000  349.000000   \n",
      "3                    FP   76.000000   84.000000   80.000000   71.000000   \n",
      "4                    FN   72.000000   74.000000   74.000000   69.000000   \n",
      "5              Accuracy    0.835373    0.824249    0.828699    0.844271   \n",
      "6             Precision    0.840671    0.827515    0.823400    0.852391   \n",
      "7           Sensitivity    0.847780    0.844864    0.834452    0.855950   \n",
      "8           Specificity    0.821600    0.800900    0.823000    0.831000   \n",
      "9              F1 score    0.844211    0.836100    0.828889    0.854167   \n",
      "10  F1 score (weighted)    0.835331    0.824107    0.828697    0.844248   \n",
      "11     F1 score (macro)    0.834841    0.823326    0.828698    0.843551   \n",
      "12    Balanced Accuracy    0.834688    0.822906    0.828730    0.843451   \n",
      "13                  MCC    0.669715    0.646857    0.657471    0.687111   \n",
      "14                  NPV    0.829400    0.820400    0.834100    0.834900   \n",
      "15              ROC_AUC    0.834688    0.822906    0.828730    0.843451   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.712344    0.692140  \n",
      "1   411.000000  388.000000  \n",
      "2   344.000000  366.000000  \n",
      "3    79.000000   72.000000  \n",
      "4    65.000000   73.000000  \n",
      "5     0.839822    0.838710  \n",
      "6     0.838776    0.843478  \n",
      "7     0.863445    0.841649  \n",
      "8     0.813200    0.835600  \n",
      "9     0.850932    0.842562  \n",
      "10    0.839635    0.838714  \n",
      "11    0.838927    0.838613  \n",
      "12    0.838342    0.838633  \n",
      "13    0.678266    0.677228  \n",
      "14    0.841100    0.833700  \n",
      "15    0.838342    0.838633  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "\n",
    "y_pred_xgb_5_cat = np.where((y_pred_xgb_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 03:56:09,888] Trial 300 finished with value: 0.6964157803650338 and parameters: {'n_estimators': 880, 'eta': 0.05849074938569992, 'max_depth': 9, 'alpha': 0.9799, 'lambda': 13.49495924081242, 'max_bin': 320}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:56:37,710] Trial 301 finished with value: 0.6946243805373635 and parameters: {'n_estimators': 879, 'eta': 0.057358636779914596, 'max_depth': 9, 'alpha': 0.9816, 'lambda': 13.263879357366129, 'max_bin': 349}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:57:06,640] Trial 302 finished with value: 0.6978761679954149 and parameters: {'n_estimators': 881, 'eta': 0.0562587841800666, 'max_depth': 9, 'alpha': 0.9997, 'lambda': 12.926803030222324, 'max_bin': 337}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:57:40,275] Trial 303 finished with value: 0.6936909548698028 and parameters: {'n_estimators': 875, 'eta': 0.055131383979434755, 'max_depth': 9, 'alpha': 0.9770000000000001, 'lambda': 14.320958866208557, 'max_bin': 351}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:58:05,677] Trial 304 finished with value: 0.6946526899003105 and parameters: {'n_estimators': 884, 'eta': 0.059857590466233936, 'max_depth': 9, 'alpha': 0.9998, 'lambda': 13.644316886114849, 'max_bin': 335}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:58:25,169] Trial 305 finished with value: 0.6897251698417035 and parameters: {'n_estimators': 488, 'eta': 0.05637214645640771, 'max_depth': 9, 'alpha': 0.9717, 'lambda': 15.239352136214526, 'max_bin': 319}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:58:56,403] Trial 306 finished with value: 0.6932809213776006 and parameters: {'n_estimators': 873, 'eta': 0.05163681098516315, 'max_depth': 9, 'alpha': 0.9666, 'lambda': 15.140484116326629, 'max_bin': 346}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:59:22,317] Trial 307 finished with value: 0.6931846463565464 and parameters: {'n_estimators': 641, 'eta': 0.05453595400462424, 'max_depth': 9, 'alpha': 0.9792000000000001, 'lambda': 14.585147375040805, 'max_bin': 306}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 03:59:51,357] Trial 308 finished with value: 0.6964578705730203 and parameters: {'n_estimators': 884, 'eta': 0.05758394591660151, 'max_depth': 9, 'alpha': 0.9551000000000001, 'lambda': 15.351501833829708, 'max_bin': 351}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:00:20,144] Trial 309 finished with value: 0.6947999267413187 and parameters: {'n_estimators': 867, 'eta': 0.05905231031656227, 'max_depth': 9, 'alpha': 0.9994000000000001, 'lambda': 12.924316796003868, 'max_bin': 333}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:00:50,024] Trial 310 finished with value: 0.6956833673803647 and parameters: {'n_estimators': 900, 'eta': 0.060990055495059446, 'max_depth': 9, 'alpha': 0.9765, 'lambda': 13.96191129547655, 'max_bin': 346}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:01:19,403] Trial 311 finished with value: 0.6932314896916968 and parameters: {'n_estimators': 885, 'eta': 0.051624666536024814, 'max_depth': 9, 'alpha': 0.9596, 'lambda': 14.509802205695996, 'max_bin': 326}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:01:49,683] Trial 312 finished with value: 0.6926874047968552 and parameters: {'n_estimators': 900, 'eta': 0.05461411179277406, 'max_depth': 9, 'alpha': 0.9807, 'lambda': 15.477694341837225, 'max_bin': 339}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:02:16,829] Trial 313 finished with value: 0.6935892849075083 and parameters: {'n_estimators': 869, 'eta': 0.0564533868578996, 'max_depth': 9, 'alpha': 0.9544, 'lambda': 13.594968893104445, 'max_bin': 353}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:02:47,602] Trial 314 finished with value: 0.6930705254896707 and parameters: {'n_estimators': 885, 'eta': 0.05193184785753153, 'max_depth': 9, 'alpha': 0.9824, 'lambda': 12.40789384903322, 'max_bin': 333}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:03:17,181] Trial 315 finished with value: 0.6956703674031297 and parameters: {'n_estimators': 867, 'eta': 0.05457168842453583, 'max_depth': 9, 'alpha': 0.9994000000000001, 'lambda': 15.265176967411119, 'max_bin': 347}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:03:46,796] Trial 316 finished with value: 0.6928764617672898 and parameters: {'n_estimators': 881, 'eta': 0.05137263819114604, 'max_depth': 9, 'alpha': 0.9625, 'lambda': 14.107918282998758, 'max_bin': 338}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:04:09,811] Trial 317 finished with value: 0.6917560040910783 and parameters: {'n_estimators': 602, 'eta': 0.05355630425064168, 'max_depth': 9, 'alpha': 0.9800000000000001, 'lambda': 15.69712074429997, 'max_bin': 330}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:04:19,271] Trial 318 finished with value: 0.6695025631512251 and parameters: {'n_estimators': 239, 'eta': 0.05703489441914538, 'max_depth': 9, 'alpha': 0.9569000000000001, 'lambda': 14.710452120768451, 'max_bin': 356}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:04:52,241] Trial 319 finished with value: 0.6967388937761958 and parameters: {'n_estimators': 899, 'eta': 0.05552065272192393, 'max_depth': 9, 'alpha': 0.9781000000000001, 'lambda': 13.064644689870617, 'max_bin': 345}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:05:23,340] Trial 320 finished with value: 0.6941500305279761 and parameters: {'n_estimators': 865, 'eta': 0.05842862781591068, 'max_depth': 9, 'alpha': 0.9525, 'lambda': 15.441928569213296, 'max_bin': 350}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:05:51,955] Trial 321 finished with value: 0.6916155956802419 and parameters: {'n_estimators': 886, 'eta': 0.060338057817176265, 'max_depth': 9, 'alpha': 0.9783000000000001, 'lambda': 14.452430257419753, 'max_bin': 323}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:06:24,061] Trial 322 finished with value: 0.6932776755780093 and parameters: {'n_estimators': 885, 'eta': 0.05269383223179318, 'max_depth': 9, 'alpha': 0.9497000000000001, 'lambda': 15.617058265144088, 'max_bin': 338}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:06:47,256] Trial 323 finished with value: 0.6898676932540985 and parameters: {'n_estimators': 541, 'eta': 0.051388667585607756, 'max_depth': 9, 'alpha': 0.9979, 'lambda': 14.572703596110147, 'max_bin': 363}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:07:10,339] Trial 324 finished with value: 0.6897286842881807 and parameters: {'n_estimators': 580, 'eta': 0.05383158780462149, 'max_depth': 9, 'alpha': 0.9994000000000001, 'lambda': 13.197794550934654, 'max_bin': 353}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:07:39,288] Trial 325 finished with value: 0.6944217236951048 and parameters: {'n_estimators': 867, 'eta': 0.05614736198867265, 'max_depth': 9, 'alpha': 0.9689000000000001, 'lambda': 13.902031146507243, 'max_bin': 312}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:08:09,778] Trial 326 finished with value: 0.6946890923942244 and parameters: {'n_estimators': 899, 'eta': 0.05084417430723042, 'max_depth': 9, 'alpha': 0.5174, 'lambda': 11.915652095200635, 'max_bin': 333}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:08:39,541] Trial 327 finished with value: 0.6939024582954536 and parameters: {'n_estimators': 868, 'eta': 0.05409629573052897, 'max_depth': 9, 'alpha': 0.9810000000000001, 'lambda': 15.674125000475462, 'max_bin': 344}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:09:10,314] Trial 328 finished with value: 0.6945369771994043 and parameters: {'n_estimators': 900, 'eta': 0.05804905839146782, 'max_depth': 9, 'alpha': 0.9396, 'lambda': 15.064081932732694, 'max_bin': 357}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:09:39,667] Trial 329 finished with value: 0.6927086709782468 and parameters: {'n_estimators': 884, 'eta': 0.055901158600488346, 'max_depth': 9, 'alpha': 0.9994000000000001, 'lambda': 13.989549525407256, 'max_bin': 336}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:10:10,571] Trial 330 finished with value: 0.6941288050235703 and parameters: {'n_estimators': 900, 'eta': 0.053071738967222636, 'max_depth': 9, 'alpha': 0.9656, 'lambda': 15.837729306841258, 'max_bin': 341}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:10:39,126] Trial 331 finished with value: 0.6955669802785011 and parameters: {'n_estimators': 864, 'eta': 0.06135860004494931, 'max_depth': 9, 'alpha': 0.9804, 'lambda': 15.01681971384261, 'max_bin': 363}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:11:13,201] Trial 332 finished with value: 0.6944020396355215 and parameters: {'n_estimators': 882, 'eta': 0.05066449870692424, 'max_depth': 9, 'alpha': 0.9499000000000001, 'lambda': 13.633023230677747, 'max_bin': 327}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:11:42,268] Trial 333 finished with value: 0.6944841133633641 and parameters: {'n_estimators': 882, 'eta': 0.05751999222035451, 'max_depth': 9, 'alpha': 0.9803000000000001, 'lambda': 15.931118344211116, 'max_bin': 349}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:11:54,972] Trial 334 finished with value: 0.6811891828281871 and parameters: {'n_estimators': 315, 'eta': 0.05356260172662201, 'max_depth': 9, 'alpha': 0.9633, 'lambda': 12.8449687606992, 'max_bin': 342}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:12:26,041] Trial 335 finished with value: 0.6965456639923314 and parameters: {'n_estimators': 900, 'eta': 0.054773921341303565, 'max_depth': 9, 'alpha': 0.9392, 'lambda': 14.707777416790945, 'max_bin': 354}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:12:53,661] Trial 336 finished with value: 0.6923091245145578 and parameters: {'n_estimators': 867, 'eta': 0.052018597106933594, 'max_depth': 8, 'alpha': 0.9996, 'lambda': 15.037303235175743, 'max_bin': 365}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:13:19,950] Trial 337 finished with value: 0.695372612390886 and parameters: {'n_estimators': 883, 'eta': 0.0593587387946932, 'max_depth': 9, 'alpha': 0.9667, 'lambda': 12.474855185913324, 'max_bin': 330}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:13:49,629] Trial 338 finished with value: 0.6959076708168648 and parameters: {'n_estimators': 858, 'eta': 0.05588465956390021, 'max_depth': 9, 'alpha': 1.0, 'lambda': 15.930473904734404, 'max_bin': 347}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:14:16,428] Trial 339 finished with value: 0.6930346583825918 and parameters: {'n_estimators': 900, 'eta': 0.06220927797617246, 'max_depth': 9, 'alpha': 0.9779, 'lambda': 14.181210390479464, 'max_bin': 337}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:14:46,908] Trial 340 finished with value: 0.6915638427349926 and parameters: {'n_estimators': 882, 'eta': 0.04941853382663808, 'max_depth': 9, 'alpha': 0.9508000000000001, 'lambda': 13.315691460358785, 'max_bin': 360}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:15:15,885] Trial 341 finished with value: 0.6936204018096157 and parameters: {'n_estimators': 859, 'eta': 0.057244678089286787, 'max_depth': 9, 'alpha': 0.9791000000000001, 'lambda': 11.66710422501036, 'max_bin': 317}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:15:49,580] Trial 342 finished with value: 0.6924386635499611 and parameters: {'n_estimators': 900, 'eta': 0.052480668238417905, 'max_depth': 9, 'alpha': 0.9641000000000001, 'lambda': 38.255008620856145, 'max_bin': 341}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:16:21,754] Trial 343 finished with value: 0.6964079385334901 and parameters: {'n_estimators': 874, 'eta': 0.05490911838616416, 'max_depth': 9, 'alpha': 0.9998, 'lambda': 14.971910462262223, 'max_bin': 300}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:16:51,754] Trial 344 finished with value: 0.6970855963320228 and parameters: {'n_estimators': 884, 'eta': 0.0504979994381673, 'max_depth': 9, 'alpha': 0.9487000000000001, 'lambda': 15.943309609962292, 'max_bin': 351}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:17:21,718] Trial 345 finished with value: 0.6975439756165803 and parameters: {'n_estimators': 900, 'eta': 0.059119549093093096, 'max_depth': 9, 'alpha': 0.9811000000000001, 'lambda': 14.384346120674698, 'max_bin': 333}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:17:47,713] Trial 346 finished with value: 0.6941834253368981 and parameters: {'n_estimators': 665, 'eta': 0.05437828050092302, 'max_depth': 9, 'alpha': 0.9631000000000001, 'lambda': 13.732220069572675, 'max_bin': 345}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:18:18,341] Trial 347 finished with value: 0.6951682832230915 and parameters: {'n_estimators': 859, 'eta': 0.05682974871225853, 'max_depth': 9, 'alpha': 0.9369000000000001, 'lambda': 15.436230629468273, 'max_bin': 323}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:18:47,730] Trial 348 finished with value: 0.6934134479685866 and parameters: {'n_estimators': 875, 'eta': 0.05172968945960714, 'max_depth': 9, 'alpha': 0.9838, 'lambda': 15.988313588714126, 'max_bin': 359}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:19:15,843] Trial 349 finished with value: 0.6948792236351402 and parameters: {'n_estimators': 883, 'eta': 0.053409909356554555, 'max_depth': 8, 'alpha': 0.9626, 'lambda': 14.52592539422867, 'max_bin': 366}. Best is trial 263 with value: 0.7085707603331093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7086\n",
      "\tBest params:\n",
      "\t\tn_estimators: 900\n",
      "\t\teta: 0.052282964474475044\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.9758\n",
      "\t\tlambda: 15.852769374672869\n",
      "\t\tmax_bin: 372\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.696313    0.701416    0.713121    0.730474   \n",
      "1                    TP  401.000000  403.000000  373.000000  410.000000   \n",
      "2                    TN  350.000000  338.000000  372.000000  349.000000   \n",
      "3                    FP   76.000000   84.000000   80.000000   71.000000   \n",
      "4                    FN   72.000000   74.000000   74.000000   69.000000   \n",
      "5              Accuracy    0.835373    0.824249    0.828699    0.844271   \n",
      "6             Precision    0.840671    0.827515    0.823400    0.852391   \n",
      "7           Sensitivity    0.847780    0.844864    0.834452    0.855950   \n",
      "8           Specificity    0.821600    0.800900    0.823000    0.831000   \n",
      "9              F1 score    0.844211    0.836100    0.828889    0.854167   \n",
      "10  F1 score (weighted)    0.835331    0.824107    0.828697    0.844248   \n",
      "11     F1 score (macro)    0.834841    0.823326    0.828698    0.843551   \n",
      "12    Balanced Accuracy    0.834688    0.822906    0.828730    0.843451   \n",
      "13                  MCC    0.669715    0.646857    0.657471    0.687111   \n",
      "14                  NPV    0.829400    0.820400    0.834100    0.834900   \n",
      "15              ROC_AUC    0.834688    0.822906    0.828730    0.843451   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.712344    0.692140    0.695690  \n",
      "1   411.000000  388.000000  402.000000  \n",
      "2   344.000000  366.000000  345.000000  \n",
      "3    79.000000   72.000000   84.000000  \n",
      "4    65.000000   73.000000   68.000000  \n",
      "5     0.839822    0.838710    0.830923  \n",
      "6     0.838776    0.843478    0.827160  \n",
      "7     0.863445    0.841649    0.855319  \n",
      "8     0.813200    0.835600    0.804200  \n",
      "9     0.850932    0.842562    0.841004  \n",
      "10    0.839635    0.838714    0.830732  \n",
      "11    0.838927    0.838613    0.830241  \n",
      "12    0.838342    0.838633    0.829757  \n",
      "13    0.678266    0.677228    0.661012  \n",
      "14    0.841100    0.833700    0.835400  \n",
      "15    0.838342    0.838633    0.829757  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_6_cat = np.where((y_pred_xgb_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 04:19:25,360] Trial 350 finished with value: 0.6271874577156036 and parameters: {'n_estimators': 143, 'eta': 0.05594835932986052, 'max_depth': 9, 'alpha': 0.9999, 'lambda': 33.522789520882284, 'max_bin': 338}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:19:55,665] Trial 351 finished with value: 0.6904402703066299 and parameters: {'n_estimators': 885, 'eta': 0.049209037878264666, 'max_depth': 9, 'alpha': 0.9807, 'lambda': 32.33928989445726, 'max_bin': 352}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:20:09,408] Trial 352 finished with value: 0.682341478074002 and parameters: {'n_estimators': 362, 'eta': 0.05863783346811289, 'max_depth': 9, 'alpha': 0.9413, 'lambda': 15.11798548888299, 'max_bin': 356}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:20:40,691] Trial 353 finished with value: 0.6905686029742617 and parameters: {'n_estimators': 867, 'eta': 0.05091649099125072, 'max_depth': 10, 'alpha': 0.9805, 'lambda': 12.617105279603402, 'max_bin': 330}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:21:09,312] Trial 354 finished with value: 0.6873314118349463 and parameters: {'n_estimators': 853, 'eta': 0.05347695165685908, 'max_depth': 9, 'alpha': 0.9645, 'lambda': 13.499427625891135, 'max_bin': 346}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:21:37,129] Trial 355 finished with value: 0.6910873556968592 and parameters: {'n_estimators': 885, 'eta': 0.055702709557949925, 'max_depth': 9, 'alpha': 0.9991000000000001, 'lambda': 16.11440586071124, 'max_bin': 340}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:22:02,453] Trial 356 finished with value: 0.690097326265575 and parameters: {'n_estimators': 900, 'eta': 0.06108380253636444, 'max_depth': 9, 'alpha': 0.9570000000000001, 'lambda': 10.767439776851452, 'max_bin': 365}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:22:20,459] Trial 357 finished with value: 0.6851303285392374 and parameters: {'n_estimators': 447, 'eta': 0.05218789096891654, 'max_depth': 9, 'alpha': 0.6305000000000001, 'lambda': 14.1424600505233, 'max_bin': 335}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:22:44,358] Trial 358 finished with value: 0.69084476066547 and parameters: {'n_estimators': 870, 'eta': 0.0631145316507132, 'max_depth': 9, 'alpha': 0.4646, 'lambda': 14.935407125183163, 'max_bin': 350}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:23:09,028] Trial 359 finished with value: 0.6899064393253684 and parameters: {'n_estimators': 888, 'eta': 0.05797697812300236, 'max_depth': 9, 'alpha': 0.9831000000000001, 'lambda': 13.266447428768794, 'max_bin': 344}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:23:37,801] Trial 360 finished with value: 0.6905388629358262 and parameters: {'n_estimators': 900, 'eta': 0.05470339918336794, 'max_depth': 10, 'alpha': 0.9322, 'lambda': 15.652035035556217, 'max_bin': 327}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:24:10,525] Trial 361 finished with value: 0.695034806296748 and parameters: {'n_estimators': 870, 'eta': 0.04983137778439681, 'max_depth': 9, 'alpha': 0.9794, 'lambda': 14.436088708253886, 'max_bin': 362}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:24:38,457] Trial 362 finished with value: 0.6884147573807773 and parameters: {'n_estimators': 854, 'eta': 0.05700285797662921, 'max_depth': 9, 'alpha': 1.0, 'lambda': 12.343245874735263, 'max_bin': 356}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:25:03,409] Trial 363 finished with value: 0.6875963182448948 and parameters: {'n_estimators': 884, 'eta': 0.06014429339680007, 'max_depth': 9, 'alpha': 0.9609000000000001, 'lambda': 11.045279681624681, 'max_bin': 339}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:25:29,736] Trial 364 finished with value: 0.6904450603452232 and parameters: {'n_estimators': 900, 'eta': 0.052754902439116076, 'max_depth': 9, 'alpha': 0.3926, 'lambda': 16.076625204791153, 'max_bin': 347}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:25:54,658] Trial 365 finished with value: 0.6905315444076886 and parameters: {'n_estimators': 873, 'eta': 0.05447223274255831, 'max_depth': 9, 'alpha': 0.9445, 'lambda': 15.22001582953689, 'max_bin': 323}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:26:23,869] Trial 366 finished with value: 0.6915783593272062 and parameters: {'n_estimators': 900, 'eta': 0.04870604771552907, 'max_depth': 9, 'alpha': 0.9784, 'lambda': 13.763486550349787, 'max_bin': 333}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:26:51,873] Trial 367 finished with value: 0.6910064553148472 and parameters: {'n_estimators': 883, 'eta': 0.05084565227057698, 'max_depth': 9, 'alpha': 0.9639000000000001, 'lambda': 16.20592271012715, 'max_bin': 367}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:27:20,754] Trial 368 finished with value: 0.6909459830269231 and parameters: {'n_estimators': 855, 'eta': 0.0558115903790099, 'max_depth': 9, 'alpha': 0.9747, 'lambda': 14.689851702054657, 'max_bin': 342}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:27:44,998] Trial 369 finished with value: 0.6892748558375075 and parameters: {'n_estimators': 872, 'eta': 0.05908929840260177, 'max_depth': 9, 'alpha': 1.0, 'lambda': 13.026021218916913, 'max_bin': 351}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:28:11,798] Trial 370 finished with value: 0.6918805373984628 and parameters: {'n_estimators': 900, 'eta': 0.052771464512645985, 'max_depth': 9, 'alpha': 0.9467000000000001, 'lambda': 15.357832717705252, 'max_bin': 358}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:28:38,014] Trial 371 finished with value: 0.6880017244899472 and parameters: {'n_estimators': 882, 'eta': 0.05740761838596523, 'max_depth': 8, 'alpha': 0.9818, 'lambda': 14.48452459620864, 'max_bin': 338}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:29:06,091] Trial 372 finished with value: 0.6922330178598959 and parameters: {'n_estimators': 858, 'eta': 0.05417120482772627, 'max_depth': 9, 'alpha': 0.9596, 'lambda': 16.274537553057744, 'max_bin': 330}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:29:36,552] Trial 373 finished with value: 0.6923711979655034 and parameters: {'n_estimators': 886, 'eta': 0.051079839527581855, 'max_depth': 9, 'alpha': 0.933, 'lambda': 13.737879894720738, 'max_bin': 362}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:30:00,742] Trial 374 finished with value: 0.6923172618838831 and parameters: {'n_estimators': 873, 'eta': 0.06630858772877943, 'max_depth': 9, 'alpha': 0.9824, 'lambda': 15.473380885681525, 'max_bin': 346}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:30:30,716] Trial 375 finished with value: 0.6919308901722786 and parameters: {'n_estimators': 900, 'eta': 0.048349838892465885, 'max_depth': 9, 'alpha': 0.9654, 'lambda': 17.285463556408345, 'max_bin': 313}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:30:53,024] Trial 376 finished with value: 0.6886371917289715 and parameters: {'n_estimators': 850, 'eta': 0.05574593242107722, 'max_depth': 9, 'alpha': 0.9855, 'lambda': 12.508810252890747, 'max_bin': 354}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:31:20,007] Trial 377 finished with value: 0.6907083879885867 and parameters: {'n_estimators': 883, 'eta': 0.05278628504946253, 'max_depth': 9, 'alpha': 0.9541000000000001, 'lambda': 14.133355370995588, 'max_bin': 366}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:31:36,194] Trial 378 finished with value: 0.6836218885706955 and parameters: {'n_estimators': 404, 'eta': 0.05736863994193435, 'max_depth': 9, 'alpha': 0.9842000000000001, 'lambda': 15.167094754900203, 'max_bin': 335}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:32:03,525] Trial 379 finished with value: 0.6917749394114824 and parameters: {'n_estimators': 868, 'eta': 0.05440761573412628, 'max_depth': 9, 'alpha': 0.9289000000000001, 'lambda': 16.15954723138591, 'max_bin': 342}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:32:31,522] Trial 380 finished with value: 0.6916454909529424 and parameters: {'n_estimators': 883, 'eta': 0.05223112165508042, 'max_depth': 9, 'alpha': 0.9704, 'lambda': 14.966213469134233, 'max_bin': 349}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:33:00,826] Trial 381 finished with value: 0.692613928866814 and parameters: {'n_estimators': 885, 'eta': 0.059286000594565895, 'max_depth': 9, 'alpha': 0.9989, 'lambda': 13.132947591277343, 'max_bin': 360}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:33:31,354] Trial 382 finished with value: 0.6928505681730353 and parameters: {'n_estimators': 864, 'eta': 0.050720459738768375, 'max_depth': 10, 'alpha': 0.9477000000000001, 'lambda': 17.32612302140948, 'max_bin': 335}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:33:58,028] Trial 383 finished with value: 0.6922090727054782 and parameters: {'n_estimators': 900, 'eta': 0.05588686701053296, 'max_depth': 9, 'alpha': 0.9984000000000001, 'lambda': 11.787102308996877, 'max_bin': 325}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:34:25,790] Trial 384 finished with value: 0.6936213889526808 and parameters: {'n_estimators': 900, 'eta': 0.054595628954710984, 'max_depth': 9, 'alpha': 0.9653, 'lambda': 15.761112580228085, 'max_bin': 343}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:34:56,261] Trial 385 finished with value: 0.6920931067908442 and parameters: {'n_estimators': 850, 'eta': 0.05019959998640189, 'max_depth': 9, 'alpha': 0.9993000000000001, 'lambda': 14.09008913246337, 'max_bin': 367}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:35:25,419] Trial 386 finished with value: 0.6940494335032001 and parameters: {'n_estimators': 869, 'eta': 0.05295314015743627, 'max_depth': 9, 'alpha': 0.9796, 'lambda': 16.492726391965284, 'max_bin': 354}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:35:54,101] Trial 387 finished with value: 0.6889329142178723 and parameters: {'n_estimators': 885, 'eta': 0.05731088853663954, 'max_depth': 9, 'alpha': 0.9486, 'lambda': 14.577565781638706, 'max_bin': 347}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:36:24,192] Trial 388 finished with value: 0.6921704637093761 and parameters: {'n_estimators': 884, 'eta': 0.04808810492451422, 'max_depth': 9, 'alpha': 0.9722000000000001, 'lambda': 15.665183925909822, 'max_bin': 330}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:36:47,603] Trial 389 finished with value: 0.6919007126074078 and parameters: {'n_estimators': 870, 'eta': 0.06094649694132975, 'max_depth': 9, 'alpha': 0.935, 'lambda': 13.825814264999321, 'max_bin': 339}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:37:13,456] Trial 390 finished with value: 0.6903988589408596 and parameters: {'n_estimators': 885, 'eta': 0.05417386358073447, 'max_depth': 9, 'alpha': 0.9845, 'lambda': 15.077803369712537, 'max_bin': 351}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:37:40,530] Trial 391 finished with value: 0.6912008902161701 and parameters: {'n_estimators': 900, 'eta': 0.05165322112998124, 'max_depth': 8, 'alpha': 0.9627, 'lambda': 16.620450400162927, 'max_bin': 369}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:38:11,543] Trial 392 finished with value: 0.6919563454817179 and parameters: {'n_estimators': 848, 'eta': 0.05597293894585978, 'max_depth': 9, 'alpha': 0.9994000000000001, 'lambda': 36.14593084320789, 'max_bin': 356}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:38:35,560] Trial 393 finished with value: 0.6896964667616154 and parameters: {'n_estimators': 869, 'eta': 0.05900059156722409, 'max_depth': 9, 'alpha': 0.9677, 'lambda': 13.191469726039788, 'max_bin': 319}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:39:06,300] Trial 394 finished with value: 0.6913585099838155 and parameters: {'n_estimators': 886, 'eta': 0.04937014442166001, 'max_depth': 9, 'alpha': 0.9809, 'lambda': 17.410965539420726, 'max_bin': 362}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:39:34,001] Trial 395 finished with value: 0.690384634547161 and parameters: {'n_estimators': 900, 'eta': 0.05312070988843336, 'max_depth': 9, 'alpha': 0.9508000000000001, 'lambda': 14.58179504280226, 'max_bin': 335}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:40:01,205] Trial 396 finished with value: 0.6883271298652246 and parameters: {'n_estimators': 868, 'eta': 0.062417744537356844, 'max_depth': 9, 'alpha': 0.9282, 'lambda': 15.856589772574772, 'max_bin': 344}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:40:26,773] Trial 397 finished with value: 0.6926811384633644 and parameters: {'n_estimators': 885, 'eta': 0.05714079828566688, 'max_depth': 10, 'alpha': 0.9839, 'lambda': 15.236954588161822, 'max_bin': 340}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:40:53,834] Trial 398 finished with value: 0.690148783929961 and parameters: {'n_estimators': 900, 'eta': 0.05126681703656712, 'max_depth': 9, 'alpha': 0.9995, 'lambda': 11.069336045811923, 'max_bin': 279}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:41:20,055] Trial 399 finished with value: 0.6877084770224493 and parameters: {'n_estimators': 852, 'eta': 0.05528015912149045, 'max_depth': 9, 'alpha': 0.9651000000000001, 'lambda': 12.412485347741825, 'max_bin': 349}. Best is trial 263 with value: 0.7085707603331093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7086\n",
      "\tBest params:\n",
      "\t\tn_estimators: 900\n",
      "\t\teta: 0.052282964474475044\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.9758\n",
      "\t\tlambda: 15.852769374672869\n",
      "\t\tmax_bin: 372\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.696313    0.701416    0.713121    0.730474   \n",
      "1                    TP  401.000000  403.000000  373.000000  410.000000   \n",
      "2                    TN  350.000000  338.000000  372.000000  349.000000   \n",
      "3                    FP   76.000000   84.000000   80.000000   71.000000   \n",
      "4                    FN   72.000000   74.000000   74.000000   69.000000   \n",
      "5              Accuracy    0.835373    0.824249    0.828699    0.844271   \n",
      "6             Precision    0.840671    0.827515    0.823400    0.852391   \n",
      "7           Sensitivity    0.847780    0.844864    0.834452    0.855950   \n",
      "8           Specificity    0.821600    0.800900    0.823000    0.831000   \n",
      "9              F1 score    0.844211    0.836100    0.828889    0.854167   \n",
      "10  F1 score (weighted)    0.835331    0.824107    0.828697    0.844248   \n",
      "11     F1 score (macro)    0.834841    0.823326    0.828698    0.843551   \n",
      "12    Balanced Accuracy    0.834688    0.822906    0.828730    0.843451   \n",
      "13                  MCC    0.669715    0.646857    0.657471    0.687111   \n",
      "14                  NPV    0.829400    0.820400    0.834100    0.834900   \n",
      "15              ROC_AUC    0.834688    0.822906    0.828730    0.843451   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.712344    0.692140    0.695690    0.699110  \n",
      "1   411.000000  388.000000  402.000000  426.000000  \n",
      "2   344.000000  366.000000  345.000000  339.000000  \n",
      "3    79.000000   72.000000   84.000000   74.000000  \n",
      "4    65.000000   73.000000   68.000000   60.000000  \n",
      "5     0.839822    0.838710    0.830923    0.850945  \n",
      "6     0.838776    0.843478    0.827160    0.852000  \n",
      "7     0.863445    0.841649    0.855319    0.876543  \n",
      "8     0.813200    0.835600    0.804200    0.820800  \n",
      "9     0.850932    0.842562    0.841004    0.864097  \n",
      "10    0.839635    0.838714    0.830732    0.850719  \n",
      "11    0.838927    0.838613    0.830241    0.849536  \n",
      "12    0.838342    0.838633    0.829757    0.848683  \n",
      "13    0.678266    0.677228    0.661012    0.699492  \n",
      "14    0.841100    0.833700    0.835400    0.849600  \n",
      "15    0.838342    0.838633    0.829757    0.848683  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "y_pred_xgb_7_cat = np.where((y_pred_xgb_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 04:41:55,751] Trial 400 finished with value: 0.6966745406277391 and parameters: {'n_estimators': 872, 'eta': 0.05333256242953155, 'max_depth': 9, 'alpha': 0.9505, 'lambda': 16.31796327738855, 'max_bin': 330}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:42:22,184] Trial 401 finished with value: 0.695230625445403 and parameters: {'n_estimators': 900, 'eta': 0.05882474803571046, 'max_depth': 8, 'alpha': 0.9824, 'lambda': 13.787997377635326, 'max_bin': 368}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:42:52,758] Trial 402 finished with value: 0.6940845894109678 and parameters: {'n_estimators': 862, 'eta': 0.04982573757944718, 'max_depth': 9, 'alpha': 0.9997, 'lambda': 14.472494615472788, 'max_bin': 359}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:43:22,515] Trial 403 finished with value: 0.692684475615012 and parameters: {'n_estimators': 884, 'eta': 0.05481253271468826, 'max_depth': 9, 'alpha': 0.9685, 'lambda': 15.453654702812681, 'max_bin': 289}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:43:49,685] Trial 404 finished with value: 0.6972989470761072 and parameters: {'n_estimators': 846, 'eta': 0.06397467570189284, 'max_depth': 9, 'alpha': 0.9405, 'lambda': 16.87684643950622, 'max_bin': 337}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:44:17,975] Trial 405 finished with value: 0.6947639929841701 and parameters: {'n_estimators': 883, 'eta': 0.04810517452956207, 'max_depth': 9, 'alpha': 0.9781000000000001, 'lambda': 13.263567903374318, 'max_bin': 343}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:44:48,222] Trial 406 finished with value: 0.6972634720035235 and parameters: {'n_estimators': 871, 'eta': 0.051952624360357885, 'max_depth': 9, 'alpha': 0.9997, 'lambda': 16.07649754465147, 'max_bin': 353}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:45:12,762] Trial 407 finished with value: 0.6922294576699505 and parameters: {'n_estimators': 883, 'eta': 0.07381151603568699, 'max_depth': 9, 'alpha': 0.9648, 'lambda': 14.711023011271074, 'max_bin': 327}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:45:30,424] Trial 408 finished with value: 0.6872895281899043 and parameters: {'n_estimators': 900, 'eta': 0.0854381210895187, 'max_depth': 9, 'alpha': 0.9502, 'lambda': 17.356096798387238, 'max_bin': 348}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:46:01,596] Trial 409 finished with value: 0.6939454874413231 and parameters: {'n_estimators': 840, 'eta': 0.05664026771707614, 'max_depth': 9, 'alpha': 0.9819, 'lambda': 28.921220266102438, 'max_bin': 367}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:46:12,627] Trial 410 finished with value: 0.6842640423565305 and parameters: {'n_estimators': 274, 'eta': 0.06095498211385044, 'max_depth': 9, 'alpha': 0.9262, 'lambda': 10.616013425274785, 'max_bin': 333}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:46:42,817] Trial 411 finished with value: 0.696411993045242 and parameters: {'n_estimators': 861, 'eta': 0.05435723369073002, 'max_depth': 9, 'alpha': 0.5952000000000001, 'lambda': 31.66447180910176, 'max_bin': 362}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:47:10,880] Trial 412 finished with value: 0.6935886470060727 and parameters: {'n_estimators': 883, 'eta': 0.05792216481554519, 'max_depth': 9, 'alpha': 0.9628000000000001, 'lambda': 12.319992878807902, 'max_bin': 341}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:47:39,933] Trial 413 finished with value: 0.6945328156298197 and parameters: {'n_estimators': 900, 'eta': 0.05247866028911565, 'max_depth': 9, 'alpha': 0.9820000000000001, 'lambda': 14.001180497710624, 'max_bin': 356}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:48:10,237] Trial 414 finished with value: 0.6918081794180354 and parameters: {'n_estimators': 624, 'eta': 0.05082470200697099, 'max_depth': 10, 'alpha': 0.9475, 'lambda': 15.492259209546214, 'max_bin': 323}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:48:44,021] Trial 415 finished with value: 0.6983739540061894 and parameters: {'n_estimators': 869, 'eta': 0.05599184482339694, 'max_depth': 9, 'alpha': 0.9838, 'lambda': 39.745594356370646, 'max_bin': 345}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:49:20,622] Trial 416 finished with value: 0.6955011327907333 and parameters: {'n_estimators': 885, 'eta': 0.0536553658280107, 'max_depth': 9, 'alpha': 0.9633, 'lambda': 34.37901498992828, 'max_bin': 336}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:49:52,284] Trial 417 finished with value: 0.696640995627552 and parameters: {'n_estimators': 857, 'eta': 0.059897066751099244, 'max_depth': 9, 'alpha': 1.0, 'lambda': 37.38083806248808, 'max_bin': 350}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:50:23,266] Trial 418 finished with value: 0.6969161972561588 and parameters: {'n_estimators': 900, 'eta': 0.048700587429572, 'max_depth': 9, 'alpha': 0.9306000000000001, 'lambda': 14.953856581792525, 'max_bin': 369}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:50:57,749] Trial 419 finished with value: 0.6829543072110049 and parameters: {'n_estimators': 884, 'eta': 0.01778537590562699, 'max_depth': 9, 'alpha': 0.9997, 'lambda': 16.22326864090079, 'max_bin': 363}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:51:25,614] Trial 420 finished with value: 0.695504976448506 and parameters: {'n_estimators': 871, 'eta': 0.0554844798040868, 'max_depth': 9, 'alpha': 0.9737, 'lambda': 13.443156631945335, 'max_bin': 341}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:51:50,599] Trial 421 finished with value: 0.6924584288951838 and parameters: {'n_estimators': 900, 'eta': 0.06861293664488091, 'max_depth': 9, 'alpha': 0.9516, 'lambda': 14.273615536237761, 'max_bin': 356}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:52:24,550] Trial 422 finished with value: 0.6971511337390246 and parameters: {'n_estimators': 872, 'eta': 0.05137093470220535, 'max_depth': 9, 'alpha': 0.9799, 'lambda': 24.958024618131134, 'max_bin': 346}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:52:51,211] Trial 423 finished with value: 0.6974197180607145 and parameters: {'n_estimators': 840, 'eta': 0.057697133648235845, 'max_depth': 9, 'alpha': 0.9685, 'lambda': 10.162894451811814, 'max_bin': 332}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:53:18,667] Trial 424 finished with value: 0.696801463317572 and parameters: {'n_estimators': 884, 'eta': 0.05363816566867502, 'max_depth': 9, 'alpha': 0.9834, 'lambda': 11.941313644121122, 'max_bin': 309}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:53:46,076] Trial 425 finished with value: 0.6914059463285641 and parameters: {'n_estimators': 857, 'eta': 0.05615919833396628, 'max_depth': 9, 'alpha': 0.9999, 'lambda': 15.525100618942046, 'max_bin': 338}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:54:18,696] Trial 426 finished with value: 0.6947996808735444 and parameters: {'n_estimators': 900, 'eta': 0.0522903883221672, 'max_depth': 9, 'alpha': 0.9435, 'lambda': 26.696784078847568, 'max_bin': 352}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:54:49,363] Trial 427 finished with value: 0.6952345176083468 and parameters: {'n_estimators': 883, 'eta': 0.05030253828952222, 'max_depth': 9, 'alpha': 0.9555, 'lambda': 16.673138963045385, 'max_bin': 359}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:55:17,406] Trial 428 finished with value: 0.6937299895843043 and parameters: {'n_estimators': 869, 'eta': 0.0591548015575289, 'max_depth': 9, 'alpha': 0.9670000000000001, 'lambda': 17.552979140762744, 'max_bin': 370}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:55:39,067] Trial 429 finished with value: 0.6906128024107197 and parameters: {'n_estimators': 886, 'eta': 0.06298069278006002, 'max_depth': 9, 'alpha': 0.9187000000000001, 'lambda': 14.924191434167783, 'max_bin': 328}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:56:08,043] Trial 430 finished with value: 0.6909788898404767 and parameters: {'n_estimators': 873, 'eta': 0.047744340198628436, 'max_depth': 9, 'alpha': 0.5571, 'lambda': 8.85733511896532, 'max_bin': 346}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:56:37,802] Trial 431 finished with value: 0.6925617925690014 and parameters: {'n_estimators': 855, 'eta': 0.05462944641957242, 'max_depth': 9, 'alpha': 0.9830000000000001, 'lambda': 16.1189062558397, 'max_bin': 338}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:57:00,015] Trial 432 finished with value: 0.6948266355162434 and parameters: {'n_estimators': 693, 'eta': 0.05760596465500356, 'max_depth': 8, 'alpha': 0.9818, 'lambda': 12.804502074062551, 'max_bin': 362}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:57:28,060] Trial 433 finished with value: 0.6918984805828771 and parameters: {'n_estimators': 885, 'eta': 0.053313674617837245, 'max_depth': 9, 'alpha': 0.9625, 'lambda': 14.217782136275584, 'max_bin': 350}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:57:56,581] Trial 434 finished with value: 0.6942950533972906 and parameters: {'n_estimators': 900, 'eta': 0.05015155322181785, 'max_depth': 9, 'alpha': 0.9375, 'lambda': 6.857796987414575, 'max_bin': 333}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:58:19,950] Trial 435 finished with value: 0.6913109769303629 and parameters: {'n_estimators': 900, 'eta': 0.08200086049846758, 'max_depth': 9, 'alpha': 0.9997, 'lambda': 15.499416787183275, 'max_bin': 366}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:58:40,466] Trial 436 finished with value: 0.6862220450925802 and parameters: {'n_estimators': 510, 'eta': 0.06111472592084633, 'max_depth': 9, 'alpha': 0.9659000000000001, 'lambda': 11.584508312464838, 'max_bin': 315}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:59:08,321] Trial 437 finished with value: 0.6952521619021121 and parameters: {'n_estimators': 861, 'eta': 0.056083096571635786, 'max_depth': 9, 'alpha': 0.9851000000000001, 'lambda': 13.777784011087913, 'max_bin': 343}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 04:59:38,727] Trial 438 finished with value: 0.6938605008889968 and parameters: {'n_estimators': 882, 'eta': 0.05214004642029968, 'max_depth': 10, 'alpha': 0.9521000000000001, 'lambda': 14.792272983296126, 'max_bin': 355}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:00:09,937] Trial 439 finished with value: 0.694152378694138 and parameters: {'n_estimators': 837, 'eta': 0.05459638768809551, 'max_depth': 8, 'alpha': 0.9832000000000001, 'lambda': 30.46322137238476, 'max_bin': 252}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:00:39,133] Trial 440 finished with value: 0.6920837472816217 and parameters: {'n_estimators': 873, 'eta': 0.05817971818748989, 'max_depth': 9, 'alpha': 0.9993000000000001, 'lambda': 17.26448683415357, 'max_bin': 371}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:01:16,682] Trial 441 finished with value: 0.6768003557535591 and parameters: {'n_estimators': 884, 'eta': 0.014396942991439982, 'max_depth': 9, 'alpha': 0.922, 'lambda': 16.409656345127374, 'max_bin': 320}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:01:36,290] Trial 442 finished with value: 0.6894322994068215 and parameters: {'n_estimators': 900, 'eta': 0.09725361985523796, 'max_depth': 9, 'alpha': 0.9653, 'lambda': 15.640076999931468, 'max_bin': 340}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:02:05,771] Trial 443 finished with value: 0.6946737296464416 and parameters: {'n_estimators': 856, 'eta': 0.049468030274405085, 'max_depth': 9, 'alpha': 0.9445, 'lambda': 13.040059076229733, 'max_bin': 327}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:02:40,641] Trial 444 finished with value: 0.6981902102924189 and parameters: {'n_estimators': 872, 'eta': 0.05321689656882099, 'max_depth': 9, 'alpha': 0.9801000000000001, 'lambda': 35.380241654345355, 'max_bin': 347}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:03:08,841] Trial 445 finished with value: 0.6925685458050593 and parameters: {'n_estimators': 884, 'eta': 0.05563057088153078, 'max_depth': 9, 'alpha': 0.9993000000000001, 'lambda': 14.560474501033418, 'max_bin': 336}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:03:33,441] Trial 446 finished with value: 0.6928473533975814 and parameters: {'n_estimators': 886, 'eta': 0.07950217815713631, 'max_depth': 9, 'alpha': 0.9652000000000001, 'lambda': 34.76643142295151, 'max_bin': 374}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:04:03,988] Trial 447 finished with value: 0.6954679630174568 and parameters: {'n_estimators': 861, 'eta': 0.05173967235616892, 'max_depth': 9, 'alpha': 0.9429000000000001, 'lambda': 16.746867928819857, 'max_bin': 358}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:04:32,920] Trial 448 finished with value: 0.6967711680435174 and parameters: {'n_estimators': 841, 'eta': 0.059166038432977366, 'max_depth': 9, 'alpha': 0.9782000000000001, 'lambda': 15.172431879608485, 'max_bin': 351}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:05:00,058] Trial 449 finished with value: 0.6921964234891712 and parameters: {'n_estimators': 886, 'eta': 0.05674817766660595, 'max_depth': 9, 'alpha': 1.0, 'lambda': 16.029993002754274, 'max_bin': 366}. Best is trial 263 with value: 0.7085707603331093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7086\n",
      "\tBest params:\n",
      "\t\tn_estimators: 900\n",
      "\t\teta: 0.052282964474475044\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.9758\n",
      "\t\tlambda: 15.852769374672869\n",
      "\t\tmax_bin: 372\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.696313    0.701416    0.713121    0.730474   \n",
      "1                    TP  401.000000  403.000000  373.000000  410.000000   \n",
      "2                    TN  350.000000  338.000000  372.000000  349.000000   \n",
      "3                    FP   76.000000   84.000000   80.000000   71.000000   \n",
      "4                    FN   72.000000   74.000000   74.000000   69.000000   \n",
      "5              Accuracy    0.835373    0.824249    0.828699    0.844271   \n",
      "6             Precision    0.840671    0.827515    0.823400    0.852391   \n",
      "7           Sensitivity    0.847780    0.844864    0.834452    0.855950   \n",
      "8           Specificity    0.821600    0.800900    0.823000    0.831000   \n",
      "9              F1 score    0.844211    0.836100    0.828889    0.854167   \n",
      "10  F1 score (weighted)    0.835331    0.824107    0.828697    0.844248   \n",
      "11     F1 score (macro)    0.834841    0.823326    0.828698    0.843551   \n",
      "12    Balanced Accuracy    0.834688    0.822906    0.828730    0.843451   \n",
      "13                  MCC    0.669715    0.646857    0.657471    0.687111   \n",
      "14                  NPV    0.829400    0.820400    0.834100    0.834900   \n",
      "15              ROC_AUC    0.834688    0.822906    0.828730    0.843451   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.712344    0.692140    0.695690    0.699110    0.701671  \n",
      "1   411.000000  388.000000  402.000000  426.000000  407.000000  \n",
      "2   344.000000  366.000000  345.000000  339.000000  338.000000  \n",
      "3    79.000000   72.000000   84.000000   74.000000   88.000000  \n",
      "4    65.000000   73.000000   68.000000   60.000000   66.000000  \n",
      "5     0.839822    0.838710    0.830923    0.850945    0.828699  \n",
      "6     0.838776    0.843478    0.827160    0.852000    0.822222  \n",
      "7     0.863445    0.841649    0.855319    0.876543    0.860465  \n",
      "8     0.813200    0.835600    0.804200    0.820800    0.793400  \n",
      "9     0.850932    0.842562    0.841004    0.864097    0.840909  \n",
      "10    0.839635    0.838714    0.830732    0.850719    0.828375  \n",
      "11    0.838927    0.838613    0.830241    0.849536    0.827683  \n",
      "12    0.838342    0.838633    0.829757    0.848683    0.826946  \n",
      "13    0.678266    0.677228    0.661012    0.699492    0.656369  \n",
      "14    0.841100    0.833700    0.835400    0.849600    0.836600  \n",
      "15    0.838342    0.838633    0.829757    0.848683    0.826946  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "y_pred_xgb_8_cat = np.where((y_pred_xgb_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:05:25,588] Trial 450 finished with value: 0.7033381784792011 and parameters: {'n_estimators': 871, 'eta': 0.08833516610652922, 'max_depth': 9, 'alpha': 0.9632000000000001, 'lambda': 13.679157115195201, 'max_bin': 343}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:05:58,290] Trial 451 finished with value: 0.7061928675981912 and parameters: {'n_estimators': 887, 'eta': 0.05420986883205453, 'max_depth': 9, 'alpha': 0.9812000000000001, 'lambda': 17.871614973020996, 'max_bin': 332}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:06:31,082] Trial 452 finished with value: 0.7072021059682347 and parameters: {'n_estimators': 852, 'eta': 0.04786790057441577, 'max_depth': 9, 'alpha': 0.9217000000000001, 'lambda': 18.216898270642535, 'max_bin': 324}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:07:08,212] Trial 453 finished with value: 0.7052367897842972 and parameters: {'n_estimators': 847, 'eta': 0.04704298727147782, 'max_depth': 10, 'alpha': 0.9414, 'lambda': 17.84372518071345, 'max_bin': 321}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:07:38,214] Trial 454 finished with value: 0.7037373421065498 and parameters: {'n_estimators': 841, 'eta': 0.0471147085445502, 'max_depth': 8, 'alpha': 0.9125000000000001, 'lambda': 18.36559122031739, 'max_bin': 316}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:08:12,760] Trial 455 finished with value: 0.7021073093199166 and parameters: {'n_estimators': 853, 'eta': 0.04888062311948296, 'max_depth': 10, 'alpha': 0.928, 'lambda': 17.94747920072716, 'max_bin': 322}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:08:46,429] Trial 456 finished with value: 0.7079293995629272 and parameters: {'n_estimators': 832, 'eta': 0.04757918543748077, 'max_depth': 10, 'alpha': 0.9339000000000001, 'lambda': 17.609491437558436, 'max_bin': 319}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:09:20,954] Trial 457 finished with value: 0.7063650817315045 and parameters: {'n_estimators': 832, 'eta': 0.04693538533603395, 'max_depth': 10, 'alpha': 0.9194, 'lambda': 17.376133091701398, 'max_bin': 326}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:09:56,547] Trial 458 finished with value: 0.7077556855841703 and parameters: {'n_estimators': 823, 'eta': 0.046247944912131016, 'max_depth': 10, 'alpha': 0.9226000000000001, 'lambda': 17.782273270985506, 'max_bin': 324}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:10:30,645] Trial 459 finished with value: 0.7049290088323873 and parameters: {'n_estimators': 807, 'eta': 0.045122260679069326, 'max_depth': 10, 'alpha': 0.9152, 'lambda': 18.532274371558813, 'max_bin': 315}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:11:04,140] Trial 460 finished with value: 0.7049704785720674 and parameters: {'n_estimators': 830, 'eta': 0.048644880010217745, 'max_depth': 10, 'alpha': 0.9059, 'lambda': 17.614523961402156, 'max_bin': 326}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:11:37,417] Trial 461 finished with value: 0.7084315105780024 and parameters: {'n_estimators': 813, 'eta': 0.0479957324299562, 'max_depth': 10, 'alpha': 0.9264, 'lambda': 17.667286240005954, 'max_bin': 324}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:12:09,039] Trial 462 finished with value: 0.707145606143779 and parameters: {'n_estimators': 789, 'eta': 0.046596809751502634, 'max_depth': 10, 'alpha': 0.908, 'lambda': 18.097489385264428, 'max_bin': 311}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:12:44,012] Trial 463 finished with value: 0.7074787703734409 and parameters: {'n_estimators': 819, 'eta': 0.04622386017854105, 'max_depth': 10, 'alpha': 0.9046000000000001, 'lambda': 17.788780568856918, 'max_bin': 309}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:13:18,429] Trial 464 finished with value: 0.7055292556526085 and parameters: {'n_estimators': 771, 'eta': 0.04600291780162035, 'max_depth': 10, 'alpha': 0.9034000000000001, 'lambda': 18.386405165553796, 'max_bin': 312}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:13:52,309] Trial 465 finished with value: 0.7065501866953274 and parameters: {'n_estimators': 762, 'eta': 0.04680152298078646, 'max_depth': 10, 'alpha': 0.8906000000000001, 'lambda': 18.245521476628227, 'max_bin': 315}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:14:27,876] Trial 466 finished with value: 0.7045437114093894 and parameters: {'n_estimators': 801, 'eta': 0.04630853698291521, 'max_depth': 10, 'alpha': 0.8955000000000001, 'lambda': 18.197012652611626, 'max_bin': 306}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:15:03,437] Trial 467 finished with value: 0.7060721551401454 and parameters: {'n_estimators': 804, 'eta': 0.04744311974523438, 'max_depth': 10, 'alpha': 0.9202, 'lambda': 18.963626305098334, 'max_bin': 301}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:15:37,813] Trial 468 finished with value: 0.7048628322818655 and parameters: {'n_estimators': 782, 'eta': 0.04699343056711124, 'max_depth': 10, 'alpha': 0.898, 'lambda': 18.656850487951232, 'max_bin': 299}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:16:14,711] Trial 469 finished with value: 0.7070300565354204 and parameters: {'n_estimators': 795, 'eta': 0.04698155959983373, 'max_depth': 10, 'alpha': 0.8872, 'lambda': 19.025776626333187, 'max_bin': 308}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:16:50,767] Trial 470 finished with value: 0.706481209502385 and parameters: {'n_estimators': 790, 'eta': 0.04704674823337579, 'max_depth': 10, 'alpha': 0.8868, 'lambda': 19.144330820503388, 'max_bin': 305}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:17:24,324] Trial 471 finished with value: 0.7050780807789685 and parameters: {'n_estimators': 797, 'eta': 0.046438377533676634, 'max_depth': 10, 'alpha': 0.8672000000000001, 'lambda': 19.358911772066968, 'max_bin': 303}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:17:58,020] Trial 472 finished with value: 0.7049440194328145 and parameters: {'n_estimators': 769, 'eta': 0.04564893948376248, 'max_depth': 10, 'alpha': 0.8766, 'lambda': 18.78902541413861, 'max_bin': 292}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:18:32,192] Trial 473 finished with value: 0.707529224589116 and parameters: {'n_estimators': 754, 'eta': 0.04739506447734878, 'max_depth': 10, 'alpha': 0.8954000000000001, 'lambda': 17.91577933843317, 'max_bin': 307}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:19:02,251] Trial 474 finished with value: 0.7068320674328301 and parameters: {'n_estimators': 749, 'eta': 0.047104714936030086, 'max_depth': 10, 'alpha': 0.887, 'lambda': 17.931324787876356, 'max_bin': 310}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:19:35,870] Trial 475 finished with value: 0.7059583679178673 and parameters: {'n_estimators': 742, 'eta': 0.0473677746252874, 'max_depth': 10, 'alpha': 0.8891, 'lambda': 18.929235833110507, 'max_bin': 307}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:20:08,287] Trial 476 finished with value: 0.7036525461554272 and parameters: {'n_estimators': 762, 'eta': 0.045862486128431044, 'max_depth': 10, 'alpha': 0.8955000000000001, 'lambda': 18.088161416358425, 'max_bin': 301}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:20:41,890] Trial 477 finished with value: 0.7057495293228697 and parameters: {'n_estimators': 787, 'eta': 0.04411654490704959, 'max_depth': 10, 'alpha': 0.8877, 'lambda': 18.012405948517564, 'max_bin': 310}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:21:15,318] Trial 478 finished with value: 0.7037947949412621 and parameters: {'n_estimators': 718, 'eta': 0.04510505867378138, 'max_depth': 10, 'alpha': 0.9092, 'lambda': 19.740310965681886, 'max_bin': 304}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:21:49,460] Trial 479 finished with value: 0.7039583726086792 and parameters: {'n_estimators': 760, 'eta': 0.04784420683228345, 'max_depth': 10, 'alpha': 0.8651000000000001, 'lambda': 18.99382772026954, 'max_bin': 298}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:22:23,964] Trial 480 finished with value: 0.7044832323640944 and parameters: {'n_estimators': 792, 'eta': 0.04747983351434958, 'max_depth': 10, 'alpha': 0.9055000000000001, 'lambda': 17.76451205024886, 'max_bin': 311}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:23:02,104] Trial 481 finished with value: 0.704591341240445 and parameters: {'n_estimators': 812, 'eta': 0.04495207065761973, 'max_depth': 10, 'alpha': 0.8815000000000001, 'lambda': 18.622313445226496, 'max_bin': 315}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:23:35,020] Trial 482 finished with value: 0.7059099721674669 and parameters: {'n_estimators': 779, 'eta': 0.04714258434922095, 'max_depth': 10, 'alpha': 0.8874000000000001, 'lambda': 18.08375348471902, 'max_bin': 308}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:24:08,133] Trial 483 finished with value: 0.7055434951634363 and parameters: {'n_estimators': 735, 'eta': 0.04798831345484497, 'max_depth': 10, 'alpha': 0.9115000000000001, 'lambda': 19.90884313661031, 'max_bin': 296}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:24:41,301] Trial 484 finished with value: 0.7043831427087548 and parameters: {'n_estimators': 753, 'eta': 0.044815866661057475, 'max_depth': 10, 'alpha': 0.9121, 'lambda': 17.512912649042917, 'max_bin': 317}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:25:17,142] Trial 485 finished with value: 0.7059046588116054 and parameters: {'n_estimators': 814, 'eta': 0.04857780856262425, 'max_depth': 10, 'alpha': 0.8645, 'lambda': 19.10802425505088, 'max_bin': 305}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:25:53,740] Trial 486 finished with value: 0.7055536958332721 and parameters: {'n_estimators': 794, 'eta': 0.04682597533473165, 'max_depth': 10, 'alpha': 0.8961, 'lambda': 17.496832414219508, 'max_bin': 311}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:26:30,567] Trial 487 finished with value: 0.7064228889151387 and parameters: {'n_estimators': 809, 'eta': 0.04439340958433107, 'max_depth': 10, 'alpha': 0.9053, 'lambda': 18.446496645146713, 'max_bin': 292}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:27:08,143] Trial 488 finished with value: 0.706875109125329 and parameters: {'n_estimators': 817, 'eta': 0.04423471341692112, 'max_depth': 10, 'alpha': 0.8717, 'lambda': 18.217690474828796, 'max_bin': 291}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:27:42,875] Trial 489 finished with value: 0.706080846988405 and parameters: {'n_estimators': 776, 'eta': 0.04464019054709154, 'max_depth': 10, 'alpha': 0.8748, 'lambda': 18.16343959246171, 'max_bin': 284}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:28:21,984] Trial 490 finished with value: 0.7078418571476937 and parameters: {'n_estimators': 823, 'eta': 0.04276076976372657, 'max_depth': 10, 'alpha': 0.8931, 'lambda': 19.724627183979734, 'max_bin': 291}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:28:57,224] Trial 491 finished with value: 0.7066675117767005 and parameters: {'n_estimators': 819, 'eta': 0.04348362525696747, 'max_depth': 10, 'alpha': 0.8563000000000001, 'lambda': 20.141421950571974, 'max_bin': 288}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:29:30,536] Trial 492 finished with value: 0.7064712986642736 and parameters: {'n_estimators': 821, 'eta': 0.04403963174076014, 'max_depth': 10, 'alpha': 0.8575, 'lambda': 20.468640449867035, 'max_bin': 287}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:30:06,249] Trial 493 finished with value: 0.705185732768058 and parameters: {'n_estimators': 816, 'eta': 0.043302555187181443, 'max_depth': 10, 'alpha': 0.8513000000000001, 'lambda': 19.93186522267399, 'max_bin': 286}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:30:42,385] Trial 494 finished with value: 0.7062677669736726 and parameters: {'n_estimators': 816, 'eta': 0.0433840807964791, 'max_depth': 10, 'alpha': 0.8612000000000001, 'lambda': 19.800658774514744, 'max_bin': 275}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:31:18,003] Trial 495 finished with value: 0.7060619014426066 and parameters: {'n_estimators': 791, 'eta': 0.04337053480690545, 'max_depth': 10, 'alpha': 0.8744000000000001, 'lambda': 20.72667659173203, 'max_bin': 292}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:31:55,833] Trial 496 finished with value: 0.7077325685564555 and parameters: {'n_estimators': 805, 'eta': 0.044924393943372734, 'max_depth': 10, 'alpha': 0.8524, 'lambda': 20.306242379558544, 'max_bin': 281}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:32:30,433] Trial 497 finished with value: 0.7065024562825143 and parameters: {'n_estimators': 793, 'eta': 0.0460580912588265, 'max_depth': 10, 'alpha': 0.8682000000000001, 'lambda': 20.272160528988902, 'max_bin': 293}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:33:05,756] Trial 498 finished with value: 0.7051146526972694 and parameters: {'n_estimators': 786, 'eta': 0.045537473633620124, 'max_depth': 10, 'alpha': 0.8430000000000001, 'lambda': 20.989622733692137, 'max_bin': 282}. Best is trial 263 with value: 0.7085707603331093.\n",
      "[I 2023-12-20 05:33:38,185] Trial 499 finished with value: 0.7047397057664815 and parameters: {'n_estimators': 766, 'eta': 0.0462214776432233, 'max_depth': 10, 'alpha': 0.8875000000000001, 'lambda': 21.628898096022102, 'max_bin': 279}. Best is trial 263 with value: 0.7085707603331093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7086\n",
      "\tBest params:\n",
      "\t\tn_estimators: 900\n",
      "\t\teta: 0.052282964474475044\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.9758\n",
      "\t\tlambda: 15.852769374672869\n",
      "\t\tmax_bin: 372\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.696313    0.701416    0.713121    0.730474   \n",
      "1                    TP  401.000000  403.000000  373.000000  410.000000   \n",
      "2                    TN  350.000000  338.000000  372.000000  349.000000   \n",
      "3                    FP   76.000000   84.000000   80.000000   71.000000   \n",
      "4                    FN   72.000000   74.000000   74.000000   69.000000   \n",
      "5              Accuracy    0.835373    0.824249    0.828699    0.844271   \n",
      "6             Precision    0.840671    0.827515    0.823400    0.852391   \n",
      "7           Sensitivity    0.847780    0.844864    0.834452    0.855950   \n",
      "8           Specificity    0.821600    0.800900    0.823000    0.831000   \n",
      "9              F1 score    0.844211    0.836100    0.828889    0.854167   \n",
      "10  F1 score (weighted)    0.835331    0.824107    0.828697    0.844248   \n",
      "11     F1 score (macro)    0.834841    0.823326    0.828698    0.843551   \n",
      "12    Balanced Accuracy    0.834688    0.822906    0.828730    0.843451   \n",
      "13                  MCC    0.669715    0.646857    0.657471    0.687111   \n",
      "14                  NPV    0.829400    0.820400    0.834100    0.834900   \n",
      "15              ROC_AUC    0.834688    0.822906    0.828730    0.843451   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.712344    0.692140    0.695690    0.699110    0.701671    0.674841  \n",
      "1   411.000000  388.000000  402.000000  426.000000  407.000000  396.000000  \n",
      "2   344.000000  366.000000  345.000000  339.000000  338.000000  340.000000  \n",
      "3    79.000000   72.000000   84.000000   74.000000   88.000000  103.000000  \n",
      "4    65.000000   73.000000   68.000000   60.000000   66.000000   60.000000  \n",
      "5     0.839822    0.838710    0.830923    0.850945    0.828699    0.818687  \n",
      "6     0.838776    0.843478    0.827160    0.852000    0.822222    0.793587  \n",
      "7     0.863445    0.841649    0.855319    0.876543    0.860465    0.868421  \n",
      "8     0.813200    0.835600    0.804200    0.820800    0.793400    0.767500  \n",
      "9     0.850932    0.842562    0.841004    0.864097    0.840909    0.829319  \n",
      "10    0.839635    0.838714    0.830732    0.850719    0.828375    0.818145  \n",
      "11    0.838927    0.838613    0.830241    0.849536    0.827683    0.817981  \n",
      "12    0.838342    0.838633    0.829757    0.848683    0.826946    0.817958  \n",
      "13    0.678266    0.677228    0.661012    0.699492    0.656369    0.639740  \n",
      "14    0.841100    0.833700    0.835400    0.849600    0.836600    0.850000  \n",
      "15    0.838342    0.838633    0.829757    0.848683    0.826946    0.817958  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_9_cat = np.where((y_pred_xgb_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.714360</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>202.400000</td>\n",
       "      <td>7.618690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>174.600000</td>\n",
       "      <td>6.785606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>4.403282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>33.700000</td>\n",
       "      <td>5.396501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.839268</td>\n",
       "      <td>0.008917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.840300</td>\n",
       "      <td>0.015852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.857352</td>\n",
       "      <td>0.021985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.819440</td>\n",
       "      <td>0.018382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.848467</td>\n",
       "      <td>0.010424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.839148</td>\n",
       "      <td>0.008899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.838515</td>\n",
       "      <td>0.008893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.838398</td>\n",
       "      <td>0.008766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.677834</td>\n",
       "      <td>0.017847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.838590</td>\n",
       "      <td>0.022014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.838398</td>\n",
       "      <td>0.008766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.714360     0.024500\n",
       "1                    TP       202.400000     7.618690\n",
       "2                    TN       174.600000     6.785606\n",
       "3                    FP        38.500000     4.403282\n",
       "4                    FN        33.700000     5.396501\n",
       "5              Accuracy         0.839268     0.008917\n",
       "6             Precision         0.840300     0.015852\n",
       "7           Sensitivity         0.857352     0.021985\n",
       "8           Specificity         0.819440     0.018382\n",
       "9              F1 score         0.848467     0.010424\n",
       "10  F1 score (weighted)         0.839148     0.008899\n",
       "11     F1 score (macro)         0.838515     0.008893\n",
       "12    Balanced Accuracy         0.838398     0.008766\n",
       "13                  MCC         0.677834     0.017847\n",
       "14                  NPV         0.838590     0.022014\n",
       "15              ROC_AUC         0.838398     0.008766"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.696313</td>\n",
       "      <td>0.701416</td>\n",
       "      <td>0.713121</td>\n",
       "      <td>0.730474</td>\n",
       "      <td>0.712344</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>0.695690</td>\n",
       "      <td>0.699110</td>\n",
       "      <td>0.701671</td>\n",
       "      <td>0.674841</td>\n",
       "      <td>0.701712</td>\n",
       "      <td>0.014744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>426.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>401.700000</td>\n",
       "      <td>14.220877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>348.100000</td>\n",
       "      <td>11.901914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>81.100000</td>\n",
       "      <td>9.492102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>68.100000</td>\n",
       "      <td>5.321863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.835373</td>\n",
       "      <td>0.824249</td>\n",
       "      <td>0.828699</td>\n",
       "      <td>0.844271</td>\n",
       "      <td>0.839822</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.830923</td>\n",
       "      <td>0.850945</td>\n",
       "      <td>0.828699</td>\n",
       "      <td>0.818687</td>\n",
       "      <td>0.834038</td>\n",
       "      <td>0.009694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.840671</td>\n",
       "      <td>0.827515</td>\n",
       "      <td>0.823400</td>\n",
       "      <td>0.852391</td>\n",
       "      <td>0.838776</td>\n",
       "      <td>0.843478</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.793587</td>\n",
       "      <td>0.832120</td>\n",
       "      <td>0.017503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.847780</td>\n",
       "      <td>0.844864</td>\n",
       "      <td>0.834452</td>\n",
       "      <td>0.855950</td>\n",
       "      <td>0.863445</td>\n",
       "      <td>0.841649</td>\n",
       "      <td>0.855319</td>\n",
       "      <td>0.876543</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.854889</td>\n",
       "      <td>0.012917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.821600</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.804200</td>\n",
       "      <td>0.820800</td>\n",
       "      <td>0.793400</td>\n",
       "      <td>0.767500</td>\n",
       "      <td>0.811120</td>\n",
       "      <td>0.020310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.844211</td>\n",
       "      <td>0.836100</td>\n",
       "      <td>0.828889</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.850932</td>\n",
       "      <td>0.842562</td>\n",
       "      <td>0.841004</td>\n",
       "      <td>0.864097</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.829319</td>\n",
       "      <td>0.843219</td>\n",
       "      <td>0.010935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.835331</td>\n",
       "      <td>0.824107</td>\n",
       "      <td>0.828697</td>\n",
       "      <td>0.844248</td>\n",
       "      <td>0.839635</td>\n",
       "      <td>0.838714</td>\n",
       "      <td>0.830732</td>\n",
       "      <td>0.850719</td>\n",
       "      <td>0.828375</td>\n",
       "      <td>0.818145</td>\n",
       "      <td>0.833870</td>\n",
       "      <td>0.009774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.834841</td>\n",
       "      <td>0.823326</td>\n",
       "      <td>0.828698</td>\n",
       "      <td>0.843551</td>\n",
       "      <td>0.838927</td>\n",
       "      <td>0.838613</td>\n",
       "      <td>0.830241</td>\n",
       "      <td>0.849536</td>\n",
       "      <td>0.827683</td>\n",
       "      <td>0.817981</td>\n",
       "      <td>0.833340</td>\n",
       "      <td>0.009587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.834688</td>\n",
       "      <td>0.822906</td>\n",
       "      <td>0.828730</td>\n",
       "      <td>0.843451</td>\n",
       "      <td>0.838342</td>\n",
       "      <td>0.838633</td>\n",
       "      <td>0.829757</td>\n",
       "      <td>0.848683</td>\n",
       "      <td>0.826946</td>\n",
       "      <td>0.817958</td>\n",
       "      <td>0.833009</td>\n",
       "      <td>0.009498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.669715</td>\n",
       "      <td>0.646857</td>\n",
       "      <td>0.657471</td>\n",
       "      <td>0.687111</td>\n",
       "      <td>0.678266</td>\n",
       "      <td>0.677228</td>\n",
       "      <td>0.661012</td>\n",
       "      <td>0.699492</td>\n",
       "      <td>0.656369</td>\n",
       "      <td>0.639740</td>\n",
       "      <td>0.667326</td>\n",
       "      <td>0.018519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.820400</td>\n",
       "      <td>0.834100</td>\n",
       "      <td>0.834900</td>\n",
       "      <td>0.841100</td>\n",
       "      <td>0.833700</td>\n",
       "      <td>0.835400</td>\n",
       "      <td>0.849600</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.836520</td>\n",
       "      <td>0.008832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.834688</td>\n",
       "      <td>0.822906</td>\n",
       "      <td>0.828730</td>\n",
       "      <td>0.843451</td>\n",
       "      <td>0.838342</td>\n",
       "      <td>0.838633</td>\n",
       "      <td>0.829757</td>\n",
       "      <td>0.848683</td>\n",
       "      <td>0.826946</td>\n",
       "      <td>0.817958</td>\n",
       "      <td>0.833009</td>\n",
       "      <td>0.009498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.696313    0.701416    0.713121    0.730474   \n",
       "1                    TP  401.000000  403.000000  373.000000  410.000000   \n",
       "2                    TN  350.000000  338.000000  372.000000  349.000000   \n",
       "3                    FP   76.000000   84.000000   80.000000   71.000000   \n",
       "4                    FN   72.000000   74.000000   74.000000   69.000000   \n",
       "5              Accuracy    0.835373    0.824249    0.828699    0.844271   \n",
       "6             Precision    0.840671    0.827515    0.823400    0.852391   \n",
       "7           Sensitivity    0.847780    0.844864    0.834452    0.855950   \n",
       "8           Specificity    0.821600    0.800900    0.823000    0.831000   \n",
       "9              F1 score    0.844211    0.836100    0.828889    0.854167   \n",
       "10  F1 score (weighted)    0.835331    0.824107    0.828697    0.844248   \n",
       "11     F1 score (macro)    0.834841    0.823326    0.828698    0.843551   \n",
       "12    Balanced Accuracy    0.834688    0.822906    0.828730    0.843451   \n",
       "13                  MCC    0.669715    0.646857    0.657471    0.687111   \n",
       "14                  NPV    0.829400    0.820400    0.834100    0.834900   \n",
       "15              ROC_AUC    0.834688    0.822906    0.828730    0.843451   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.712344    0.692140    0.695690    0.699110    0.701671    0.674841   \n",
       "1   411.000000  388.000000  402.000000  426.000000  407.000000  396.000000   \n",
       "2   344.000000  366.000000  345.000000  339.000000  338.000000  340.000000   \n",
       "3    79.000000   72.000000   84.000000   74.000000   88.000000  103.000000   \n",
       "4    65.000000   73.000000   68.000000   60.000000   66.000000   60.000000   \n",
       "5     0.839822    0.838710    0.830923    0.850945    0.828699    0.818687   \n",
       "6     0.838776    0.843478    0.827160    0.852000    0.822222    0.793587   \n",
       "7     0.863445    0.841649    0.855319    0.876543    0.860465    0.868421   \n",
       "8     0.813200    0.835600    0.804200    0.820800    0.793400    0.767500   \n",
       "9     0.850932    0.842562    0.841004    0.864097    0.840909    0.829319   \n",
       "10    0.839635    0.838714    0.830732    0.850719    0.828375    0.818145   \n",
       "11    0.838927    0.838613    0.830241    0.849536    0.827683    0.817981   \n",
       "12    0.838342    0.838633    0.829757    0.848683    0.826946    0.817958   \n",
       "13    0.678266    0.677228    0.661012    0.699492    0.656369    0.639740   \n",
       "14    0.841100    0.833700    0.835400    0.849600    0.836600    0.850000   \n",
       "15    0.838342    0.838633    0.829757    0.848683    0.826946    0.817958   \n",
       "\n",
       "           ave        std  \n",
       "0     0.701712   0.014744  \n",
       "1   401.700000  14.220877  \n",
       "2   348.100000  11.901914  \n",
       "3    81.100000   9.492102  \n",
       "4    68.100000   5.321863  \n",
       "5     0.834038   0.009694  \n",
       "6     0.832120   0.017503  \n",
       "7     0.854889   0.012917  \n",
       "8     0.811120   0.020310  \n",
       "9     0.843219   0.010935  \n",
       "10    0.833870   0.009774  \n",
       "11    0.833340   0.009587  \n",
       "12    0.833009   0.009498  \n",
       "13    0.667326   0.018519  \n",
       "14    0.836520   0.008832  \n",
       "15    0.833009   0.009498  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_xgb0</th>\n",
       "      <th>y_pred_xgb1</th>\n",
       "      <th>y_pred_xgb2</th>\n",
       "      <th>y_pred_xgb3</th>\n",
       "      <th>y_pred_xgb4</th>\n",
       "      <th>y_pred_xgb_ave</th>\n",
       "      <th>y_pred_xgb_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3921050</td>\n",
       "      <td>0</td>\n",
       "      <td>6.17</td>\n",
       "      <td>5.715065</td>\n",
       "      <td>5.710178</td>\n",
       "      <td>5.713224</td>\n",
       "      <td>5.744065</td>\n",
       "      <td>5.650393</td>\n",
       "      <td>5.783821</td>\n",
       "      <td>0.174953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL270476</td>\n",
       "      <td>1</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.779842</td>\n",
       "      <td>6.810263</td>\n",
       "      <td>6.766532</td>\n",
       "      <td>6.821313</td>\n",
       "      <td>6.652802</td>\n",
       "      <td>6.771792</td>\n",
       "      <td>0.056242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3664128</td>\n",
       "      <td>2</td>\n",
       "      <td>7.62</td>\n",
       "      <td>7.129533</td>\n",
       "      <td>7.077412</td>\n",
       "      <td>7.113159</td>\n",
       "      <td>7.443276</td>\n",
       "      <td>7.137381</td>\n",
       "      <td>7.253460</td>\n",
       "      <td>0.204080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4456250</td>\n",
       "      <td>3</td>\n",
       "      <td>5.26</td>\n",
       "      <td>5.845671</td>\n",
       "      <td>6.146075</td>\n",
       "      <td>5.704451</td>\n",
       "      <td>5.991127</td>\n",
       "      <td>5.541411</td>\n",
       "      <td>5.748122</td>\n",
       "      <td>0.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2408818</td>\n",
       "      <td>4</td>\n",
       "      <td>6.32</td>\n",
       "      <td>6.046592</td>\n",
       "      <td>6.088707</td>\n",
       "      <td>6.207484</td>\n",
       "      <td>6.108141</td>\n",
       "      <td>5.992596</td>\n",
       "      <td>6.127253</td>\n",
       "      <td>0.108073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4250302</td>\n",
       "      <td>4487</td>\n",
       "      <td>10.25</td>\n",
       "      <td>9.153563</td>\n",
       "      <td>9.422625</td>\n",
       "      <td>9.378964</td>\n",
       "      <td>9.293154</td>\n",
       "      <td>9.494086</td>\n",
       "      <td>9.498732</td>\n",
       "      <td>0.352601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL483893</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.621792</td>\n",
       "      <td>7.651759</td>\n",
       "      <td>7.516408</td>\n",
       "      <td>7.731095</td>\n",
       "      <td>7.819549</td>\n",
       "      <td>7.695100</td>\n",
       "      <td>0.111179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL3655914</td>\n",
       "      <td>4489</td>\n",
       "      <td>6.69</td>\n",
       "      <td>6.644430</td>\n",
       "      <td>6.719728</td>\n",
       "      <td>6.684826</td>\n",
       "      <td>6.609362</td>\n",
       "      <td>6.621881</td>\n",
       "      <td>6.661705</td>\n",
       "      <td>0.039425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL467876</td>\n",
       "      <td>4490</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.337439</td>\n",
       "      <td>7.231032</td>\n",
       "      <td>7.286335</td>\n",
       "      <td>7.270732</td>\n",
       "      <td>7.203191</td>\n",
       "      <td>7.288122</td>\n",
       "      <td>0.065481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4458544</td>\n",
       "      <td>4491</td>\n",
       "      <td>7.37</td>\n",
       "      <td>7.031149</td>\n",
       "      <td>7.117301</td>\n",
       "      <td>7.147614</td>\n",
       "      <td>6.972671</td>\n",
       "      <td>6.307689</td>\n",
       "      <td>6.991070</td>\n",
       "      <td>0.329895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_xgb0  y_pred_xgb1  \\\n",
       "0         CHEMBL3921050            0     6.17     5.715065     5.710178   \n",
       "1          CHEMBL270476            1     6.80     6.779842     6.810263   \n",
       "2         CHEMBL3664128            2     7.62     7.129533     7.077412   \n",
       "3         CHEMBL4456250            3     5.26     5.845671     6.146075   \n",
       "4         CHEMBL2408818            4     6.32     6.046592     6.088707   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "4487      CHEMBL4250302         4487    10.25     9.153563     9.422625   \n",
       "4488       CHEMBL483893         4488     7.83     7.621792     7.651759   \n",
       "4489      CHEMBL3655914         4489     6.69     6.644430     6.719728   \n",
       "4490       CHEMBL467876         4490     7.40     7.337439     7.231032   \n",
       "4491      CHEMBL4458544         4491     7.37     7.031149     7.117301   \n",
       "\n",
       "      y_pred_xgb2  y_pred_xgb3  y_pred_xgb4  y_pred_xgb_ave  y_pred_xgb_std  \n",
       "0        5.713224     5.744065     5.650393        5.783821        0.174953  \n",
       "1        6.766532     6.821313     6.652802        6.771792        0.056242  \n",
       "2        7.113159     7.443276     7.137381        7.253460        0.204080  \n",
       "3        5.704451     5.991127     5.541411        5.748122        0.291500  \n",
       "4        6.207484     6.108141     5.992596        6.127253        0.108073  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "4487     9.378964     9.293154     9.494086        9.498732        0.352601  \n",
       "4488     7.516408     7.731095     7.819549        7.695100        0.111179  \n",
       "4489     6.684826     6.609362     6.621881        6.661705        0.039425  \n",
       "4490     7.286335     7.270732     7.203191        7.288122        0.065481  \n",
       "4491     7.147614     6.972671     6.307689        6.991070        0.329895  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where((y_pred_optimized_xgb >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id, xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "xgb_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1124bd7-9f1e-4bf2-9f96-468dd7034eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.712577</td>\n",
       "      <td>0.023918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.836689</td>\n",
       "      <td>0.015774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.840652</td>\n",
       "      <td>0.022426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.850831</td>\n",
       "      <td>0.024673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.821040</td>\n",
       "      <td>0.026468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.845376</td>\n",
       "      <td>0.016258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>0.015762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.835903</td>\n",
       "      <td>0.015788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.835935</td>\n",
       "      <td>0.015745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.672629</td>\n",
       "      <td>0.031665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.832742</td>\n",
       "      <td>0.025264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.835935</td>\n",
       "      <td>0.015745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.712577     0.023918\n",
       "1              Accuracy         0.836689     0.015774\n",
       "2             Precision         0.840652     0.022426\n",
       "3           Sensitivity         0.850831     0.024673\n",
       "4           Specificity         0.821040     0.026468\n",
       "5              F1 score         0.845376     0.016258\n",
       "6   F1 score (weighted)         0.836600     0.015762\n",
       "7      F1 score (macro)         0.835903     0.015788\n",
       "8     Balanced Accuracy         0.835935     0.015745\n",
       "9                   MCC         0.672629     0.031665\n",
       "10                  NPV         0.832742     0.025264\n",
       "11              ROC_AUC         0.835935     0.015745"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6569 with a standard deviation of 0.0236\n",
      "XGBoost optimized model r2_score 0.7120 with a standard deviation of 0.0210\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb.joblib']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"OUTPUT/xgb_reg.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.649669     0.027233\n",
      "1                    TP       193.900000     6.999206\n",
      "2                    TN       170.400000     8.921883\n",
      "3                    FP        42.700000     3.888730\n",
      "4                    FN        42.200000     6.795423\n",
      "5              Accuracy         0.810993     0.014677\n",
      "6             Precision         0.819698     0.012074\n",
      "7           Sensitivity         0.821538     0.026219\n",
      "8           Specificity         0.799330     0.020868\n",
      "9              F1 score         0.820355     0.013351\n",
      "10  F1 score (weighted)         0.810960     0.014626\n",
      "11     F1 score (macro)         0.810273     0.015021\n",
      "12    Balanced Accuracy         0.810431     0.014821\n",
      "13                  MCC         0.621124     0.030010\n",
      "14                  NPV         0.801690     0.029070\n",
      "15              ROC_AUC         0.810431     0.014821\n",
      "CPU times: user 5.05 s, sys: 12.4 s, total: 17.4 s\n",
      "Wall time: 757 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=16)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=16)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:44:17,979] A new study created in memory with name: KNNregressor\n",
      "[I 2023-12-20 05:44:20,269] Trial 0 finished with value: 0.6289826848833393 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 0 with value: 0.6289826848833393.\n",
      "[I 2023-12-20 05:44:22,360] Trial 1 finished with value: 0.6355712732730321 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 68}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:24,607] Trial 2 finished with value: 0.5475692374892493 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:26,936] Trial 3 finished with value: 0.5508250840738482 and parameters: {'n_neighbors': 19, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 60}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:28,798] Trial 4 finished with value: 0.6287994523210143 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 72}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:29,654] Trial 5 finished with value: 0.6329367338660347 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:30,446] Trial 6 finished with value: 0.5562045516357272 and parameters: {'n_neighbors': 18, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:32,613] Trial 7 finished with value: 0.6354670023442666 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:33,367] Trial 8 finished with value: 0.6289826848833393 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:34,091] Trial 9 finished with value: 0.6235105395441558 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:34,783] Trial 10 finished with value: 0.5303250815116312 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:37,002] Trial 11 finished with value: 0.6091088546804018 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 63}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:38,965] Trial 12 finished with value: 0.6091088546804018 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 46}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:41,131] Trial 13 finished with value: 0.6091088546804018 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 59}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:43,460] Trial 14 finished with value: 0.6355712732730321 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 76}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:45,562] Trial 15 finished with value: 0.6355712732730321 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:46,911] Trial 16 finished with value: 0.5334106893117723 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 77}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:47,977] Trial 17 finished with value: 0.604327069200618 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:49,199] Trial 18 finished with value: 0.5766879182551328 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:49,868] Trial 19 finished with value: 0.5384610124904846 and parameters: {'n_neighbors': 21, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:50,279] Trial 20 finished with value: 0.6355712732730321 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:50,728] Trial 21 finished with value: 0.6355712732730321 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 80}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:51,450] Trial 22 finished with value: 0.621206480894309 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 88}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:53,837] Trial 23 finished with value: 0.6355712732730321 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:44:56,675] Trial 24 finished with value: 0.604327069200618 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 83}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:00,009] Trial 25 finished with value: 0.5725374932263062 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 64}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:02,841] Trial 26 finished with value: 0.621206480894309 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 54}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:05,285] Trial 27 finished with value: 0.6289826848833393 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 73}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:06,228] Trial 28 finished with value: 0.604327069200618 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:07,912] Trial 29 finished with value: 0.6289826848833393 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:10,484] Trial 30 finished with value: 0.6329367338660347 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 66}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:12,106] Trial 31 finished with value: 0.6355712732730321 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:12,781] Trial 32 finished with value: 0.6355712732730321 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:15,545] Trial 33 finished with value: 0.621206480894309 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 78}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:17,393] Trial 34 finished with value: 0.6289826848833393 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 70}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:20,379] Trial 35 finished with value: 0.5165777014891432 and parameters: {'n_neighbors': 25, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 55}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:23,584] Trial 36 finished with value: 0.6355712732730321 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 59}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:24,941] Trial 37 finished with value: 0.6146562062697609 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 74}. Best is trial 1 with value: 0.6355712732730321.\n",
      "[I 2023-12-20 05:45:25,743] Trial 38 finished with value: 0.6465768494641717 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:26,505] Trial 39 finished with value: 0.6465768494641717 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:27,270] Trial 40 finished with value: 0.6465768494641717 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:28,026] Trial 41 finished with value: 0.6465768494641717 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:28,767] Trial 42 finished with value: 0.6465768494641717 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:29,567] Trial 43 finished with value: 0.6308703830450133 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:30,375] Trial 44 finished with value: 0.6465768494641717 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:31,199] Trial 45 finished with value: 0.6421130803001148 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:32,075] Trial 46 finished with value: 0.6246467548331875 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:32,846] Trial 47 finished with value: 0.6465768494641717 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:33,657] Trial 48 finished with value: 0.6132597228174979 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:34,527] Trial 49 finished with value: 0.6308703830450133 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 38 with value: 0.6465768494641717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6466\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 8\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 83\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.635546\n",
      "1                    TP  373.000000\n",
      "2                    TN  341.000000\n",
      "3                    FP   85.000000\n",
      "4                    FN  100.000000\n",
      "5              Accuracy    0.794216\n",
      "6             Precision    0.814410\n",
      "7           Sensitivity    0.788584\n",
      "8           Specificity    0.800500\n",
      "9              F1 score    0.801289\n",
      "10  F1 score (weighted)    0.794338\n",
      "11     F1 score (macro)    0.793955\n",
      "12    Balanced Accuracy    0.794526\n",
      "13                  MCC    0.588353\n",
      "14                  NPV    0.773200\n",
      "15              ROC_AUC    0.794526\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_0_cat = np.where((y_pred_knn_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:45:35,599] Trial 50 finished with value: 0.6302558896832988 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:36,376] Trial 51 finished with value: 0.6319357125401476 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:37,123] Trial 52 finished with value: 0.6327958919332486 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:38,027] Trial 53 finished with value: 0.6345259794347854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:38,964] Trial 54 finished with value: 0.6319357125401476 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:39,871] Trial 55 finished with value: 0.6334814090702995 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:40,736] Trial 56 finished with value: 0.6241472485772463 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:41,634] Trial 57 finished with value: 0.6319357125401476 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:42,453] Trial 58 finished with value: 0.5930008087597629 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:43,184] Trial 59 finished with value: 0.6151230772038128 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:43,936] Trial 60 finished with value: 0.6334814090702995 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:44,767] Trial 61 finished with value: 0.6302558896832988 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:45,593] Trial 62 finished with value: 0.6319357125401476 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:46,486] Trial 63 finished with value: 0.6327958919332486 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:47,269] Trial 64 finished with value: 0.6334814090702995 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:50,339] Trial 65 finished with value: 0.6145108455201742 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:51,215] Trial 66 finished with value: 0.6319357125401476 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:52,065] Trial 67 finished with value: 0.6345259794347854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:52,939] Trial 68 finished with value: 0.6327958919332486 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:53,814] Trial 69 finished with value: 0.5570763922321837 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:56,440] Trial 70 finished with value: 0.6293236204913096 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:57,306] Trial 71 finished with value: 0.6302558896832988 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:58,123] Trial 72 finished with value: 0.6302558896832988 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:58,960] Trial 73 finished with value: 0.6345259794347854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:45:59,760] Trial 74 finished with value: 0.6319357125401476 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:00,499] Trial 75 finished with value: 0.6270970426674106 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:01,219] Trial 76 finished with value: 0.6327958919332486 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:01,972] Trial 77 finished with value: 0.6334814090702995 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:02,867] Trial 78 finished with value: 0.5790922224321987 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:03,699] Trial 79 finished with value: 0.6345259794347854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:06,291] Trial 80 finished with value: 0.6233080324173214 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:07,227] Trial 81 finished with value: 0.6319357125401476 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:08,038] Trial 82 finished with value: 0.6302558896832988 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:08,879] Trial 83 finished with value: 0.6334814090702995 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:09,699] Trial 84 finished with value: 0.6345259794347854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:10,500] Trial 85 finished with value: 0.6319357125401476 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:11,316] Trial 86 finished with value: 0.6270970426674106 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:12,123] Trial 87 finished with value: 0.6329495279356163 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:14,910] Trial 88 finished with value: 0.609976621776241 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 88}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:15,839] Trial 89 finished with value: 0.6000132641561431 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:16,675] Trial 90 finished with value: 0.6327958919332486 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:17,456] Trial 91 finished with value: 0.6214965280425988 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 74}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:17,877] Trial 92 finished with value: 0.6142370956005265 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 76}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:19,365] Trial 93 finished with value: 0.6183615025966452 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 80}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:22,125] Trial 94 finished with value: 0.6077644328519314 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 90}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:22,954] Trial 95 finished with value: 0.6214965280425988 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:25,509] Trial 96 finished with value: 0.6293236204913096 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:26,387] Trial 97 finished with value: 0.6345259794347854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:27,183] Trial 98 finished with value: 0.6000203501874752 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:29,471] Trial 99 finished with value: 0.6233080324173214 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 82}. Best is trial 38 with value: 0.6465768494641717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6466\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 8\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 83\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.635546    0.641951\n",
      "1                    TP  373.000000  388.000000\n",
      "2                    TN  341.000000  325.000000\n",
      "3                    FP   85.000000   97.000000\n",
      "4                    FN  100.000000   89.000000\n",
      "5              Accuracy    0.794216    0.793103\n",
      "6             Precision    0.814410    0.800000\n",
      "7           Sensitivity    0.788584    0.813417\n",
      "8           Specificity    0.800500    0.770100\n",
      "9              F1 score    0.801289    0.806653\n",
      "10  F1 score (weighted)    0.794338    0.792974\n",
      "11     F1 score (macro)    0.793955    0.792082\n",
      "12    Balanced Accuracy    0.794526    0.791780\n",
      "13                  MCC    0.588353    0.584291\n",
      "14                  NPV    0.773200    0.785000\n",
      "15              ROC_AUC    0.794526    0.791780\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_1_cat = np.where((y_pred_knn_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:46:30,573] Trial 100 finished with value: 0.6321896626638315 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:32,819] Trial 101 finished with value: 0.6132379031791084 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 66}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:34,598] Trial 102 finished with value: 0.6070460004580231 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 86}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:35,916] Trial 103 finished with value: 0.6121925053680879 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 91}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:38,213] Trial 104 finished with value: 0.6132379031791084 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 84}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:39,171] Trial 105 finished with value: 0.6070460004580231 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:40,452] Trial 106 finished with value: 0.6244010542757833 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 68}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:42,644] Trial 107 finished with value: 0.6111743605144768 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 88}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:43,621] Trial 108 finished with value: 0.6299015927141138 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:44,480] Trial 109 finished with value: 0.6195743490087414 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:45,299] Trial 110 finished with value: 0.6318992242696344 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:47,539] Trial 111 finished with value: 0.6132379031791084 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 82}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:49,830] Trial 112 finished with value: 0.6121925053680879 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 62}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:51,492] Trial 113 finished with value: 0.6132379031791084 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 68}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:52,122] Trial 114 finished with value: 0.6111743605144768 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 58}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:53,631] Trial 115 finished with value: 0.6121925053680879 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 78}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:54,571] Trial 116 finished with value: 0.6299015927141138 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:55,429] Trial 117 finished with value: 0.5998936351431527 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:58,221] Trial 118 finished with value: 0.6240622620071546 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:46:59,171] Trial 119 finished with value: 0.6084318138456805 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:00,081] Trial 120 finished with value: 0.6277703067658891 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:02,398] Trial 121 finished with value: 0.6132379031791084 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:04,393] Trial 122 finished with value: 0.6070460004580231 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:06,418] Trial 123 finished with value: 0.6121925053680879 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 70}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:09,039] Trial 124 finished with value: 0.6132379031791084 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 79}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:10,461] Trial 125 finished with value: 0.6121925053680879 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 83}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:11,234] Trial 126 finished with value: 0.6321896626638315 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:13,894] Trial 127 finished with value: 0.595991965311389 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:14,857] Trial 128 finished with value: 0.6318992242696344 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:17,905] Trial 129 finished with value: 0.6180093472279149 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 88}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:18,862] Trial 130 finished with value: 0.6132379031791084 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:21,381] Trial 131 finished with value: 0.6121925053680879 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 74}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:24,488] Trial 132 finished with value: 0.6132379031791084 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 72}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:26,586] Trial 133 finished with value: 0.6111743605144768 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 80}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:27,572] Trial 134 finished with value: 0.6121925053680879 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 79}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:28,383] Trial 135 finished with value: 0.5634066584039032 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:30,284] Trial 136 finished with value: 0.6070460004580231 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:31,245] Trial 137 finished with value: 0.6321896626638315 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:33,395] Trial 138 finished with value: 0.6229132840149634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 82}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:34,323] Trial 139 finished with value: 0.6026637663222676 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:35,159] Trial 140 finished with value: 0.631086218316835 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:37,687] Trial 141 finished with value: 0.6132379031791084 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 67}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:41,395] Trial 142 finished with value: 0.6132379031791084 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 65}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:44,694] Trial 143 finished with value: 0.6121925053680879 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 73}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:47,663] Trial 144 finished with value: 0.6111743605144768 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 71}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:49,313] Trial 145 finished with value: 0.6111743605144768 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:50,202] Trial 146 finished with value: 0.6164884991393229 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:51,935] Trial 147 finished with value: 0.5998936351431527 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 78}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:54,003] Trial 148 finished with value: 0.6229132840149634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:54,994] Trial 149 finished with value: 0.631086218316835 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 38 with value: 0.6465768494641717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6466\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 8\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 83\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.635546    0.641951    0.676110\n",
      "1                    TP  373.000000  388.000000  369.000000\n",
      "2                    TN  341.000000  325.000000  358.000000\n",
      "3                    FP   85.000000   97.000000   94.000000\n",
      "4                    FN  100.000000   89.000000   78.000000\n",
      "5              Accuracy    0.794216    0.793103    0.808676\n",
      "6             Precision    0.814410    0.800000    0.796976\n",
      "7           Sensitivity    0.788584    0.813417    0.825503\n",
      "8           Specificity    0.800500    0.770100    0.792000\n",
      "9              F1 score    0.801289    0.806653    0.810989\n",
      "10  F1 score (weighted)    0.794338    0.792974    0.808635\n",
      "11     F1 score (macro)    0.793955    0.792082    0.808648\n",
      "12    Balanced Accuracy    0.794526    0.791780    0.808769\n",
      "13                  MCC    0.588353    0.584291    0.617808\n",
      "14                  NPV    0.773200    0.785000    0.821100\n",
      "15              ROC_AUC    0.794526    0.791780    0.808769\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_2_cat = np.where((y_pred_knn_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:47:56,096] Trial 150 finished with value: 0.6115972275656645 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:47:58,328] Trial 151 finished with value: 0.6298989191668013 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 89}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:00,792] Trial 152 finished with value: 0.6219562128124304 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:03,629] Trial 153 finished with value: 0.6298989191668013 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:05,169] Trial 154 finished with value: 0.606989939914574 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:06,216] Trial 155 finished with value: 0.6340825438876094 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 91}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:07,028] Trial 156 finished with value: 0.6298989191668013 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:08,755] Trial 157 finished with value: 0.567558363683586 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 88}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:09,648] Trial 158 finished with value: 0.6170600546687796 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:12,454] Trial 159 finished with value: 0.6340825438876094 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 80}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:13,408] Trial 160 finished with value: 0.6115972275656645 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:17,238] Trial 161 finished with value: 0.6298989191668013 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 63}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:21,078] Trial 162 finished with value: 0.6298989191668013 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 53}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:23,260] Trial 163 finished with value: 0.6170600546687796 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 58}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:26,667] Trial 164 finished with value: 0.6219562128124304 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 60}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:27,394] Trial 165 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:29,108] Trial 166 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:31,333] Trial 167 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:35,051] Trial 168 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 94}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:37,443] Trial 169 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:41,165] Trial 170 finished with value: 0.6340825438876094 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:43,851] Trial 171 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:46,392] Trial 172 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 94}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:47,960] Trial 173 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:48,415] Trial 174 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:48,870] Trial 175 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:49,325] Trial 176 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:50,628] Trial 177 finished with value: 0.6390388483103719 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 94}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:52,817] Trial 178 finished with value: 0.6340825438876094 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:54,881] Trial 179 finished with value: 0.6274679971493262 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:57,684] Trial 180 finished with value: 0.6426587300091879 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:58,558] Trial 181 finished with value: 0.6426587300091879 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:48:59,406] Trial 182 finished with value: 0.6426587300091879 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:00,296] Trial 183 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:01,147] Trial 184 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:02,028] Trial 185 finished with value: 0.6374076282281793 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:02,942] Trial 186 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:03,827] Trial 187 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:04,682] Trial 188 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:05,532] Trial 189 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:06,358] Trial 190 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:07,120] Trial 191 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:07,920] Trial 192 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:08,697] Trial 193 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:09,492] Trial 194 finished with value: 0.6374076282281793 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:10,296] Trial 195 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:11,172] Trial 196 finished with value: 0.6374076282281793 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:11,951] Trial 197 finished with value: 0.6374076282281793 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:12,714] Trial 198 finished with value: 0.635676566406625 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 38 with value: 0.6465768494641717.\n",
      "[I 2023-12-20 05:49:13,552] Trial 199 finished with value: 0.639195114726359 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 38 with value: 0.6465768494641717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6466\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 8\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 83\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.635546    0.641951    0.676110    0.681134\n",
      "1                    TP  373.000000  388.000000  369.000000  401.000000\n",
      "2                    TN  341.000000  325.000000  358.000000  350.000000\n",
      "3                    FP   85.000000   97.000000   94.000000   70.000000\n",
      "4                    FN  100.000000   89.000000   78.000000   78.000000\n",
      "5              Accuracy    0.794216    0.793103    0.808676    0.835373\n",
      "6             Precision    0.814410    0.800000    0.796976    0.851380\n",
      "7           Sensitivity    0.788584    0.813417    0.825503    0.837161\n",
      "8           Specificity    0.800500    0.770100    0.792000    0.833300\n",
      "9              F1 score    0.801289    0.806653    0.810989    0.844211\n",
      "10  F1 score (weighted)    0.794338    0.792974    0.808635    0.835456\n",
      "11     F1 score (macro)    0.793955    0.792082    0.808648    0.834841\n",
      "12    Balanced Accuracy    0.794526    0.791780    0.808769    0.835247\n",
      "13                  MCC    0.588353    0.584291    0.617808    0.669815\n",
      "14                  NPV    0.773200    0.785000    0.821100    0.817800\n",
      "15              ROC_AUC    0.794526    0.791780    0.808769    0.835247\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_3_cat = np.where((y_pred_knn_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:49:14,594] Trial 200 finished with value: 0.6469465511004584 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:15,468] Trial 201 finished with value: 0.6469465511004584 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:16,264] Trial 202 finished with value: 0.6469465511004584 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:17,044] Trial 203 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:17,903] Trial 204 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:18,716] Trial 205 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:19,518] Trial 206 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:20,284] Trial 207 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:21,102] Trial 208 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:21,907] Trial 209 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:22,766] Trial 210 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:23,584] Trial 211 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:24,274] Trial 212 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:25,004] Trial 213 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:25,827] Trial 214 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:26,592] Trial 215 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:27,324] Trial 216 finished with value: 0.6417725072828928 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:28,091] Trial 217 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:28,946] Trial 218 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:29,835] Trial 219 finished with value: 0.6417725072828928 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:30,728] Trial 220 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:31,607] Trial 221 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:32,494] Trial 222 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:33,308] Trial 223 finished with value: 0.6430513874878276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:34,061] Trial 224 finished with value: 0.6417725072828928 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:34,877] Trial 225 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:35,736] Trial 226 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:36,554] Trial 227 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:37,387] Trial 228 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:38,198] Trial 229 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:39,020] Trial 230 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:39,894] Trial 231 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:40,610] Trial 232 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:41,336] Trial 233 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:42,130] Trial 234 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:42,951] Trial 235 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:43,773] Trial 236 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:44,594] Trial 237 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:45,446] Trial 238 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:46,232] Trial 239 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:47,037] Trial 240 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:47,780] Trial 241 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:48,622] Trial 242 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:49,472] Trial 243 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:50,300] Trial 244 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:51,174] Trial 245 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:52,036] Trial 246 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:52,932] Trial 247 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:53,812] Trial 248 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6469465511004584.\n",
      "[I 2023-12-20 05:49:54,652] Trial 249 finished with value: 0.6461161500710887 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6469465511004584.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6469\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 97\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.635546    0.641951    0.676110    0.681134   \n",
      "1                    TP  373.000000  388.000000  369.000000  401.000000   \n",
      "2                    TN  341.000000  325.000000  358.000000  350.000000   \n",
      "3                    FP   85.000000   97.000000   94.000000   70.000000   \n",
      "4                    FN  100.000000   89.000000   78.000000   78.000000   \n",
      "5              Accuracy    0.794216    0.793103    0.808676    0.835373   \n",
      "6             Precision    0.814410    0.800000    0.796976    0.851380   \n",
      "7           Sensitivity    0.788584    0.813417    0.825503    0.837161   \n",
      "8           Specificity    0.800500    0.770100    0.792000    0.833300   \n",
      "9              F1 score    0.801289    0.806653    0.810989    0.844211   \n",
      "10  F1 score (weighted)    0.794338    0.792974    0.808635    0.835456   \n",
      "11     F1 score (macro)    0.793955    0.792082    0.808648    0.834841   \n",
      "12    Balanced Accuracy    0.794526    0.791780    0.808769    0.835247   \n",
      "13                  MCC    0.588353    0.584291    0.617808    0.669815   \n",
      "14                  NPV    0.773200    0.785000    0.821100    0.817800   \n",
      "15              ROC_AUC    0.794526    0.791780    0.808769    0.835247   \n",
      "\n",
      "          Set4  \n",
      "0     0.649843  \n",
      "1   397.000000  \n",
      "2   337.000000  \n",
      "3    86.000000  \n",
      "4    79.000000  \n",
      "5     0.816463  \n",
      "6     0.821946  \n",
      "7     0.834034  \n",
      "8     0.796700  \n",
      "9     0.827946  \n",
      "10    0.816367  \n",
      "11    0.815642  \n",
      "12    0.815362  \n",
      "13    0.631383  \n",
      "14    0.810100  \n",
      "15    0.815362  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_4_cat = np.where((y_pred_knn_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:49:55,682] Trial 250 finished with value: 0.6566178244224047 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 250 with value: 0.6566178244224047.\n",
      "[I 2023-12-20 05:49:56,565] Trial 251 finished with value: 0.6566178244224047 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 250 with value: 0.6566178244224047.\n",
      "[I 2023-12-20 05:49:57,345] Trial 252 finished with value: 0.6566178244224047 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 250 with value: 0.6566178244224047.\n",
      "[I 2023-12-20 05:49:58,061] Trial 253 finished with value: 0.6566178244224047 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 250 with value: 0.6566178244224047.\n",
      "[I 2023-12-20 05:49:58,769] Trial 254 finished with value: 0.6566178244224047 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 250 with value: 0.6566178244224047.\n",
      "[I 2023-12-20 05:49:59,545] Trial 255 finished with value: 0.6597890415031527 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:00,272] Trial 256 finished with value: 0.6597890415031527 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:00,998] Trial 257 finished with value: 0.6597890415031527 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:01,836] Trial 258 finished with value: 0.6597890415031527 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:02,640] Trial 259 finished with value: 0.6597890415031527 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:03,509] Trial 260 finished with value: 0.6597890415031527 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:04,312] Trial 261 finished with value: 0.6597890415031527 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:07,343] Trial 262 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:09,541] Trial 263 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:10,043] Trial 264 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:12,305] Trial 265 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:14,308] Trial 266 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:14,725] Trial 267 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:15,140] Trial 268 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:15,550] Trial 269 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:15,955] Trial 270 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:17,461] Trial 271 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:18,901] Trial 272 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:21,693] Trial 273 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:24,237] Trial 274 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:27,277] Trial 275 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:29,306] Trial 276 finished with value: 0.6516755836767205 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:31,773] Trial 277 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:32,645] Trial 278 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:33,201] Trial 279 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:35,239] Trial 280 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:37,531] Trial 281 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:40,094] Trial 282 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:42,229] Trial 283 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:44,582] Trial 284 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:46,921] Trial 285 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:50,044] Trial 286 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:51,493] Trial 287 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:52,965] Trial 288 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:57,013] Trial 289 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:50:59,985] Trial 290 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:01,678] Trial 291 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:05,487] Trial 292 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:07,300] Trial 293 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:07,812] Trial 294 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:09,463] Trial 295 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:12,209] Trial 296 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:13,951] Trial 297 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:16,509] Trial 298 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:20,355] Trial 299 finished with value: 0.6539644638799896 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6598\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.635546    0.641951    0.676110    0.681134   \n",
      "1                    TP  373.000000  388.000000  369.000000  401.000000   \n",
      "2                    TN  341.000000  325.000000  358.000000  350.000000   \n",
      "3                    FP   85.000000   97.000000   94.000000   70.000000   \n",
      "4                    FN  100.000000   89.000000   78.000000   78.000000   \n",
      "5              Accuracy    0.794216    0.793103    0.808676    0.835373   \n",
      "6             Precision    0.814410    0.800000    0.796976    0.851380   \n",
      "7           Sensitivity    0.788584    0.813417    0.825503    0.837161   \n",
      "8           Specificity    0.800500    0.770100    0.792000    0.833300   \n",
      "9              F1 score    0.801289    0.806653    0.810989    0.844211   \n",
      "10  F1 score (weighted)    0.794338    0.792974    0.808635    0.835456   \n",
      "11     F1 score (macro)    0.793955    0.792082    0.808648    0.834841   \n",
      "12    Balanced Accuracy    0.794526    0.791780    0.808769    0.835247   \n",
      "13                  MCC    0.588353    0.584291    0.617808    0.669815   \n",
      "14                  NPV    0.773200    0.785000    0.821100    0.817800   \n",
      "15              ROC_AUC    0.794526    0.791780    0.808769    0.835247   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.649843    0.634471  \n",
      "1   397.000000  375.000000  \n",
      "2   337.000000  357.000000  \n",
      "3    86.000000   81.000000  \n",
      "4    79.000000   86.000000  \n",
      "5     0.816463    0.814238  \n",
      "6     0.821946    0.822368  \n",
      "7     0.834034    0.813449  \n",
      "8     0.796700    0.815100  \n",
      "9     0.827946    0.817884  \n",
      "10    0.816367    0.814259  \n",
      "11    0.815642    0.814164  \n",
      "12    0.815362    0.814259  \n",
      "13    0.631383    0.628377  \n",
      "14    0.810100    0.805900  \n",
      "15    0.815362    0.814259  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_5_cat = np.where((y_pred_knn_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:51:23,405] Trial 300 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:24,884] Trial 301 finished with value: 0.5680560338215636 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:26,382] Trial 302 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:29,302] Trial 303 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:30,677] Trial 304 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:32,369] Trial 305 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:34,618] Trial 306 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:35,955] Trial 307 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:38,703] Trial 308 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:42,351] Trial 309 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:43,397] Trial 310 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:44,713] Trial 311 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:48,807] Trial 312 finished with value: 0.5989120963795639 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:51,334] Trial 313 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:52,448] Trial 314 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:54,463] Trial 315 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:56,195] Trial 316 finished with value: 0.6107543317656772 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:51:58,203] Trial 317 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:02,373] Trial 318 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:04,830] Trial 319 finished with value: 0.5572508351266519 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:08,228] Trial 320 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:10,630] Trial 321 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:13,402] Trial 322 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:15,208] Trial 323 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:16,261] Trial 324 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:18,597] Trial 325 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:20,939] Trial 326 finished with value: 0.5357940697014172 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:24,943] Trial 327 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:27,442] Trial 328 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:30,891] Trial 329 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:33,007] Trial 330 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:35,447] Trial 331 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:37,435] Trial 332 finished with value: 0.5884636849350829 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:37,860] Trial 333 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:38,773] Trial 334 finished with value: 0.6523398953450832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:41,104] Trial 335 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:44,691] Trial 336 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:46,970] Trial 337 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:49,187] Trial 338 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:53,504] Trial 339 finished with value: 0.6523398953450832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:57,026] Trial 340 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:57,943] Trial 341 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:52:58,368] Trial 342 finished with value: 0.6523398953450832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:00,300] Trial 343 finished with value: 0.5405327036516288 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:02,448] Trial 344 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:06,157] Trial 345 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:08,132] Trial 346 finished with value: 0.6515715098374768 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:11,694] Trial 347 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:13,967] Trial 348 finished with value: 0.6523398953450832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:16,416] Trial 349 finished with value: 0.646821130305159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6598\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.635546    0.641951    0.676110    0.681134   \n",
      "1                    TP  373.000000  388.000000  369.000000  401.000000   \n",
      "2                    TN  341.000000  325.000000  358.000000  350.000000   \n",
      "3                    FP   85.000000   97.000000   94.000000   70.000000   \n",
      "4                    FN  100.000000   89.000000   78.000000   78.000000   \n",
      "5              Accuracy    0.794216    0.793103    0.808676    0.835373   \n",
      "6             Precision    0.814410    0.800000    0.796976    0.851380   \n",
      "7           Sensitivity    0.788584    0.813417    0.825503    0.837161   \n",
      "8           Specificity    0.800500    0.770100    0.792000    0.833300   \n",
      "9              F1 score    0.801289    0.806653    0.810989    0.844211   \n",
      "10  F1 score (weighted)    0.794338    0.792974    0.808635    0.835456   \n",
      "11     F1 score (macro)    0.793955    0.792082    0.808648    0.834841   \n",
      "12    Balanced Accuracy    0.794526    0.791780    0.808769    0.835247   \n",
      "13                  MCC    0.588353    0.584291    0.617808    0.669815   \n",
      "14                  NPV    0.773200    0.785000    0.821100    0.817800   \n",
      "15              ROC_AUC    0.794526    0.791780    0.808769    0.835247   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.649843    0.634471    0.639424  \n",
      "1   397.000000  375.000000  382.000000  \n",
      "2   337.000000  357.000000  349.000000  \n",
      "3    86.000000   81.000000   80.000000  \n",
      "4    79.000000   86.000000   88.000000  \n",
      "5     0.816463    0.814238    0.813126  \n",
      "6     0.821946    0.822368    0.826840  \n",
      "7     0.834034    0.813449    0.812766  \n",
      "8     0.796700    0.815100    0.813500  \n",
      "9     0.827946    0.817884    0.819742  \n",
      "10    0.816367    0.814259    0.813187  \n",
      "11    0.815642    0.814164    0.812874  \n",
      "12    0.815362    0.814259    0.813143  \n",
      "13    0.631383    0.628377    0.625876  \n",
      "14    0.810100    0.805900    0.798600  \n",
      "15    0.815362    0.814259    0.813143  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_6_cat = np.where((y_pred_knn_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:53:20,342] Trial 350 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:22,455] Trial 351 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:24,592] Trial 352 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:26,354] Trial 353 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:27,245] Trial 354 finished with value: 0.6474218855525604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:30,493] Trial 355 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:32,326] Trial 356 finished with value: 0.6498688771596486 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:34,905] Trial 357 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:35,722] Trial 358 finished with value: 0.6483729940480587 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:36,564] Trial 359 finished with value: 0.5958756811885048 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:39,033] Trial 360 finished with value: 0.6143140303238942 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:39,918] Trial 361 finished with value: 0.6474218855525604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:41,657] Trial 362 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:42,592] Trial 363 finished with value: 0.6483729940480587 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:46,673] Trial 364 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:47,634] Trial 365 finished with value: 0.6474218855525604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:50,132] Trial 366 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:50,983] Trial 367 finished with value: 0.6483729940480587 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:52,680] Trial 368 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:53,479] Trial 369 finished with value: 0.6498688771596486 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:55,898] Trial 370 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:56,868] Trial 371 finished with value: 0.6474218855525604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:53:59,384] Trial 372 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:00,283] Trial 373 finished with value: 0.6498688771596486 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:02,966] Trial 374 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:06,357] Trial 375 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:07,198] Trial 376 finished with value: 0.6474218855525604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:09,060] Trial 377 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:09,812] Trial 378 finished with value: 0.6498688771596486 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:10,746] Trial 379 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:11,620] Trial 380 finished with value: 0.6474218855525604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:13,866] Trial 381 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:14,750] Trial 382 finished with value: 0.6492821620712429 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:19,320] Trial 383 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:20,248] Trial 384 finished with value: 0.6498688771596486 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:23,076] Trial 385 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:23,961] Trial 386 finished with value: 0.6474218855525604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:26,285] Trial 387 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:27,091] Trial 388 finished with value: 0.6483729940480587 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:28,907] Trial 389 finished with value: 0.639947498292258 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:29,796] Trial 390 finished with value: 0.6483729940480587 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:31,748] Trial 391 finished with value: 0.6433843084641178 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:32,690] Trial 392 finished with value: 0.6498688771596486 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:35,443] Trial 393 finished with value: 0.6421114238233396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:36,413] Trial 394 finished with value: 0.6492821620712429 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:39,467] Trial 395 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:40,404] Trial 396 finished with value: 0.6498688771596486 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:43,280] Trial 397 finished with value: 0.6397294176712497 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:44,206] Trial 398 finished with value: 0.6498688771596486 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:45,972] Trial 399 finished with value: 0.5970594253527304 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6598\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.635546    0.641951    0.676110    0.681134   \n",
      "1                    TP  373.000000  388.000000  369.000000  401.000000   \n",
      "2                    TN  341.000000  325.000000  358.000000  350.000000   \n",
      "3                    FP   85.000000   97.000000   94.000000   70.000000   \n",
      "4                    FN  100.000000   89.000000   78.000000   78.000000   \n",
      "5              Accuracy    0.794216    0.793103    0.808676    0.835373   \n",
      "6             Precision    0.814410    0.800000    0.796976    0.851380   \n",
      "7           Sensitivity    0.788584    0.813417    0.825503    0.837161   \n",
      "8           Specificity    0.800500    0.770100    0.792000    0.833300   \n",
      "9              F1 score    0.801289    0.806653    0.810989    0.844211   \n",
      "10  F1 score (weighted)    0.794338    0.792974    0.808635    0.835456   \n",
      "11     F1 score (macro)    0.793955    0.792082    0.808648    0.834841   \n",
      "12    Balanced Accuracy    0.794526    0.791780    0.808769    0.835247   \n",
      "13                  MCC    0.588353    0.584291    0.617808    0.669815   \n",
      "14                  NPV    0.773200    0.785000    0.821100    0.817800   \n",
      "15              ROC_AUC    0.794526    0.791780    0.808769    0.835247   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.649843    0.634471    0.639424    0.647010  \n",
      "1   397.000000  375.000000  382.000000  413.000000  \n",
      "2   337.000000  357.000000  349.000000  326.000000  \n",
      "3    86.000000   81.000000   80.000000   87.000000  \n",
      "4    79.000000   86.000000   88.000000   73.000000  \n",
      "5     0.816463    0.814238    0.813126    0.822024  \n",
      "6     0.821946    0.822368    0.826840    0.826000  \n",
      "7     0.834034    0.813449    0.812766    0.849794  \n",
      "8     0.796700    0.815100    0.813500    0.789300  \n",
      "9     0.827946    0.817884    0.819742    0.837728  \n",
      "10    0.816367    0.814259    0.813187    0.821754  \n",
      "11    0.815642    0.814164    0.812874    0.820342  \n",
      "12    0.815362    0.814259    0.813143    0.819570  \n",
      "13    0.631383    0.628377    0.625876    0.641089  \n",
      "14    0.810100    0.805900    0.798600    0.817000  \n",
      "15    0.815362    0.814259    0.813143    0.819570  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_7_cat = np.where((y_pred_knn_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:54:47,010] Trial 400 finished with value: 0.6494847122584546 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:49,330] Trial 401 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:50,218] Trial 402 finished with value: 0.647507203600221 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:52,764] Trial 403 finished with value: 0.6417927802099428 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:56,920] Trial 404 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:54:57,910] Trial 405 finished with value: 0.6498234531892699 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:00,745] Trial 406 finished with value: 0.6440231044888772 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:01,662] Trial 407 finished with value: 0.5789175473070589 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:02,901] Trial 408 finished with value: 0.6358691049498725 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:03,738] Trial 409 finished with value: 0.6498234531892699 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:06,080] Trial 410 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:07,042] Trial 411 finished with value: 0.647507203600221 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:09,771] Trial 412 finished with value: 0.6417927802099428 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:10,706] Trial 413 finished with value: 0.6494847122584546 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:13,136] Trial 414 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:13,981] Trial 415 finished with value: 0.6498234531892699 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:16,731] Trial 416 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:17,828] Trial 417 finished with value: 0.647507203600221 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:20,813] Trial 418 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:21,640] Trial 419 finished with value: 0.6277570589755107 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:22,748] Trial 420 finished with value: 0.6417927802099428 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 56}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:23,624] Trial 421 finished with value: 0.6494847122584546 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:25,720] Trial 422 finished with value: 0.6358691049498725 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:26,611] Trial 423 finished with value: 0.6498234531892699 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:28,812] Trial 424 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:29,668] Trial 425 finished with value: 0.5885204759174387 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:32,096] Trial 426 finished with value: 0.6417927802099428 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:33,067] Trial 427 finished with value: 0.6494847122584546 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:35,471] Trial 428 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:36,378] Trial 429 finished with value: 0.6498234531892699 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:38,587] Trial 430 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:41,677] Trial 431 finished with value: 0.6358691049498725 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:42,604] Trial 432 finished with value: 0.6494847122584546 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:44,769] Trial 433 finished with value: 0.6214752369690684 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:45,665] Trial 434 finished with value: 0.6498234531892699 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:47,471] Trial 435 finished with value: 0.6358691049498725 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:48,409] Trial 436 finished with value: 0.6492657180805589 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:51,911] Trial 437 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:53,606] Trial 438 finished with value: 0.6498234531892699 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:55,545] Trial 439 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:56,474] Trial 440 finished with value: 0.6498234531892699 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:57,443] Trial 441 finished with value: 0.6440231044888772 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:55:58,276] Trial 442 finished with value: 0.647507203600221 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:00,390] Trial 443 finished with value: 0.6417927802099428 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:01,303] Trial 444 finished with value: 0.6492657180805589 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:03,777] Trial 445 finished with value: 0.6358691049498725 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:04,640] Trial 446 finished with value: 0.6494847122584546 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:06,654] Trial 447 finished with value: 0.6417927802099428 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:07,536] Trial 448 finished with value: 0.6471434220810195 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:09,094] Trial 449 finished with value: 0.6398575796400168 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 255 with value: 0.6597890415031527.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6598\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.635546    0.641951    0.676110    0.681134   \n",
      "1                    TP  373.000000  388.000000  369.000000  401.000000   \n",
      "2                    TN  341.000000  325.000000  358.000000  350.000000   \n",
      "3                    FP   85.000000   97.000000   94.000000   70.000000   \n",
      "4                    FN  100.000000   89.000000   78.000000   78.000000   \n",
      "5              Accuracy    0.794216    0.793103    0.808676    0.835373   \n",
      "6             Precision    0.814410    0.800000    0.796976    0.851380   \n",
      "7           Sensitivity    0.788584    0.813417    0.825503    0.837161   \n",
      "8           Specificity    0.800500    0.770100    0.792000    0.833300   \n",
      "9              F1 score    0.801289    0.806653    0.810989    0.844211   \n",
      "10  F1 score (weighted)    0.794338    0.792974    0.808635    0.835456   \n",
      "11     F1 score (macro)    0.793955    0.792082    0.808648    0.834841   \n",
      "12    Balanced Accuracy    0.794526    0.791780    0.808769    0.835247   \n",
      "13                  MCC    0.588353    0.584291    0.617808    0.669815   \n",
      "14                  NPV    0.773200    0.785000    0.821100    0.817800   \n",
      "15              ROC_AUC    0.794526    0.791780    0.808769    0.835247   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.649843    0.634471    0.639424    0.647010    0.656149  \n",
      "1   397.000000  375.000000  382.000000  413.000000  399.000000  \n",
      "2   337.000000  357.000000  349.000000  326.000000  340.000000  \n",
      "3    86.000000   81.000000   80.000000   87.000000   86.000000  \n",
      "4    79.000000   86.000000   88.000000   73.000000   74.000000  \n",
      "5     0.816463    0.814238    0.813126    0.822024    0.822024  \n",
      "6     0.821946    0.822368    0.826840    0.826000    0.822680  \n",
      "7     0.834034    0.813449    0.812766    0.849794    0.843552  \n",
      "8     0.796700    0.815100    0.813500    0.789300    0.798100  \n",
      "9     0.827946    0.817884    0.819742    0.837728    0.832985  \n",
      "10    0.816367    0.814259    0.813187    0.821754    0.821868  \n",
      "11    0.815642    0.814164    0.812874    0.820342    0.821255  \n",
      "12    0.815362    0.814259    0.813143    0.819570    0.820837  \n",
      "13    0.631383    0.628377    0.625876    0.641089    0.642804  \n",
      "14    0.810100    0.805900    0.798600    0.817000    0.821300  \n",
      "15    0.815362    0.814259    0.813143    0.819570    0.820837  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_8_cat = np.where((y_pred_knn_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:56:10,118] Trial 450 finished with value: 0.6580275148577343 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:10,973] Trial 451 finished with value: 0.6544876311001973 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:11,819] Trial 452 finished with value: 0.6544876311001973 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:12,687] Trial 453 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:13,535] Trial 454 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:14,373] Trial 455 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:15,181] Trial 456 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:15,919] Trial 457 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:16,674] Trial 458 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:17,427] Trial 459 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:18,193] Trial 460 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:18,977] Trial 461 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:19,787] Trial 462 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:20,588] Trial 463 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:21,401] Trial 464 finished with value: 0.6480414631014724 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:22,191] Trial 465 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:22,955] Trial 466 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:23,682] Trial 467 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:24,508] Trial 468 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:25,354] Trial 469 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:26,153] Trial 470 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:26,962] Trial 471 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:27,849] Trial 472 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:28,666] Trial 473 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:29,419] Trial 474 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:30,157] Trial 475 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:30,847] Trial 476 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:31,526] Trial 477 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:32,327] Trial 478 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:33,111] Trial 479 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:33,956] Trial 480 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:34,751] Trial 481 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:35,667] Trial 482 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:36,487] Trial 483 finished with value: 0.6480414631014724 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:37,283] Trial 484 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:38,140] Trial 485 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:38,980] Trial 486 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:39,752] Trial 487 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:40,640] Trial 488 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:41,501] Trial 489 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:42,239] Trial 490 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:43,102] Trial 491 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:43,955] Trial 492 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:44,733] Trial 493 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:45,500] Trial 494 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:46,277] Trial 495 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:47,086] Trial 496 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:47,894] Trial 497 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:48,672] Trial 498 finished with value: 0.6544521686337166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 255 with value: 0.6597890415031527.\n",
      "[I 2023-12-20 05:56:49,436] Trial 499 finished with value: 0.6517682808028843 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 255 with value: 0.6597890415031527.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6598\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.635546    0.641951    0.676110    0.681134   \n",
      "1                    TP  373.000000  388.000000  369.000000  401.000000   \n",
      "2                    TN  341.000000  325.000000  358.000000  350.000000   \n",
      "3                    FP   85.000000   97.000000   94.000000   70.000000   \n",
      "4                    FN  100.000000   89.000000   78.000000   78.000000   \n",
      "5              Accuracy    0.794216    0.793103    0.808676    0.835373   \n",
      "6             Precision    0.814410    0.800000    0.796976    0.851380   \n",
      "7           Sensitivity    0.788584    0.813417    0.825503    0.837161   \n",
      "8           Specificity    0.800500    0.770100    0.792000    0.833300   \n",
      "9              F1 score    0.801289    0.806653    0.810989    0.844211   \n",
      "10  F1 score (weighted)    0.794338    0.792974    0.808635    0.835456   \n",
      "11     F1 score (macro)    0.793955    0.792082    0.808648    0.834841   \n",
      "12    Balanced Accuracy    0.794526    0.791780    0.808769    0.835247   \n",
      "13                  MCC    0.588353    0.584291    0.617808    0.669815   \n",
      "14                  NPV    0.773200    0.785000    0.821100    0.817800   \n",
      "15              ROC_AUC    0.794526    0.791780    0.808769    0.835247   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.649843    0.634471    0.639424    0.647010    0.656149    0.612768  \n",
      "1   397.000000  375.000000  382.000000  413.000000  399.000000  381.000000  \n",
      "2   337.000000  357.000000  349.000000  326.000000  340.000000  330.000000  \n",
      "3    86.000000   81.000000   80.000000   87.000000   86.000000  113.000000  \n",
      "4    79.000000   86.000000   88.000000   73.000000   74.000000   75.000000  \n",
      "5     0.816463    0.814238    0.813126    0.822024    0.822024    0.790879  \n",
      "6     0.821946    0.822368    0.826840    0.826000    0.822680    0.771255  \n",
      "7     0.834034    0.813449    0.812766    0.849794    0.843552    0.835526  \n",
      "8     0.796700    0.815100    0.813500    0.789300    0.798100    0.744900  \n",
      "9     0.827946    0.817884    0.819742    0.837728    0.832985    0.802105  \n",
      "10    0.816367    0.814259    0.813187    0.821754    0.821868    0.790376  \n",
      "11    0.815642    0.814164    0.812874    0.820342    0.821255    0.790204  \n",
      "12    0.815362    0.814259    0.813143    0.819570    0.820837    0.790224  \n",
      "13    0.631383    0.628377    0.625876    0.641089    0.642804    0.583252  \n",
      "14    0.810100    0.805900    0.798600    0.817000    0.821300    0.814800  \n",
      "15    0.815362    0.814259    0.813143    0.819570    0.820837    0.790224  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_9_cat = np.where((y_pred_knn_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.661129</td>\n",
       "      <td>0.029776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>195.400000</td>\n",
       "      <td>5.891614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>172.700000</td>\n",
       "      <td>8.083591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>40.400000</td>\n",
       "      <td>2.633122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>40.700000</td>\n",
       "      <td>5.457920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.819455</td>\n",
       "      <td>0.010744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.828734</td>\n",
       "      <td>0.008207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.827878</td>\n",
       "      <td>0.020079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.810200</td>\n",
       "      <td>0.014135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.828140</td>\n",
       "      <td>0.009078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.819468</td>\n",
       "      <td>0.010646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.818836</td>\n",
       "      <td>0.011022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.819036</td>\n",
       "      <td>0.010677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.638041</td>\n",
       "      <td>0.022004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.809270</td>\n",
       "      <td>0.024743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.819036</td>\n",
       "      <td>0.010677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.661129     0.029776\n",
       "1                    TP       195.400000     5.891614\n",
       "2                    TN       172.700000     8.083591\n",
       "3                    FP        40.400000     2.633122\n",
       "4                    FN        40.700000     5.457920\n",
       "5              Accuracy         0.819455     0.010744\n",
       "6             Precision         0.828734     0.008207\n",
       "7           Sensitivity         0.827878     0.020079\n",
       "8           Specificity         0.810200     0.014135\n",
       "9              F1 score         0.828140     0.009078\n",
       "10  F1 score (weighted)         0.819468     0.010646\n",
       "11     F1 score (macro)         0.818836     0.011022\n",
       "12    Balanced Accuracy         0.819036     0.010677\n",
       "13                  MCC         0.638041     0.022004\n",
       "14                  NPV         0.809270     0.024743\n",
       "15              ROC_AUC         0.819036     0.010677"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.635546</td>\n",
       "      <td>0.641951</td>\n",
       "      <td>0.676110</td>\n",
       "      <td>0.681134</td>\n",
       "      <td>0.649843</td>\n",
       "      <td>0.634471</td>\n",
       "      <td>0.639424</td>\n",
       "      <td>0.647010</td>\n",
       "      <td>0.656149</td>\n",
       "      <td>0.612768</td>\n",
       "      <td>0.647441</td>\n",
       "      <td>0.020125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>387.800000</td>\n",
       "      <td>14.281301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>341.300000</td>\n",
       "      <td>12.055888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>87.900000</td>\n",
       "      <td>11.532081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>8.563488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.794216</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.808676</td>\n",
       "      <td>0.835373</td>\n",
       "      <td>0.816463</td>\n",
       "      <td>0.814238</td>\n",
       "      <td>0.813126</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.790879</td>\n",
       "      <td>0.811012</td>\n",
       "      <td>0.014507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.814410</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.796976</td>\n",
       "      <td>0.851380</td>\n",
       "      <td>0.821946</td>\n",
       "      <td>0.822368</td>\n",
       "      <td>0.826840</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.822680</td>\n",
       "      <td>0.771255</td>\n",
       "      <td>0.815386</td>\n",
       "      <td>0.021624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.788584</td>\n",
       "      <td>0.813417</td>\n",
       "      <td>0.825503</td>\n",
       "      <td>0.837161</td>\n",
       "      <td>0.834034</td>\n",
       "      <td>0.813449</td>\n",
       "      <td>0.812766</td>\n",
       "      <td>0.849794</td>\n",
       "      <td>0.843552</td>\n",
       "      <td>0.835526</td>\n",
       "      <td>0.825379</td>\n",
       "      <td>0.018383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.770100</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.796700</td>\n",
       "      <td>0.815100</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.789300</td>\n",
       "      <td>0.798100</td>\n",
       "      <td>0.744900</td>\n",
       "      <td>0.795350</td>\n",
       "      <td>0.024554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.801289</td>\n",
       "      <td>0.806653</td>\n",
       "      <td>0.810989</td>\n",
       "      <td>0.844211</td>\n",
       "      <td>0.827946</td>\n",
       "      <td>0.817884</td>\n",
       "      <td>0.819742</td>\n",
       "      <td>0.837728</td>\n",
       "      <td>0.832985</td>\n",
       "      <td>0.802105</td>\n",
       "      <td>0.820153</td>\n",
       "      <td>0.015152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.794338</td>\n",
       "      <td>0.792974</td>\n",
       "      <td>0.808635</td>\n",
       "      <td>0.835456</td>\n",
       "      <td>0.816367</td>\n",
       "      <td>0.814259</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.821754</td>\n",
       "      <td>0.821868</td>\n",
       "      <td>0.790376</td>\n",
       "      <td>0.810921</td>\n",
       "      <td>0.014566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.793955</td>\n",
       "      <td>0.792082</td>\n",
       "      <td>0.808648</td>\n",
       "      <td>0.834841</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.814164</td>\n",
       "      <td>0.812874</td>\n",
       "      <td>0.820342</td>\n",
       "      <td>0.821255</td>\n",
       "      <td>0.790204</td>\n",
       "      <td>0.810400</td>\n",
       "      <td>0.014448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.794526</td>\n",
       "      <td>0.791780</td>\n",
       "      <td>0.808769</td>\n",
       "      <td>0.835247</td>\n",
       "      <td>0.815362</td>\n",
       "      <td>0.814259</td>\n",
       "      <td>0.813143</td>\n",
       "      <td>0.819570</td>\n",
       "      <td>0.820837</td>\n",
       "      <td>0.790224</td>\n",
       "      <td>0.810372</td>\n",
       "      <td>0.014399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.588353</td>\n",
       "      <td>0.584291</td>\n",
       "      <td>0.617808</td>\n",
       "      <td>0.669815</td>\n",
       "      <td>0.631383</td>\n",
       "      <td>0.628377</td>\n",
       "      <td>0.625876</td>\n",
       "      <td>0.641089</td>\n",
       "      <td>0.642804</td>\n",
       "      <td>0.583252</td>\n",
       "      <td>0.621305</td>\n",
       "      <td>0.028471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.773200</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.821100</td>\n",
       "      <td>0.817800</td>\n",
       "      <td>0.810100</td>\n",
       "      <td>0.805900</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.821300</td>\n",
       "      <td>0.814800</td>\n",
       "      <td>0.806480</td>\n",
       "      <td>0.016282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.794526</td>\n",
       "      <td>0.791780</td>\n",
       "      <td>0.808769</td>\n",
       "      <td>0.835247</td>\n",
       "      <td>0.815362</td>\n",
       "      <td>0.814259</td>\n",
       "      <td>0.813143</td>\n",
       "      <td>0.819570</td>\n",
       "      <td>0.820837</td>\n",
       "      <td>0.790224</td>\n",
       "      <td>0.810372</td>\n",
       "      <td>0.014399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.635546    0.641951    0.676110    0.681134   \n",
       "1                    TP  373.000000  388.000000  369.000000  401.000000   \n",
       "2                    TN  341.000000  325.000000  358.000000  350.000000   \n",
       "3                    FP   85.000000   97.000000   94.000000   70.000000   \n",
       "4                    FN  100.000000   89.000000   78.000000   78.000000   \n",
       "5              Accuracy    0.794216    0.793103    0.808676    0.835373   \n",
       "6             Precision    0.814410    0.800000    0.796976    0.851380   \n",
       "7           Sensitivity    0.788584    0.813417    0.825503    0.837161   \n",
       "8           Specificity    0.800500    0.770100    0.792000    0.833300   \n",
       "9              F1 score    0.801289    0.806653    0.810989    0.844211   \n",
       "10  F1 score (weighted)    0.794338    0.792974    0.808635    0.835456   \n",
       "11     F1 score (macro)    0.793955    0.792082    0.808648    0.834841   \n",
       "12    Balanced Accuracy    0.794526    0.791780    0.808769    0.835247   \n",
       "13                  MCC    0.588353    0.584291    0.617808    0.669815   \n",
       "14                  NPV    0.773200    0.785000    0.821100    0.817800   \n",
       "15              ROC_AUC    0.794526    0.791780    0.808769    0.835247   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.649843    0.634471    0.639424    0.647010    0.656149    0.612768   \n",
       "1   397.000000  375.000000  382.000000  413.000000  399.000000  381.000000   \n",
       "2   337.000000  357.000000  349.000000  326.000000  340.000000  330.000000   \n",
       "3    86.000000   81.000000   80.000000   87.000000   86.000000  113.000000   \n",
       "4    79.000000   86.000000   88.000000   73.000000   74.000000   75.000000   \n",
       "5     0.816463    0.814238    0.813126    0.822024    0.822024    0.790879   \n",
       "6     0.821946    0.822368    0.826840    0.826000    0.822680    0.771255   \n",
       "7     0.834034    0.813449    0.812766    0.849794    0.843552    0.835526   \n",
       "8     0.796700    0.815100    0.813500    0.789300    0.798100    0.744900   \n",
       "9     0.827946    0.817884    0.819742    0.837728    0.832985    0.802105   \n",
       "10    0.816367    0.814259    0.813187    0.821754    0.821868    0.790376   \n",
       "11    0.815642    0.814164    0.812874    0.820342    0.821255    0.790204   \n",
       "12    0.815362    0.814259    0.813143    0.819570    0.820837    0.790224   \n",
       "13    0.631383    0.628377    0.625876    0.641089    0.642804    0.583252   \n",
       "14    0.810100    0.805900    0.798600    0.817000    0.821300    0.814800   \n",
       "15    0.815362    0.814259    0.813143    0.819570    0.820837    0.790224   \n",
       "\n",
       "           ave        std  \n",
       "0     0.647441   0.020125  \n",
       "1   387.800000  14.281301  \n",
       "2   341.300000  12.055888  \n",
       "3    87.900000  11.532081  \n",
       "4    82.000000   8.563488  \n",
       "5     0.811012   0.014507  \n",
       "6     0.815386   0.021624  \n",
       "7     0.825379   0.018383  \n",
       "8     0.795350   0.024554  \n",
       "9     0.820153   0.015152  \n",
       "10    0.810921   0.014566  \n",
       "11    0.810400   0.014448  \n",
       "12    0.810372   0.014399  \n",
       "13    0.621305   0.028471  \n",
       "14    0.806480   0.016282  \n",
       "15    0.810372   0.014399  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_knn0</th>\n",
       "      <th>y_pred_knn1</th>\n",
       "      <th>y_pred_knn2</th>\n",
       "      <th>y_pred_knn3</th>\n",
       "      <th>y_pred_knn4</th>\n",
       "      <th>y_pred_knn_ave</th>\n",
       "      <th>y_pred_knn_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3921050</td>\n",
       "      <td>0</td>\n",
       "      <td>6.17</td>\n",
       "      <td>5.881382</td>\n",
       "      <td>5.788784</td>\n",
       "      <td>5.881382</td>\n",
       "      <td>5.918187</td>\n",
       "      <td>5.806020</td>\n",
       "      <td>5.907626</td>\n",
       "      <td>0.125682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL270476</td>\n",
       "      <td>1</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.332799</td>\n",
       "      <td>6.597422</td>\n",
       "      <td>6.320035</td>\n",
       "      <td>6.597422</td>\n",
       "      <td>6.413524</td>\n",
       "      <td>6.510200</td>\n",
       "      <td>0.171357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3664128</td>\n",
       "      <td>2</td>\n",
       "      <td>7.62</td>\n",
       "      <td>7.134899</td>\n",
       "      <td>7.185277</td>\n",
       "      <td>7.138537</td>\n",
       "      <td>7.107074</td>\n",
       "      <td>7.141257</td>\n",
       "      <td>7.221174</td>\n",
       "      <td>0.179828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4456250</td>\n",
       "      <td>3</td>\n",
       "      <td>5.26</td>\n",
       "      <td>5.953141</td>\n",
       "      <td>6.236653</td>\n",
       "      <td>5.688565</td>\n",
       "      <td>5.953141</td>\n",
       "      <td>5.651759</td>\n",
       "      <td>5.790543</td>\n",
       "      <td>0.306329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2408818</td>\n",
       "      <td>4</td>\n",
       "      <td>6.32</td>\n",
       "      <td>6.247131</td>\n",
       "      <td>6.247131</td>\n",
       "      <td>6.247131</td>\n",
       "      <td>6.247131</td>\n",
       "      <td>6.245302</td>\n",
       "      <td>6.258971</td>\n",
       "      <td>0.027301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4250302</td>\n",
       "      <td>4487</td>\n",
       "      <td>10.25</td>\n",
       "      <td>9.234397</td>\n",
       "      <td>9.224191</td>\n",
       "      <td>9.234397</td>\n",
       "      <td>9.224191</td>\n",
       "      <td>9.228971</td>\n",
       "      <td>9.399358</td>\n",
       "      <td>0.380442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL483893</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.750982</td>\n",
       "      <td>7.831410</td>\n",
       "      <td>7.701820</td>\n",
       "      <td>7.731933</td>\n",
       "      <td>7.796240</td>\n",
       "      <td>7.773731</td>\n",
       "      <td>0.049050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL3655914</td>\n",
       "      <td>4489</td>\n",
       "      <td>6.69</td>\n",
       "      <td>6.718561</td>\n",
       "      <td>6.749571</td>\n",
       "      <td>6.718561</td>\n",
       "      <td>6.718561</td>\n",
       "      <td>6.718561</td>\n",
       "      <td>6.718969</td>\n",
       "      <td>0.017206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL467876</td>\n",
       "      <td>4490</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>7.183333</td>\n",
       "      <td>0.096896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4458544</td>\n",
       "      <td>4491</td>\n",
       "      <td>7.37</td>\n",
       "      <td>7.364371</td>\n",
       "      <td>7.452308</td>\n",
       "      <td>7.501392</td>\n",
       "      <td>7.461284</td>\n",
       "      <td>6.825435</td>\n",
       "      <td>7.329132</td>\n",
       "      <td>0.230555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_knn0  y_pred_knn1  \\\n",
       "0         CHEMBL3921050            0     6.17     5.881382     5.788784   \n",
       "1          CHEMBL270476            1     6.80     6.332799     6.597422   \n",
       "2         CHEMBL3664128            2     7.62     7.134899     7.185277   \n",
       "3         CHEMBL4456250            3     5.26     5.953141     6.236653   \n",
       "4         CHEMBL2408818            4     6.32     6.247131     6.247131   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "4487      CHEMBL4250302         4487    10.25     9.234397     9.224191   \n",
       "4488       CHEMBL483893         4488     7.83     7.750982     7.831410   \n",
       "4489      CHEMBL3655914         4489     6.69     6.718561     6.749571   \n",
       "4490       CHEMBL467876         4490     7.40     7.140000     7.140000   \n",
       "4491      CHEMBL4458544         4491     7.37     7.364371     7.452308   \n",
       "\n",
       "      y_pred_knn2  y_pred_knn3  y_pred_knn4  y_pred_knn_ave  y_pred_knn_std  \n",
       "0        5.881382     5.918187     5.806020        5.907626        0.125682  \n",
       "1        6.320035     6.597422     6.413524        6.510200        0.171357  \n",
       "2        7.138537     7.107074     7.141257        7.221174        0.179828  \n",
       "3        5.688565     5.953141     5.651759        5.790543        0.306329  \n",
       "4        6.247131     6.247131     6.245302        6.258971        0.027301  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "4487     9.234397     9.224191     9.228971        9.399358        0.380442  \n",
       "4488     7.701820     7.731933     7.796240        7.773731        0.049050  \n",
       "4489     6.718561     6.718561     6.718561        6.718969        0.017206  \n",
       "4490     7.140000     7.140000     7.140000        7.183333        0.096896  \n",
       "4491     7.501392     7.461284     6.825435        7.329132        0.230555  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where((y_pred_optimized_knn >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id, knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "knn_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "efa46682-fb4e-45a7-be4c-9b5504c6dba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.660319</td>\n",
       "      <td>0.033209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.820214</td>\n",
       "      <td>0.018991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.829614</td>\n",
       "      <td>0.021078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.828289</td>\n",
       "      <td>0.031439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.811450</td>\n",
       "      <td>0.024286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.828559</td>\n",
       "      <td>0.019493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.820204</td>\n",
       "      <td>0.018912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.819522</td>\n",
       "      <td>0.019002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.819871</td>\n",
       "      <td>0.018801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.639908</td>\n",
       "      <td>0.038220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.810460</td>\n",
       "      <td>0.032717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.819871</td>\n",
       "      <td>0.018801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.660319     0.033209\n",
       "1              Accuracy         0.820214     0.018991\n",
       "2             Precision         0.829614     0.021078\n",
       "3           Sensitivity         0.828289     0.031439\n",
       "4           Specificity         0.811450     0.024286\n",
       "5              F1 score         0.828559     0.019493\n",
       "6   F1 score (weighted)         0.820204     0.018912\n",
       "7      F1 score (macro)         0.819522     0.019002\n",
       "8     Balanced Accuracy         0.819871     0.018801\n",
       "9                   MCC         0.639908     0.038220\n",
       "10                  NPV         0.810460     0.032717\n",
       "11              ROC_AUC         0.819871     0.018801"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAG8CAYAAADaV3/fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACOlElEQVR4nO3deXhU5dkG8PvMko0khBAgCQECBiygKOqnVVRAq61IpSiiKK0LKmUrrSJhEZEqS8AdgVqxbrgjiFutuGBdernUpSqKBohsCcmQjZBtlvP9cWYmc7aZM8lJJnNy/67LSzLre2YmOc+87/M+jyCKoggiIiIiC7PFegBERERE7Y0BDxEREVkeAx4iIiKyPAY8REREZHkMeIiIiMjyGPAQERGR5THgISIiIstjwENERESWx4CHiIiILI8BTwQ333wzBEHApZdeCq/XG+vhEBERUSt0qYDn2muvhSAIEAQBDocD/fv3x4wZM1BVVaV5++XLl+ORRx7Bww8/jP/85z+YPn266jY7duzAhAkTkJOTg27duuHkk0/G008/3d6HgqamJsyZMwdZWVno1q0bLrnkEhw4cCDsffLz84PHH/rfrFmzgrfRul4QBKxZs6ZNz01ERBRLXSrgAYDf/OY3KC0tRUlJCTZu3IhXX30VM2fOVN3u73//O+655x5s374dN910E/79739j+/btKCwslN3u448/xogRI/DSSy/hf//7H66//nr84Q9/wKuvvtqux/HnP/8ZW7duxXPPPYcPP/wQdXV1GD9+fNhZqM8++wylpaXB/7Zv3w4AuPzyy4O3Cb2+tLQU//jHPyAIAi677LI2PTcREVFMiV3INddcI06YMEF22c033yxmZmbKLnvxxRfF7Oxs8csvv5Rd/vPPP4sFBQViUVFR2OcZN26ceN1115kxZE3V1dWi0+kUn3vuueBlBw8eFG02m/jmm28afpy5c+eKxx13nOjz+XRvM2HCBPG8884z/bmJiIg6Upeb4Qm1Z88evPnmm3A6nbLLJ02ahNLSUpx88smyy/v374+ffvoJ8+fPD/u4NTU1yMzMDHub4cOHIzU1Vfe/4cOH6973v//9L9xuNy688MLgZbm5uTjhhBPw8ccfh33egObmZmzatAnXX389BEHQvM3hw4fx+uuvY9q0aaY+NxERUUdzxHoAHe21115DamoqvF4vGhsbAQD33nuvaY+/efNmfPbZZ3j44YfD3u6NN96A2+3WvV4ZhIUqKytDQkICevToIbu8T58+KCsrMzTOl19+GdXV1bj22mt1b/PEE08gLS0Nl156qanPTURE1NFiHvDs3LkTr7zyCvbu3YuqqirMmzcPp59+OgDA4/Hgueeew5dffony8nKkpKTgxBNPxFVXXRVxBkXP2LFjsWHDBtTX12Pjxo348ccfMWfOHFOOZceOHbj22mvxyCOPhJ2hAYABAwaY8pyhRFHUna1RevTRR3HRRRchNzdX9zb/+Mc/cPXVVyMpKcnU5yYiIupoMV/SampqQn5+Pq6//nrVdc3Nzdi7dy8uu+wyFBUV4ZZbbkFpaSlWr17d6ufr1q0bCgoKMGLECDz44INoamrCsmXL2nIIAID3338fv/3tb3HvvffiD3/4Q8Tbt2VJKzs7G83NzardZeXl5ejTp0/E5/7555/x9ttv44YbbtC9zQcffIBdu3apbtPW5yYiIoqFmM/wjBw5EiNHjtS8LiUlBUuWLJFddt1112HRokVwuVzIyspq8/MvXboUF110EWbMmBF2tiOcHTt2YPz48SgqKsJNN91k6D5tWdI69dRT4XQ6sX37dkyePBmAtLvq22+/NRQMPvbYY+jduzcuvvhi3ds8+uijOPXUU3HSSSeZ+txERESxEPOAJ1r19fUQBAEpKSm6t3G73apgQi+AGDNmDIYPH44VK1bgoYceino8O3bswMUXX4y5c+fisssuC+axJCQkhF12a8uSVvfu3TFt2jTccsst6NmzJzIzMzFv3jyceOKJ+NWvfhW83fnnn4+JEydi9uzZwct8Ph8ee+wxXHPNNXA4tN/+2tpavPjii7jnnnta/dxERESdSVwFPM3NzXjmmWcwatSosAHP1q1bsXnz5uDPo0aNwty5c3Vvf/PNN+O6665DYWEh+vXrF9WYHn/8cdTX12PlypVYuXJl8PLRo0djx44dUT1WNO677z44HA5MnjwZDQ0NOP/88/H444/DbrcHb7N79264XC7Z/d5++23s27dPcwkx4LnnnoMoipgyZUqrn5uIiKgzEURRFGM9iIDJkyfLkpZDeTwe3HvvvThy5AiWLl0a1QyPIAhITk5GVVUVPB5Pu4w9VgRBQFZWFlwuFzrRW2kKHlt8svKxAdY+Ph5bfLLysTkcDtWu4FY/limP0s48Hg/uu+8+VFRU4Pbbbw8b7ADS8pXWEpbH4wmbNxOPAjuj3G635T7oPLb4ZOVjA6x9fDy2+GTlYzNTzHdpRRIIdsrKyrBkyRKkpaXFekhEREQUZ2I+w9PY2CgrWFdeXo6SkhKkpqaiR48euPfee7F3714UFhbC5/OhuroaAJCamqqbdEtEREQUKuYRw+7du2V1cJ588kkAUtLv5Zdfjs8//xwAVO0cli5dGrG4HxERERHQCQKe4cOH44UXXtC9Ptx1REREREZ0+hweIiIiorZiwENERESWx4CHiIiILI8BDxEREVkeAx4iIiKyPAY8REREZHkMeIiIiMjyGPAQERGR5THgISIiIstjwENERESWx4CHiIiILI8BDxEREVkeAx4iIiKyPAY8REREZHkMeIiIiMjyGPAQERGR5THgISIiIstjwENERESWx4CHiIiILI8BDxEREVkeAx4iIiKyPAY8REREZHkMeIiIiMjyGPAQERGR5THgISIiIstjwENERESWx4CHiIiILI8BDxEREVmeI9YDICKKhlhbBd+GVUB1JZCRCduMhRDSM2I9LCLq5DjDQ0RxxbdhFVD8PeA6DBR/D9+GlbEeEhHFAQY8RBRfqivD/0xEpCHmS1o7d+7EK6+8gr1796Kqqgrz5s3D6aefHrz+k08+wdtvv409e/bg6NGjWL16NfLz82M3YCKKrYxMaXYn9GcioghiPsPT1NSE/Px8XH/99brXH3/88bjqqqs6eGRE1BnZZiwECoYCWX2AgqHSz0REEcR8hmfkyJEYOXKk7vXnnnsuAKC8vNzwY7rdbrjd7uDPgiAgOTkZgiBAEITWD7YTChyP1Y4L4LHFq/Y+NqF7D9gWrG6Xxzb0/Hzv4hKPLT6ZeUwxD3jaw9atW7F58+bgzwMHDkRRURGysrJiOKr2lZ2dHeshtBseW3yy8rEB1j4+Hlt8svKxmcGSAc/EiRMxfvz44M+BCNHlcslmfqxAEARkZ2ejrKwMoijGejim4rHFJysfG2Dt4+OxxScrH5vT6TRtssKSAY/T6YTT6VRdLoqi5T4MATy2+MRji19WPj4eW3yy4rGZeTwxT1omIiIiam8MeIiIiMjyYr6k1djYiLKysuDP5eXlKCkpQWpqKrKyslBXVweXy4XKSqm42KFDhwAAGRkZyMjIiMWQiYiIKM7EPODZvXs3li1bFvz5ySefBACMHj0as2bNwueff47169cHr7///vsBAJMmTcLkyZM7dKxEREQUn2Ie8AwfPhwvvPCC7vVjxozBmDFjOm5AREREZDnM4SEiIiLLY8BDRERElseAh4iIiCyPAQ8RERFZHgMeIiIisjwGPERERGR5DHiIiIjI8hjwEBERkeUx4CEiIiLLY8BDRERElseAh4iIiCyPAQ8RERFZHgMeIiIisjwGPERERGR5DHiIiIjI8hjwEBERkeUx4CEiIiLLY8BDRERElseAh4iIiCyPAQ8RERFZHgMeIiIisjxHtHf47rvv8MUXX2DXrl2orKxEc3Mz0tLSkJeXhxNOOAFnnnkm0tPT22OsRERERK1iOODZsWMHtm3bhkOHDiEpKQkDBgzAoEGDkJCQgLq6Ouzbtw+ffvopnnzySZx55pm44oor0KtXr/YcOxEREZEhhgKewsJClJeX45xzzsGsWbMwaNAg2Gzq1bC6ujp8+umneP/99/GXv/wFs2fPxi9/+UvTB01EREQUDUMBzymnnILf/va3SElJCXu71NRUnHfeeTjvvPOwc+dO1NXVmTJIIiIiorYwFPBcccUVUT/wsGHDor4PERERUXvgLi0iIiKyPEMzPDt37ozqQTm7Q0RERJ2JoYBn2bJlUT3o888/36rBEBEREbUHw9vSU1JScOaZZ+LEE0+EIAjtOSYiIiIiUxkKeGbOnIkdO3bgnXfewddff42xY8dizJgxyMrKavMAdu7ciVdeeQV79+5FVVUV5s2bh9NPPz14vSiKePHFF/HOO++grq4OgwcPxrRp09CvX782PzcRERF1DYYCntGjR2P06NE4fPgw3n33XbzzzjvYvHkzhg8fjvPPPx+nn346HI6oizYDAJqampCfn4+xY8finnvuUV2/bds2vP7665g5cyZycnKwZcsW3HXXXbj//vuRnJzcquckIiKiriWqKKVPnz6YMmUKrrjiCnz11Vd499138dBDDyEpKQmTJk3CuHHjoh7AyJEjMXLkSM3rRFHEG2+8gYkTJ+KMM84AAMyaNQs33ngjPvzwQ1xwwQWa93O73XC73cGfBUFAcnIyBEGw3HJc4HisdlwAjy1eWfnYAGsfH48tPnWFYzNDq6ZlbDYbTjnlFAwZMgSvvfYaXn75ZezcubNVAU845eXlqK6uxkknnRS8zOl0YtiwYdi1a5duwLN161Zs3rw5+PPAgQNRVFRkyhJcZ5WdnR3rIbQbHlt8svKxAdY+Ph5bfLLysZmhVQHPV199hffeew+ff/45EhIScN555+HCCy80e2yorq4GAHTv3l12effu3eFyuXTvN3HiRIwfPz74cyBCdLlcspkfKxAEAdnZ2SgrK4MoirEejql4bPHJyscGWPv4eGzxycrH5nQ6TZusMBzwlJeX491338X777+PyspKDBs2DNOnT8cvf/lLJCQkmDIYPcoprUhvqNPphNPpVF0uiqLlPgwBPLb4xGOLX1Y+Ph5bfLLisZl5PIbr8Hz//ffIzMzE6NGjMXbsWPTp08e0QejJyMgAIM309OjRI3h5bW2tataHiIiISI/hSsvJycno378/fv75Zzz++OO6txUEAfPnzzdlcL1790ZGRgb+97//YeDAgQAAj8eDnTt34uqrrzblOYiIiMj6DAU8gfWz/fv3R7xttBnVjY2NKCsrC/5cXl6OkpISpKamIisrC+PGjcPWrVuRk5OD7OxsbN26FYmJiTj77LOjeh4iIiLqugwFPOvWrWu3AezevVvWuuLJJ58EINX+mTVrFiZMmIDm5mZs3LgRx44dQ0FBARYvXswaPERERGRY66oFmmj48OF44YUXdK8XBAGTJ0/G5MmTO3BUREREZCVtDngOHTqEffv2IT09HUOHDrVk4SMiIiKKb4YDnjfffBMfffQRHA4HzjnnHJx33nnYtGkTXnvtteC2sYKCAixZsgRJSUntNmAiIiKiaBkKeN5//3089thj6NWrF5KSkvDwww+joqICr7/+Os4//3wMGDAAe/fuxXvvvYfXXnsNkyZNau9xExERERlmKOB56623cOaZZ2Lu3LkQBAEvv/wynn/+eVxyySWYMmVK8HYpKSn4z3/+w4CHiIiIOhWbkRsdOnQI5557bjA/Z+zYsfD5fDjxxBNltxsxYkTYlg9EREREsWAo4Kmvr0d6enrw57S0NADSjE6olJQUNDY2mjg8IiIiorYzFPAQERERxTPDu7S+++47HDlyBEBLM6/vvvsOFRUVwduUlpaaPDwiIiKitjMc8DzzzDOqyzZt2mTqYIiIiIjag6GAZ+nSpe09DiIi6qLE2ir4NqwCqiuBjEzYZiyEkJ4R62GRxRgKeIYNG9be4yAioi7Kt2EVUPy99IPrMHwbVsJeWBTbQZHlxLyXFhERdXHVleF/biecWepaDAU8Pp8P77//Pvr06ROc7RFFEatXr5bdLiUlBbNmzYLNxs1fRNS1dbWTaZuONyMTcB1u+Tk1Hd6iwnZ/7Tiz1LUYiky++OIL/P3vf0dqamrwMlEU8cUXX2DPnj3Yt28f9u3bh08++QQff/xxuw2WiCheBE+mrsNA8ffwbVgZ6yEZ4q06As+q+fAuvBHeokKItdWG7teW47XNWAgUDAWy+kj/BzrmtYvRzBLFhqEZnh07duCMM85A//79VdcVFhZi0KBBAIAnn3wSH3/8Mc4++2xzR0lEFCPKmQv7zEVATk7kO8bpydS1Yn7rZj3acLxCeobsObwLb2z1Y0VFObOUkdk+z0OdgqEZnt27d+O0006LeLuhQ4di7969bR4UEVFnoZy58K5fYeyOypNnO55MxdoqeIsKo56V0eKtVLQHMhpsmHm8HfTaKWeWbDMWBq8Ta6vgXX4LvDMuk/5bfkubXleKPUMzPDU1NcjKypJdJggCLrroImRkZAQvS0tLQ21trakDJCKKqShmLmSzQanpQP5goK42mIfSXszMRbFnZsFbdrDlAoPBhm3GQmnpKSTvBjCW26O8jTB1FsRN61SPZTblzFIo34ZVQMlPLReU/MQcnzhnKOBxOp2qHlmCIODaa6+VXdbY2AiHgxu/iMhColj2UAYeKBgK+8pH2nmAMHX5LGvxGhxaOjfqYEMveDASjPnW3tUSXLgOQ3z8AdgX39PqYzCF1msYJ8uSpM1QdNKnTx/8+OOPOPnkk8Pe7scff0SfPn3MGBcRUaegnLmwz1ykf+NY5e2YmItiz8iEY8HqYAuhNjPymhwoCf9zLChf08BlFLcM5fCcfPLJ2L59O2pqanRvU11dje3bt+OUU04xbXBERLEWmLmwr3wE9sKi8NujOzBvJ1S4XJSYi9FrEkqsrYJn1XwcmjYBnlXzDeXi2GYslJYkHU7pv/zBnet1pagZmuG5+OKL8e6772LJkiWYOnUqTj75ZCQkJAAAmpub8eWXXwb7ao0bN679RktE1Inp5bG0t3C5KLFm6DXJy5fny+TlmzqGwLKaFwBw0FAujpCeEftlNTKVoYCne/fumD9/PtasWYN77rkHNpsN6enpAIDa2lr4fL7gbQKXExF1NZ058GgPRhKSjbwmtjlLWhUoGi52GKclAshchjOMhwwZggceeABvv/02vvnmG7hc0tbF/v37Y8SIETj//PORkpLSbgMlIqLOxazdYa0NFA0/P+vtEKLspZWSkoJLLrkEl1xySXuNh4ioU+hqrSFaRTlTUlIsFQ3sqNfL4MxNYFnNXlcLb2o6c3G6qKibXs2ePRslJSWa1+3btw+zZ89u65iIiGIuXltDdKhURQqDxx3V69XmgokGE6KF9Aw4FqxG7qPb4FiwmoFrFxV10ZyKigp4PB7N69xuNyoqKto8KCJqO85QtFGc5X10uvfbwOvV1iWxWCWJU3wytUrg4cOHkZycbOZDElErsRN0G8VZ3kdM3u+6MJX162oh1laHD7o0lsQi3idEV0sSp7Yx3Dz0/fffD/68ceNGVWDT3NyMn3/+GcOGDTN3hETUOnE2Q9HZxN3sQQe838pZJKSmy4NCwQaIPunfjQ2Rgy5lUOlxMzCndmMo4Glubpb1yDp27BjcbrfsNk6nE2eddRYmT55s7giJqHXibIais4m72QON99vsZS5V64z8wVLBw0AAVOkCKkPSGiIEXbYZC+ErnCbl/iju0+mW6CjuGQp4LrzwQlx44YUAgFmzZuGWW25Bfn5+e46LiNoo7mYouhizT+ha77dvw0pzl7mUAUxdraxXmLeoUB7wRAiyhfQMIL+gZYwh94nVkiwDLeuKOodn3bp17TGOsBoaGvD888/j008/RU1NDQYOHIhrr70WBQUFHT4WongRdzMUFmLkpGn2CV3z/Y6wzKUcp33mIiAnR/9JIswatibI1r1POy7RhXt/mPtmXa1OWq6pqUFFRQWam5tV15mdx/O3v/0N+/fvx+zZs5GZmYl///vfuPPOO3HfffchM5PT9ERkvrZ80zd00uyIHKsIAYpynN5brsF+QQASEiAsWANb3/6y20cOaKJvOKobmLfjkmzY94e5b5YVdcBTVVWFhx56CN9++63ubZ5//vk2DSpUc3MzPvnkE8yfPz8YSE2ePBmfffYZ3nrrLVx55ZWq+7jdblmOkSAISE5OhiAIEATBtLF1BoHjsdpxATy2eGWVY/NqnBQdC1YbOz6Nk6bq9hondLNfM9vUWfCtuhVobgISEmH7/Sz5cyjHKfqkmKWxAeKqWyE89ILsaqF7D9gWrJbfpaYK3kAQVFcLNDZIV7gOw7d4Ouwr/q4bKMru659hCtzWPnMRvOtXyK9rw+sje9/CvT8d8L6YzSq/c1rMPKaoA55HH30Ue/fuxdVXX40BAwbA6XSaNhgtXq8XPp9P9TwJCQn44YcfNO+zdetWbN68OfjzwIEDUVRUhKysrHYdayxlZ2fHegjthscWn+L92A7V1fqbTUrsdbXICVnuCXd8h3tnoznkpJnQOxt9FEtF3mUPwLX8VngrXbBnZiFr8RrYTU4sL1tdCF8gAGlsgP3p9ci+78mW6zN7wh16cg/V3CQ7Xj2H770N3tAcnFCNDbBvvBt91jwa+b6uw/Lb5uTAe8f9cK2YL71GG+825TXKzs4O+/50xPvSXuL9d669RR3wfP/99/j973+PsWPHtsd4VJKTkzFkyBC89NJL6Nu3LzIyMvDhhx+iuLhY982dOHEixo8fH/w5ECG6XC7V7rJ4JwgCsrOzUVZWBlGMfjq5M+OxxSerHJs3NR3AQdnPpaWlho5PvGEeEDI74b1hHkpLS9U3vPkuCAB8AMobmoCG0rCzHqrniXBbz56fZLd37/lJNg5PuL+HPh/2z54CHK0NOw5PeZn+YwBo/nEnDu363tB9m8vL5ONbNT84y+YtO4hD0yaEnTEKJ/R980V6fzTel87MKr9zWpxOp2mTFa3K4enZs6cpT27U7NmzsWHDBvzxj3+EzWbDwIEDMWrUKOzdu1fz9k6nU3PmSRRFy30YAnhs8YnH1nlp5auEHk/Y40vrrspLMfpaeBU7q7zrV+gmzapuO/96IL8gbL5RYBxibRWwvyT8YPb+pDsOWY5TqNBaPADgcesfg9ZW+tDXSfnYjQ1hXw8jRFFs0/vTmcX775wWM48n6oDnzDPPxBdffIERI0aYNohIsrOzsWzZMjQ2NqKhoQE9evTAfffdh969e3fYGIioa9FKphVrq+DdsEpa7vI3oTRjy7IseNBJmtVKolbd1uMO9rGyFxYBeflAScgsT15+8J++Davk9W8i8VdBBkT41t4FlBRDlaSclAzMvh24/3bN2jpKEZOglQFRmMfSonzNvMseMHxfsh5DAc+ePXuC/z7zzDPx8MMPw+fz4bTTTkNqaqrq9oMGDTJvhCGSkpKQlJSEuro6fP3115g6dWq7PA8RkZbA7h4pt+egaVuWZbuGlPTq0iyeDmTnqQMCQCoACMA2Z4kUUFS6gPo6oLYa3qJCCFNn+gOWKHjcUpFAh6MlMVmpsQF4+Und2jpKkUon2GYshG/RTUBTY8uFyoalYShfM9fyW4Gb7zJ8f7IWQwHPwoXqWgr/+te/8K9//Uvz9mbu0gKAr776CgCQm5uLsrIyPPXUU8jNzcWYMWNMfR4iorDaa8uy8nEcTilICFeXJhB0FAwFin+AbLalvs7/D/9ltdXSjEtjA1BZAXHVfGOzOw4HENos2uOOfL+SYgi33Qdx07qI9Xgibf0X0jOAnH7yWapoKF4zb6UL1tvHREYZCnhmzJjR3uMIq76+Hs8++yyOHDmC1NRUnHHGGZgyZQocDlN7nxIRhRdFbZio6vgoHze/QD3zobW846907C2cJq9wnCLNvOvOHIXOmISTP1h/5kmPxw1x0zpDM1+G6hUpG5SGa1iqpHjN7JlZ8IW5OVmboYgh1jMpZ511Fs4666yYjoGISJg6U5odaW6WivNNnaV722gq9spyWVLTAY8H3oU3tizf1NVK/05MkgcrgYArM0se8GT6d7XozkAJMFIkMDiukmL5zE5iEuD1b9rPyQNK98tngozOfBmZMWtDAUJljlDW4jXSrivqkjhFQkRkkPj4gy1LSY0NEB9/AFh8j/aNo1j+Cs1l8RYVyht0BgSadTocqqUi3eRfrVkhAMjoAfTsDezZBfh05jzsDgjpGbDNWCAlKR8okS7Py4dtzhLZbJVszIHnNcJAMNMSZErFE8MFmar7hryugiBI9XQ6+RZzaj9RBzzr16/Xvc5msyElJQUFBQU4/fTTueRERJ2CaQ0hAyd9vZ9DtXZmItzsSF0tbAtXS8dSWSElLqekAplZmsdkm7EQvrV3qndU9ewNe2GROlAJ5fXAu/wW6d+hOTQOh/bztKJRre7MVsh7JG5aLw8yN60D2NuKWiHqiOS7775DfX096uvrYbPZkJaWhqNHj8Ln8yElJQUA8PrrryM3NxdLly5FRkaG2WMmoi6o3XtbmUyzjk9tVcTZEt1ZGf91qrwcfyKy1jEJ6RnSjFBosCPYgrMksjHWVAFuRW/Ekp/89w+9TNqerkwuthcWBd8j38pbDb1H4Wa2gsfTToni7Ire9diivcMtt9yC5ORkzJ07F08//TT+/ve/4+mnn8af/vQnJCcnY/HixfjrX/+Kuro6PPvss+0xZiKyKLG2Ct6iQngX3ghvUaG/7oskeKJ3HQ7WmjHMrJNmSB0bzZ9DBE7m9pWPwF5YBCE9QzqGkp9adjuV/KQ6DtuMhdIuLdmFNqBgqHbtnYDKCtVrJ9ZWqbefiz5plkQxRuhVXfZ4FT+7dV/71rxHgfcce3bJrwgcp3JmzKQ2D236PFFcinqG58knn8Rvf/tbWRKxzWbDqFGjUFNTgyeeeAJ33nknJkyYgFdffdXUwRKRtbVbF2uTOm8H6trYQwoPGhGcTVCe1AGgulI126AqGDjo+JbXQW8GqP5YsP5O4LWDx6O9jVzztdNJYrbbAUGQP47GLI/m4xp4j3R3kvnfo9Yul0XEruhdTtQBz+7du3HZZZdpXtevX7/grE5+fj6OHj3attERUdcS7iRk4m6dSCdNveUOIT0DjgWrkZOTg9LSUlXZe737RSosqAz0kD9Yqq9TWREMZLxFhbDNWNhyLK5yaRlKAOBMUM/QVFcC1Ue0n7OuVr0LTBAArTL+DkfLjqwAj1vKDVImULfmPVK+5zYbMOj44HukV5ywzUtSJgXBFD+iXtJKTk7Gd999p3ndt99+i+TkZABAc3Nz8N9ERIaEWb6wzVgoBQFZfVqWdwzSWl4KR7XcMe9aeAunqZbZIt4vsEyiN3uQmCTl0yivr6uVTvLpPYI5Oij+Hr61d7YEAFm9pZ5VPp+0Vd3rkT9GRqZ8q3hAUrL0mK7D0ixSyU/SvzV7FgnSY2vNEh0oUR1rq94j5Xvun82K+j2KckmqLZ8nik9Rz/CcffbZ2LZtG0RRxJlnnonu3bujpqYGH3/8MV599VWMGzcOgNSOom/fvqYPmIisK9xMTKQ2BKZSBiCiTwo6KivgXb8CeOApY/cLzUPRWoZqapTyaZTX19XCd/Bndf5N6K6w0Lo7WrSCHUCa1dFrDaHkcBjvt1Vd2ar3qNVLVm1ckurQzxN1ClEHPFdddRWqqqrw8ssv4+WXX5ZdN2rUKEyZMgUAMGTIEJx88slmjJGIuohOcxIKt1Mq5MSqyr1JTddcJtEt4Od/PNvCNdIW89Dt18vmQpVXI4pSgm91JVAV7gQvaLdjsDuA5JQw91NQ5hIlJUvHGJg9Cr2ulUtCrX7PuSRFURLEVvZeP3DgAHbu3Im6ujqkpqZi2LBhyMvLM3t8pqqoqIBbbydCnBIEQTefIN7x2OJTRx9bNLkcRm8r1lbLA5BQ+YPRb92zKC0thWfVfHlujkZhQFmBvoU3qgOpxCQgrbt0n4izKcYqJKt6YAX0GwiUl6pbS9hs6gKEdjtsqx9Tzb4Ejkesrda9riNE+/z8nYtPTqcTvXr1MuWxWl0ZMC8vr9MHOERkfZpdxP2zEMqToNF6PEJ6BmzLH/bPyvwkDx5Cu09q5d6sfER/sFozR02NxntbGYl3HA4gpz+wf4/6OrtDXWsHAAYdD+zfKx+Hwxn2yWI9G2fk+ZUBrnfZAx00OuqMok5aJiLqVLS6iOslsram3UNGT/kVNdU4fOs0eBbcoG5kGWFZRbPGTiiHU0qi1ZOYFPbxAUizTHa79nUHSgCfOogRps4CUrrJL0zpFve1apTjdy2/NdZDohgyNMNzxRVXYPny5SgoKMAVV1wR9raCIOC5554zZXBERBEZzLfRvG1IgCKbDQjdrq0Maurr0Lzz65afQxtpejza9Wn8hPQMIL9Af4t6fgGEqTMg3vEn9XUFQyFMnSUlOWvlAgGAwyHlC63UObHrLJmJm9YBDfXyCxvqW5UY3B4VjFv9mIrxeitdsgk66loMBTyTJk1CZqb0h+Gyyy6DIPAjQ2QmlrnXF3xtKl1AfZ2qd5Rsl8/RGvmyTCBw8Qu3I0hVCydUaLJupUue2+P1tgQS/srJoW0WlO+pqn8UIAVV/tv4Fk/XfiGKv4d4uNT/2P78leIfIFt2cjilz40qCIywFlZdCSSlyI8rKUWdhK14PbW0RxsPvceM+HujeB3smVnQaZVKXYChgOfyyy8P/nvy5MntNhiirioWvZ7iRaTeUbJ+TMtv0d6d5Bc27yPc7IXHA9vCNRDSM6RdUuG2hJcUS8nJdbUtAUTIexox96S5Sf+6DcvhLRjasiusRyZQFVJcMCUVgDqwg6tcvwghIN1G2Qi1sR5AK3Y+tUcFY53HjPR7o3wdshavQXlDmNeXLM3UduY7d+7Eiy++iKVLl5r5sETWxzL3+vReC632BsrlJ+XP4YRbGvO44Vs8HbblD8M+cxFsf1sJ9x5/YGW3y5eKPG7txyn5Cd4Z/ir1Wk1DAxISw9fJCZ2FEhRpmJlZANSBnXeOIhVBEIABBfKZpeW3qGd4lEGQkddT+TqmprdspTcwe6k1a6O7FBnh9yb0dRAEAfaMTKChNPIxkCWZmrRcW1uLnTt3mvmQRF1DOzVItAS918IfhMgqH7fhdZRV3g1sLw/V2ADfhpVSiwmHs6UBaFOjtOSV1Sd8QnKgr1Wgaahi7MEmmkkpUiBjM/DnWfQBEFr6Xe3ZBe+cK+A7uE9+O//MT1CPLNgX3yOvPO0PllqOt16d82Pg9VRWMAagmfis1yhWK1Fatyoyf28oCtylRdQJsMy9vuBrk9kLUKac+oOQYLBQ6ZKCj8xerXgdQ3NhHEDeQPVN/DMI3kCTzoDUdGk7en6B/PJw+Y7+sQcET/TVR6RAZtDxwIzFxsbt9UqtIXw+qWjhynkt19ZWSblPoerrVIFG4HW2Z/eVXm9lkORwGno9lW08VLNCyuWoSG04Qio4K1uD8PeGomHqkhYRtU6sa5p0ZrIcnaJC9Q6n6kp1nk9eftSvp2YDz0DfqQD/DII9MwvesoOqy2U5I6E5PHoCJ/+DJf4EZPl1toIh8CnHYERI4rZvwyr5/QWb9LN/+35oblFoY1TPqvnyXKX8gqgS6WVLU6EiLUdFUUGZvzcUDc7wEFGnobfMEWCbsVAKQkJlZKpPnsXfay/thKNRRNC2/GH/8pZT+s+/7Txr8RrNmYXQmQgjO5oCJ3NxVSFUu6hS07WDlR49pdfAHu77asjMkvK4lJNOOjlSbZ09CQaQgWUxh9PQchRnbai9cIaHiDqNSLtuZBWQQ5JafRtWqhOFGxsgrroVWPu8sSfXmFkQ0jPkDTRLfoJ3/QrYH3gKjgWrw5fx10qC7jcQqCiTdmIlJEoF/wD9nVnKYKRHTynXJnRL+4ESda5Nv3z9cSjHrBOYtXn2RGNmJ9wOKmXQSGQ2QwHPvHnzIt8IQENDlNOuREShDOxW0zohBk+eyuWucFu8FXRr9OiMSaypgjdMLyfbjIXw3b8U2F8CQJTyeQ7tB7z+NhWNDVLBv8Ii7Z1ZdbXqOjjHjrYsM7kOSzMhyoDG4YTtz8ukMdZW+dtihNTh6aheSxGWpowGNqxRRWYxFPCkpqYaKjaYlpaG3r17t3lQRNRFtbIDduDk6Z1zhTxwSEg0/NS6J2CdMXlDAyy9Hl52B2SBhlfR0NMfPAkL1kBc9if/rquQ51E2AFX23CopljqaywIeh1RpWaujuZZotu5HIVyRx2iwRhWZxVDAc8cdd7TzMIgonrTXt+62niSFBWukZazAktGCNaaPyT5zkXSFVg+vQCJw4TRpx9aBveEf3L+cZOvbH+Ldj6uX6vRaRAQElrL6DWyZSQoZR9ht8gGKoNJbdURKWG7je2vG0pRYWyUFdaG06i8RGcAcHiKKWnt9627NSVIVfC1/2OSToc4SUIRChdLrE2Fm3N+DSzeADPccAcF+XwaWqhxOaUYocD+NoNK1Yn6nmVHxbVilzk/yuDnLE+ditUxpKOBxuVzIysqKfEOFysrKYA8uIrKQTlQZ2qzgS6ytgm/tXS3VhXP6SQX9QpOCXYfhXb8CeOAp2H4/S5qBCbdl3G5XL2OF2r83fBuKqTMhrpof/jkyMvVbXeTlS0nXUZxYVDWGYln1W++5WYk8rsVqmdJQwDN37lz86le/wkUXXYTs7Oywt/V4PPjss8+wZcsWnHHGGZg0aZIpAyWiTqSVuTZmCgYoyhyVVp4MfRtWyR9r/x7tGwZq5zy1LnJ9nH4DgdL96tybIFF7Bqe6EmJtVfhgx2YDBh3f0nBUebukZP32FVojqa2Cd8Mq4IgieNJ4b6P5ht6mb/N6M1ysqBzfYvSFyVDAc9ttt+GJJ57Am2++iYKCAgwfPhwDBw5E9+7d4XQ6UVdXh8OHD+PHH3/E119/jcbGRowbNw7jx49v7/ETUQy0Jtcm0okv2hOjKkAJCDkZRvWYRv/o1tXi0LQJUkPOsATY5iyR8nmipVWDR2nQ8S19ohasgXjHHMiWtVLTo1omUBVvBICk5Jat83q3jfANvS3f5oOfs0qXVC06JRXIzGJtnngXoy9MhgKeoUOHYtWqVfjyyy+xfft2/POf/0Rzc7Pqdr1798avf/1rXHDBBejRo4fpgyWizqE1uTaRTnxGToyyAEYrQHE4IUyd2dKsUmepSFOkfBmHU1oeamyQV1nWk5gI8WiNeqeVUVrHJwhAz97B4E32eiQlaVaFVtINApVLWUBw67w4Y4HsPqrbhgsW2/BtvrPX5OGW+dYxawdftKJKWh45ciRGjhwJj8eDkpISVFVVobm5GWlpacjLy2O+DhHpi3TiM3Bi1JyFCJVfAHHTev3b+Hf4AKLqRKWqm5OQCPTJBRrq5bumZLMuIfVtlJoapSUpI8nESrXVUoFBZQBmt0tVnP3UrTYEKSjLyQM8Hik/SHEi1g0slf22ApStO1yHtatd6+kEy5/thVvmWydWgWyrdmk5HA4UFBREvqEJvF4vXnzxRXzwwQeorq5Gjx49MGbMGFx66aWwGekmTEStZuo32EgnvgjXa25RtjtaGnTm5Ufeyu1xw7f2Tiko0DpRJSYhGKA0NwGJSbD9+Q74NqySHldVsyZCMBNtD6yA+jrYFt8D3y1/CH87VbKyKCVYH/i5paaP8kSsvM+eXS1d2rXGq9W6IyVVSog28A09Vt/mO0QnSt6nyDr9tvRt27Zh+/btmDVrFvLy8rBnzx6sX78eKSkpGDduXKyHR2RpZn6DjXTi07s+GHTt/Um942ngYPV4Ii1NHShRB1t7f4L3xglQBTAlP8FXeIN8a7Tg/6IVWiTQbEkpUmCZmCRPePZ45TVo6o9p3185ttATsfI+Pp/0HitnbQAp8VmrdUdmluHPQWdflmoTC89eWVGnD3h+/PFHnHbaaTjllFMASHlCH374IXbv3q17H7fbDbe75Q+UIAhITk6GIAiGKkbHk8DxWO24AB5bp6DxDTbSmPWOTejeA7YFq/Xvp3O9V3cZS4B95iLV89hnLpK2jgf6TSkTmz1u9UyN3tZxrfyb9gx0AhqOSceVmqbY4SXCt2ElHIHXKVlnVkYpI7PldeqWqn2fbqnSVvwDewG3R9pSn5MHQRDkr6m/+GKn/+wqtMfvXGd5XeLm70krmHlMnT7g+cUvfoHt27fj0KFDyM3NRUlJCXbt2oVrrrlG9z5bt27F5s2bgz8PHDgQRUVFraolFC8ilQuIZzy22DncOxvNId9gE3pno09OjqH7mnVsh+pq4dW6wulE7vFD1Zfn5AAPPAUAOHzrNKi3V6D1S03tQEhKgdhYL7+wqQm9kxPh6pOLZuU28eLv4bnxEn9T08gJ0QnDTkLW4jWw+2cfDms9JoCEPrnos+ZR6TXb+bUUBO79CfaNd6PPmkeDr2mAt+oIXCvmw1vpgj0zS/YcnZmpv3Mhn7XOoLP/PYk1QQzb7jf2RFHEs88+i23btsFms8Hn8+HKK6/ExIkTde+jN8Pjcrlkl1uBIAjIzs5GWVlZ+M7NcYjHFhuyppip6VJe7tHalm+wEXJ4lMembLKpfAyt6yGKLZeF7rSSP5H0nzMB6JMD1NerHt+z4IbIlYpjKSER9lUb4b359+rrkpIhzFkC8e7bWj+r5HDC8bctsovE2mppViKw1btbKtAjC/aZi2Dr3gO+xdPlu9Cy+sCxaqPqoT2r5stn3gqGtsw8dUKd+Xeurax8bE6n07TJik4/w/Pxxx/jgw8+wJ/+9Cf069cPJSUlePzxx4PJy1qcTiecTnUPGVEULfdhCOCxxafOeGzKppgoGCrbGWR0vIFjUz6ed/0K2AuLWnJzSorVlYwB+clUsAHdewCN9VLCbNURKQgQRWnJZ9/elvvf/AdpOcZuAzTKZ3QeAuD1wvvgX7WvbmyA+OBf27aElpevfr/Suuvm1IiiCHtmljzgycjUfs81ljs722dZS2f8nTOLFY/NzOMxJeBpbm5GRUUFcnJyTN85tWnTJkyYMAGjRo0CAPTv3x8VFRV4+eWXdQMe6lpYC6P1r4HW/UzfeaLzeLpbzLWeT/QBWb2DJ2rv9N+F2SDl70quuQ7WCSQl+2es/OMM181ct0KzAQVDo9oRFai0bKupksYYqcgfE3YpzkQdnfzzn/+U5cfs2bMHM2bMwM0334y5c+fC5dIoXtUGTU1NqiDKZrNZLoql1gueOF2HgeLvpR0lXUyk10CsrYK3qBDehTfCW1Tor0Wjcz/liauuVnW/qGhtPwf0A6mMTO2TZ2ixu4TE6MfRWbQlfygxScrdCdTbSUzSvZ29sKhVlZZ9FWXSGP07sfQewzZjIZA/2F+Q0Ql4PK37fBB1kKgDnnfffRfdunUL/vz0008jNTUV11xzDURRxJYtW8LcO3qnnnoqtmzZgi+++ALl5eX49NNP8dprr+H//u//TH0eimOshRHxNdANiDTuZ5uxECgYCmT1aZmNaEMwKXu80FkHZVDjcAavt81Y2LL9O6CyAt45V8BbOA3I7N1SfyeQx2N1gg3Cwrth37AF9ke2wb5hC2wr/i69tsqZdXdz9AFqlL9HQnqGP3HaLf1X8lOX/LJB8SPqgMflcqFv374AgIaGBuzcuRNXXXUVxo0bh8mTJ+Prr782dYDXX389fvnLX2Ljxo34y1/+gqeeegoXXHABrrzySlOfh+KY3gxCVxLpNdA7mWncL1A3xb7yESClm/x6rfYDEWnPxqoCoaJHgzMKQnoG0KOn+k6NDVLhvEM/S/k7gPR/i21G0CT6IG5aJ7soWONm0PHy2/pr60QVgLTm94hfNiiORJ3D43a7YbfbAUg1ckRRxIknnggA6NWrF6qrq00dYHJyMq699lpce+21pj4uWYelK7kaFPE10Mm3iHg/ZZG6+rrWNfkMLV64eDpsyx+WFaSTOp/fKRUFBKQqvukZGpWE9XSRJW6dgCL4Pu7ZJQU7EW4f7jHsdbXwpqYb+z1iHg/FkagDnqysLHz//fcYPnw4PvvsM+Tn5yMlJQUAUFtbG/w3UUexdCVXgyK9BnqBTcTXLkVRpC4l1XD15eB28z275Fc0NsiCHkCj83nJT/58Fae8ynFXYrPJgxcAOFojr7Qc5A/4bHb5faIIQIT0DDgWrEZOTg5KS0sN5UnyywbFk6gDnnPOOQebN2/GZ599hp9//hm//31L/Yjdu3cjx2BRMiLqOEaCQs0dW5lZ8lmWzCzDyxiy7ehKjQ0tPa30Op+3ZYdSZ6FsDWGUIKiDFwBoaoRv3rUQlj4IIS2t5f1S1ipyOIH8gnYPQPhlg+JJ1AHPpZdeCrvdjl27duH000/HRRddFLxu//79OOOMM0wdIBG1P7G2Cr7Ff2w5afpnbmTf4FPTgaYmwFUuv7PeLEKk5ZQDJdadvcnqI70ulS51wJOULL2WR8pb8pCURFH/tRF9EFfdCjEvXz+gzMhkIEKkEHXAIwgCfve732leV1hY2NbxEFEM+DasUm+Xrq6UfYP3FhWqa8b4m0sGBGq5HKqrVferEgTFCd7CeTeVFdLx98qWz5AlJQeX8rwzLjXUGkJTY4O6c3yo6kp4iwq7ZE0qIj2tLjxYX1+PH3/8EUePHsXIkSORmppq5riIyCBTCi9qzcYoT5pat0lNlz1XIL8nWPMvMJuRkSnNdOzf23Jf5cne7tBv4hlvfD4pKNm/tyXQE2zA7NtDXi+DTRH1lsWUM0BJydJrGtgm7t+l1ZVnepS/G95lD8R6SBRDrSqLvHnzZkyfPh0rV67EQw89hPJyaYr7r3/9K15++WUzx0dEEbS18KJYW6WejQFaTpqF06TZndR09W2UlymDotR02Fc+AtuMBUDpgfAD8e/+tJzg9nkf8FBIG4m8fP37CLaW7for/g7hjoekgEbJ4Wy53fKHI5cjiHboOgUr44Xyd8O1/NZYD4liKOqA51//+hc2b96MsWPHYsGCBbLrTjnlFHzxxRemDY6ovcX7H3QA6pNapSuqY1IvZylmHvyBDwD9yr4BGidcb1EhfPcvi5yv4+2svSBM1NgQfE9sc5a01CFyKCbbe/SEfeUjwbpEtr79YVv+N3XQk18gu53ZNanivoq54nfD26o6UmQVUQc8b775JsaPH4/rr78eJ510kuy6wHZGongR93/QAfVJrb5OfkyLp4cPepQBk/LkG1BXC6R1l1+2b7csqBKmzpSflAPB0v49kY/DKstZkRR/D9/aO2XbuZE3UH6bTHV3aM3A1N/OIRC4o9Ilvf6ZvaLupaUp3gsLKn437BqvK3UdUQc85eXlqkAnIDk5GfX19W0eFFGHifc/6FBXLEaKIp+usSEYyGnOaCkDJrvd3yNJEfgEcnFCBSr6+oMq8fEH29YrKh5pLsVFyM85UCILSgFot98IpfpsisF2DsHAvbLCUB8sw+K8irnydyNr8ZpYD4liKOqk5ZSUFNTU1GheV15ejvR0jXV+os4qDivFaiUphyamSt/0FRWKtTqUh249Xzy9JVBpapSCnbyB8l1Z+/cC/QZKwdC+3fIaMf5igl0u2AG0l+L6DQw/q6Vc3qut1pzVkVF+VgN0Es7bSqytkpKgHU7pgrz8uCssGLrLUBAE2DMygQauQnRVUc/wnHDCCdi2bRsaG1t2DQiCAK/Xi+3bt+vO/hB1RrqNLQ2KRQ6Q1jJc6Djg8ahzbfQ6lJcUQzxao94xVVIsnYRDeT1SAORwqHs3AV0z2AkEA0p2u/S50rteSbkMqbG0GvysKh9Tq7u8CYF7sPp1YNeXw8Et7hTXop7hueKKK7Bw4ULcfPPNOP300wFIeT0lJSVwuVz4y1/+YvogidpLWyvFGm2zYCqNZTjlOIJLUsqS/8pZAo8b4rK50g6iUB63fqPQ6krYFq7pujM6RuzbLQWFRvqBabXP0JihCXxWxdpqzXYOprd4sMByL1GoqGd4srOzceedd6Jv377417/+BQD497//jbS0NCxbtgxZWUwKoy6knU8KhnJuMjLVz1tXK20F918nzQJVSydC5QyBMthpuUL74tR06brsPBiuJWMlDodUWwfQ33nmz21CfV3kx/O41TNsGjM0gc+Cb6W0tdq2cE1Ink47FHGM8/wdIiVBNNIhTofb7cbRo0eRmpqKhIQEM8fVLioqKuB2W6uUvSAIUTX7iyfxcGzeokJ5ef+CoYZmeIwem9bjazVs9Cn7Vjmc0ok5dAYmUARQ2XcpWvmDpf8rqy5Hy5kAuJvb9hixkJRs/PXL6Ak01hu7vcMZXJ7SKh4Z7rMW6XNotDhl6OfSV1Ol+pzF85JWPPw9aS0rH5vT6USvXr1MeaxWV1oODCQzk1E/dV3t0S1adnLSyLnxrbxVeq6FayCkZ8iTSz0eAGJL3kWoxoaWE29CItDcjFbNDOzbDfiivJ9WtWAhjmaHBEH6LyERSE5RBzD+AMM7e7L8OBuOwbbi7y2fkXDBZn5B+GA53GxihJnG1iy9sjEoWU3UAc/mzZsj3mbSpEmtGgxRvGmPk4Ls5KTkcUs5OK7DUg5Na2dsDAc7AuB0ymdilB28IykYCmHqTIh3zFGMoSm6x+lIdoc0Q9YtTVqWamyQKiY3NkgNVEOF9hNT1hLyemSfEVn+TaBKdV2tbrAsC36V1bBDl5gi7TZkPg5R9AHPiy++GPE2DHi6NlN6O1l4PErK8amShQNLHdWV8lmb0BkbXQK0AxujMzRi25adBBvgKpcSo+NJbn8gMVH9mgOA6IOQnAKxW5rG50k5a6X8OeR1dzgifhZVwW9Ib7LQACniTGMcll8gMlvUAc/zzz+vuqyurg6ffvop3njjDVW7Cep6YrJzKY7Go6TaYaXVPqCwSJ2noSe0YafHI8+1cTilHk4lxZCdfFWdzA1ISgYaGxE2eBJ9QPWR6B63M4hQGdrWvQeEu/4WzJcIBq3K11DRL0v1WQzM0oUETvIlTcVrl5IK+8pHVOPRm2kMPlagAnNKKpCZFXf1dIjM0KYcnoDU1FScd955qK2txWOPPYZbb2WDti6tFdPnmrMw3Xt06HhiNhOkHE9KqnSiVHxbl32LP1ojzxVJTJLaPijGrbWF2bf2TqiCnYxMoMpgYOJwSEs+bjfaZXdQHPDVVEFccENI0vgqddJ4foE6sFC+14FZupBAPOyS5rGj0Y1T+Vh5+Z0q2CfqSKYEPAEFBQXYunWrmQ9J8agV0+eaFYAXrO7Q8cRsJkg5Pn9bAKXQb/He5bfIZ25y+sG++J6w9wk6UCL/2e4AevY2FvAkJUvb0du6QyvOiQ31QEN9yyyNMh8pI1P7s5Oarl0tGWgJhsJ9QVBuX4+EuTtEQVHX4QmnpKQESUkRuimT5bWqenE7/mE2PJ4YnRxa9XopE1iVP0dJmDozcid0QDrh7tvdpufq9JRLipE0NqgTuf1d4qOqvB0IxMN9QdDZ2aZb8bsT1NKJRTVyIi1Rz/C8//77qsvcbjf27duH9957D+ecc44pA6P41aqdS+2YVGl4PDFK7GypoCstqQW3nYdbUlOOta4WYm21sSW4vHz5DE1evtT0U7ltXIteoT0r8Xik1hBaPbIEG9BvALBvb4THcAe7ostm3pSBqaL2DhBh6VKRExSgNzvZHmUTotXZc+io64g64Fm/fr3m5U6nE+eccw5+//vft3lQ1PWY/Ye5Nfk4sT45+Nbe1RKIuA6rT5YhVA0//R3RNYvNVVYA9ceCCavCtXMhblonz+spnNbehxc/PG6p3o4y4HE4YSt6VGpCufFuNJeXRS4JcKAEvoMlEFcVSsteypQnjdo7utvYw30mdWYnO0UtHS6rUScRdcDz0EMPqS5zOp3IyMgwYzzURZn9h7m9C63JAiqNeiqtSnZW5tbs3yvtzNII2oT0DOl5Q0+21ZXqui2h1zc2AJUVEDetUwVGUeeGWJ3W7E5+AYT0DAiCgD5rHm2pRrzopjCzYyLEZX+Wt+8QbEDPXqoARi9IN3N2MtwXAbG2Ct4Nq3Corhbe1HTzkva5JZ46iahzeHr16qX6j8EOxZJmjkA7f6uUdSwv+Un6L0yn60g0gw6vN3wHbY38DNm49GYetKrwdtHdVrqUBQQhaM6uCOkZ2sFRgN2h7lUmAPaVj4T0wZLI3rsoP0dG88BUz7F4ejCnJnCdt+xgqz/HbRkbUXszdZcWUSxozea0+7fKcAFUK4IrzaDDYZcHQXt2wVtUGPzmrdlTa6WBkhCp6bKZI92u6NTC4QjWyFHOgqgJQFbvls+gcvYnIVH7OTTaiHgX3mho1tDwTJDGtvjg7Gc7fUnoFMtqRDAY8MyaNQuCwb43giBg7dq1bRoUUVQ0/lDbFq5pVT6O4dwfZUClvC5aymMIFggMSS72d+AOnKCUJxKxtsrYbq1D+1q2UWsVOiQ1f7JwcBYEAHBQyn2y2+XJ3PkFwdwr7+zJqocSFqzRfg7lZyq0jYhZib5an9vAZ49LT2RxhgKeYcOGGQ54iDqcxh9qI98qtYIbo7k/stmVMD2RtJ4DEFtmCZJSpBsqAx5/0TpZYnJASbHmjizfhlXGemopa8akpEq1dfbtjr5PVjwTBH+w4oVqds3uAPoNlOVVibXVUgJ4qECTVp2WD6qlMYcDtr79NYcj+0wpW1qYNNui+ZnyBzaB57eHnb0iil+GZ3iIOqvW7q7SXAozOK0v30njD2qMPgcQMkugEFKhVzMxGQA8bu1ArLUnxcws/2C7ULADSG0g9JK1+w2U/h8IOkp+kpKT9aSmy1o+BD8TqscP98VR3mdLFvCYNNsipGfAtvxhzd8XIT0DjgWrkZOTg9LS0mDbDCKrYA4Pxb1W5whoBTcmVYkOjkfrOcIFFsoKvXpLZ5UVqh1cYZfZwhCmzoL40J1R38+ykpJhm7MEvsLr5ZeHq1OkzItS9jAL0KmjA+g0Ck3pJpUUqHTJ8reUoinDwJwa6qpaHfDU19fj0KFDaG5Wd1IeNmxYmwalNGvWLFRUVKguv/DCC3HDDTeY+lzUhWgEN62aLQo3K6QVQCm3nyvHFCI4npJi+Tf+yiMtycaBVhzKZbbQXJ0wxE3rWh0sWUZCItDcDEAERBHi0Vr/UlcEDgeQP1gKcEIbwDqc8tvZbMCg48N/npSfo0AD2EpXsKSA3hIri/sRRRZ1wOP1evHII4/g/fffh0/nm6pWR/W2WLlypey59u3bh7vuugtnnnmmqc9DXYtWcGN2lWjNnVTLb9HJtREgTJUvH7dUYa6Wmn4GZw0Uyw3+kyEqK4C6o9EFL5Uu2BbfA9+8a9VbqLuCpGT/rJv/NW1qhLjqVsBuC7/lHADyBkqd7BfeGP52g46P/LnS+hwZ3Tll4eJ+MWvqS5YTdcDz+uuv47///S9mzJiBdevWYdq0abDb7XjnnXdQX1+P6667zvRBpqeny35++eWX0adPH92ZJLfbDbe75duwIAhITk6GIAiWS74OHI/Vjgto/2MTuvcI26BUrKmCNyRYsc9cpPmH1j5zEbwP/rVl5sbjAY7WSEXqNJ7Dl5mlTn6VnhHiXX+BN79A9lxiTZUUzISbGfIve7TKsaOwde+BLhPqJCZJycSB/Bqt4LO5CXAmRA54fi6WlrLSFE1B8/Kl2Z9KF1BfF1yS0vsMAf7P0foVss+bd/0KdUK+1u+DVuK+wd+b0M+5kJEJ77IHOtXfE6/G7JWjFb+3/FsZn8w8JkGMMjNt3rx5OO+88/Cb3/wGU6ZMwcqVKzFo0CAAwPLlyzFw4EBcddVVpg1QyePxYPr06bj44otx6aWXat7mhRdewObNm4M/Dxw4EEVFnN6l6By+dRqad34d/Dlh2Enos+bRNt/WW10J1/Jb4ak4DJ/rsJQ8qyAkpyBn48uwZ2SqHlt126RkCKlp8LnKIx9UYpJmLkru02/h0O9/Y53EZWcC4FYvtwPSe+OtdEkF9gIEQfY+CMkpEFLT4asoC7mNTco51niNnEOGQXA44a10wZ6ZhazFazTfu3CfCy2Bz4rycVt7Oy1tHWN7OzRtguy9smf3Re6j23Rv39mPh2In6hmew4cPIz8/Pxh1hc6kXHDBBXjsscfaNeD59NNPcezYMYwZM0b3NhMnTsT48eODPwfG6nK5ZOO1AkEQkJ2djbKyMsvtqojlsYk1VfD+uFN2WXN5GUpLSzVv7ykvU9320A87pW+agW/5/l5W9pmLYLtlOfpmZ+PA3N9DDE1UDTx/Qz0OLZ0Lx4LVqsdW3baxAWKjgcafgG7i7aFpEzQDr3hlL3oU3pu1+/o1lx2UZsRC9RsIlJdKMzsJiRAWrIbvqXVAaMBz3PFAlQs4op6dc+/5EfbVj0FIz4APQHlDE9BQqv5c/LgT+6+5OOyMocrNd0lxVsjjKok1VfA2NwMeD7zNzTh8+DCEhsj5W4D6s+utdHWqvyfe1HQAB2U/6/0eAtq/i6WlpfxbGaecTieysrJMeayoW0skJSXB4/FAEASkpqbKkokTEhJQV1dnysD0vPfeezj55JORman/7cXpdCIlJSX4X3KyVFhNFEVL/tfVj81XUwnPqvnwLLgBnlXz4aupavPzejesVHcGz8jUH6dGmwfvhpXSVHxlRTDpFMXfw7t+RfDYbDMXSWX3lUmugNQbS+uxNbXxj1xjg6UCHu+KedKsjZb6Y/JlrKRkCNf/RVqGyuwl/T9Qh0bZEqGHzh9ejyf4vob9XASKCRZ/D++im0z5rAY/ryEtI7zzrzf8u6Acoz0zq91/v6L5T+t9iOZ4Qn9vga79tzJe/zNL1AFPbm4uysulqfMhQ4bg9ddfx5EjR1BTU4Nt27YhNzfXtMEpVVRU4H//+x/OP//8dnsOij9t6UGkS6PycbgdNponR73E0ZJieBbcgMO3Sh3K7YVFEG67V1oyCRVSEC70sckAnaVC2GzqQDY1HeKm9arPUCBhPLTvlTB1pvp9CtB4v2XvnTKo9bd10OwFFy3lc3vchn8XlJ+vrMXyStDt8vsVBa33IRz27iI9US9pnXXWWTh06BAAYPLkyVi6dClmzpwpPZjDgVtuucXcEYZ477330L17d5xyyint9hwUh9pjh4oyCdTfKVuP5u4uva3e/m/5za7DwPoVsM1YAHHVfPkOqaTkkD/UihN3QqKh7eakwedT5+C4yoGqI/LLQhLAVR3o9XayaczEhX4uvEWF8jo7AFBSDF/hDS1BmOuw1K4itPikgmb1br3PmoHfhdAxCoIg5f6ELpvF2Q4w1hkiPVEHPL/+9a+D/x44cCDuvfdefPbZZxAEASNGjGi3GR6fz4cdO3Zg9OjRsNvt7fIcFKfaoQeQMHWmFIQEcjqmRl9t3DZjoXTyUs4ohKqu1G4JkZoebFbpWzS9JffGdVh/qYZaSVS3gKhvWZpXFQRU8vc9izSToNnWQeuzETI7Y7Tmjm69JjMqNLPHFllEmystZ2Vl4aKLLjJjLGF98803cLlcGDt2bLs/F8UX24yFUo2akG3hWr2moiFuWt9yYmpskIrzFRaFrQmi+c07v0BdPVfZx0jrG3Ogcu/eH9Vbo01c0yYdKakt/w43o1Ew1PBsgqqtg7JfllIUNXdk9Zpa0WYlnNa2biHqbKIOeBYsWICxY8di1KhRSE1NjXwHk5x00kl44YUXOuz5KH4I6Rny3kMlP0VVaVYzUFGeVIp/gLdwmvTNPxCwKCrahv3m7X9sYeosKXiqroQzsyfcjY3SkkqoxCTDVZJJh2BTLz0ptp6HlRmSnKyc4UhKhj0jU7PBZqQieeGXuATIli/1ZlLCzLi0x3IOl4jIKqIOeGw2G/7xj3/gySefxP/93/9h7NixGDFihCULHlEciSLPQHlSkvU9ch2WZovqapX30i4WGPo8Yb55h/JNnQFxVSHcWjkXghC+ZxMZ070HkNVbFniIR2sgLpurCIQEKVjunQMcKZdq9yiWMFUtOwQADfWaTxtNi4dwwXC4mRTOuBC1TtQBz4oVK3Do0CG8++67+OCDD/Cf//wHmZmZGD16NMaMGYPs7Oz2GCdReBrfevW+bStPSqrdMwdKwi81KJ83zBhCBcdT/AN0t5Fzucocytf+aI06MVywQVj6IGx9+0uzLYf8gWZjQ7DitbLdSGBWRlpkPKgOaJTVrsNUv9acOTEwk8IZF6LWiXpbOiBtTZ86dSo2bNiAwsJCDB48GK+++irmzp2LpUuXmj1Gooi0tqKqttMuni5t+TVtl4kQbBkg1lbrbocNbDv2Fd7gD7QY1LSrQAAb8t6Lq+arE8NFnzSjAhjf1h1pJrG+LvzPccaULfNEnUSbkpZtNhtOOeUUnHLKKfjhhx/wwAMP4IcffjBrbESGaX7rVZ6MGhukXTIexY6cQN8jrSWusPzLXCFdrO0hic2+lbdG+XgUmQA47Or3MJTDAezbLb9Ms1krWj4jRrd1R9qxlJIqf66UjstzbA/swk5W0qaAp6GhAR999BF27NiBn376CQkJCRg1apRZY6M40Kk7GWudxEJPRg6nZr0TsbZau3N4v4FSQrHWDpuQE2PEJTNqA1E/2LE7pGBHL7jR4l/6hMcjvU8eD8IlDgfyZ+x1tcGkZdnvgHJGJ9OckvgxE2c1eIjCaVXA8+233+K9997Dp59+iubmZhQUFOCGG27AqFGjkJKSYvYYqRPzrb1LlfBrX3xPbAflZ5uxEL5FN+knAWdkan5bFdIzgB491UnKdof+DpujNfDOuEz6N/NwOl7+YNjm3CYtG4ay2QCbXZ2TZbcDA4e0lDQInYFLTAK6pcm6nAeCYiE9A44Fq5GTk4PS0lKIoqj+LCQlS8nNVkgoZg0espCoA55Zs2bB5XKhe/fuuPDCCzF27Fjk5eW1x9goHgRq3wSUFEesgSPWVkmBUuC+efmwzVli+syQkJ4B5PTTX04K98c7M0sd8ITs3ArulKmskKr0httZlZMnnUT37LJON/JOyLdhlTqwGXS89H9l4UCvr2VmT/kZ9npb3n9/D7SwSznKWY/UdNhXPhL1+DvjbCl3hJGVRB3w5Ofn47rrrsMpp5wCm61VOc/UQWLzB1SMuM7v27BKHoREWTcnKqrt5X7+1g16r5FmVVx/gCS7z7G6yDM6Ph/shUXarQXIHAf2Auk95JeF9D/z3XIN5Mnior9O0gLtJbJolnJMmgXpjPky3BFGVhJ1xHLrrbfitNNOY7ATBzqk6V9evvqySOv8Wte3MTdAdzeJ8uTjcEo7qJY/LN+irtjJFaiKq7Xryrf2rpb7GKmZc/BneP86F9i3R/v6hETpORISW/8CdHUej7oflr//mZCeIeVfKQX6WCl3zeXla3bc1mNas0rmyxC1K0YtFqI86atqgLTDH1DbnCVSzkKoSN9wta7XuEysqcLhW6fBs+CGiFti9YI71cmo6FF5x2WtnVz++0ozPQuC7R+kztbVwP694Y9Py/69+pWTm5thW7iay13hJCZFvo2ixg5+94eWn7X673nc6iUwh1NaXp06U/pc22xAUnLYXmrRdvPWFUWQFQ1uLSeStLmXFnUeqt1B0QYiraDqD2RgnV/V+yovH8LUmVKQFvIYvrV3wms0IVrj23Fw6amyAqg/Bng90jJVSiqgF/AAQElxSxsJt7ulsaR/mUHV26rNRPjmXaffhZukXCyHI3zRxlCiD3jor8Da56Wf9ZY2lfLy/TN/KzV7qbWn9sqXUS2Vrb1TVoahM+QKEXUEBjxWojx5p6RK0/PtnHAY7Tq/kJ6hClxk+S2BP8olxfI7KpNLQ2nkUai6XIecwDTbRAR43PrXV1dKswXK7tptxWAnvH27pQTk9O6A0RmK0Bk1jX5YYbevx2B5qd3yZZRjD60k3klyhYg6AgMeK1H+Uc/Mip8/ZFp/lHW+yWslGmt9O/atvNX48zucLZ3LI3Ww9rVihichkc1A28Ln0074TkyS+o9pBS8hOVG6fauUO+cCM0HK36W62oi7D2Mp7AYFvaKKAcwVoi6COTwWYlrypEGm5gYYWW7zJ0hr5esE821S0/3JqNOAozXGnz+/AMLsxZGXqzzu1tXZye2vnThLbeP1SDOZScnSLi3BFsy7wewlwc9noHN9IM/G1re/9GUgsG09wP85tM1YKF8SDsnt6ozCbVBQ/l1QbTQI2X3oWTUfh6ZNgGfVfOb6kOVwhsdCOnoLaTTbaCNtkVd+A1e1Y0hKlhKkAd3lBtV2d49bOmmlpEr5OMkpUpfr0ByeutqWGaHF09tvaYmtJdqHx9Oy/JiXD/s9TwSv8i6/JWJRTL28GSE9Q90mIkwj0JgLswSn/Lsg1lZrHnPg91m3MSpRnDMU8MyaNQuCIBh+0IceeqjVA6I4EkWeQ+TgSD5rIlw7F+KmdbIS/oDYktgcKjA7pPX8zU1SYcPF92guR8j6XkXTkoDM40wA3M3a1wV2Z4Vu/w9UMg7T3gOAOudLIwcs7JeEeGoEGkUtIN1j5rZ4sjhDS1rDhg2T/efz+VBZWYlevXqhoKAAvXr1QmVlJURRxLBhw9p7zNRZKP+o1tXqL29F+GOqnJIXN62DY8Fq5D66DY4Fq+U1cwInuUBNncDSndYfeX/uh95yhOx5jYoi+O86WvmaZPaCbdVG7fsXDIVtxd9hW/F3+VLt8oelSsb5BfLbm70LUdn4sxM3AjVlObudtsUTdRaGZ3gC/v3vf2PXrl148MEHkZXV0hivoqICd911FwOeLkS2HFBXK82QNDZoz+BE+gZq5Nul8jJF8rBsu7sy8bikGL6DP0PctF56nNR06XJlV20j0jKA2qqWnwXBP0HVRXtoJSVLy4XKwn9GZGZJM28Oh/w9czhlnx+tGYmI27jz8uVLiVpFMiOMTbZbrxM3AjVjOVurMSqRlUSdw/Pyyy/j8ssvlwU7ANCrVy9MmjQJW7ZswZgxY8waH3VioX9kvQtvlC8JKYKTiCcnI1PyytuEzN4ECr4FcjRUbRw8boh3/AnBoCSaGR0lZTJ0aBKzwymdWA/s1e/qbTVhlwIFaRu/3S4FqKIovW+CIO2i+t3vpfdKq9qxBqPtUsRAQBroVO/v1xaNrtZHSqsxKpGVRB3wHD58WLcjerdu3VBeXt7mQVEcihCwRPoGGunkItZWSQGEw6mevQktMhhIev7d74G7F0PZP6nNEhL9O7V0rg+MLac/sF+nlYQVpaRKwUxork3+YNgX3yO9N4v/KN+WL4pSoPTQnfKAKRAwwh9EK4IarVww24wFqiBIlcDucES9pbw9NgF0xgahRF1F1AFPr1698O677+KUU05RXffOO++gV69epgyM4ktbvw1HOrmoTmChlEUGXYelE2l7LDH1ygaOlIef1eiKO7Iys6Sdb6HHXrpfCloCy51alLWJMjL9FZV1Etw1lj61giDVjqoY7bBSBeKhuw9Z9I+oQ0Ud8Pzud7/Dhg0bsHDhQowaNQoZGRmorq7GRx99hD179uCPf/xje4yTOjHlH3Vh6kxV8GP0W6xYWyU15/TvqDk8aAjEPy5Qn+gChQL1igyGK/LncAK9c4DS/dHX1Dn4M2CzS0syguDfYeRuXTHCeJU/WKp2XF8nzexkZkGYOhPinX+R366pMXJz1YREdUf6cPlcWjOJWrfvJDusVIF4YIktgDuhiDpM1AFPID/nueeew1NPPRW8PCMjA9OnT8fYsWNNGxy1TUdNnyv/qIur5recxMIsO2iNRTmT0/zjd8D6FVKSceiJLi9fXlNFeSJUnkiDBGnJpDXBTnCQ/uDmuF/AXlgkr/didYlJmv3MvEWF0bXbsDukgLFXtvTv0HpIG1bqLo+q+rB5POrPRkamlCcU+v673ZpLZO0uUkDDnVBEHaZVhQfHjBmD0aNH49ChQzh69CjS0tKQm5sbVa0ean/RFAZsE+UfdeXsyp5d8C2a3vJt33UYvkU3wbbi7+oTj97urMCuKh2arQMef0AjEBGNByeCAAwo0L99Zy5EF63MXtKylFb7BggARCmIzOgJ740TpJ8Tk4A5twMvPyW1aDDKFtKLbP9eKddn5SMtV4dZHlXt6Cr5SZpxKhiqyOFZKd9h5fVIQVFHLyMpA/G8fFXjTiLqGK2utCwIAvr27WvmWMhsHVVILNLsis+nXtpoatQ+8Si/rQceXzl2f1uL0BOH8rG8jjYWEu8T+Hz7T/hKlRXwzrrcGj2ywi35FPyiJYgIDYiaGoG7F0X/XKG9qwBVQcCIycLKz0JdrSxgAhRBU6QChe1IK3hjkjJRbLTqjHDw4EG8+OKL2LlzJ44ePYrly5dj0KBBePHFFzF06FCccMIJZo+TWiOK6qttYbgxo5KRE49gA5ob1betqmyZYVF8aw8u5UUz66DFdTh8I1HAGsEO0FJDSUugcKMZgUJSsvSaRrllX7Y8G2jwGaDxuZaVTFCWKOjAZaSObvdCRPqiDnhKSkpw++23Izk5GcOGDcN//vOf4HWNjY3Yvn07A55OoqPqiGj+UTeS26I48Yi1Very/6IP2LdXfV9lz6uQk7FsKc+IxCTt5FrWIWlR/INUSydaggAc9wv5ctPaO+Wfi5x+UlBSWQHUHwsmQutuRwda2ksY+Fyb+Xsg1lbBu2EVDoUU5+OMDVF8iDrgefrppzFgwADcdtttcDgcsoCnoKAAn3zyiakDpNbrdN8uExJbZnzy8lUnHt+GVZFnVPRUV8JbVCg9ZlQzEQKEhXdr5/tE6pzepYjRJSUH7yYqZt5WSju8Ak1dM7Ok2Z7QYKaxAaiskDre5xdAmDoTKCmWP25qumoZS4+ZvwdssEkUv6IOeHbt2oU5c+YgMTERPsVyRffu3VFdXW3W2CjeKZce0jPCn6TasmTicUtLL4XTpKRQo/ILIKSlQdQ6mdsEwMdZnrZp2cigmqXJy5d2uS28Ufuu/vdUXDVfHQjHancTG2wSxa2oAx5RFOHQOaEcO3YMTqdT8zrqeDGv6qrMITpSAd/BfbD17a89RmWApKKRPKysvOxx+3/WSTRWsM1ZIs067NdYNguXf0TG9Mtv+bdesKD8nCgp86QcztjtbuqgvDgiMp+hbumhBgwYgE8//VTzuq+++gqDBg1q86CUKisr8eCDD+L666/H1KlTceutt2LPni5Utr+VlB3I9TqGtxfbjIVS0nGA6IO4Sl4gUDbGxgbp9pm91AXapAdo+ae/U7puQ0iDszyaVXkpOoINsNmk/6d29/9sB5KSIUy7peV2Ot24g52+M3tJS13K9y4hUf5zfkHM8mYCY7Vn9219V3IiiomoZ3jGjRuHBx54AImJiTj33HMBAC6XC99++y3ee+893HzzzaYOsK6uDkuWLMHw4cOxaNEipKenh+3nRSEMTL+35yyQkJ6hnmhRfltXjkn0SXkdujVh/DIyYS8sglhbLQUtJcWKmR6PdBKONEsTTXIzqSUkArn9WzrU1/kbq4oAGhuk3Ch/oUK95GFljk3wPVXu+usEtWvYYJMofkUd8Jx11lkoKyvDiy++iH/+858AgHvuuQd2ux2TJ0/GaaedZuoAt23bhp49e2LmzJnBy3r37m3qc1iWgen39ixOKNZWqVeVlN/WtZYzqishzL4N4spb/bunBCAhQR4s+Y8lcLIUa6vhWzw9ZGu12JJ/UzBU+j+DG3MkJgFp3aX3oKkx/E68kF13RpOH9Xb9ERG1Ravq8Fx66aUYPXo0vv76a1RXVyM9PR0nnXRSuzQO/fzzz3HSSSfh3nvvxc6dO5GZmYkLL7wQv/rVr3Tv43a74Xa3fNsXBAHJyckQBMFy1aADx6N1XPaZi+BdvyL4zdg+c5H6dhqzQGa9Rt4Nq+TbxwUbbAvXyB7fPnMRvItuUvVTEjetD9kqLkqzCCEVajWPRa+2S2UFUFtjyjFZjmBTb/HX43AC+QXSa++fBfT88dIIdxJlBSJD7xuvwv3OxTseW3zqCsdmymOJUc7J7ty5E4MGDUJSUpLqusbGRuzZswfDhg0zbYBXX301AODiiy/GmWeeieLiYjz++OO46aabMHr0aM37vPDCC9i8eXPw54EDB6KoiN8QtRy+dRqad34d/Dlh2Enos+bRNj2mt+oIXCvmo/mHb+VNNQUBfdY9h4QBx8lvX10J1/Jb4Skvha/S5Z8VEmV1cOzZfZH76DbDxyEjCHFfU8eW1Rs+V7nJD2qTXuvQgMfhlALL5mYICYmw5+ZBrD8Ge2YWshavgd0/sxZ8j/Ve8wDFa2/G58tsgWPxVrpUx0lE1hF1wHPFFVdg+fLlKCgoUF23Z88eLFy4EM8//7xpA5wyZQqOO+443HXXXcHL/vGPf2D37t1Yvny55n30ZnhcLpfscisQBAHZ2dkoKytrVT6BWFutngVq4zdwz6r5+stHSclwPPQCxJoqeEPyNDRnekIlJgE5/YJNJkPHKdZUwVs4rfU1fKiFsgjjwMGwz16ifq/Wr4i8RKg1e5TVB45VG80fdysEP4PK/K+CoXAsWK17v7b+znVmPLb4ZOVjczqdyMrKMuWx2thsSM7j8cBmi3rjV1g9evRAXl6e7LK8vLywBQ6dTqfm9nhRFC33YQho9bGldVflS7QucApJfg5Xm6S5CaIoSieakNwh7/oV4ds0hOaKuA7D++BfW5a46moZ7JhBq+L0/hLt90rrPQ4JShN6Z6O57JC8gScgLVd2kt9Br7I3WEB1paEx8u9JfOKxxRczj8dQwFNfX4/6+vrgz9XV1XC55Ft5m5ub8f777yMjI8O0wQHA8ccfj0OHDskuO3ToULvkC1HrGW7nEEhaVp4ww3Xp1hLYFUTm0WqvAWjv9tNKNk/rDvvieyAIAvrk5GD/3N/LA56k5M61jVsvMOdyFpElGQp4Xn/9dVlOzJo1a3RvO3HixLaPKsTFF1+MJUuWYMuWLTjrrLNQXFyMd955BzfddJOpz0Nt3KKuPHk4nNKJIzkFKC8F3M1SsDN7SUsSqx6bTbpt7xxgH+stxVRevjSTptjtZ5uxULErDqpAQZk03+n6TimDNn9SdqcKyojINIYCnpNOOglJSUkQRRFPP/00fvOb36jW1JxOJ/r3729qwjIg9eeaN28ennnmGbz00kvo3bs3rrnmGpxzzjmmPk9XoRXUAKJ0WWgug84Wdb37q6ok5xdobkFWda5WstmAQcdLMwOHDwFOpzTJk9UHKDsoPZdgk4KhQ/va8lJ0PCN1gdpFpKrTOtcnJMI2ZwkAqOrnCOkZsC1/OGxTzk7Xy01Bqy5QpwrIiMhUhgKeIUOGYMiQIQCApqYmnH/++cjM7Lhp31NPPRWnnnpqhz2flWnV3QGgm8tg+P6h3/TDLV2oHlNxshV1xnL4UMvtRF/0wU5n2KnVmmAnmm3jemw2+W45JYdde0u/IAQDAK3AJVYBjZGZSCO36ewBGRGZK+oM48svv7xDgx1qHbG2Ct6iQngX3ghvUSHE2mrpCq18jGhyGYzcPzVd/5uy8jETEhBsMJmYBHTvoX2/tp70Yx3stFY0x+1wIrRZZ1C4YAcA7Drfe9zNxp+7AxlpmRLrtipE1PlEHfA88cQTePDBBzWve/DBB/HUU0+1eVDUdrp/8FPT5TdMTVcHIf4+VZqzNFr9kHR6JGkFXcG+SVl9pL5JzU0Iztz0GwhkRVtFW2jp4xQv7Pb2eVyPG0YapsLhaHkPCoYC3dK0b6esit1ZGOlYzq7mRKQQ9Vni888/x4gRIzSvO+mkk/D555+3eVBkgij+4MuCkIKhsBU9CnthkeYsjeq2MxZqXgZoB12BZQT7ykfUwVd1pXTfpGTjx5lfIOX8xFOBUW+EGRcVjYPr0bPlNdeboQlIVBQJzRsYfA/shUVS3zKlpGQIC1o2J+jOGMaCMsCuq1WPRycIJ6KuK+qAp7KyUreXVa9evXDkyJE2D4pMoHFS8C68UdbbCACwfy98hdOkhOXU9LCJm2JtFXxr75RuW10ZzPsIDWJsMxbAt2Gl9FwlxfIH2LNLfrJUjrG6UgqKFqyRTuZ6J/Kk5JbZCUAKqmKSDNxRNGZtevZuCRy1OsM7nMHXSFh4tzwg9SciB6gC1nuehH3t87D17R+8TWdaIlIFxY0NqvHoBeFE1HVFXXgwKSlJVYMnwOVyaRb8o44n24FSVyslFWtVMfaGJKuW/BS2eai0k+unsLcPW4/H5wueLO2FRS1jDOwO87iB4u8hbloHe2ERvLMny8cHSCfy7DzY5iyBkJ4hBVZW0G8QUH5IvxYO0LLVX7kjSqsekXKXXJjkXEPJuzozhprJwXp5WCYR0jOk2cHQz7NifExIJiKlqGd4Bg8ejNdeew0exa4Oj8eD119/Hccff7xpg6PWC7t0FG79J1yug06uhFhbBe/yW+CdcZmx4oP+xwmelDRmegBoJ8163FKgtXi6FOwctUZTUNuf75C22keitePIq57dMn1GQ2eJKGYzP1yyIqIoRT3Dc9lll2Hp0qW45ZZbcN555yEzMxNHjhzBe++9B5fLhRtvtMg37jilWSdHVWDNoV+lOCNTf0uvVnXdjEz1zI+Swyl/PuVzKGv4BE5eCYn6vbX0ZqzilG/lrUCk5qD+GTDVLJzetnITadWsARCz5GDd8RAR6Yg64Bk8eDDmz5+PRx99FM8880zw8j59+mD+/PmaTUWp4/jW3iXrOeVbeyeEa/8EcdV8aUdUQiLQKwfYr1HBODEJwtRZmrV2gktQa+9syQPKy5cuW3lr+EEFqvWGnJx8yj5GiUktybwej7Sja+Ea+Jb9yeL5OX7KQDIcZVCRN1AVcIZbmgxltLq27hKRMgjuoJkWvfG0qVq4AWJtFbwbVuFQXS28EXLeiKhzaVXz0JNPPhlr165FaWkpamtrkZ6ejpwcA9Px1P6USckHSiBuWt8yG9LYIOWK5A+WZlYC+T0A0NQIcdM63W/tQnoG7IvvCV4snVxWhv9Wn5QczLfReswgr7dlFsifG+RYsBpC7xx4yw5GPu6uRBFU2OYskRLPQ2fRDM60qILbtXe2BKeBpVB/h3qtk3tnm2nRC9bNfnwpND9o+uMTUftpU7f0nJwcBjrxQNmxuqkRcDhgX/mIlAejTP5Ufms/UgHfwX2yXTtibRV8i/+oWFYSpOUVu0Oq7ZKZpf8NODVdMauh2InkP2HbM7O6VsATqbKyYIMwdZZqJgN5+fJZHqMzLcrAqKQYwfci9P3RCR46XXJwey+xsb4PUdwyFPDs3LkTgwYNQlJSEnbu3Bnx9mb306IoKE98eflA2QH17QJ/qJXBTV0thNlLIC77U8uJV/RBXHUrsPb54M18G1apc2iyektJ0hGItVVA6X75hTY7gJA8FP/sQtbiNTh09YURH9MyuvcAGuv185NEH8S7/gLR4Wi5jeuwNGNXMDT6mRZVXlaYwoXxcHJv7yW2GC3hEVHbGQp4li1bhuXLl6OgoADLli2LePvnn38+4m2ofdjmLFEtMfiW36I+gfr/UKu6Xjc2QHz8AfUsQ1MjvHOuaMkDSkpRP7lqN5g234ZV6u3XOu0P7BmZ0sk8XFK0lTQcA3rnAvv3Qgo+NBp7Brbwh6qr1Q02w+W12GYsVC+H6YmDk3t7L7EFHt8eksNDRPHBUMCzdOlS5OXlBf9NnZfmEkNmlnxZK6S5p2ZNE2UeECD1ogrNA4qwQ0p5khWmzpRyifR6dyl3GdXVwnegBPtnTza2G6szNAc1g9cjTygPt6MuVJhgJFxei5CeIVWrDk0gT0qWPhMaOTydXXsvsQnpGXAsWI2cnByUlpZCtMJnjqiLMBTwhC5Rcbkq/si+9fpPYr6Vt+pvW2+tkO3lypOsuPLW8EX1lFLT4Vs13/jW88504mlLh3Nl4Gck2AnXnR6ImHeiNSvCnUdEZDVtSlqmzkOsqYJ3w0ppJqf+GJCSKksaDnzr9RYVqr7tK0948Hg0lpA0llaUQmcZlCdZVbAjADYh/JbzaAKkziQnDzi0z9zHtDuk5qoAsG+3/HUL150eiJh30ppZkfbe/k1EZDZDAc/69esNP6AgCJgxY0arB0St41XWtWlsACor1DtrNL7tK094Ym21PK8HAPrl+/NKtAj+pRepfo5ukcJQDod6KSVUbVXnmrWJhtnBDgAIQrAkgCxoBSLm1rRHXkt7b/8mIjKboYDnu+++k/1cX1+P+vp62Gw2pKWl4ejRo/D5fEhJSUG3bt3aZaAUgd4OGuXlykAkNV06gSq+qduWP6yd/ByaC2SztVRDDrR8CNRxqXRJuSDNTdqzOIGihWvvBA7sVS/lVMXBjqB2pT+jpioAGRpoaj2SKqCt0nzPo8Lt2UQUZwwFPOvWrQv+u7i4GPfccw+mTZuGs846CzabDT6fDx9//DE2bdqEP//5z+01VgpHb0ZFWaROuXzV1KSqzGybc5v2coUy+XnQ8dJtlAnPsrwTRd8uhxPIL2h5TP/MkOr2rc2B6UySkqNrfyHYgJ69Wt6X0OTlvPyWmwVfN3mhRqMzLKbMznB7NhHFmaibhz711FP47W9/i7PPPhs2m3R3m82Gs88+G+PHj8cTTzxh+iApMvvMRVIdlsxe0ok2sxdQMFS1fBHaVNReWKSuh3OgRN0QcvF0qdXDjIXSc2T1aXls1VZ05ayE/2eHU9penpcvLbUtng5v4TR/oTvl7eN0KUspNV06Zh2OQUOklhoAAAHIGwDbwjVSG48/3yF/recskd+5LTMsJszOaH4WiIg6saiTlvfs2YNJkyZpXte/f3/W4IkRI4mnYm2V1GsrsBSS0097F5DyBNjYEJwFiDgTYFfO2ASf3b/lOiQPyELNPzWF9g3TaNXQe9kDOLR0rn+2RQT27w2+zqHvp6yFh97OumhmWEyYnel0FZaJiCKIOuBJTk7GN998gxNPPFF13TfffIPk5GRTBkbmU3U112ogGmj0qVwe05sFUHY675YmLceoiuV5gP0l2o/hcEon3dC+Xlawfy/Eo7XagUFtNVzL/qJO2tZ4nbWWoNqSiNzZ+l8REXWEqAOec889F6+88gq8Xi/OPvtsZGRkoLq6Gh988AHeeOMNjB8/vj3GSWaItHThcAaXTlS7tPRmAZSzBZlZQG21Tv0YnaUqh0NKbO7Mu7Im3wi8silMQKaRZNzUqGrJEeDdsBJerR1qWq+zgZ110eDsDBF1RVEHPFOmTEFNTQ1ee+01vPbaa7LrzjnnHEyZMsW0wVH0fAdLIK4qDLaAEBasaWn6aWSrOKDepZWaDng8UqNRZdXk5BQpD8XdLD3f1FlSawqtVhCJSfLaOg6HNLtjoHJzzIUNdqC/bb+xQXsHlVbw6XBqz7YwQZiIqM2iDnjsdjtmzZqFiRMn4ttvv0VdXR1SU1MxfPhw9O3btz3GSFEQV85vCSoaGyCunAdxxcPSskilSwo6vB4AAtAnFzi4D8GZiZBcHdlsRdkBWaNK3arJgT5c7ib55TY70H8QcNEk4G9FLTuwklKAuqMmHn07Chfs2B2w/XmZFCAW/wDlTI/mLiit4DO/QHN7eLglKBYAJCIyptWVlnNzc5Gbm2vmWMgMykCkqVGeAwIABUNhLyySarEol2H8Mw+q+4R7jlBaMzs+rzT7sWGl/HJl/k+86jcwuEzkO7gP4h1zIHtdNWZz7DMXwf63VWje86N0gb8ukZZwS1AsAEhEZEyrAh63240dO3bgu+++Q11dHaZNm4acnBx89tln6N+/P/r06WP2OMkwZS6JoL8NWWtZJbBcYnYhOa/Wzq1Ozu4IP26bDRh0vCxQsfXtD2/BLyJWQhbSM9Dnvifa3oCSBQCJiAyJug5PbW0tFixYgI0bN+L777/HN998g4YGabr/s88+w6uvvmr6ICkK/fLlPyckqGvlBE7AyhNxaBNKVX0dPyHqj0z8ihSkDTo+uIU8VIfWqFG+h8zvISLSFPUMz6ZNm1BfX4+VK1diwIABuOqqq4LXDR8+HNu2bTN1gBQdYdrNEJfNbcmTafbn0xQMVeWAaHZRX34LcOyo/rKVFSogaxEEY7vEEpOAtO5ht3O3dhdUa/JxuMWciMiYqAOeL774AldffTUGDRoEn6JHUs+ePXHkyBHTBkfREzetVwcldbWwr3xEdVvdLupdUWKSgZ1iAmwr/t5uScGtycfhFnMiImOiXp9oaGhAr169NK/zeDyqIIg6WKVLfZmRZQ4DNXrgcLZuTPEgJTXy8ensojIN83GIiNpN1DM8vXv3xo8//ogTTjhBdV1xcbHpO7deeOEFbN68WXZZ9+7d8cgj6hkLAlBfJ/9ZsBlb5ghXoycpuaUuj9mzQIKtcyyTZWZJ/+kdX2KSup+V2Vhvh4io3UQd8Jx99tnYtm0b+vXrh1NOOQUAIAgCiouL8c9//hMTJ040fZD9+vXDkiUtJ5tA01LSkJIqX5qxG3utgrkglS4ph8fjkfJa8vJhm7NEKkYYuE1JsU4lZR2B2j/KHlv9BkKYdgvEZXNiV2VZsdNK1feqtgqoPwakpAZbOrTXLA/zcYiI2k/UAc+ECROwa9cu3H333ejWrRsAYPny5Th69ChOPvlkjBs3zvRB2mw2ZGRkmP64lpSZBVRWtPzs8bQqFyQ0gTZwog9ud0/P8C+dGQxSvF6pdYRSYpK0jfuW5cDdi4w9ltl8YjAQ08qH8RYVSsfa2CB1edd4Lc0q/sd8HCKi9hN1wONwOLBw4UJ8/PHH+OKLL1BTU4O0tDSceuqpOOuss9pl9qWsrAzTp0+Hw+HA4MGDMWXKlLC1ftxuN9zulhkIQRCQnJwMQRAgCILp44ulwPEE/m+fuQje+dfLZ2CqK6M+bq9GAi08Hu3CgoC6bUQovdmgPbvgnT05fCHDtnLodW8PEIGSn6TeYanpQEYm7DMXtQQsWn2sFK+l1mvlWLA67LCU75uVWPnYAGsfH48tPnWFYzPlscQoqp41NzfjzjvvxOWXX44RI0aYNohwvvzySzQ1NSE3NxfV1dXYsmULDh48iHvvvRdpaWma91Hm/QwcOBBFRV3nm/PhW6eheefXwZ8Thp2EPmsejeoxDk2bAG/ZweDP9t7Z8FYc1l96SkyCIAgQO0tPLLsDQkICRI8bcEex/Ab562XktVS9Vtl9kfsoyzMQEXUmUc3wJCQkYN++fbDb7e01HpWRI0cG/92/f38MGTIEc+bMwfvvv6/bmX3ixImy6wIRosvlks38WIEgCMjOzkZZWVmwYq94wzxg/YrgEov3hnkoLS2N6nG9qekAWk7i3tqa8Hk2TY1GF7jMpZf0LAgQG+pbfnY4gbx8qRD1/pKwOUjN5WXB18vIa6l6rVLTI77eWu+bVVj52ABrHx+PLT5Z+dicTieysrJMeayol7SGDBmC4uJiDB8+3JQBRCspKQn9+/cPe0JxOp1wOtVbjEVRtNyHIUB2bGnd1XkmGscdzD2pdEm7u1JSgcws2GYslBJo194JHCiRbtwZA8WkZOC6PwMbViFiPlFGJuyL7wEAiLXV8mOz2+XLahmZUb2WWsnGRj9nXeYzaUFWPj4eW3yy4rGZeTxRBzy///3vsWbNGmRkZOCMM85AUlKSaYMxwu124+DBgxg6dGiHPm9n5q06As+q+bpJs3pJtaoGocrEXIcjut1YoZKSgcZGGE5s1pKQKM3gNGkskzmc0lb5xdPVz5GUrB53yBZvIT0jGPwA/gCoDbujmGxMRNT5RR3w3HbbbfB4PFi/fj3Wr1+PxMREVVLRE088YdoAn3zySZx22mnIyspCTU0NXnrpJTQ0NGD06NGmPUe8c62YH7ZCr24FX73CduGai9odUiAU6DOllRAs2KRgZPkt8h1j0XA4YV+1EVi9QJYfExQoAhhonREqNI/I4QTyC8IGMQxYiIisL+qA54wzzujQTPDKyko88MADqK2tRXp6OgYPHozly5frVnvuirzK6sqRKvYGftYrNhjaXFR5vSAAySlATbWUD6OVQxOo/aPcIh8Nh/TRtGdmqQOepGQIU2dJ/05IVLSEUHSLz8hkMENERNEHPLNmzWqPcej685//3KHPF49UQUFdLbwLb2xZntGp4CsrNqjI4Qlev3i6PKDwuFsCJr3VKo9HypEBoApAjGpsgHf9CvRZ9gAOLZ0rL3bY2ABx0zqgsAjCgjUQV90qzfQkJAK9soH9e1XHSkREXZvhbenNzc349NNP4XK5kJ6ejtNOOw3p6entPT5TVVRUWHKXVu/kRCkoqK4E6mrlAUrBUM2kWqOF8WT5LdWVrc/paQ2HE/as3tIuqEqXfLYoq49mQ1StfJx27X/VSoIgICcnB6WlpZZLMrTysQHWPj4eW3yy8rE5nU7TVnQMzfBUVlZi6dKlKC8vD1721FNPYeHChRgyZIgpA6HWs2dkwrFgNURRlGZ2QgOe6sqwOSqRqgTHtKO6x+2fuTooJSKH0pm5YT4OERFpMRTwPPfcc6isrMRll12GwYMHo7S0FFu3bsXGjRuxenX4irLUwaJsQKlKaF57p5Q/oxEA2WYshK/w+giVi9tJSqpUR4d9poiIqBUMBTzffPMNJk6ciEmTJgGQigFmZ2ejqKgI1dXV7HPVieg1oNSdyVEmNB8oaVm2Uu34EqW+WLHAmRsiImoDQ42vqqurMWzYMNllgZ9ramrMHxW1WmBJx7ZQmnnzrbwV3qJC+NbeJc3kuA4Dxd9LQREQOal3zy54iwr9uTGrzO1q7vBvcU/s2FpORETU9Ria4fH5fEhISJBdFvjZG6tv/BSkVXhQuVQFh6LytH9mRzkjpGoQ6vNJAdLi6dKykpaERMBma6lWbLNL29dz8oDDh9S1chKTgDm3Ay8/JT1vajrwc3H4YKquNopXhIiISM7wtvRDhw7JOqH7fL7g5UqDBg0yYWhklFbhQd2iggHBmR15kCFcOxfisj+pa+uEawqamg570aMtSc2BooQHflY/Tv5g2BffI0+A1qoFZA8pbigbLxERUfQMBzzr1q3TvHzt2rWqy55//vnWj4iipll4UJm8nJevSkYG1EnL4qZ1+qVzPG5pJsenCGLSM6QAZs8u+eVaTT3raiHWVkl1dfRk9YFt4Ropf6iuFt7UdCYpExFRmxgKeGbMmNHe46A2UBUeDC5rGahHo1WFOUxBQRXBP+tndLt6RqYUZIWr55ORCSE9A44Fqy1bW4KIiDqWoYBnzJgx7TwMaousxWtaCg+GBDeGdjWlpstnglLTpZyb2mr57RxO7SBFgPq2gEbLCSHY08q38lb5be0OoN9AKU+HW86JiKgdRN1agjqf0MKDbVa6X31ZUrK0JKY1i+PzAVUu9eU9ekq9tLRmmJTLbYIAOBywLVzTKasiExFR/DO0LZ0sTLn7qalRniwMACmp0qxLwVBpNkZJK9DKzIK9sAj2lY/AXlgkC2SCjxXYOeZxy7fKExERmYwBT1entfvJoyg1kJnVskTWo2fkx0xKDrssFXws5XNXVsBbVAjvwhuDtX+IiIjMwICnq/vdVI0LQ2ZslMFLpO3hgg225Q8bW5pSPlb9MVlxRO/6FZEfg4iIyAAGPF3dQ3eFvz41Xb0clT9YWo5yOAG7XX77Hj0N5+EEl7ay+kj/VxY2jFRLiIiIyCAmLXd1yirISopZGCE9A/bF9wR/VnVQz8wy/NTKnWTeokKgskL3uYmIiFqLMzxdXUKi/OfEJNmsS6Qt4spZmrZsKVc+ln3molY/FhERUSjO8FiMbld0HcKCNRBX3SrN9CQkQliwBra+/Q0/n+F6P614LEEQTHlcIiIiBjwWo2wV4duwMmxAYuvbH1jLViBERGRtDHisRqNVRLSzPkRERFbDHB6rUSb6BnpXhWz3ZoE/IiLqahjwWIxmErFWg1AiIqIuhEtaFqOZRKzsXcXt3kRE1MVwhqcLMHPrOBERUTziDE8XYObWcSIionjEGR4iIiKyPAY8REREZHkMeIiIiMjyGPAQERGR5THgISIiIsuLu4Bn69atmDx5Mh5//PFYD4WIiIjiRFwFPMXFxXj77bcxYMCAWA+FiIiI4kjcBDyNjY1Yu3Ytpk+fjm7dusV6OERERBRH4qbw4MaNGzFy5EiMGDECW7ZsCXtbt9sNt9sd/FkQBCQnJ0MQBAiC0N5D7VCB47HacQE8tnhl5WMDrH18PLb41BWOzQxxEfB89NFH2Lt3L1auNNble+vWrdi8eXPw54EDB6KoqAhZWVntNcSYy87OjvUQ2g2PLT5Z+dgAax8fjy0+WfnYzNDpAx6Xy4XHH38cixcvRkJCgqH7TJw4EePHjw/+HIgQXS6XbObHCgRBQHZ2NsrKyiCKYqyHYyoeW3yy8rEB1j4+Hlt8svKxOZ1O0yYrOn3As2fPHtTU1GDBggXBy3w+H77//nu8+eabeOaZZ2CzyVORnE4nnE6n6rFEUbTchyGAxxafeGzxy8rHx2OLT1Y8NjOPp9MHPCeeeCLuvvtu2WUbNmxAbm4uJkyYoAp2iIiIiJQ6fcCTnJyM/v37yy5LTExEWlqa6nIiIiIiLZweISIiIsvr9DM8Wu64445YD4GIiIjiCGd4iIiIyPIY8BAREZHlMeAhIiIiy2PAQ0RERJbHgIeIiIgsjwEPERERWR4DHiIiIrI8BjxERERkeQx4iIiIyPIY8BAREZHlMeAhIiIiy2PAQ0RERJbHgIeIiIgsjwEPERERWR4DHiIiIrI8BjxERERkeQx4iIiIyPIY8BAREZHlMeAhIiIiy2PAQ0RERJbHgIeIiIgsjwEPERERWR4DHiIiIrI8BjxERERkeQx4iIiIyPIY8BAREZHlMeAhIiIiy2PAQ0RERJbHgIeIiIgsjwEPERERWZ4j1gOI5K233sJbb72FiooKAEBeXh4mTZqEkSNHxnhkREREFC86fcCTmZmJq666CtnZ2QCA999/H6tXr8bq1avRr1+/GI+OiIiI4kGnD3hOO+002c9TpkzBW2+9hZ9++okBDxERERnS6QOeUD6fD//5z3/Q1NSEIUOG6N7O7XbD7XYHfxYEAcnJyXA44upwDREEAQDgdDohimKMR2MuHlt8svKxAdY+Ph5bfLLysZl53hbEOHh19u3bh8WLF8PtdiMpKQl/+tOfcMopp+je/oUXXsDmzZuDP48aNQpz587tiKESERGRydxuN5xOZ5seIy52aeXm5mLNmjVYvnw5LrzwQqxbtw4HDhzQvf3EiRPx+OOPB/+bOnUqHnjgATQ0NHTgqDtGQ0MDCgsLeWxxhscWv6x8fDy2+GT1Y3vggQdkqzatFRcBj8PhQHZ2No477jhcddVVyM/PxxtvvKF7e6fTiZSUlOB/ycnJ+Oijjyw31QcAoihi7969PLY4w2OLX1Y+Ph5bfLL6sX300UemPFZcBDxKoiiaEu0RERFR19DpA55nnnkG33//PcrLy7Fv3z48++yz+O6773DOOefEemhEREQUJzr9tqWamho89NBDqKqqQkpKCgYMGIDFixdjxIgRhh/D6XRi0qRJbU546ox4bPGJxxa/rHx8PLb4xGMzJi52aRERERG1Radf0iIiIiJqKwY8REREZHkMeIiIiMjyGPAQERGR5XX6XVpt8dZbb+Gtt95CRUUFACAvLw+TJk3CyJEjYzwyc23duhXPPvssxo0bh2uvvTbWw2kzZWsQAOjevTseeeSRGI3IXJWVldi0aRO++uorNDc3IycnBzNmzMCgQYNiPbQ2mTVrVvB3LdSFF16IG264IQYjMo/X68WLL76IDz74ANXV1ejRowfGjBmDSy+9FDZb/H9vbGhowPPPP49PP/0UNTU1GDhwIK699loUFBTEemhR2blzJ1555RXs3bsXVVVVmDdvHk4//fTg9aIo4sUXX8Q777yDuro6DB48GNOmTYubRtSRju+TTz7B22+/jT179uDo0aNYvXo18vPzYzfgKIQ7No/Hg+eeew5ffvklysvLkZKSghNPPBFXXXUVMjMzDT+HpQOezMxMXHXVVcjOzgYAvP/++1i9ejVWr14dNx/wSIqLi/H2229jwIABsR6Kqfr164clS5YEf7bCSQUA6urqsGTJEgwfPhyLFi1Ceno6Dh8+jJSUlFgPrc1WrlwJn88X/Hnfvn246667cOaZZ8ZwVObYtm0btm/fjlmzZiEvLw979uzB+vXrkZKSgnHjxsV6eG32t7/9Dfv378fs2bORmZmJf//737jzzjtx3333RXVCibWmpibk5+dj7NixuOeee1TXb9u2Da+//jpmzpyJnJwcbNmyBXfddRfuv/9+JCcnx2DE0Yl0fE1NTTj++OPxy1/+Eg8//HAMRth64Y6tubkZe/fuxWWXXYb8/HzU1dXhiSeewOrVq7Fq1SrDz2HpgOe0006T/TxlyhS89dZb+OmnnywR8DQ2NmLt2rWYPn06tmzZEuvhmMpmsyEjIyPWwzDdtm3b0LNnT8ycOTN4We/evWM4IvOkp6fLfn755ZfRp08fDBs2LEYjMs+PP/6I0047Ldi0uHfv3vjwww+xe/fuGI+s7Zqbm/HJJ59g/vz5wfdq8uTJ+Oyzz/DWW2/hyiuvjPEIjRs5cqTuDL4oinjjjTcwceJEnHHGGQCkWckbb7wRH374IS644IKOHGqrhDs+ADj33HMBAOXl5R01JNOEO7aUlBTZF2AAuO6667Bo0SK4XC5kZWUZeg5rfG02wOfz4aOPPkJTUxOGDBkS6+GYYuPGjRg5cmRURRjjRVlZGaZPn45Zs2bh/vvvx+HDh2M9JFN8/vnnGDRoEO69917ccMMNmD9/Pt5+++1YD8t0Ho8HH3zwAcaOHQtBEGI9nDb7xS9+gW+//RaHDh0CAJSUlGDXrl2WWB73er3w+Xyqwm4JCQn44YcfYjQq85WXl6O6uhonnXRS8DKn04lhw4Zh165dMRwZtUZ9fT0EQYhqdtzSMzyANK2+ePFiuN1uJCUlYd68ecjLy4v1sNrso48+wt69e7Fy5cpYD8V0gwcPxqxZs5Cbm4vq6mps2bIFt912G+69916kpaXFenhtUl5eju3bt+Piiy/GxIkTUVxcjMceewxOpxOjR4+O9fBM8+mnn+LYsWMYM2ZMrIdiigkTJqC+vh5/+ctfYLPZ4PP5cOWVV+Lss8+O9dDaLDk5GUOGDMFLL72Evn37IiMjAx9++CGKi4uD6QBWUF1dDUDKBwzVvXt3uFyuGIyIWqu5uRnPPPMMRo0axYAnVG5uLtasWYNjx47hk08+wbp167Bs2bK4DnpcLhcef/xxLF68GAkJCbEejulCvzX3798fQ4YMwZw5c/D+++9j/PjxMRxZ2/l8Phx33HG46qqrAAADBw7E/v378dZbb1kq4Hnvvfdw8sknx1X+Rzgff/wxPvjgA/zpT39Cv379UFJSgscffzyYvBzvZs+ejQ0bNuCPf/wjbDYbBg4ciFGjRmHv3r2xHprplDOObDYQXzweD+6//36Iohj1ZgjLBzwOhyP4LeW4447D7t278cYbb+Cmm26K8chab8+ePaipqcGCBQuCl/l8Pnz//fd488038cwzz1gmyRcAkpKS0L9/f5SWlsZ6KG3Wo0cPVbCdl5eHTz75JEYjMl9FRQX+97//Yd68ebEeimk2bdqECRMmYNSoUQCkQLyiogIvv/yyJQKe7OxsLFu2DI2NjWhoaECPHj1w3333WSa/DEAwJzCwyy6gtrZWNetDnZPH48F9992HiooK3H777VFv9rB8wKMkiiLcbnesh9EmJ554Iu6++27ZZRs2bEBubi4mTJhgqWAHANxuNw4ePIihQ4fGeihtdvzxxwfzQAIOHTqEXr16xWhE5nvvvffQvXv3YIKvFTQ1Nal+r2w2m+VmB5KSkpCUlIS6ujp8/fXXmDp1aqyHZJrevXsjIyMD//vf/zBw4EAA0gl0586duPrqq2M8OookEOyUlZVh6dKlrUpvsHTA88wzz2DkyJHo2bMnGhsb8dFHH+G7777D4sWLYz20NklOTkb//v1llyUmJiItLU11eTx68skncdpppyErKws1NTV46aWX0NDQYIkln4svvhhLlizBli1bcNZZZ6G4uBjvvPNOXM84hvL5fNixYwdGjx4Nu90e6+GY5tRTT8WWLVuQlZWFvLw8lJSU4LXXXsPYsWNjPTRTfPXVVwCkFICysjI89dRTyM3NjbvZq8bGRpSVlQV/Li8vR0lJCVJTU5GVlYVx48Zh69atyMnJQXZ2NrZu3YrExMS4ycWKdHx1dXVwuVyorKwEgOCXq4yMjE6/6zXcsfXo0QP33nsv9u7di8LCQvh8vmBOVmpqKhwOY6GMpbulb9iwAd9++y2qqqqQkpKCAQMGYMKECZbc1XTHHXcgPz/fEoUH77//fnz//feora1Feno6Bg8ejCuvvDKu865C/fe//8UzzzyDsrIy9O7dGxdffDF+9atfxXpYpvj666+xfPly3H///cjNzY31cEyjLMyXmZmJUaNGYdKkSYb/2HZmH3/8MZ599lkcOXIEqampOOOMMzBlypS4qw/13XffYdmyZarLR48ejVmzZgULD7799ts4duwYCgoKMG3atLj5ohjp+Hbs2IH169errp80aRImT57cEUNstXDHdvnll2P27Nma91u6dCmGDx9u6DksHfAQERERAV2oDg8RERF1XQx4iIiIyPIY8BAREZHlMeAhIiIiy2PAQ0RERJbHgIeIiIgsjwEPERERWR4DHiIiIrK8+C8RSkSmMFqJNZrKpvFg3bp12LlzJ9atWxfroRBRO2LAQ0QAgLvuukv280svvYTvvvsOt99+u+xyq7T4IKKuhQEPEQEAhgwZIvs5PT0dgiCoLldqampCYmJiew6NiKjNGPAQkWF33HEHjh49imnTpuGZZ55BSUkJTjvtNPz5z3/G5MmTNZsUzpo1C8OGDcOsWbOCl1VXV+OFF17AF198EWzGOWbMGFx66aVhu6yvXr0aJSUleOihh2CzyVMQFy1aBK/Xi6KiIgDAm2++if/85z84ePAgmpqa0Lt3b5x77rm4+OKLwzb8LC8vx+zZszFz5kxVt3CtYywtLcULL7yAb775BvX19ejTpw9+/etf4ze/+U3wNj6fD1u3bsW///1vuFwuOJ1OZGVl4bzzzsO4ceP0X3AiMg0DHiKKSlVVFdauXYsJEyZgypQpEAQhqvtXV1dj4cKFsNlsmDRpEvr06YMff/wRW7ZsQUVFBWbOnKl73/POOw+rV6/Gt99+ixEjRgQvP3jwIIqLi3HdddcFLzt8+DBGjRqF3r17w+Fw4Oeff8aWLVtw8ODBsM8RjQMHDuC2225DVlYW/vCHPyAjIwNfffUVHnvsMRw9ehSXX345AOCVV17Biy++iEsvvRTDhg2Dx+PBoUOHcOzYMVPGQUSRMeAhoqjU1dXh5ptvxgknnNCq+7/wwgs4duwY7r33XmRlZQEATjzxRCQkJOCpp57CJZdcopsnNHLkSHTv3h07duyQBTzvvfceHA4Hzj777OBl11xzTfDfPp8PQ4cORVpaGtavX48//OEPSE1NbdX4Qz3xxBNITk7GX//6V6SkpAAARowYAY/Hg5dffhkXXXQRUlNT8cMPP6B///6ymaGTTz65zc9PRMZxWzoRRaVbt26tDnYA4IsvvsDw4cPRo0cPeL3e4H8jR44EAOzcuVP3vna7Heeccw4++eQT1NfXA5CCmQ8++ACnnXYa0tLSgrfdu3cvioqKcP311+PKK6/ElClT8NBDD8Hn86G0tLTV4w9obm7Gt99+i//7v/9DYmKi6ljcbjd++uknAEBBQQF+/vlnbNy4EV999VVw7ETUcTjDQ0RR6dGjR5vuX1NTg//+97+YMmWK5vW1tbVh73/eeefhtddew0cffYQLLrgAX331FaqqqjB27NjgbVwuF26//Xbk5ubi2muvRe/eveF0OlFcXIxHH30Uzc3NbToGQJrp8nq9ePPNN/Hmm29q3ubo0aMAgIkTJyIpKQkffPABtm/fDpvNhqFDh+Lqq6/Gcccd1+axEFFkDHiIKCp6OTtOpxMej0d1eeCkH5CWloYBAwbgyiuv1HycSAFVXl4eCgoKsGPHDlxwwQXYsWMHevTogZNOOil4m08//RRNTU2YN28eevXqFby8pKQk7GMDQEJCAgDA7XaHPY5u3brBZrPh3HPPxa9//WvNx+rduzcAaWZq/PjxGD9+PI4dO4ZvvvkGzz77LJYvX44NGzZwlxtRB2DAQ0Sm6NWrF37++WfZZd9++y0aGxtll51yyin48ssv0adPn1bn0YwZMwYbN27EDz/8gP/+97+4+OKLZbu2AkGZ0+kMXiaKIt55552Ij929e3c4nU7VsXz22WeynxMTEzF8+HDs3bsXAwYMCLvzK1S3bt3wy1/+EpWVlXj88cdRUVHB2kZEHYABDxGZ4txzz8Xzzz+P559/HsOGDcOBAwfw5ptvBpN5A6644gp88803WLJkCS666CLk5uaiubkZFRUV+PLLL3HjjTeiZ8+eYZ/r7LPPxpNPPokHHngAbrdbtX18xIgRcDgceOCBB3DJJZfA7XbjrbfeMrQrShAEnHPOOXjvvfeQnZ2NAQMGoLi4GB9++KHqttdddx2WLFmC22+/HRdeeCF69eqFhoYGlJWV4b///S+WLl0KAFi1ahX69++PQYMGIT09HS6XC6+//jp69eqF7OzsiGMiorZjwENEprjkkktQX1+PHTt24NVXX0VBQQH+8pe/YM2aNbLb9ejRAytXrsRLL72EV155BUeOHEFycjJ69+6Nk08+Gd26dYv4XCkpKTj99NPx4Ycf4vjjj0dubq7s+r59++KWW27Bc889h7vvvhtpaWk4++yzMX78eKxYsSLi4//hD38AAGzbtg2NjY044YQTsGDBAlktIUBaXisqKsJLL72E5557DjU1NejWrRtycnKCSdgAcMIJJ+CTTz7BO++8g4aGBmRkZGDEiBG47LLLDM8MEVHbCKIoirEeBBEREVF74rZ0IiIisjwGPERERGR5DHiIiIjI8hjwEBERkeUx4CEiIiLLY8BDRERElseAh4iIiCyPAQ8RERFZHgMeIiIisjwGPERERGR5DHiIiIjI8v4fSlH6f4404TsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6553 with a standard deviation of 0.0199\n",
      "KNN optimized model r2_score 0.6650 with a standard deviation of 0.0218\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn.joblib']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"OUTPUT/knn_reg.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.684736     0.020457\n",
      "1                    TP       200.400000     7.515909\n",
      "2                    TN       173.800000     4.709329\n",
      "3                    FP        39.300000     4.448470\n",
      "4                    FN        35.700000     4.029061\n",
      "5              Accuracy         0.833037     0.011300\n",
      "6             Precision         0.835924     0.019541\n",
      "7           Sensitivity         0.848787     0.016122\n",
      "8           Specificity         0.815910     0.015700\n",
      "9              F1 score         0.842150     0.013387\n",
      "10  F1 score (weighted)         0.832979     0.011273\n",
      "11     F1 score (macro)         0.832367     0.011051\n",
      "12    Balanced Accuracy         0.832348     0.010790\n",
      "13                  MCC         0.665184     0.021877\n",
      "14                  NPV         0.829750     0.016865\n",
      "15              ROC_AUC         0.832348     0.010790\n",
      "CPU times: user 44.5 s, sys: 3.9 ms, total: 44.5 s\n",
      "Wall time: 44.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:57:46,177] A new study created in memory with name: SVM_regressor_CV\n",
      "[I 2023-12-20 05:58:13,293] Trial 0 finished with value: 0.6593861304397464 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 0 with value: 0.6593861304397464.\n",
      "[I 2023-12-20 05:58:36,509] Trial 1 finished with value: 0.6566779176516353 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 0 with value: 0.6593861304397464.\n",
      "[I 2023-12-20 05:59:01,269] Trial 2 finished with value: 0.6715341867502411 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 2 with value: 0.6715341867502411.\n",
      "[I 2023-12-20 05:59:27,456] Trial 3 finished with value: 0.036476146431117254 and parameters: {'C': 64.0, 'gamma': 2.0}. Best is trial 2 with value: 0.6715341867502411.\n",
      "[I 2023-12-20 05:59:51,527] Trial 4 finished with value: 0.5626830814991028 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 2 with value: 0.6715341867502411.\n",
      "[I 2023-12-20 06:00:18,337] Trial 5 finished with value: 0.1394339951859795 and parameters: {'C': 0.03125, 'gamma': 0.00390625}. Best is trial 2 with value: 0.6715341867502411.\n",
      "[I 2023-12-20 06:00:41,791] Trial 6 finished with value: 0.28498260910658535 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 2 with value: 0.6715341867502411.\n",
      "[I 2023-12-20 06:01:06,780] Trial 7 finished with value: 0.015429428140041779 and parameters: {'C': 0.0078125, 'gamma': 0.0625}. Best is trial 2 with value: 0.6715341867502411.\n",
      "[I 2023-12-20 06:01:31,853] Trial 8 finished with value: 0.0009111469637155811 and parameters: {'C': 0.0625, 'gamma': 1.0}. Best is trial 2 with value: 0.6715341867502411.\n",
      "[I 2023-12-20 06:01:56,472] Trial 9 finished with value: 0.6843984166339483 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.6843984166339483.\n",
      "[I 2023-12-20 06:02:23,213] Trial 10 finished with value: 0.6887887958440939 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:02:48,922] Trial 11 finished with value: 0.6887887958440939 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:03:13,226] Trial 12 finished with value: 0.6887887958440939 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:03:40,932] Trial 13 finished with value: 0.03309073133595643 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:04:05,960] Trial 14 finished with value: 0.6843110329347271 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:04:28,683] Trial 15 finished with value: 0.5323971686254124 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:04:54,335] Trial 16 finished with value: 0.03283307309843014 and parameters: {'C': 0.25, 'gamma': 0.25}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:05:21,050] Trial 17 finished with value: 0.04778975752383348 and parameters: {'C': 0.015625, 'gamma': 0.001953125}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:05:48,687] Trial 18 finished with value: 0.004641833391671968 and parameters: {'C': 0.125, 'gamma': 8.0}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:06:12,802] Trial 19 finished with value: 0.24595240173353666 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:06:37,052] Trial 20 finished with value: 0.05347650688943286 and parameters: {'C': 0.5, 'gamma': 6.103515625e-05}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:07:01,866] Trial 21 finished with value: 0.6887887958440939 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:07:27,187] Trial 22 finished with value: 0.6887887958440939 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:07:51,495] Trial 23 finished with value: 0.41389176390667937 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:08:17,374] Trial 24 finished with value: 0.05825958317154325 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:08:41,932] Trial 25 finished with value: 0.24728427552606097 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:09:06,095] Trial 26 finished with value: 0.6887887958440939 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:09:31,630] Trial 27 finished with value: 0.07414939166949619 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:09:56,550] Trial 28 finished with value: 0.6843110329347271 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:10:20,595] Trial 29 finished with value: 0.5776260180856391 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:10:45,563] Trial 30 finished with value: 0.025588873291533597 and parameters: {'C': 0.5, 'gamma': 2.0}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:11:09,778] Trial 31 finished with value: 0.6887887958440939 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 10 with value: 0.6887887958440939.\n",
      "[I 2023-12-20 06:11:33,523] Trial 32 finished with value: 0.6905429710671578 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:11:55,210] Trial 33 finished with value: 0.6566779176516353 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:12:18,284] Trial 34 finished with value: 0.6753731762996641 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:12:39,431] Trial 35 finished with value: 0.41388652038832163 and parameters: {'C': 2.0, 'gamma': 0.00048828125}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:13:05,427] Trial 36 finished with value: 0.1394339951859795 and parameters: {'C': 0.03125, 'gamma': 0.00390625}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:13:31,446] Trial 37 finished with value: 0.10417322523670965 and parameters: {'C': 0.25, 'gamma': 0.125}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:13:56,053] Trial 38 finished with value: 0.22580443288709534 and parameters: {'C': 0.125, 'gamma': 0.0625}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:14:21,372] Trial 39 finished with value: 0.0009111469637155811 and parameters: {'C': 0.0625, 'gamma': 1.0}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:14:46,218] Trial 40 finished with value: 0.13123559509284996 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:15:11,335] Trial 41 finished with value: 0.6887887958440939 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:15:35,975] Trial 42 finished with value: 0.6847138719855689 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:16:03,141] Trial 43 finished with value: 0.03309073133595643 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 32 with value: 0.6905429710671578.\n",
      "[I 2023-12-20 06:16:27,246] Trial 44 finished with value: 0.6923633479626572 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:16:50,018] Trial 45 finished with value: 0.41391717875664885 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:17:19,189] Trial 46 finished with value: 0.034333475686722834 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:17:44,058] Trial 47 finished with value: 0.09788316965562513 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:18:08,149] Trial 48 finished with value: 0.6843984166339483 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:18:34,897] Trial 49 finished with value: 0.036303731157253105 and parameters: {'C': 1.0, 'gamma': 2.0}. Best is trial 44 with value: 0.6923633479626572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6924\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.690919\n",
      "1                    TP  403.000000\n",
      "2                    TN  348.000000\n",
      "3                    FP   78.000000\n",
      "4                    FN   70.000000\n",
      "5              Accuracy    0.835373\n",
      "6             Precision    0.837838\n",
      "7           Sensitivity    0.852008\n",
      "8           Specificity    0.816900\n",
      "9              F1 score    0.844864\n",
      "10  F1 score (weighted)    0.835283\n",
      "11     F1 score (macro)    0.834754\n",
      "12    Balanced Accuracy    0.834455\n",
      "13                  MCC    0.669641\n",
      "14                  NPV    0.832500\n",
      "15              ROC_AUC    0.834455\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_0_cat = np.where((y_pred_svm_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 06:19:03,987] Trial 50 finished with value: 0.10003611793267146 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:19:29,345] Trial 51 finished with value: 0.6096502729025851 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:19:53,450] Trial 52 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:20:17,704] Trial 53 finished with value: 0.3291300613454229 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:20:41,354] Trial 54 finished with value: 0.005787489501689091 and parameters: {'C': 0.03125, 'gamma': 0.0001220703125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:21:06,987] Trial 55 finished with value: 0.3175009230124525 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:21:31,479] Trial 56 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:21:56,637] Trial 57 finished with value: -0.0016638425386193423 and parameters: {'C': 0.0078125, 'gamma': 0.5}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:22:22,448] Trial 58 finished with value: 0.657352115767894 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:22:45,539] Trial 59 finished with value: 0.6644317273492976 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:23:08,638] Trial 60 finished with value: 0.6805519072484911 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:23:32,074] Trial 61 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:23:55,059] Trial 62 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:24:20,055] Trial 63 finished with value: 0.5190885397400723 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:24:43,156] Trial 64 finished with value: 0.6021249138264121 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:25:07,096] Trial 65 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:25:33,817] Trial 66 finished with value: 0.10871393481014961 and parameters: {'C': 0.25, 'gamma': 0.125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:25:59,384] Trial 67 finished with value: 0.31072707170265973 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:26:23,324] Trial 68 finished with value: 0.4744915735265269 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:26:48,563] Trial 69 finished with value: 0.5514470155463038 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:27:12,620] Trial 70 finished with value: 0.041325277884484 and parameters: {'C': 1.0, 'gamma': 4.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:27:37,485] Trial 71 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:28:03,024] Trial 72 finished with value: 0.051698699573503336 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:28:27,149] Trial 73 finished with value: 0.13402895560783848 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:28:53,123] Trial 74 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:29:16,577] Trial 75 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:29:42,066] Trial 76 finished with value: 0.09531381127959122 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:30:06,104] Trial 77 finished with value: 0.47063456898191147 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:30:28,775] Trial 78 finished with value: 0.5340816500678012 and parameters: {'C': 2.0, 'gamma': 0.001953125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:30:56,606] Trial 79 finished with value: 0.0405025292263146 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:31:22,079] Trial 80 finished with value: 0.07671142322805916 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:31:46,260] Trial 81 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:32:10,776] Trial 82 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:32:33,990] Trial 83 finished with value: 0.3291300613454229 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:32:57,881] Trial 84 finished with value: 0.005787489501689091 and parameters: {'C': 0.03125, 'gamma': 0.0001220703125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:33:21,414] Trial 85 finished with value: 0.6793105953521401 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:33:46,166] Trial 86 finished with value: 0.24750465133018143 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:34:09,615] Trial 87 finished with value: 0.005689970333749228 and parameters: {'C': 0.0625, 'gamma': 0.5}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:34:34,065] Trial 88 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:34:59,093] Trial 89 finished with value: 0.033286143846809925 and parameters: {'C': 0.5, 'gamma': 2.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:35:22,130] Trial 90 finished with value: 0.6656957085143863 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:35:45,215] Trial 91 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:36:08,934] Trial 92 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:36:33,793] Trial 93 finished with value: 0.6816562475411623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:36:58,376] Trial 94 finished with value: 0.6681949257070163 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:37:22,554] Trial 95 finished with value: 0.5198655367788036 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:37:46,498] Trial 96 finished with value: 0.5190885397400723 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:38:10,548] Trial 97 finished with value: 0.6776589207727877 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:38:36,021] Trial 98 finished with value: 0.31072707170265973 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:39:01,518] Trial 99 finished with value: 0.005733214696976419 and parameters: {'C': 0.015625, 'gamma': 0.125}. Best is trial 44 with value: 0.6923633479626572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6924\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.690919    0.729222\n",
      "1                    TP  403.000000  408.000000\n",
      "2                    TN  348.000000  348.000000\n",
      "3                    FP   78.000000   74.000000\n",
      "4                    FN   70.000000   69.000000\n",
      "5              Accuracy    0.835373    0.840934\n",
      "6             Precision    0.837838    0.846473\n",
      "7           Sensitivity    0.852008    0.855346\n",
      "8           Specificity    0.816900    0.824600\n",
      "9              F1 score    0.844864    0.850886\n",
      "10  F1 score (weighted)    0.835283    0.840875\n",
      "11     F1 score (macro)    0.834754    0.840223\n",
      "12    Balanced Accuracy    0.834455    0.839995\n",
      "13                  MCC    0.669641    0.680498\n",
      "14                  NPV    0.832500    0.834500\n",
      "15              ROC_AUC    0.834455    0.839995\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_1_cat = np.where((y_pred_svm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 06:39:30,218] Trial 100 finished with value: 0.5607542974981008 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:39:54,985] Trial 101 finished with value: 0.6834547348920956 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:40:21,490] Trial 102 finished with value: 0.6834547348920956 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:40:46,844] Trial 103 finished with value: 0.6834547348920956 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:41:10,528] Trial 104 finished with value: 0.40357758177244474 and parameters: {'C': 1.0, 'gamma': 0.0009765625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:41:35,422] Trial 105 finished with value: 0.6846467577772148 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:42:02,443] Trial 106 finished with value: 0.04843030318172751 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:42:27,700] Trial 107 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:42:54,112] Trial 108 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:43:21,334] Trial 109 finished with value: 0.03900503404561932 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:43:46,106] Trial 110 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:44:11,555] Trial 111 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:44:37,784] Trial 112 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:45:02,744] Trial 113 finished with value: 0.5075465831299295 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:45:32,085] Trial 114 finished with value: 0.04037063974954545 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:45:57,926] Trial 115 finished with value: 0.0970998993467575 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:46:21,648] Trial 116 finished with value: 0.6837467083963107 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:46:48,839] Trial 117 finished with value: 0.6048201363684493 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:47:12,972] Trial 118 finished with value: 0.6846467577772148 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:47:39,464] Trial 119 finished with value: 0.6826143998440934 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:48:05,778] Trial 120 finished with value: 0.0001911076438653403 and parameters: {'C': 0.03125, 'gamma': 3.0517578125e-05}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:48:30,930] Trial 121 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:48:57,266] Trial 122 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:49:22,097] Trial 123 finished with value: 0.4031502316059973 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:49:47,208] Trial 124 finished with value: 0.000178162790780545 and parameters: {'C': 0.0078125, 'gamma': 0.0001220703125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:50:11,269] Trial 125 finished with value: 0.6846467577772148 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:50:35,726] Trial 126 finished with value: 0.3193406656559192 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:51:02,267] Trial 127 finished with value: 0.059303315373874364 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:51:26,079] Trial 128 finished with value: 0.6041100681346296 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:51:50,746] Trial 129 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:52:19,773] Trial 130 finished with value: 0.04179906250587456 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:52:44,602] Trial 131 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:53:11,617] Trial 132 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:53:37,743] Trial 133 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:54:03,167] Trial 134 finished with value: 0.6706633812243815 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:54:29,863] Trial 135 finished with value: 0.46785783671398357 and parameters: {'C': 0.25, 'gamma': 0.0078125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:54:54,360] Trial 136 finished with value: 0.6837467083963107 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:55:20,633] Trial 137 finished with value: 0.09935348013825782 and parameters: {'C': 0.125, 'gamma': 0.00048828125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:55:45,957] Trial 138 finished with value: 0.6846467577772148 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:56:12,245] Trial 139 finished with value: 0.1360051446241113 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:56:42,004] Trial 140 finished with value: 0.6371968382390397 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:57:08,519] Trial 141 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:57:34,610] Trial 142 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:57:59,647] Trial 143 finished with value: 0.6847319395109344 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:58:28,003] Trial 144 finished with value: 0.28630045291763195 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:58:52,197] Trial 145 finished with value: 0.6846467577772148 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:59:17,254] Trial 146 finished with value: 0.538935413494352 and parameters: {'C': 1.0, 'gamma': 0.0625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 06:59:42,352] Trial 147 finished with value: 0.6846467577772148 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:00:08,286] Trial 148 finished with value: 0.5647108045437124 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:00:35,036] Trial 149 finished with value: 0.04843030318172751 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 44 with value: 0.6923633479626572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6924\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.690919    0.729222    0.719859\n",
      "1                    TP  403.000000  408.000000  381.000000\n",
      "2                    TN  348.000000  348.000000  374.000000\n",
      "3                    FP   78.000000   74.000000   78.000000\n",
      "4                    FN   70.000000   69.000000   66.000000\n",
      "5              Accuracy    0.835373    0.840934    0.839822\n",
      "6             Precision    0.837838    0.846473    0.830065\n",
      "7           Sensitivity    0.852008    0.855346    0.852349\n",
      "8           Specificity    0.816900    0.824600    0.827400\n",
      "9              F1 score    0.844864    0.850886    0.841060\n",
      "10  F1 score (weighted)    0.835283    0.840875    0.839805\n",
      "11     F1 score (macro)    0.834754    0.840223    0.839812\n",
      "12    Balanced Accuracy    0.834455    0.839995    0.839891\n",
      "13                  MCC    0.669641    0.680498    0.679924\n",
      "14                  NPV    0.832500    0.834500    0.850000\n",
      "15              ROC_AUC    0.834455    0.839995    0.839891\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_2_cat = np.where((y_pred_svm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:01:03,244] Trial 150 finished with value: 0.6845189427911448 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:01:28,218] Trial 151 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:01:54,422] Trial 152 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:02:20,095] Trial 153 finished with value: 0.027749534793615305 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:02:46,518] Trial 154 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:03:11,712] Trial 155 finished with value: 0.6784690367640496 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:03:37,030] Trial 156 finished with value: 0.32486565054666616 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:04:02,625] Trial 157 finished with value: 0.07491551123814576 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:04:29,040] Trial 158 finished with value: 0.21369623033228025 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:04:57,384] Trial 159 finished with value: 0.02725105767825825 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:05:24,128] Trial 160 finished with value: 0.6062676889535286 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:05:50,059] Trial 161 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:06:17,191] Trial 162 finished with value: 0.09231739853023062 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:06:41,061] Trial 163 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:07:04,053] Trial 164 finished with value: 0.6799928132892382 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:07:27,255] Trial 165 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:07:51,288] Trial 166 finished with value: 0.6784690367640496 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:08:13,020] Trial 167 finished with value: 0.24297741157161115 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:08:35,221] Trial 168 finished with value: 0.054179268985541006 and parameters: {'C': 0.5, 'gamma': 6.103515625e-05}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:08:56,420] Trial 169 finished with value: 0.012230333503897073 and parameters: {'C': 0.0625, 'gamma': 0.0001220703125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:09:20,328] Trial 170 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:09:43,941] Trial 171 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:10:07,192] Trial 172 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:10:30,622] Trial 173 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:10:54,755] Trial 174 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:11:16,854] Trial 175 finished with value: 0.013741779555384703 and parameters: {'C': 0.25, 'gamma': 2.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:11:40,231] Trial 176 finished with value: 0.6784690367640496 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:12:04,196] Trial 177 finished with value: 0.05516600371120887 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:12:26,885] Trial 178 finished with value: 0.6815068969068288 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:12:52,356] Trial 179 finished with value: 0.37502927031026295 and parameters: {'C': 0.125, 'gamma': 0.0078125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:13:19,188] Trial 180 finished with value: 0.6784690367640496 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:13:46,714] Trial 181 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:14:13,359] Trial 182 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:14:38,983] Trial 183 finished with value: 0.672879442261051 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:15:04,878] Trial 184 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:15:31,054] Trial 185 finished with value: 0.1314934051797778 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:15:56,161] Trial 186 finished with value: 0.5499747046361716 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:16:21,886] Trial 187 finished with value: 0.6800570365227248 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:16:46,928] Trial 188 finished with value: 0.6784690367640496 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:17:11,882] Trial 189 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:17:35,393] Trial 190 finished with value: 0.5485717314958184 and parameters: {'C': 1.0, 'gamma': 0.00390625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:18:01,061] Trial 191 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:18:25,997] Trial 192 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:18:53,807] Trial 193 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:19:19,878] Trial 194 finished with value: 0.6784690367640496 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:19:45,288] Trial 195 finished with value: 0.6794152628008806 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:20:11,344] Trial 196 finished with value: 0.5629934071290019 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:20:38,738] Trial 197 finished with value: 0.28346682607572815 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:21:03,369] Trial 198 finished with value: 0.6845189427911448 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:21:27,296] Trial 199 finished with value: 0.5606929722497374 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 44 with value: 0.6923633479626572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6924\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.690919    0.729222    0.719859    0.721196\n",
      "1                    TP  403.000000  408.000000  381.000000  406.000000\n",
      "2                    TN  348.000000  348.000000  374.000000  354.000000\n",
      "3                    FP   78.000000   74.000000   78.000000   66.000000\n",
      "4                    FN   70.000000   69.000000   66.000000   73.000000\n",
      "5              Accuracy    0.835373    0.840934    0.839822    0.845384\n",
      "6             Precision    0.837838    0.846473    0.830065    0.860169\n",
      "7           Sensitivity    0.852008    0.855346    0.852349    0.847599\n",
      "8           Specificity    0.816900    0.824600    0.827400    0.842900\n",
      "9              F1 score    0.844864    0.850886    0.841060    0.853838\n",
      "10  F1 score (weighted)    0.835283    0.840875    0.839805    0.845454\n",
      "11     F1 score (macro)    0.834754    0.840223    0.839812    0.844865\n",
      "12    Balanced Accuracy    0.834455    0.839995    0.839891    0.845228\n",
      "13                  MCC    0.669641    0.680498    0.679924    0.689833\n",
      "14                  NPV    0.832500    0.834500    0.850000    0.829000\n",
      "15              ROC_AUC    0.834455    0.839995    0.839891    0.845228\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_3_cat = np.where((y_pred_svm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:21:59,065] Trial 200 finished with value: 0.039119466108999815 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:22:24,077] Trial 201 finished with value: 0.6860419335322965 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:22:47,816] Trial 202 finished with value: 0.6860419335322965 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:23:15,342] Trial 203 finished with value: 0.6860419335322965 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:23:43,529] Trial 204 finished with value: 0.05165771349843875 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:24:10,056] Trial 205 finished with value: 0.6860419335322965 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:24:36,499] Trial 206 finished with value: 0.6860419335322965 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:25:04,045] Trial 207 finished with value: 0.07657613146803703 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:25:31,743] Trial 208 finished with value: 0.21559828967346562 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:25:57,482] Trial 209 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:26:24,778] Trial 210 finished with value: 0.5469542724196549 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:26:49,971] Trial 211 finished with value: 0.6851136262338091 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:27:14,943] Trial 212 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:27:40,812] Trial 213 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:28:05,916] Trial 214 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:28:30,987] Trial 215 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:29:00,530] Trial 216 finished with value: 0.03736876666124811 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:29:26,920] Trial 217 finished with value: 0.10133467211494032 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:29:52,704] Trial 218 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:30:19,488] Trial 219 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:30:55,856] Trial 220 finished with value: 0.6175856517395848 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:31:23,005] Trial 221 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:31:49,271] Trial 222 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:32:15,706] Trial 223 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:32:40,864] Trial 224 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:33:06,100] Trial 225 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:33:31,486] Trial 226 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:34:00,467] Trial 227 finished with value: 0.5248340110711698 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:34:26,026] Trial 228 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:34:53,396] Trial 229 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:35:19,331] Trial 230 finished with value: 0.5446226425082068 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:35:44,306] Trial 231 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:36:11,957] Trial 232 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:36:36,925] Trial 233 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:37:03,478] Trial 234 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:37:28,990] Trial 235 finished with value: 0.5482737706113591 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:37:53,867] Trial 236 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:38:21,001] Trial 237 finished with value: 0.06463096556886487 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:38:47,135] Trial 238 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:39:12,464] Trial 239 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:39:41,770] Trial 240 finished with value: 0.042373413449747634 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:40:06,835] Trial 241 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:40:32,400] Trial 242 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:40:56,773] Trial 243 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:41:20,987] Trial 244 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:41:45,577] Trial 245 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:42:09,997] Trial 246 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:42:36,106] Trial 247 finished with value: 0.6851136262338091 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:43:02,994] Trial 248 finished with value: 0.2813569104020173 and parameters: {'C': 0.0625, 'gamma': 0.0078125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:43:29,733] Trial 249 finished with value: 0.685345136576814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 44 with value: 0.6923633479626572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6924\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.690919    0.729222    0.719859    0.721196   \n",
      "1                    TP  403.000000  408.000000  381.000000  406.000000   \n",
      "2                    TN  348.000000  348.000000  374.000000  354.000000   \n",
      "3                    FP   78.000000   74.000000   78.000000   66.000000   \n",
      "4                    FN   70.000000   69.000000   66.000000   73.000000   \n",
      "5              Accuracy    0.835373    0.840934    0.839822    0.845384   \n",
      "6             Precision    0.837838    0.846473    0.830065    0.860169   \n",
      "7           Sensitivity    0.852008    0.855346    0.852349    0.847599   \n",
      "8           Specificity    0.816900    0.824600    0.827400    0.842900   \n",
      "9              F1 score    0.844864    0.850886    0.841060    0.853838   \n",
      "10  F1 score (weighted)    0.835283    0.840875    0.839805    0.845454   \n",
      "11     F1 score (macro)    0.834754    0.840223    0.839812    0.844865   \n",
      "12    Balanced Accuracy    0.834455    0.839995    0.839891    0.845228   \n",
      "13                  MCC    0.669641    0.680498    0.679924    0.689833   \n",
      "14                  NPV    0.832500    0.834500    0.850000    0.829000   \n",
      "15              ROC_AUC    0.834455    0.839995    0.839891    0.845228   \n",
      "\n",
      "          Set4  \n",
      "0     0.710927  \n",
      "1   404.000000  \n",
      "2   344.000000  \n",
      "3    79.000000  \n",
      "4    72.000000  \n",
      "5     0.832036  \n",
      "6     0.836439  \n",
      "7     0.848739  \n",
      "8     0.813200  \n",
      "9     0.842544  \n",
      "10    0.831948  \n",
      "11    0.831284  \n",
      "12    0.830989  \n",
      "13    0.662670  \n",
      "14    0.826900  \n",
      "15    0.830989  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_4_cat = np.where((y_pred_svm_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:44:01,368] Trial 250 finished with value: 0.5725572227392377 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 44 with value: 0.6923633479626572.\n",
      "[I 2023-12-20 07:44:28,328] Trial 251 finished with value: 0.7018736313932994 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:44:54,171] Trial 252 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:45:20,893] Trial 253 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:45:47,516] Trial 254 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:46:14,318] Trial 255 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:46:41,106] Trial 256 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:47:08,793] Trial 257 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:47:34,536] Trial 258 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:47:58,866] Trial 259 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:48:25,441] Trial 260 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:48:50,168] Trial 261 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:49:16,525] Trial 262 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:49:42,622] Trial 263 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:50:06,198] Trial 264 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:50:30,489] Trial 265 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:50:54,347] Trial 266 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:51:18,061] Trial 267 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:51:42,013] Trial 268 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:52:05,500] Trial 269 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:52:28,195] Trial 270 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:52:51,938] Trial 271 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:53:17,567] Trial 272 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:53:40,896] Trial 273 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:54:03,989] Trial 274 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:54:28,616] Trial 275 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:54:51,999] Trial 276 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:55:18,709] Trial 277 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:55:44,903] Trial 278 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:56:09,923] Trial 279 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:56:34,692] Trial 280 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:57:01,128] Trial 281 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:57:26,072] Trial 282 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:57:52,088] Trial 283 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:58:17,031] Trial 284 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:58:41,867] Trial 285 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:59:07,730] Trial 286 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:59:32,002] Trial 287 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 07:59:57,661] Trial 288 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:00:22,064] Trial 289 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:00:48,839] Trial 290 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:01:15,853] Trial 291 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:01:40,019] Trial 292 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:02:06,189] Trial 293 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:02:30,156] Trial 294 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:02:55,676] Trial 295 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:03:22,030] Trial 296 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:03:46,992] Trial 297 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:04:11,111] Trial 298 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:04:36,626] Trial 299 finished with value: 0.6880213768595599 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7019\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.690919    0.729222    0.719859    0.721196   \n",
      "1                    TP  403.000000  408.000000  381.000000  406.000000   \n",
      "2                    TN  348.000000  348.000000  374.000000  354.000000   \n",
      "3                    FP   78.000000   74.000000   78.000000   66.000000   \n",
      "4                    FN   70.000000   69.000000   66.000000   73.000000   \n",
      "5              Accuracy    0.835373    0.840934    0.839822    0.845384   \n",
      "6             Precision    0.837838    0.846473    0.830065    0.860169   \n",
      "7           Sensitivity    0.852008    0.855346    0.852349    0.847599   \n",
      "8           Specificity    0.816900    0.824600    0.827400    0.842900   \n",
      "9              F1 score    0.844864    0.850886    0.841060    0.853838   \n",
      "10  F1 score (weighted)    0.835283    0.840875    0.839805    0.845454   \n",
      "11     F1 score (macro)    0.834754    0.840223    0.839812    0.844865   \n",
      "12    Balanced Accuracy    0.834455    0.839995    0.839891    0.845228   \n",
      "13                  MCC    0.669641    0.680498    0.679924    0.689833   \n",
      "14                  NPV    0.832500    0.834500    0.850000    0.829000   \n",
      "15              ROC_AUC    0.834455    0.839995    0.839891    0.845228   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.710927    0.691306  \n",
      "1   404.000000  395.000000  \n",
      "2   344.000000  363.000000  \n",
      "3    79.000000   75.000000  \n",
      "4    72.000000   66.000000  \n",
      "5     0.832036    0.843159  \n",
      "6     0.836439    0.840426  \n",
      "7     0.848739    0.856833  \n",
      "8     0.813200    0.828800  \n",
      "9     0.842544    0.848550  \n",
      "10    0.831948    0.843103  \n",
      "11    0.831284    0.842960  \n",
      "12    0.830989    0.842800  \n",
      "13    0.662670    0.686090  \n",
      "14    0.826900    0.846200  \n",
      "15    0.830989    0.842800  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_5_cat = np.where((y_pred_svm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:05:05,669] Trial 300 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:05:30,432] Trial 301 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:05:57,196] Trial 302 finished with value: 0.2892785299119814 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:06:22,847] Trial 303 finished with value: 0.5137578696120787 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:06:52,260] Trial 304 finished with value: 0.6508160745428875 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:07:19,294] Trial 305 finished with value: 0.5672682525268754 and parameters: {'C': 2.0, 'gamma': 0.0625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:07:46,000] Trial 306 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:08:14,009] Trial 307 finished with value: 0.044020874574893495 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:08:40,194] Trial 308 finished with value: 0.3284899355450631 and parameters: {'C': 0.5, 'gamma': 0.0009765625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:09:08,102] Trial 309 finished with value: 0.00628603749950829 and parameters: {'C': 0.125, 'gamma': 4.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:09:34,052] Trial 310 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:10:00,739] Trial 311 finished with value: 0.6789546085936321 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:10:25,721] Trial 312 finished with value: 0.47526632191557594 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:10:54,011] Trial 313 finished with value: 0.0958363419205449 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:11:21,710] Trial 314 finished with value: 0.6017686056751212 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:11:46,626] Trial 315 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:12:17,492] Trial 316 finished with value: -0.0021776720098156234 and parameters: {'C': 0.015625, 'gamma': 8.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:12:42,812] Trial 317 finished with value: 0.6584896964942835 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:13:09,606] Trial 318 finished with value: 0.40948602954103375 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:13:38,993] Trial 319 finished with value: 0.2494584926300197 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:14:04,406] Trial 320 finished with value: 0.3324516809572867 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:14:30,293] Trial 321 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:14:56,627] Trial 322 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:15:22,865] Trial 323 finished with value: 0.6801701804119622 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:15:51,517] Trial 324 finished with value: -0.0025050678545966766 and parameters: {'C': 0.0078125, 'gamma': 0.5}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:16:18,823] Trial 325 finished with value: 0.6766916234636137 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:16:47,565] Trial 326 finished with value: -0.0006973232868302981 and parameters: {'C': 0.03125, 'gamma': 2.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:17:15,346] Trial 327 finished with value: 0.5268512306623141 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:17:42,508] Trial 328 finished with value: 0.6860255316316144 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:18:07,971] Trial 329 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:18:33,885] Trial 330 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:19:00,287] Trial 331 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:19:27,969] Trial 332 finished with value: 0.28305385993288057 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:19:57,571] Trial 333 finished with value: 0.6508160745428875 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:20:24,025] Trial 334 finished with value: 0.678962901479453 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:20:49,820] Trial 335 finished with value: 0.5695498848426428 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:21:16,360] Trial 336 finished with value: 0.2892785299119814 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:21:43,148] Trial 337 finished with value: 0.5655527382333483 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:22:09,835] Trial 338 finished with value: 0.04402087106224546 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:22:35,996] Trial 339 finished with value: 0.6801701804119622 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:23:00,614] Trial 340 finished with value: 0.6062821192262644 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:23:28,213] Trial 341 finished with value: 0.03229247527783276 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:23:54,577] Trial 342 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:24:21,769] Trial 343 finished with value: 0.05561326883523856 and parameters: {'C': 0.125, 'gamma': 0.000244140625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:24:50,008] Trial 344 finished with value: 0.0327201639923612 and parameters: {'C': 0.25, 'gamma': 0.25}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:25:17,018] Trial 345 finished with value: 0.6766916234636137 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:25:45,279] Trial 346 finished with value: 0.6017686056751212 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:26:10,646] Trial 347 finished with value: 0.6805814480357282 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:26:36,784] Trial 348 finished with value: 0.6789546085936321 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:27:08,723] Trial 349 finished with value: -0.0021776720098156234 and parameters: {'C': 0.015625, 'gamma': 8.0}. Best is trial 251 with value: 0.7018736313932994.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7019\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.690919    0.729222    0.719859    0.721196   \n",
      "1                    TP  403.000000  408.000000  381.000000  406.000000   \n",
      "2                    TN  348.000000  348.000000  374.000000  354.000000   \n",
      "3                    FP   78.000000   74.000000   78.000000   66.000000   \n",
      "4                    FN   70.000000   69.000000   66.000000   73.000000   \n",
      "5              Accuracy    0.835373    0.840934    0.839822    0.845384   \n",
      "6             Precision    0.837838    0.846473    0.830065    0.860169   \n",
      "7           Sensitivity    0.852008    0.855346    0.852349    0.847599   \n",
      "8           Specificity    0.816900    0.824600    0.827400    0.842900   \n",
      "9              F1 score    0.844864    0.850886    0.841060    0.853838   \n",
      "10  F1 score (weighted)    0.835283    0.840875    0.839805    0.845454   \n",
      "11     F1 score (macro)    0.834754    0.840223    0.839812    0.844865   \n",
      "12    Balanced Accuracy    0.834455    0.839995    0.839891    0.845228   \n",
      "13                  MCC    0.669641    0.680498    0.679924    0.689833   \n",
      "14                  NPV    0.832500    0.834500    0.850000    0.829000   \n",
      "15              ROC_AUC    0.834455    0.839995    0.839891    0.845228   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.710927    0.691306    0.700637  \n",
      "1   404.000000  395.000000  400.000000  \n",
      "2   344.000000  363.000000  349.000000  \n",
      "3    79.000000   75.000000   80.000000  \n",
      "4    72.000000   66.000000   70.000000  \n",
      "5     0.832036    0.843159    0.833148  \n",
      "6     0.836439    0.840426    0.833333  \n",
      "7     0.848739    0.856833    0.851064  \n",
      "8     0.813200    0.828800    0.813500  \n",
      "9     0.842544    0.848550    0.842105  \n",
      "10    0.831948    0.843103    0.833042  \n",
      "11    0.831284    0.842960    0.832609  \n",
      "12    0.830989    0.842800    0.832292  \n",
      "13    0.662670    0.686090    0.665426  \n",
      "14    0.826900    0.846200    0.832900  \n",
      "15    0.830989    0.842800    0.832292  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_6_cat = np.where((y_pred_svm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:27:38,723] Trial 350 finished with value: 0.40988971418304565 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:28:05,991] Trial 351 finished with value: 0.33328583971337944 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:28:32,065] Trial 352 finished with value: 0.6599622581747567 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:28:57,823] Trial 353 finished with value: 0.2497544684044129 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:29:22,911] Trial 354 finished with value: 0.057561484542022236 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:29:50,034] Trial 355 finished with value: 0.03705374922743717 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:30:16,242] Trial 356 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:30:43,104] Trial 357 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:31:07,910] Trial 358 finished with value: 0.6866057107047595 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:31:30,475] Trial 359 finished with value: 0.6745111889890987 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:31:53,055] Trial 360 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:32:14,532] Trial 361 finished with value: 0.18229904183647822 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:32:37,813] Trial 362 finished with value: 0.05868911960514393 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:33:01,413] Trial 363 finished with value: 0.29155449859609484 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:33:23,139] Trial 364 finished with value: 0.5262953808081599 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:33:46,126] Trial 365 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:34:09,874] Trial 366 finished with value: 0.661378082985037 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:34:32,086] Trial 367 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:34:53,613] Trial 368 finished with value: 0.13639102748985316 and parameters: {'C': 0.0625, 'gamma': 0.0625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:35:17,802] Trial 369 finished with value: 0.034199821990564525 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:35:39,405] Trial 370 finished with value: 0.5658562127808132 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:36:02,712] Trial 371 finished with value: 0.04443448071831343 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:36:25,178] Trial 372 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:36:53,545] Trial 373 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:37:20,626] Trial 374 finished with value: 0.47594402281018694 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:37:49,090] Trial 375 finished with value: 0.025461403006087437 and parameters: {'C': 0.5, 'gamma': 8.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:38:14,661] Trial 376 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:38:39,558] Trial 377 finished with value: 0.6067479302507424 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:39:02,738] Trial 378 finished with value: 0.6745111889890987 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:39:28,473] Trial 379 finished with value: 0.5184029571115596 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:39:57,279] Trial 380 finished with value: 0.2497544684044129 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:40:26,333] Trial 381 finished with value: 0.01785341264493904 and parameters: {'C': 0.125, 'gamma': 0.25}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:40:54,974] Trial 382 finished with value: 0.40988971418304565 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:41:19,480] Trial 383 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:41:46,234] Trial 384 finished with value: 0.06196783420208211 and parameters: {'C': 2.0, 'gamma': 0.5}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:42:14,892] Trial 385 finished with value: -0.0006831465563087557 and parameters: {'C': 0.015625, 'gamma': 6.103515625e-05}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:42:40,057] Trial 386 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:43:03,531] Trial 387 finished with value: 0.03747845086893459 and parameters: {'C': 1.0, 'gamma': 2.0}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:43:27,048] Trial 388 finished with value: 0.6693069328192592 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:43:54,174] Trial 389 finished with value: 0.6866057107047595 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:44:22,577] Trial 390 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:44:51,039] Trial 391 finished with value: 0.5262953808081599 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:45:18,382] Trial 392 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:45:40,981] Trial 393 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:46:06,104] Trial 394 finished with value: 0.6713445805207179 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:46:31,011] Trial 395 finished with value: 0.6896925177779785 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:46:54,659] Trial 396 finished with value: 0.07915987518811575 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:47:18,143] Trial 397 finished with value: 0.22097590659647434 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:47:40,682] Trial 398 finished with value: 0.6896925177779785 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:48:01,997] Trial 399 finished with value: 0.6958626752497825 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7019\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.690919    0.729222    0.719859    0.721196   \n",
      "1                    TP  403.000000  408.000000  381.000000  406.000000   \n",
      "2                    TN  348.000000  348.000000  374.000000  354.000000   \n",
      "3                    FP   78.000000   74.000000   78.000000   66.000000   \n",
      "4                    FN   70.000000   69.000000   66.000000   73.000000   \n",
      "5              Accuracy    0.835373    0.840934    0.839822    0.845384   \n",
      "6             Precision    0.837838    0.846473    0.830065    0.860169   \n",
      "7           Sensitivity    0.852008    0.855346    0.852349    0.847599   \n",
      "8           Specificity    0.816900    0.824600    0.827400    0.842900   \n",
      "9              F1 score    0.844864    0.850886    0.841060    0.853838   \n",
      "10  F1 score (weighted)    0.835283    0.840875    0.839805    0.845454   \n",
      "11     F1 score (macro)    0.834754    0.840223    0.839812    0.844865   \n",
      "12    Balanced Accuracy    0.834455    0.839995    0.839891    0.845228   \n",
      "13                  MCC    0.669641    0.680498    0.679924    0.689833   \n",
      "14                  NPV    0.832500    0.834500    0.850000    0.829000   \n",
      "15              ROC_AUC    0.834455    0.839995    0.839891    0.845228   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.710927    0.691306    0.700637    0.699005  \n",
      "1   404.000000  395.000000  400.000000  415.000000  \n",
      "2   344.000000  363.000000  349.000000  343.000000  \n",
      "3    79.000000   75.000000   80.000000   70.000000  \n",
      "4    72.000000   66.000000   70.000000   71.000000  \n",
      "5     0.832036    0.843159    0.833148    0.843159  \n",
      "6     0.836439    0.840426    0.833333    0.855670  \n",
      "7     0.848739    0.856833    0.851064    0.853909  \n",
      "8     0.813200    0.828800    0.813500    0.830500  \n",
      "9     0.842544    0.848550    0.842105    0.854789  \n",
      "10    0.831948    0.843103    0.833042    0.843173  \n",
      "11    0.831284    0.842960    0.832609    0.842147  \n",
      "12    0.830989    0.842800    0.832292    0.842209  \n",
      "13    0.662670    0.686090    0.665426    0.684295  \n",
      "14    0.826900    0.846200    0.832900    0.828500  \n",
      "15    0.830989    0.842800    0.832292    0.842209  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_7_cat = np.where((y_pred_svm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:48:28,791] Trial 400 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:48:52,834] Trial 401 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:49:18,383] Trial 402 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:49:46,348] Trial 403 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:50:13,687] Trial 404 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:50:42,714] Trial 405 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:51:10,977] Trial 406 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:51:38,026] Trial 407 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:51:59,216] Trial 408 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:52:21,195] Trial 409 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:52:43,126] Trial 410 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:53:05,511] Trial 411 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:53:27,786] Trial 412 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:53:49,149] Trial 413 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:54:10,166] Trial 414 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:54:32,554] Trial 415 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:54:54,683] Trial 416 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:55:16,068] Trial 417 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:55:38,064] Trial 418 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:56:00,413] Trial 419 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:56:22,336] Trial 420 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:56:45,202] Trial 421 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:57:08,828] Trial 422 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:57:31,292] Trial 423 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:57:54,242] Trial 424 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:58:16,642] Trial 425 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:58:39,327] Trial 426 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:59:01,197] Trial 427 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:59:23,848] Trial 428 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 08:59:46,014] Trial 429 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:00:08,231] Trial 430 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:00:30,906] Trial 431 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:00:53,210] Trial 432 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:01:15,355] Trial 433 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:01:37,706] Trial 434 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:02:00,175] Trial 435 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:02:21,511] Trial 436 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:02:44,773] Trial 437 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:03:07,120] Trial 438 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:03:29,129] Trial 439 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:03:50,991] Trial 440 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:04:13,126] Trial 441 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:04:35,742] Trial 442 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:04:58,802] Trial 443 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:05:21,761] Trial 444 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:05:44,821] Trial 445 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:06:06,501] Trial 446 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:06:29,007] Trial 447 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:06:50,779] Trial 448 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n",
      "[I 2023-12-20 09:07:12,171] Trial 449 finished with value: 0.700200442203562 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7018736313932994.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7019\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.690919    0.729222    0.719859    0.721196   \n",
      "1                    TP  403.000000  408.000000  381.000000  406.000000   \n",
      "2                    TN  348.000000  348.000000  374.000000  354.000000   \n",
      "3                    FP   78.000000   74.000000   78.000000   66.000000   \n",
      "4                    FN   70.000000   69.000000   66.000000   73.000000   \n",
      "5              Accuracy    0.835373    0.840934    0.839822    0.845384   \n",
      "6             Precision    0.837838    0.846473    0.830065    0.860169   \n",
      "7           Sensitivity    0.852008    0.855346    0.852349    0.847599   \n",
      "8           Specificity    0.816900    0.824600    0.827400    0.842900   \n",
      "9              F1 score    0.844864    0.850886    0.841060    0.853838   \n",
      "10  F1 score (weighted)    0.835283    0.840875    0.839805    0.845454   \n",
      "11     F1 score (macro)    0.834754    0.840223    0.839812    0.844865   \n",
      "12    Balanced Accuracy    0.834455    0.839995    0.839891    0.845228   \n",
      "13                  MCC    0.669641    0.680498    0.679924    0.689833   \n",
      "14                  NPV    0.832500    0.834500    0.850000    0.829000   \n",
      "15              ROC_AUC    0.834455    0.839995    0.839891    0.845228   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.710927    0.691306    0.700637    0.699005    0.697855  \n",
      "1   404.000000  395.000000  400.000000  415.000000  408.000000  \n",
      "2   344.000000  363.000000  349.000000  343.000000  342.000000  \n",
      "3    79.000000   75.000000   80.000000   70.000000   84.000000  \n",
      "4    72.000000   66.000000   70.000000   71.000000   65.000000  \n",
      "5     0.832036    0.843159    0.833148    0.843159    0.834260  \n",
      "6     0.836439    0.840426    0.833333    0.855670    0.829268  \n",
      "7     0.848739    0.856833    0.851064    0.853909    0.862579  \n",
      "8     0.813200    0.828800    0.813500    0.830500    0.802800  \n",
      "9     0.842544    0.848550    0.842105    0.854789    0.845596  \n",
      "10    0.831948    0.843103    0.833042    0.843173    0.834002  \n",
      "11    0.831284    0.842960    0.832609    0.842147    0.833362  \n",
      "12    0.830989    0.842800    0.832292    0.842209    0.832698  \n",
      "13    0.662670    0.686090    0.665426    0.684295    0.667476  \n",
      "14    0.826900    0.846200    0.832900    0.828500    0.840300  \n",
      "15    0.830989    0.842800    0.832292    0.842209    0.832698  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_8_cat = np.where((y_pred_svm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 09:07:36,975] Trial 450 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:07:57,741] Trial 451 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:08:18,741] Trial 452 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:08:39,253] Trial 453 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:08:59,611] Trial 454 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:09:20,941] Trial 455 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:09:41,985] Trial 456 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:10:03,074] Trial 457 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:10:24,870] Trial 458 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:10:46,930] Trial 459 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:11:06,902] Trial 460 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:11:27,347] Trial 461 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:11:47,948] Trial 462 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:12:09,667] Trial 463 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:12:29,910] Trial 464 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:12:50,898] Trial 465 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:13:12,386] Trial 466 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:13:32,469] Trial 467 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:13:53,332] Trial 468 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:14:14,234] Trial 469 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:14:35,028] Trial 470 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:14:55,758] Trial 471 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:15:16,105] Trial 472 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:15:37,144] Trial 473 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:15:58,159] Trial 474 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:16:19,101] Trial 475 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:16:39,883] Trial 476 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:17:00,814] Trial 477 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:17:22,810] Trial 478 finished with value: 0.29605463623945916 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:17:43,465] Trial 479 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:18:04,891] Trial 480 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:18:26,536] Trial 481 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:18:48,821] Trial 482 finished with value: 0.6393738099706526 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:19:09,641] Trial 483 finished with value: 0.5436866949329379 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:19:33,227] Trial 484 finished with value: 0.039501256656618446 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:19:55,681] Trial 485 finished with value: 0.5771759949934347 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:20:17,370] Trial 486 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:20:40,282] Trial 487 finished with value: 0.04825659509640897 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:21:01,837] Trial 488 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:21:21,924] Trial 489 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:21:43,327] Trial 490 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:22:05,523] Trial 491 finished with value: 0.10051732809743619 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:22:26,103] Trial 492 finished with value: 0.5906250770550079 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:22:47,279] Trial 493 finished with value: 0.4207015933510355 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:23:12,338] Trial 494 finished with value: 0.039166503576924215 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:23:34,064] Trial 495 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:23:54,844] Trial 496 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:24:15,371] Trial 497 finished with value: 0.7055574334986049 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:24:36,929] Trial 498 finished with value: 0.2571807753953764 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 450 with value: 0.7055574334986049.\n",
      "[I 2023-12-20 09:24:58,139] Trial 499 finished with value: 0.1731416144111163 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 450 with value: 0.7055574334986049.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7056\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.690919    0.729222    0.719859    0.721196   \n",
      "1                    TP  403.000000  408.000000  381.000000  406.000000   \n",
      "2                    TN  348.000000  348.000000  374.000000  354.000000   \n",
      "3                    FP   78.000000   74.000000   78.000000   66.000000   \n",
      "4                    FN   70.000000   69.000000   66.000000   73.000000   \n",
      "5              Accuracy    0.835373    0.840934    0.839822    0.845384   \n",
      "6             Precision    0.837838    0.846473    0.830065    0.860169   \n",
      "7           Sensitivity    0.852008    0.855346    0.852349    0.847599   \n",
      "8           Specificity    0.816900    0.824600    0.827400    0.842900   \n",
      "9              F1 score    0.844864    0.850886    0.841060    0.853838   \n",
      "10  F1 score (weighted)    0.835283    0.840875    0.839805    0.845454   \n",
      "11     F1 score (macro)    0.834754    0.840223    0.839812    0.844865   \n",
      "12    Balanced Accuracy    0.834455    0.839995    0.839891    0.845228   \n",
      "13                  MCC    0.669641    0.680498    0.679924    0.689833   \n",
      "14                  NPV    0.832500    0.834500    0.850000    0.829000   \n",
      "15              ROC_AUC    0.834455    0.839995    0.839891    0.845228   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.710927    0.691306    0.700637    0.699005    0.697855    0.686044  \n",
      "1   404.000000  395.000000  400.000000  415.000000  408.000000  398.000000  \n",
      "2   344.000000  363.000000  349.000000  343.000000  342.000000  341.000000  \n",
      "3    79.000000   75.000000   80.000000   70.000000   84.000000  102.000000  \n",
      "4    72.000000   66.000000   70.000000   71.000000   65.000000   58.000000  \n",
      "5     0.832036    0.843159    0.833148    0.843159    0.834260    0.822024  \n",
      "6     0.836439    0.840426    0.833333    0.855670    0.829268    0.796000  \n",
      "7     0.848739    0.856833    0.851064    0.853909    0.862579    0.872807  \n",
      "8     0.813200    0.828800    0.813500    0.830500    0.802800    0.769800  \n",
      "9     0.842544    0.848550    0.842105    0.854789    0.845596    0.832636  \n",
      "10    0.831948    0.843103    0.833042    0.843173    0.834002    0.821470  \n",
      "11    0.831284    0.842960    0.832609    0.842147    0.833362    0.821306  \n",
      "12    0.830989    0.842800    0.832292    0.842209    0.832698    0.821279  \n",
      "13    0.662670    0.686090    0.665426    0.684295    0.667476    0.646585  \n",
      "14    0.826900    0.846200    0.832900    0.828500    0.840300    0.854600  \n",
      "15    0.830989    0.842800    0.832292    0.842209    0.832698    0.821279  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_9_cat = np.where((y_pred_svm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7056\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.717112</td>\n",
       "      <td>0.022899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>201.200000</td>\n",
       "      <td>7.539525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>174.900000</td>\n",
       "      <td>4.557046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>38.200000</td>\n",
       "      <td>4.516636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>34.900000</td>\n",
       "      <td>4.408325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.837262</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.840258</td>\n",
       "      <td>0.020427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.852201</td>\n",
       "      <td>0.018035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.821110</td>\n",
       "      <td>0.016152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.846046</td>\n",
       "      <td>0.015470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.837219</td>\n",
       "      <td>0.013791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.836632</td>\n",
       "      <td>0.013641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.836651</td>\n",
       "      <td>0.013572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.673666</td>\n",
       "      <td>0.027264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.833770</td>\n",
       "      <td>0.019349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.836651</td>\n",
       "      <td>0.013572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.717112     0.022899\n",
       "1                    TP       201.200000     7.539525\n",
       "2                    TN       174.900000     4.557046\n",
       "3                    FP        38.200000     4.516636\n",
       "4                    FN        34.900000     4.408325\n",
       "5              Accuracy         0.837262     0.013800\n",
       "6             Precision         0.840258     0.020427\n",
       "7           Sensitivity         0.852201     0.018035\n",
       "8           Specificity         0.821110     0.016152\n",
       "9              F1 score         0.846046     0.015470\n",
       "10  F1 score (weighted)         0.837219     0.013791\n",
       "11     F1 score (macro)         0.836632     0.013641\n",
       "12    Balanced Accuracy         0.836651     0.013572\n",
       "13                  MCC         0.673666     0.027264\n",
       "14                  NPV         0.833770     0.019349\n",
       "15              ROC_AUC         0.836651     0.013572"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.690919</td>\n",
       "      <td>0.729222</td>\n",
       "      <td>0.719859</td>\n",
       "      <td>0.721196</td>\n",
       "      <td>0.710927</td>\n",
       "      <td>0.691306</td>\n",
       "      <td>0.700637</td>\n",
       "      <td>0.699005</td>\n",
       "      <td>0.697855</td>\n",
       "      <td>0.686044</td>\n",
       "      <td>0.704697</td>\n",
       "      <td>0.014739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>401.800000</td>\n",
       "      <td>9.259230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>350.600000</td>\n",
       "      <td>10.521935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>9.697651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>4.422166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.835373</td>\n",
       "      <td>0.840934</td>\n",
       "      <td>0.839822</td>\n",
       "      <td>0.845384</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.843159</td>\n",
       "      <td>0.833148</td>\n",
       "      <td>0.843159</td>\n",
       "      <td>0.834260</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.836930</td>\n",
       "      <td>0.007019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.830065</td>\n",
       "      <td>0.860169</td>\n",
       "      <td>0.836439</td>\n",
       "      <td>0.840426</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.855670</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.836568</td>\n",
       "      <td>0.017602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.852008</td>\n",
       "      <td>0.855346</td>\n",
       "      <td>0.852349</td>\n",
       "      <td>0.847599</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.856833</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.853909</td>\n",
       "      <td>0.862579</td>\n",
       "      <td>0.872807</td>\n",
       "      <td>0.855323</td>\n",
       "      <td>0.007480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.827400</td>\n",
       "      <td>0.842900</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.828800</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.830500</td>\n",
       "      <td>0.802800</td>\n",
       "      <td>0.769800</td>\n",
       "      <td>0.817040</td>\n",
       "      <td>0.020040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.844864</td>\n",
       "      <td>0.850886</td>\n",
       "      <td>0.841060</td>\n",
       "      <td>0.853838</td>\n",
       "      <td>0.842544</td>\n",
       "      <td>0.848550</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.854789</td>\n",
       "      <td>0.845596</td>\n",
       "      <td>0.832636</td>\n",
       "      <td>0.845687</td>\n",
       "      <td>0.006664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.835283</td>\n",
       "      <td>0.840875</td>\n",
       "      <td>0.839805</td>\n",
       "      <td>0.845454</td>\n",
       "      <td>0.831948</td>\n",
       "      <td>0.843103</td>\n",
       "      <td>0.833042</td>\n",
       "      <td>0.843173</td>\n",
       "      <td>0.834002</td>\n",
       "      <td>0.821470</td>\n",
       "      <td>0.836815</td>\n",
       "      <td>0.007178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.834754</td>\n",
       "      <td>0.840223</td>\n",
       "      <td>0.839812</td>\n",
       "      <td>0.844865</td>\n",
       "      <td>0.831284</td>\n",
       "      <td>0.842960</td>\n",
       "      <td>0.832609</td>\n",
       "      <td>0.842147</td>\n",
       "      <td>0.833362</td>\n",
       "      <td>0.821306</td>\n",
       "      <td>0.836332</td>\n",
       "      <td>0.007104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.834455</td>\n",
       "      <td>0.839995</td>\n",
       "      <td>0.839891</td>\n",
       "      <td>0.845228</td>\n",
       "      <td>0.830989</td>\n",
       "      <td>0.842800</td>\n",
       "      <td>0.832292</td>\n",
       "      <td>0.842209</td>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.821279</td>\n",
       "      <td>0.836184</td>\n",
       "      <td>0.007223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.669641</td>\n",
       "      <td>0.680498</td>\n",
       "      <td>0.679924</td>\n",
       "      <td>0.689833</td>\n",
       "      <td>0.662670</td>\n",
       "      <td>0.686090</td>\n",
       "      <td>0.665426</td>\n",
       "      <td>0.684295</td>\n",
       "      <td>0.667476</td>\n",
       "      <td>0.646585</td>\n",
       "      <td>0.673244</td>\n",
       "      <td>0.013289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.826900</td>\n",
       "      <td>0.846200</td>\n",
       "      <td>0.832900</td>\n",
       "      <td>0.828500</td>\n",
       "      <td>0.840300</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>0.837540</td>\n",
       "      <td>0.009735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.834455</td>\n",
       "      <td>0.839995</td>\n",
       "      <td>0.839891</td>\n",
       "      <td>0.845228</td>\n",
       "      <td>0.830989</td>\n",
       "      <td>0.842800</td>\n",
       "      <td>0.832292</td>\n",
       "      <td>0.842209</td>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.821279</td>\n",
       "      <td>0.836184</td>\n",
       "      <td>0.007223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.690919    0.729222    0.719859    0.721196   \n",
       "1                    TP  403.000000  408.000000  381.000000  406.000000   \n",
       "2                    TN  348.000000  348.000000  374.000000  354.000000   \n",
       "3                    FP   78.000000   74.000000   78.000000   66.000000   \n",
       "4                    FN   70.000000   69.000000   66.000000   73.000000   \n",
       "5              Accuracy    0.835373    0.840934    0.839822    0.845384   \n",
       "6             Precision    0.837838    0.846473    0.830065    0.860169   \n",
       "7           Sensitivity    0.852008    0.855346    0.852349    0.847599   \n",
       "8           Specificity    0.816900    0.824600    0.827400    0.842900   \n",
       "9              F1 score    0.844864    0.850886    0.841060    0.853838   \n",
       "10  F1 score (weighted)    0.835283    0.840875    0.839805    0.845454   \n",
       "11     F1 score (macro)    0.834754    0.840223    0.839812    0.844865   \n",
       "12    Balanced Accuracy    0.834455    0.839995    0.839891    0.845228   \n",
       "13                  MCC    0.669641    0.680498    0.679924    0.689833   \n",
       "14                  NPV    0.832500    0.834500    0.850000    0.829000   \n",
       "15              ROC_AUC    0.834455    0.839995    0.839891    0.845228   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.710927    0.691306    0.700637    0.699005    0.697855    0.686044   \n",
       "1   404.000000  395.000000  400.000000  415.000000  408.000000  398.000000   \n",
       "2   344.000000  363.000000  349.000000  343.000000  342.000000  341.000000   \n",
       "3    79.000000   75.000000   80.000000   70.000000   84.000000  102.000000   \n",
       "4    72.000000   66.000000   70.000000   71.000000   65.000000   58.000000   \n",
       "5     0.832036    0.843159    0.833148    0.843159    0.834260    0.822024   \n",
       "6     0.836439    0.840426    0.833333    0.855670    0.829268    0.796000   \n",
       "7     0.848739    0.856833    0.851064    0.853909    0.862579    0.872807   \n",
       "8     0.813200    0.828800    0.813500    0.830500    0.802800    0.769800   \n",
       "9     0.842544    0.848550    0.842105    0.854789    0.845596    0.832636   \n",
       "10    0.831948    0.843103    0.833042    0.843173    0.834002    0.821470   \n",
       "11    0.831284    0.842960    0.832609    0.842147    0.833362    0.821306   \n",
       "12    0.830989    0.842800    0.832292    0.842209    0.832698    0.821279   \n",
       "13    0.662670    0.686090    0.665426    0.684295    0.667476    0.646585   \n",
       "14    0.826900    0.846200    0.832900    0.828500    0.840300    0.854600   \n",
       "15    0.830989    0.842800    0.832292    0.842209    0.832698    0.821279   \n",
       "\n",
       "           ave        std  \n",
       "0     0.704697   0.014739  \n",
       "1   401.800000   9.259230  \n",
       "2   350.600000  10.521935  \n",
       "3    78.600000   9.697651  \n",
       "4    68.000000   4.422166  \n",
       "5     0.836930   0.007019  \n",
       "6     0.836568   0.017602  \n",
       "7     0.855323   0.007480  \n",
       "8     0.817040   0.020040  \n",
       "9     0.845687   0.006664  \n",
       "10    0.836815   0.007178  \n",
       "11    0.836332   0.007104  \n",
       "12    0.836184   0.007223  \n",
       "13    0.673244   0.013289  \n",
       "14    0.837540   0.009735  \n",
       "15    0.836184   0.007223  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_svm0</th>\n",
       "      <th>y_pred_svm1</th>\n",
       "      <th>y_pred_svm2</th>\n",
       "      <th>y_pred_svm3</th>\n",
       "      <th>y_pred_svm4</th>\n",
       "      <th>y_pred_svm_ave</th>\n",
       "      <th>y_pred_svm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3921050</td>\n",
       "      <td>0</td>\n",
       "      <td>6.17</td>\n",
       "      <td>5.688394</td>\n",
       "      <td>5.641491</td>\n",
       "      <td>5.702819</td>\n",
       "      <td>5.750936</td>\n",
       "      <td>5.650278</td>\n",
       "      <td>5.767320</td>\n",
       "      <td>0.183640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL270476</td>\n",
       "      <td>1</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.488829</td>\n",
       "      <td>6.475655</td>\n",
       "      <td>6.606188</td>\n",
       "      <td>6.577611</td>\n",
       "      <td>6.592361</td>\n",
       "      <td>6.590107</td>\n",
       "      <td>0.106326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3664128</td>\n",
       "      <td>2</td>\n",
       "      <td>7.62</td>\n",
       "      <td>7.103311</td>\n",
       "      <td>7.099764</td>\n",
       "      <td>7.116153</td>\n",
       "      <td>7.143908</td>\n",
       "      <td>7.013923</td>\n",
       "      <td>7.182843</td>\n",
       "      <td>0.199512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4456250</td>\n",
       "      <td>3</td>\n",
       "      <td>5.26</td>\n",
       "      <td>5.931354</td>\n",
       "      <td>5.970633</td>\n",
       "      <td>5.674359</td>\n",
       "      <td>5.933590</td>\n",
       "      <td>5.693925</td>\n",
       "      <td>5.743977</td>\n",
       "      <td>0.246313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2408818</td>\n",
       "      <td>4</td>\n",
       "      <td>6.32</td>\n",
       "      <td>6.139761</td>\n",
       "      <td>6.149006</td>\n",
       "      <td>6.153160</td>\n",
       "      <td>6.132491</td>\n",
       "      <td>6.127830</td>\n",
       "      <td>6.170375</td>\n",
       "      <td>0.067482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4250302</td>\n",
       "      <td>4487</td>\n",
       "      <td>10.25</td>\n",
       "      <td>9.267433</td>\n",
       "      <td>9.375296</td>\n",
       "      <td>9.331982</td>\n",
       "      <td>9.406212</td>\n",
       "      <td>9.380394</td>\n",
       "      <td>9.501886</td>\n",
       "      <td>0.337503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL483893</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.598687</td>\n",
       "      <td>7.592764</td>\n",
       "      <td>7.533434</td>\n",
       "      <td>7.531419</td>\n",
       "      <td>7.550638</td>\n",
       "      <td>7.606157</td>\n",
       "      <td>0.103519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL3655914</td>\n",
       "      <td>4489</td>\n",
       "      <td>6.69</td>\n",
       "      <td>6.337966</td>\n",
       "      <td>6.346410</td>\n",
       "      <td>6.242492</td>\n",
       "      <td>6.346422</td>\n",
       "      <td>6.233569</td>\n",
       "      <td>6.366143</td>\n",
       "      <td>0.152382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL467876</td>\n",
       "      <td>4490</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.239907</td>\n",
       "      <td>7.240081</td>\n",
       "      <td>7.239908</td>\n",
       "      <td>7.240138</td>\n",
       "      <td>7.239870</td>\n",
       "      <td>7.266651</td>\n",
       "      <td>0.059636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4458544</td>\n",
       "      <td>4491</td>\n",
       "      <td>7.37</td>\n",
       "      <td>7.267848</td>\n",
       "      <td>7.313077</td>\n",
       "      <td>7.389416</td>\n",
       "      <td>7.274397</td>\n",
       "      <td>6.407695</td>\n",
       "      <td>7.170405</td>\n",
       "      <td>0.344043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_svm0  y_pred_svm1  \\\n",
       "0         CHEMBL3921050            0     6.17     5.688394     5.641491   \n",
       "1          CHEMBL270476            1     6.80     6.488829     6.475655   \n",
       "2         CHEMBL3664128            2     7.62     7.103311     7.099764   \n",
       "3         CHEMBL4456250            3     5.26     5.931354     5.970633   \n",
       "4         CHEMBL2408818            4     6.32     6.139761     6.149006   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "4487      CHEMBL4250302         4487    10.25     9.267433     9.375296   \n",
       "4488       CHEMBL483893         4488     7.83     7.598687     7.592764   \n",
       "4489      CHEMBL3655914         4489     6.69     6.337966     6.346410   \n",
       "4490       CHEMBL467876         4490     7.40     7.239907     7.240081   \n",
       "4491      CHEMBL4458544         4491     7.37     7.267848     7.313077   \n",
       "\n",
       "      y_pred_svm2  y_pred_svm3  y_pred_svm4  y_pred_svm_ave  y_pred_svm_std  \n",
       "0        5.702819     5.750936     5.650278        5.767320        0.183640  \n",
       "1        6.606188     6.577611     6.592361        6.590107        0.106326  \n",
       "2        7.116153     7.143908     7.013923        7.182843        0.199512  \n",
       "3        5.674359     5.933590     5.693925        5.743977        0.246313  \n",
       "4        6.153160     6.132491     6.127830        6.170375        0.067482  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "4487     9.331982     9.406212     9.380394        9.501886        0.337503  \n",
       "4488     7.533434     7.531419     7.550638        7.606157        0.103519  \n",
       "4489     6.242492     6.346422     6.233569        6.366143        0.152382  \n",
       "4490     7.239908     7.240138     7.239870        7.266651        0.059636  \n",
       "4491     7.389416     7.274397     6.407695        7.170405        0.344043  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where((y_pred_optimized_svm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id, svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "svm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2f1c2fff-1c92-4518-83ac-c4e880fe4590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.713483</td>\n",
       "      <td>0.025011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.839981</td>\n",
       "      <td>0.017639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.846828</td>\n",
       "      <td>0.023409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.849836</td>\n",
       "      <td>0.026737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.829298</td>\n",
       "      <td>0.028402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.847938</td>\n",
       "      <td>0.017028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.839942</td>\n",
       "      <td>0.017650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.839303</td>\n",
       "      <td>0.017792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.839567</td>\n",
       "      <td>0.017829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.679471</td>\n",
       "      <td>0.035712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.832984</td>\n",
       "      <td>0.028926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.839567</td>\n",
       "      <td>0.017829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.713483     0.025011\n",
       "1              Accuracy         0.839981     0.017639\n",
       "2             Precision         0.846828     0.023409\n",
       "3           Sensitivity         0.849836     0.026737\n",
       "4           Specificity         0.829298     0.028402\n",
       "5              F1 score         0.847938     0.017028\n",
       "6   F1 score (weighted)         0.839942     0.017650\n",
       "7      F1 score (macro)         0.839303     0.017792\n",
       "8     Balanced Accuracy         0.839567     0.017829\n",
       "9                   MCC         0.679471     0.035712\n",
       "10                  NPV         0.832984     0.028926\n",
       "11              ROC_AUC         0.839567     0.017829"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.6861 with a standard deviation of 0.0214\n",
      "SVM optimized model r2_score 0.7140 with a standard deviation of 0.0221\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm.joblib']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, \"OUTPUT/svm_reg.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6e46b761-0f0c-48c9-9d17-d49420ceb22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "82af877f-1e9a-49d6-a15d-c0324d767704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "108a44a5-038b-4d83-877e-55a291431d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/PredResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    rf_5preds.to_excel(writer, sheet_name=\"RF\", )\n",
    "    lgbm_5preds.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    xgb_5preds.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    knn_5preds.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    svm_5preds.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
