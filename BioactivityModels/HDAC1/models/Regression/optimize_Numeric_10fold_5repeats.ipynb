{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1 = Path(HERE).resolve().parents[1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4176702</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[9413970, 7836485, 4703614, 8711586, 1857203, ...</td>\n",
       "      <td>6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL272401</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[6242194, 3827191, 2475943, 5652600, 2748816, ...</td>\n",
       "      <td>7.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL118</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5726680, 25018265, 38173396, 23403255, 169510...</td>\n",
       "      <td>5.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3655939</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[8343543, 2478511, 6486141, 299249, 11932100, ...</td>\n",
       "      <td>6.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3621537</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5976924, 15365489, 24097203, 9391761, 6651314...</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4176702  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       CHEMBL272401  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2          CHEMBL118  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3655939  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3621537  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC1  \n",
       "0  [9413970, 7836485, 4703614, 8711586, 1857203, ...           6.50  \n",
       "1  [6242194, 3827191, 2475943, 5652600, 2748816, ...           7.26  \n",
       "2  [5726680, 25018265, 38173396, 23403255, 169510...           5.93  \n",
       "3  [8343543, 2478511, 6486141, 299249, 11932100, ...           6.28  \n",
       "4  [5976924, 15365489, 24097203, 9391761, 6651314...           5.88  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1/\"HDAC1_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type</th>\n",
       "      <th>Standard_Value_HDAC1</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL327146</td>\n",
       "      <td>O=C(CCCCCC(C(=O)Nc1ccc2ncccc2c1)C(=O)Nc1ccc2nc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL116620</td>\n",
       "      <td>O=C(/C=C/c1cccc(C(C(=O)Nc2ccccc2)C(=O)Nc2ccccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2093007</td>\n",
       "      <td>C/C=C1\\NC(=O)[C@@H](CSC)NC(=O)[C@@H](C(C)C)CC(...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6300.00</td>\n",
       "      <td>5.20</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL316457</td>\n",
       "      <td>CC(C)c1cc(C(C)C)c(S(=O)(=O)Nc2ccc(/C=C/C(=O)NO...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>600.00</td>\n",
       "      <td>6.22</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL269692</td>\n",
       "      <td>O=C(NCc1ccc(C(=O)NO)cc1)OCc1cccnc1</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3000.00</td>\n",
       "      <td>5.52</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4649511</td>\n",
       "      <td>O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3.30</td>\n",
       "      <td>8.48</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL4637976</td>\n",
       "      <td>O=C(CCCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>130.00</td>\n",
       "      <td>6.89</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL4638227</td>\n",
       "      <td>CC(=O)Nc1ccc2c(c1)CN(C(=O)[C@H](N)Cc1ccc(Cl)cc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>8800.00</td>\n",
       "      <td>5.06</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL4444219</td>\n",
       "      <td>CCCNNC(=O)/C=C/c1ccc(CNCCc2c(C)[nH]c3ccccc23)cc1</td>\n",
       "      <td>IC50</td>\n",
       "      <td>4.85</td>\n",
       "      <td>8.43</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>28.00</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0          CHEMBL327146  O=C(CCCCCC(C(=O)Nc1ccc2ncccc2c1)C(=O)Nc1ccc2nc...   \n",
       "1          CHEMBL116620  O=C(/C=C/c1cccc(C(C(=O)Nc2ccccc2)C(=O)Nc2ccccc...   \n",
       "2         CHEMBL2093007  C/C=C1\\NC(=O)[C@@H](CSC)NC(=O)[C@@H](C(C)C)CC(...   \n",
       "3          CHEMBL316457  CC(C)c1cc(C(C)C)c(S(=O)(=O)Nc2ccc(/C=C/C(=O)NO...   \n",
       "4          CHEMBL269692                 O=C(NCc1ccc(C(=O)NO)cc1)OCc1cccnc1   \n",
       "...                 ...                                                ...   \n",
       "4487      CHEMBL4649511        O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO   \n",
       "4488      CHEMBL4637976       O=C(CCCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO   \n",
       "4489      CHEMBL4638227  CC(=O)Nc1ccc2c(c1)CN(C(=O)[C@H](N)Cc1ccc(Cl)cc...   \n",
       "4490      CHEMBL4444219   CCCNNC(=O)/C=C/c1ccc(CNCCc2c(C)[nH]c3ccccc23)cc1   \n",
       "4491      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "\n",
       "      type  Standard_Value_HDAC1  pChEMBL_HDAC1          label  \n",
       "0     IC50                  1.00           9.00  Single points  \n",
       "1     IC50                  1.00           9.00  Single points  \n",
       "2     IC50               6300.00           5.20  Single points  \n",
       "3     IC50                600.00           6.22  Single points  \n",
       "4     IC50               3000.00           5.52  Single points  \n",
       "...    ...                   ...            ...            ...  \n",
       "4487  IC50                  3.30           8.48    Dual-binder  \n",
       "4488  IC50                130.00           6.89  Single points  \n",
       "4489  IC50               8800.00           5.06  Single points  \n",
       "4490  IC50                  4.85           8.43  Single points  \n",
       "4491    Ki                 28.00           7.55    Dual-binder  \n",
       "\n",
       "[4492 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1/\"HDAC1_dataset.csv\", )\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL2047606</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[15114361, 3821889, 7998790, 4833708, 16951091...</td>\n",
       "      <td>5.44</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL217781</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[3492127, 2201708, 2475943, 7125875, 5802687, ...</td>\n",
       "      <td>7.48</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL2105763</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4277439, 2478511, 1683617, 1179055, 492964, 2...</td>\n",
       "      <td>9.92</td>\n",
       "      <td>HDAC1-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL3415969</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1821977, 756812, 526260, 718486, 1799526, 521...</td>\n",
       "      <td>5.75</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL467066</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[8781729, 10511, 1052992, 2183376, 2852937, 37...</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                           fp_MACCS  \\\n",
       "4487      CHEMBL2047606  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4488       CHEMBL217781  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4489      CHEMBL2105763  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4490      CHEMBL3415969  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4491       CHEMBL467066  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_Morgan3  \\\n",
       "4487  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4488  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4489  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4490  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4491  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MorganF  \\\n",
       "4487  [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "4488  [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "4489  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4490  [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4491  [1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                fp_MAP4  pChEMBL_HDAC1  \\\n",
       "4487  [15114361, 3821889, 7998790, 4833708, 16951091...           5.44   \n",
       "4488  [3492127, 2201708, 2475943, 7125875, 5802687, ...           7.48   \n",
       "4489  [4277439, 2478511, 1683617, 1179055, 492964, 2...           9.92   \n",
       "4490  [1821977, 756812, 526260, 718486, 1799526, 521...           5.75   \n",
       "4491  [8781729, 10511, 1052992, 2183376, 2852937, 37...           7.55   \n",
       "\n",
       "                label  \n",
       "4487    Single points  \n",
       "4488    Single points  \n",
       "4489  HDAC1-selective  \n",
       "4490   Semi-selective  \n",
       "4491    Single points  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4176702</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[9413970, 7836485, 4703614, 8711586, 1857203, ...</td>\n",
       "      <td>6.50</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL272401</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[6242194, 3827191, 2475943, 5652600, 2748816, ...</td>\n",
       "      <td>7.26</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL118</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5726680, 25018265, 38173396, 23403255, 169510...</td>\n",
       "      <td>5.93</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3655939</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[8343543, 2478511, 6486141, 299249, 11932100, ...</td>\n",
       "      <td>6.28</td>\n",
       "      <td>Semi-selective</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4176702  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       CHEMBL272401  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2          CHEMBL118  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3655939  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC1  \\\n",
       "0  [9413970, 7836485, 4703614, 8711586, 1857203, ...           6.50   \n",
       "1  [6242194, 3827191, 2475943, 5652600, 2748816, ...           7.26   \n",
       "2  [5726680, 25018265, 38173396, 23403255, 169510...           5.93   \n",
       "3  [8343543, 2478511, 6486141, 299249, 11932100, ...           6.28   \n",
       "\n",
       "            label  Class  \n",
       "0   Single points    0.0  \n",
       "1     Dual-binder    3.0  \n",
       "2      Non-binder    4.0  \n",
       "3  Semi-selective    5.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for activity\n",
    "df[\"activity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as active if pchembl value is >=6.6 0 otherwise\n",
    "df.loc[df[df.pChEMBL_HDAC1 >= 6.6].index, \"activity\"] = 1.0\n",
    "\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"pChEMBL_HDAC1\"].values\n",
    "Y_cat =  df[\"activity\"].values\n",
    "Y_class = df['Class'].values\n",
    "indices =  np.array(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['pChEMBL_HDAC1'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['pChEMBL_HDAC1'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.678596     0.016382\n",
      "1                    TP       200.400000     7.545418\n",
      "2                    TN       170.700000     7.832269\n",
      "3                    FP        42.400000     7.074210\n",
      "4                    FN        35.700000     3.802046\n",
      "5              Accuracy         0.826134     0.018331\n",
      "6             Precision         0.825612     0.026919\n",
      "7           Sensitivity         0.848774     0.015849\n",
      "8           Specificity         0.801250     0.030982\n",
      "9              F1 score         0.836830     0.017572\n",
      "10  F1 score (weighted)         0.825949     0.018476\n",
      "11     F1 score (macro)         0.825230     0.018496\n",
      "12    Balanced Accuracy         0.825011     0.018713\n",
      "13                  MCC         0.651279     0.036591\n",
      "14                  NPV         0.826920     0.018560\n",
      "15              ROC_AUC         0.825011     0.018713\n",
      "CPU times: user 3min 31s, sys: 307 ms, total: 3min 31s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=16,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6) , 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=16, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=16, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 09:23:32,911] A new study created in memory with name: RFRegressor\n",
      "[I 2023-12-06 09:24:14,468] Trial 0 finished with value: 0.659267076967515 and parameters: {'n_estimators': 357}. Best is trial 0 with value: 0.659267076967515.\n",
      "[I 2023-12-06 09:25:14,525] Trial 1 finished with value: 0.6597452991802513 and parameters: {'n_estimators': 528}. Best is trial 1 with value: 0.6597452991802513.\n",
      "[I 2023-12-06 09:26:23,037] Trial 2 finished with value: 0.6600084438778697 and parameters: {'n_estimators': 607}. Best is trial 2 with value: 0.6600084438778697.\n",
      "[I 2023-12-06 09:28:14,469] Trial 3 finished with value: 0.660259062615891 and parameters: {'n_estimators': 984}. Best is trial 3 with value: 0.660259062615891.\n",
      "[I 2023-12-06 09:30:05,087] Trial 4 finished with value: 0.6603616349543182 and parameters: {'n_estimators': 965}. Best is trial 4 with value: 0.6603616349543182.\n",
      "[I 2023-12-06 09:31:13,017] Trial 5 finished with value: 0.6599258509045776 and parameters: {'n_estimators': 582}. Best is trial 4 with value: 0.6603616349543182.\n",
      "[I 2023-12-06 09:31:46,681] Trial 6 finished with value: 0.6584360756242915 and parameters: {'n_estimators': 280}. Best is trial 4 with value: 0.6603616349543182.\n",
      "[I 2023-12-06 09:32:08,791] Trial 7 finished with value: 0.6571747739506136 and parameters: {'n_estimators': 177}. Best is trial 4 with value: 0.6603616349543182.\n",
      "[I 2023-12-06 09:32:52,817] Trial 8 finished with value: 0.659203642148527 and parameters: {'n_estimators': 371}. Best is trial 4 with value: 0.6603616349543182.\n",
      "[I 2023-12-06 09:34:30,859] Trial 9 finished with value: 0.660330557513887 and parameters: {'n_estimators': 852}. Best is trial 4 with value: 0.6603616349543182.\n",
      "[I 2023-12-06 09:36:01,737] Trial 10 finished with value: 0.6599838515359258 and parameters: {'n_estimators': 788}. Best is trial 4 with value: 0.6603616349543182.\n",
      "[I 2023-12-06 09:37:56,261] Trial 11 finished with value: 0.6602749974114854 and parameters: {'n_estimators': 996}. Best is trial 4 with value: 0.6603616349543182.\n",
      "[I 2023-12-06 09:39:28,988] Trial 12 finished with value: 0.6599830973182883 and parameters: {'n_estimators': 804}. Best is trial 4 with value: 0.6603616349543182.\n",
      "[I 2023-12-06 09:41:03,470] Trial 13 finished with value: 0.6601368896541925 and parameters: {'n_estimators': 819}. Best is trial 4 with value: 0.6603616349543182.\n",
      "[I 2023-12-06 09:42:46,774] Trial 14 finished with value: 0.6603782714957658 and parameters: {'n_estimators': 897}. Best is trial 14 with value: 0.6603782714957658.\n",
      "[I 2023-12-06 09:44:10,749] Trial 15 finished with value: 0.6600634130179174 and parameters: {'n_estimators': 727}. Best is trial 14 with value: 0.6603782714957658.\n",
      "[I 2023-12-06 09:45:55,620] Trial 16 finished with value: 0.6603950316345228 and parameters: {'n_estimators': 908}. Best is trial 16 with value: 0.6603950316345228.\n",
      "[I 2023-12-06 09:47:16,496] Trial 17 finished with value: 0.6600680813614519 and parameters: {'n_estimators': 697}. Best is trial 16 with value: 0.6603950316345228.\n",
      "[I 2023-12-06 09:48:57,923] Trial 18 finished with value: 0.6603391779911061 and parameters: {'n_estimators': 888}. Best is trial 16 with value: 0.6603950316345228.\n",
      "[I 2023-12-06 09:49:55,501] Trial 19 finished with value: 0.6598029350133958 and parameters: {'n_estimators': 497}. Best is trial 16 with value: 0.6603950316345228.\n",
      "[I 2023-12-06 09:51:12,382] Trial 20 finished with value: 0.6601333264284804 and parameters: {'n_estimators': 671}. Best is trial 16 with value: 0.6603950316345228.\n",
      "[I 2023-12-06 09:53:01,216] Trial 21 finished with value: 0.6603720507343969 and parameters: {'n_estimators': 944}. Best is trial 16 with value: 0.6603950316345228.\n",
      "[I 2023-12-06 09:54:44,103] Trial 22 finished with value: 0.6603491124845551 and parameters: {'n_estimators': 921}. Best is trial 16 with value: 0.6603950316345228.\n",
      "[I 2023-12-06 09:56:28,273] Trial 23 finished with value: 0.6604065801827325 and parameters: {'n_estimators': 902}. Best is trial 23 with value: 0.6604065801827325.\n",
      "[I 2023-12-06 09:57:55,052] Trial 24 finished with value: 0.6600628604736316 and parameters: {'n_estimators': 748}. Best is trial 23 with value: 0.6604065801827325.\n",
      "[I 2023-12-06 09:59:33,941] Trial 25 finished with value: 0.6603270250385925 and parameters: {'n_estimators': 876}. Best is trial 23 with value: 0.6604065801827325.\n",
      "[I 2023-12-06 10:00:48,934] Trial 26 finished with value: 0.6601372459164858 and parameters: {'n_estimators': 668}. Best is trial 23 with value: 0.6604065801827325.\n",
      "[I 2023-12-06 10:02:28,060] Trial 27 finished with value: 0.6603496870051566 and parameters: {'n_estimators': 867}. Best is trial 23 with value: 0.6604065801827325.\n",
      "[I 2023-12-06 10:03:56,611] Trial 28 finished with value: 0.6599777120960871 and parameters: {'n_estimators': 763}. Best is trial 23 with value: 0.6604065801827325.\n",
      "[I 2023-12-06 10:05:39,553] Trial 29 finished with value: 0.6603492000711892 and parameters: {'n_estimators': 922}. Best is trial 23 with value: 0.6604065801827325.\n",
      "[I 2023-12-06 10:06:31,332] Trial 30 finished with value: 0.6596977683402441 and parameters: {'n_estimators': 440}. Best is trial 23 with value: 0.6604065801827325.\n",
      "[I 2023-12-06 10:08:18,053] Trial 31 finished with value: 0.6604116645347684 and parameters: {'n_estimators': 949}. Best is trial 31 with value: 0.6604116645347684.\n",
      "[I 2023-12-06 10:09:53,606] Trial 32 finished with value: 0.6601699946968201 and parameters: {'n_estimators': 829}. Best is trial 31 with value: 0.6604116645347684.\n",
      "[I 2023-12-06 10:11:37,109] Trial 33 finished with value: 0.660412110482207 and parameters: {'n_estimators': 915}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:13:29,897] Trial 34 finished with value: 0.6602410507637138 and parameters: {'n_estimators': 993}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:15:18,044] Trial 35 finished with value: 0.660369368067971 and parameters: {'n_estimators': 945}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:16:26,983] Trial 36 finished with value: 0.660010236027813 and parameters: {'n_estimators': 595}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:18:01,950] Trial 37 finished with value: 0.6603493046750596 and parameters: {'n_estimators': 847}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:19:51,460] Trial 38 finished with value: 0.6603815930015724 and parameters: {'n_estimators': 955}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:20:08,992] Trial 39 finished with value: 0.6563691113370355 and parameters: {'n_estimators': 137}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:21:40,408] Trial 40 finished with value: 0.659926185474969 and parameters: {'n_estimators': 793}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:23:30,452] Trial 41 finished with value: 0.6604060690073197 and parameters: {'n_estimators': 959}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:25:14,158] Trial 42 finished with value: 0.6603916981409705 and parameters: {'n_estimators': 911}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:27:07,401] Trial 43 finished with value: 0.6602474910804148 and parameters: {'n_estimators': 988}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:27:36,671] Trial 44 finished with value: 0.6580368862345368 and parameters: {'n_estimators': 240}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:29:21,563] Trial 45 finished with value: 0.660369368067971 and parameters: {'n_estimators': 945}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:31:00,878] Trial 46 finished with value: 0.660302259518833 and parameters: {'n_estimators': 862}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:32:36,368] Trial 47 finished with value: 0.6601780125728964 and parameters: {'n_estimators': 828}. Best is trial 33 with value: 0.660412110482207.\n",
      "[I 2023-12-06 10:34:18,934] Trial 48 finished with value: 0.660424465293637 and parameters: {'n_estimators': 901}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:36:13,352] Trial 49 finished with value: 0.6602737582812641 and parameters: {'n_estimators': 997}. Best is trial 48 with value: 0.660424465293637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6604\n",
      "\tBest params:\n",
      "\t\tn_estimators: 901\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.668049\n",
      "1                    TP  410.000000\n",
      "2                    TN  341.000000\n",
      "3                    FP   89.000000\n",
      "4                    FN   59.000000\n",
      "5              Accuracy    0.835373\n",
      "6             Precision    0.821643\n",
      "7           Sensitivity    0.874200\n",
      "8           Specificity    0.793000\n",
      "9              F1 score    0.847107\n",
      "10  F1 score (weighted)    0.834948\n",
      "11     F1 score (macro)    0.834397\n",
      "12    Balanced Accuracy    0.833612\n",
      "13                  MCC    0.670675\n",
      "14                  NPV    0.852500\n",
      "15              ROC_AUC    0.833612\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_0_cat = np.where((y_pred_rf_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 10:37:54,246] Trial 50 finished with value: 0.6524884277401968 and parameters: {'n_estimators': 782}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:39:36,757] Trial 51 finished with value: 0.6525701448755248 and parameters: {'n_estimators': 895}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:41:26,207] Trial 52 finished with value: 0.6526766529953709 and parameters: {'n_estimators': 958}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:43:10,092] Trial 53 finished with value: 0.6525031345152434 and parameters: {'n_estimators': 909}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:44:30,070] Trial 54 finished with value: 0.6521304014688827 and parameters: {'n_estimators': 715}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:46:06,538] Trial 55 finished with value: 0.6524720739127592 and parameters: {'n_estimators': 849}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:47:55,414] Trial 56 finished with value: 0.6526516389604693 and parameters: {'n_estimators': 963}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:49:38,325] Trial 57 finished with value: 0.6524974433473597 and parameters: {'n_estimators': 926}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:51:10,863] Trial 58 finished with value: 0.6523725396997847 and parameters: {'n_estimators': 814}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:52:50,915] Trial 59 finished with value: 0.6525636572084834 and parameters: {'n_estimators': 886}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:54:05,361] Trial 60 finished with value: 0.6521978551375498 and parameters: {'n_estimators': 660}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:55:48,268] Trial 61 finished with value: 0.6525512305894093 and parameters: {'n_estimators': 910}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:57:27,798] Trial 62 finished with value: 0.6526034457535029 and parameters: {'n_estimators': 877}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 10:59:18,298] Trial 63 finished with value: 0.652621673954877 and parameters: {'n_estimators': 973}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:01:03,148] Trial 64 finished with value: 0.652499427062631 and parameters: {'n_estimators': 924}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:02:29,322] Trial 65 finished with value: 0.6524273754577326 and parameters: {'n_estimators': 760}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:04:04,467] Trial 66 finished with value: 0.6524980969701393 and parameters: {'n_estimators': 853}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:05:51,342] Trial 67 finished with value: 0.6525678780086984 and parameters: {'n_estimators': 934}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:06:39,129] Trial 68 finished with value: 0.6512934284983437 and parameters: {'n_estimators': 406}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:07:49,940] Trial 69 finished with value: 0.6518442672489114 and parameters: {'n_estimators': 630}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:09:32,342] Trial 70 finished with value: 0.6525701448755247 and parameters: {'n_estimators': 895}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:11:21,860] Trial 71 finished with value: 0.6526027260959422 and parameters: {'n_estimators': 968}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:13:08,208] Trial 72 finished with value: 0.6526419759533311 and parameters: {'n_estimators': 954}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:15:00,893] Trial 73 finished with value: 0.6526024070701605 and parameters: {'n_estimators': 996}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:16:01,961] Trial 74 finished with value: 0.651445741113823 and parameters: {'n_estimators': 524}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:17:36,634] Trial 75 finished with value: 0.6524512895145086 and parameters: {'n_estimators': 826}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:19:18,013] Trial 76 finished with value: 0.6525537681111947 and parameters: {'n_estimators': 905}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:21:05,819] Trial 77 finished with value: 0.6526434060051971 and parameters: {'n_estimators': 943}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:22:44,547] Trial 78 finished with value: 0.6525602655994119 and parameters: {'n_estimators': 871}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:23:23,417] Trial 79 finished with value: 0.6511338860190997 and parameters: {'n_estimators': 328}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:25:15,124] Trial 80 finished with value: 0.6526086529220111 and parameters: {'n_estimators': 975}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:26:57,907] Trial 81 finished with value: 0.6524878226566967 and parameters: {'n_estimators': 922}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:28:28,644] Trial 82 finished with value: 0.6525617681861926 and parameters: {'n_estimators': 841}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:30:10,149] Trial 83 finished with value: 0.6525886837686358 and parameters: {'n_estimators': 894}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:31:44,029] Trial 84 finished with value: 0.6524630699293776 and parameters: {'n_estimators': 803}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:33:23,718] Trial 85 finished with value: 0.6525578455326824 and parameters: {'n_estimators': 870}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:35:12,536] Trial 86 finished with value: 0.6525678780086985 and parameters: {'n_estimators': 934}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:37:03,090] Trial 87 finished with value: 0.6525724636726707 and parameters: {'n_estimators': 970}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:38:34,117] Trial 88 finished with value: 0.6525086819971826 and parameters: {'n_estimators': 778}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:40:27,776] Trial 89 finished with value: 0.6526213391478288 and parameters: {'n_estimators': 1000}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:42:12,975] Trial 90 finished with value: 0.6525463279877087 and parameters: {'n_estimators': 904}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:43:58,441] Trial 91 finished with value: 0.6526567839944479 and parameters: {'n_estimators': 948}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:45:44,220] Trial 92 finished with value: 0.6525872058183653 and parameters: {'n_estimators': 918}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:47:34,963] Trial 93 finished with value: 0.6526429472860261 and parameters: {'n_estimators': 951}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:49:15,686] Trial 94 finished with value: 0.6525784020157934 and parameters: {'n_estimators': 884}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:50:52,449] Trial 95 finished with value: 0.6525301859561898 and parameters: {'n_estimators': 842}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:52:42,605] Trial 96 finished with value: 0.6526231629954805 and parameters: {'n_estimators': 976}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:54:22,666] Trial 97 finished with value: 0.6525428452429851 and parameters: {'n_estimators': 860}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:56:12,594] Trial 98 finished with value: 0.6526377318608192 and parameters: {'n_estimators': 944}. Best is trial 48 with value: 0.660424465293637.\n",
      "[I 2023-12-06 11:57:59,874] Trial 99 finished with value: 0.6525484177767533 and parameters: {'n_estimators': 920}. Best is trial 48 with value: 0.660424465293637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6604\n",
      "\tBest params:\n",
      "\t\tn_estimators: 901\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.668049    0.674609\n",
      "1                    TP  410.000000  406.000000\n",
      "2                    TN  341.000000  345.000000\n",
      "3                    FP   89.000000   79.000000\n",
      "4                    FN   59.000000   69.000000\n",
      "5              Accuracy    0.835373    0.835373\n",
      "6             Precision    0.821643    0.837113\n",
      "7           Sensitivity    0.874200    0.854737\n",
      "8           Specificity    0.793000    0.813700\n",
      "9              F1 score    0.847107    0.845833\n",
      "10  F1 score (weighted)    0.834948    0.835248\n",
      "11     F1 score (macro)    0.834397    0.834611\n",
      "12    Balanced Accuracy    0.833612    0.834208\n",
      "13                  MCC    0.670675    0.669431\n",
      "14                  NPV    0.852500    0.833300\n",
      "15              ROC_AUC    0.833612    0.834208\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_1_cat = np.where((y_pred_rf_1 >= 6.6), 1, 0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 11:59:54,265] Trial 100 finished with value: 0.6636536278418805 and parameters: {'n_estimators': 887}. Best is trial 100 with value: 0.6636536278418805.\n",
      "[I 2023-12-06 12:01:36,533] Trial 101 finished with value: 0.6636399967906441 and parameters: {'n_estimators': 893}. Best is trial 100 with value: 0.6636536278418805.\n",
      "[I 2023-12-06 12:03:15,134] Trial 102 finished with value: 0.6636754628289712 and parameters: {'n_estimators': 888}. Best is trial 102 with value: 0.6636754628289712.\n",
      "[I 2023-12-06 12:04:56,000] Trial 103 finished with value: 0.6636145897103678 and parameters: {'n_estimators': 883}. Best is trial 102 with value: 0.6636754628289712.\n",
      "[I 2023-12-06 12:06:37,290] Trial 104 finished with value: 0.6636196450605425 and parameters: {'n_estimators': 884}. Best is trial 102 with value: 0.6636754628289712.\n",
      "[I 2023-12-06 12:08:11,666] Trial 105 finished with value: 0.6636299937667597 and parameters: {'n_estimators': 807}. Best is trial 102 with value: 0.6636754628289712.\n",
      "[I 2023-12-06 12:09:46,425] Trial 106 finished with value: 0.6636221341001824 and parameters: {'n_estimators': 809}. Best is trial 102 with value: 0.6636754628289712.\n",
      "[I 2023-12-06 12:11:13,942] Trial 107 finished with value: 0.6637658119175528 and parameters: {'n_estimators': 748}. Best is trial 107 with value: 0.6637658119175528.\n",
      "[I 2023-12-06 12:12:39,379] Trial 108 finished with value: 0.6638493651229009 and parameters: {'n_estimators': 751}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:14:02,561] Trial 109 finished with value: 0.6636758349283696 and parameters: {'n_estimators': 718}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:15:28,399] Trial 110 finished with value: 0.6636959948796177 and parameters: {'n_estimators': 740}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:16:51,243] Trial 111 finished with value: 0.6636644678471645 and parameters: {'n_estimators': 723}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:18:15,742] Trial 112 finished with value: 0.6636380831387083 and parameters: {'n_estimators': 729}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:19:37,846] Trial 113 finished with value: 0.6636537377198899 and parameters: {'n_estimators': 717}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:21:00,232] Trial 114 finished with value: 0.6636607444902685 and parameters: {'n_estimators': 733}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:22:23,467] Trial 115 finished with value: 0.6636758349283696 and parameters: {'n_estimators': 718}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:23:43,676] Trial 116 finished with value: 0.6636529788864689 and parameters: {'n_estimators': 721}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:25:03,172] Trial 117 finished with value: 0.663708102884632 and parameters: {'n_estimators': 705}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:26:22,168] Trial 118 finished with value: 0.6637664608972208 and parameters: {'n_estimators': 697}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:27:40,887] Trial 119 finished with value: 0.6638103453859395 and parameters: {'n_estimators': 688}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:29:00,407] Trial 120 finished with value: 0.6638159638483387 and parameters: {'n_estimators': 693}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:30:19,573] Trial 121 finished with value: 0.6638086643104254 and parameters: {'n_estimators': 682}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:31:39,223] Trial 122 finished with value: 0.6637744174144695 and parameters: {'n_estimators': 696}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:32:57,746] Trial 123 finished with value: 0.6637853178041128 and parameters: {'n_estimators': 689}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:34:16,899] Trial 124 finished with value: 0.663810685214546 and parameters: {'n_estimators': 691}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:35:35,370] Trial 125 finished with value: 0.6638106852145459 and parameters: {'n_estimators': 691}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:36:54,538] Trial 126 finished with value: 0.66378028857539 and parameters: {'n_estimators': 684}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:38:14,278] Trial 127 finished with value: 0.6637776381343551 and parameters: {'n_estimators': 687}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:39:34,637] Trial 128 finished with value: 0.6638159638483387 and parameters: {'n_estimators': 693}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:40:49,270] Trial 129 finished with value: 0.6635999066892567 and parameters: {'n_estimators': 649}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:42:09,145] Trial 130 finished with value: 0.6637776381343551 and parameters: {'n_estimators': 687}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:43:28,595] Trial 131 finished with value: 0.6637776381343551 and parameters: {'n_estimators': 687}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:44:46,246] Trial 132 finished with value: 0.6637999702510327 and parameters: {'n_estimators': 683}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:46:05,565] Trial 133 finished with value: 0.6638086643104255 and parameters: {'n_estimators': 682}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:47:11,397] Trial 134 finished with value: 0.6637836688671894 and parameters: {'n_estimators': 569}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:48:23,166] Trial 135 finished with value: 0.663639496284884 and parameters: {'n_estimators': 618}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:49:28,084] Trial 136 finished with value: 0.6638030673860258 and parameters: {'n_estimators': 574}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:50:35,286] Trial 137 finished with value: 0.6637966057014577 and parameters: {'n_estimators': 577}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:51:41,952] Trial 138 finished with value: 0.6638055103177603 and parameters: {'n_estimators': 579}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:52:48,289] Trial 139 finished with value: 0.6637967619735268 and parameters: {'n_estimators': 571}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:53:51,020] Trial 140 finished with value: 0.6635175959882318 and parameters: {'n_estimators': 537}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:54:59,309] Trial 141 finished with value: 0.6638086185324814 and parameters: {'n_estimators': 592}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:56:07,367] Trial 142 finished with value: 0.663756904713724 and parameters: {'n_estimators': 591}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:57:12,539] Trial 143 finished with value: 0.6637944188924437 and parameters: {'n_estimators': 566}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:58:11,362] Trial 144 finished with value: 0.6634709892584703 and parameters: {'n_estimators': 502}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 12:59:26,138] Trial 145 finished with value: 0.6636130394336119 and parameters: {'n_estimators': 651}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:00:30,919] Trial 146 finished with value: 0.663810278576112 and parameters: {'n_estimators': 559}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:01:43,904] Trial 147 finished with value: 0.6635238995468268 and parameters: {'n_estimators': 631}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:02:52,328] Trial 148 finished with value: 0.6637989603913053 and parameters: {'n_estimators': 601}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:04:03,231] Trial 149 finished with value: 0.6637299646617348 and parameters: {'n_estimators': 607}. Best is trial 108 with value: 0.6638493651229009.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6638\n",
      "\tBest params:\n",
      "\t\tn_estimators: 751\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.668049    0.674609    0.679188\n",
      "1                    TP  410.000000  406.000000  411.000000\n",
      "2                    TN  341.000000  345.000000  342.000000\n",
      "3                    FP   89.000000   79.000000   86.000000\n",
      "4                    FN   59.000000   69.000000   60.000000\n",
      "5              Accuracy    0.835373    0.835373    0.837597\n",
      "6             Precision    0.821643    0.837113    0.826962\n",
      "7           Sensitivity    0.874200    0.854737    0.872611\n",
      "8           Specificity    0.793000    0.813700    0.799100\n",
      "9              F1 score    0.847107    0.845833    0.849174\n",
      "10  F1 score (weighted)    0.834948    0.835248    0.837235\n",
      "11     F1 score (macro)    0.834397    0.834611    0.836635\n",
      "12    Balanced Accuracy    0.833612    0.834208    0.835838\n",
      "13                  MCC    0.670675    0.669431    0.674686\n",
      "14                  NPV    0.852500    0.833300    0.850700\n",
      "15              ROC_AUC    0.833612    0.834208    0.835838\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_2_cat = np.where((y_pred_rf_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 13:05:28,931] Trial 150 finished with value: 0.6504685897970758 and parameters: {'n_estimators': 665}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:06:34,995] Trial 151 finished with value: 0.6507481925203087 and parameters: {'n_estimators': 559}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:07:42,991] Trial 152 finished with value: 0.6507255754296157 and parameters: {'n_estimators': 578}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:08:57,789] Trial 153 finished with value: 0.6503004248160842 and parameters: {'n_estimators': 636}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:10:00,090] Trial 154 finished with value: 0.6505743303669316 and parameters: {'n_estimators': 526}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:11:11,074] Trial 155 finished with value: 0.6504757865542602 and parameters: {'n_estimators': 604}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:12:17,860] Trial 156 finished with value: 0.6508183670881029 and parameters: {'n_estimators': 586}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:13:21,977] Trial 157 finished with value: 0.6506864794669237 and parameters: {'n_estimators': 542}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:14:34,732] Trial 158 finished with value: 0.6504651101631266 and parameters: {'n_estimators': 618}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:15:33,743] Trial 159 finished with value: 0.6505182123806643 and parameters: {'n_estimators': 506}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:16:52,360] Trial 160 finished with value: 0.6504154165441896 and parameters: {'n_estimators': 672}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:18:00,189] Trial 161 finished with value: 0.6507505159591604 and parameters: {'n_estimators': 576}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:19:06,199] Trial 162 finished with value: 0.6507579705248703 and parameters: {'n_estimators': 560}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:20:01,060] Trial 163 finished with value: 0.650479184444744 and parameters: {'n_estimators': 463}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:21:15,872] Trial 164 finished with value: 0.6503710417698002 and parameters: {'n_estimators': 649}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:22:44,380] Trial 165 finished with value: 0.6500508324178167 and parameters: {'n_estimators': 759}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:23:49,645] Trial 166 finished with value: 0.6507058554310354 and parameters: {'n_estimators': 555}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:25:00,712] Trial 167 finished with value: 0.6504005559918914 and parameters: {'n_estimators': 609}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:26:13,352] Trial 168 finished with value: 0.6504377934754071 and parameters: {'n_estimators': 629}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:27:23,648] Trial 169 finished with value: 0.6506602252729092 and parameters: {'n_estimators': 597}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:28:24,327] Trial 170 finished with value: 0.6506260527007074 and parameters: {'n_estimators': 514}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:29:42,610] Trial 171 finished with value: 0.6504036713038222 and parameters: {'n_estimators': 671}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:31:00,401] Trial 172 finished with value: 0.6504685897970758 and parameters: {'n_estimators': 665}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:32:04,780] Trial 173 finished with value: 0.6505404567001553 and parameters: {'n_estimators': 547}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:33:19,489] Trial 174 finished with value: 0.6502857847460998 and parameters: {'n_estimators': 637}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:34:41,630] Trial 175 finished with value: 0.6501219449716136 and parameters: {'n_estimators': 703}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:35:48,449] Trial 176 finished with value: 0.6507666723157252 and parameters: {'n_estimators': 568}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:37:05,096] Trial 177 finished with value: 0.6504296625371058 and parameters: {'n_estimators': 654}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:38:34,321] Trial 178 finished with value: 0.6502181699173628 and parameters: {'n_estimators': 772}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:39:42,941] Trial 179 finished with value: 0.650777519130533 and parameters: {'n_estimators': 585}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:41:08,073] Trial 180 finished with value: 0.6500769866533427 and parameters: {'n_estimators': 744}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:42:15,420] Trial 181 finished with value: 0.6507684860645171 and parameters: {'n_estimators': 574}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:43:17,276] Trial 182 finished with value: 0.6505686724174201 and parameters: {'n_estimators': 532}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:44:12,766] Trial 183 finished with value: 0.6503692169457312 and parameters: {'n_estimators': 480}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:45:29,522] Trial 184 finished with value: 0.6504120233234165 and parameters: {'n_estimators': 679}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:46:49,832] Trial 185 finished with value: 0.650088498536646 and parameters: {'n_estimators': 702}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:47:54,566] Trial 186 finished with value: 0.6506928118324581 and parameters: {'n_estimators': 556}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:49:03,735] Trial 187 finished with value: 0.650767494234899 and parameters: {'n_estimators': 594}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:50:17,045] Trial 188 finished with value: 0.6505104407420085 and parameters: {'n_estimators': 623}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:51:23,430] Trial 189 finished with value: 0.6507559081683567 and parameters: {'n_estimators': 570}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:52:39,201] Trial 190 finished with value: 0.650396662599922 and parameters: {'n_estimators': 647}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:53:58,042] Trial 191 finished with value: 0.6504038376408523 and parameters: {'n_estimators': 673}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:55:17,005] Trial 192 finished with value: 0.6503594826454969 and parameters: {'n_estimators': 683}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:56:39,548] Trial 193 finished with value: 0.6501735756269207 and parameters: {'n_estimators': 708}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:57:49,791] Trial 194 finished with value: 0.6503721437973965 and parameters: {'n_estimators': 611}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 13:59:09,020] Trial 195 finished with value: 0.650110146065819 and parameters: {'n_estimators': 695}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:00:12,899] Trial 196 finished with value: 0.6506348332491702 and parameters: {'n_estimators': 545}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:01:29,838] Trial 197 finished with value: 0.650440468315973 and parameters: {'n_estimators': 659}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:02:53,142] Trial 198 finished with value: 0.6502154611336274 and parameters: {'n_estimators': 730}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:04:02,124] Trial 199 finished with value: 0.650838141617042 and parameters: {'n_estimators': 588}. Best is trial 108 with value: 0.6638493651229009.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6638\n",
      "\tBest params:\n",
      "\t\tn_estimators: 751\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.668049    0.674609    0.679188    0.685368\n",
      "1                    TP  410.000000  406.000000  411.000000  399.000000\n",
      "2                    TN  341.000000  345.000000  342.000000  338.000000\n",
      "3                    FP   89.000000   79.000000   86.000000   97.000000\n",
      "4                    FN   59.000000   69.000000   60.000000   65.000000\n",
      "5              Accuracy    0.835373    0.835373    0.837597    0.819800\n",
      "6             Precision    0.821643    0.837113    0.826962    0.804435\n",
      "7           Sensitivity    0.874200    0.854737    0.872611    0.859914\n",
      "8           Specificity    0.793000    0.813700    0.799100    0.777000\n",
      "9              F1 score    0.847107    0.845833    0.849174    0.831250\n",
      "10  F1 score (weighted)    0.834948    0.835248    0.837235    0.819363\n",
      "11     F1 score (macro)    0.834397    0.834611    0.836635    0.818966\n",
      "12    Balanced Accuracy    0.833612    0.834208    0.835838    0.818463\n",
      "13                  MCC    0.670675    0.669431    0.674686    0.640028\n",
      "14                  NPV    0.852500    0.833300    0.850700    0.838700\n",
      "15              ROC_AUC    0.833612    0.834208    0.835838    0.818463\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_3_cat = np.where((y_pred_rf_3 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 14:05:28,553] Trial 200 finished with value: 0.6587275894896664 and parameters: {'n_estimators': 683}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:06:47,689] Trial 201 finished with value: 0.6589186366549036 and parameters: {'n_estimators': 696}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:08:03,579] Trial 202 finished with value: 0.6587111425555079 and parameters: {'n_estimators': 682}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:09:21,954] Trial 203 finished with value: 0.6590459645096863 and parameters: {'n_estimators': 713}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:10:24,562] Trial 204 finished with value: 0.6585975150089483 and parameters: {'n_estimators': 561}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:11:36,371] Trial 205 finished with value: 0.6586886892727986 and parameters: {'n_estimators': 646}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:12:52,595] Trial 206 finished with value: 0.6587993920097562 and parameters: {'n_estimators': 668}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:14:18,089] Trial 207 finished with value: 0.6588646138006822 and parameters: {'n_estimators': 751}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:15:17,930] Trial 208 finished with value: 0.6586374944011757 and parameters: {'n_estimators': 525}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:16:26,741] Trial 209 finished with value: 0.658781051302811 and parameters: {'n_estimators': 602}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:17:32,442] Trial 210 finished with value: 0.6586814173062514 and parameters: {'n_estimators': 575}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:18:50,419] Trial 211 finished with value: 0.6588791343646812 and parameters: {'n_estimators': 691}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:20:07,684] Trial 212 finished with value: 0.6588484518612123 and parameters: {'n_estimators': 687}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:21:24,660] Trial 213 finished with value: 0.6589811953591421 and parameters: {'n_estimators': 710}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:22:38,366] Trial 214 finished with value: 0.6587654183940239 and parameters: {'n_estimators': 664}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:23:59,344] Trial 215 finished with value: 0.6589506695325027 and parameters: {'n_estimators': 728}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:25:10,709] Trial 216 finished with value: 0.6588731018282983 and parameters: {'n_estimators': 629}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:26:27,363] Trial 217 finished with value: 0.6587161565752983 and parameters: {'n_estimators': 678}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:27:29,121] Trial 218 finished with value: 0.658566118468431 and parameters: {'n_estimators': 545}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:28:36,346] Trial 219 finished with value: 0.6587354641952955 and parameters: {'n_estimators': 587}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:29:03,218] Trial 220 finished with value: 0.658426575893464 and parameters: {'n_estimators': 224}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:30:21,430] Trial 221 finished with value: 0.6588272074567915 and parameters: {'n_estimators': 689}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:31:41,004] Trial 222 finished with value: 0.6589249374009885 and parameters: {'n_estimators': 705}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:32:55,813] Trial 223 finished with value: 0.6587926624199753 and parameters: {'n_estimators': 657}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:34:12,897] Trial 224 finished with value: 0.6587065657797009 and parameters: {'n_estimators': 680}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:35:30,776] Trial 225 finished with value: 0.6588487737960793 and parameters: {'n_estimators': 695}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:36:52,158] Trial 226 finished with value: 0.658938228442032 and parameters: {'n_estimators': 731}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:37:56,301] Trial 227 finished with value: 0.6585975150089483 and parameters: {'n_estimators': 561}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:39:05,409] Trial 228 finished with value: 0.6587907397517982 and parameters: {'n_estimators': 610}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:40:17,884] Trial 229 finished with value: 0.6587813450180435 and parameters: {'n_estimators': 638}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:41:38,155] Trial 230 finished with value: 0.6589811953591421 and parameters: {'n_estimators': 710}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:42:55,785] Trial 231 finished with value: 0.6588272074567916 and parameters: {'n_estimators': 689}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:44:10,235] Trial 232 finished with value: 0.6587461753899173 and parameters: {'n_estimators': 669}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:45:29,916] Trial 233 finished with value: 0.6589636269876236 and parameters: {'n_estimators': 701}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:46:35,411] Trial 234 finished with value: 0.6585660279620502 and parameters: {'n_estimators': 570}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:47:57,443] Trial 235 finished with value: 0.6590596591168814 and parameters: {'n_estimators': 719}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:49:11,634] Trial 236 finished with value: 0.6587984191040809 and parameters: {'n_estimators': 656}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:50:36,289] Trial 237 finished with value: 0.6587678014792673 and parameters: {'n_estimators': 742}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:51:53,973] Trial 238 finished with value: 0.6587196558497261 and parameters: {'n_estimators': 681}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:52:58,647] Trial 239 finished with value: 0.6587006177802597 and parameters: {'n_estimators': 586}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:54:17,632] Trial 240 finished with value: 0.6589755314805104 and parameters: {'n_estimators': 699}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:55:36,958] Trial 241 finished with value: 0.6589231726027067 and parameters: {'n_estimators': 697}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:56:52,416] Trial 242 finished with value: 0.6587282855035343 and parameters: {'n_estimators': 670}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:58:09,324] Trial 243 finished with value: 0.6588574318875556 and parameters: {'n_estimators': 690}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 14:59:28,876] Trial 244 finished with value: 0.6590484052481413 and parameters: {'n_estimators': 715}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 15:00:45,821] Trial 245 finished with value: 0.6587364456245728 and parameters: {'n_estimators': 675}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 15:01:48,340] Trial 246 finished with value: 0.658566118468431 and parameters: {'n_estimators': 545}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 15:02:52,677] Trial 247 finished with value: 0.6585937915784963 and parameters: {'n_estimators': 572}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 15:04:04,022] Trial 248 finished with value: 0.6587373692194003 and parameters: {'n_estimators': 641}. Best is trial 108 with value: 0.6638493651229009.\n",
      "[I 2023-12-06 15:05:12,683] Trial 249 finished with value: 0.6587726802498133 and parameters: {'n_estimators': 600}. Best is trial 108 with value: 0.6638493651229009.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6638\n",
      "\tBest params:\n",
      "\t\tn_estimators: 751\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.668049    0.674609    0.679188    0.685368   \n",
      "1                    TP  410.000000  406.000000  411.000000  399.000000   \n",
      "2                    TN  341.000000  345.000000  342.000000  338.000000   \n",
      "3                    FP   89.000000   79.000000   86.000000   97.000000   \n",
      "4                    FN   59.000000   69.000000   60.000000   65.000000   \n",
      "5              Accuracy    0.835373    0.835373    0.837597    0.819800   \n",
      "6             Precision    0.821643    0.837113    0.826962    0.804435   \n",
      "7           Sensitivity    0.874200    0.854737    0.872611    0.859914   \n",
      "8           Specificity    0.793000    0.813700    0.799100    0.777000   \n",
      "9              F1 score    0.847107    0.845833    0.849174    0.831250   \n",
      "10  F1 score (weighted)    0.834948    0.835248    0.837235    0.819363   \n",
      "11     F1 score (macro)    0.834397    0.834611    0.836635    0.818966   \n",
      "12    Balanced Accuracy    0.833612    0.834208    0.835838    0.818463   \n",
      "13                  MCC    0.670675    0.669431    0.674686    0.640028   \n",
      "14                  NPV    0.852500    0.833300    0.850700    0.838700   \n",
      "15              ROC_AUC    0.833612    0.834208    0.835838    0.818463   \n",
      "\n",
      "          Set4  \n",
      "0     0.668521  \n",
      "1   411.000000  \n",
      "2   351.000000  \n",
      "3    78.000000  \n",
      "4    59.000000  \n",
      "5     0.847608  \n",
      "6     0.840491  \n",
      "7     0.874468  \n",
      "8     0.818200  \n",
      "9     0.857143  \n",
      "10    0.847393  \n",
      "11    0.846927  \n",
      "12    0.846325  \n",
      "13    0.694616  \n",
      "14    0.856100  \n",
      "15    0.846325  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_4_cat = np.where((y_pred_rf_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 15:06:45,720] Trial 250 finished with value: 0.6665330247051002 and parameters: {'n_estimators': 721}. Best is trial 250 with value: 0.6665330247051002.\n",
      "[I 2023-12-06 15:08:09,897] Trial 251 finished with value: 0.6665461409601424 and parameters: {'n_estimators': 727}. Best is trial 251 with value: 0.6665461409601424.\n",
      "[I 2023-12-06 15:09:38,564] Trial 252 finished with value: 0.6665299231318824 and parameters: {'n_estimators': 769}. Best is trial 251 with value: 0.6665461409601424.\n",
      "[I 2023-12-06 15:11:10,825] Trial 253 finished with value: 0.6665701097226138 and parameters: {'n_estimators': 791}. Best is trial 253 with value: 0.6665701097226138.\n",
      "[I 2023-12-06 15:12:36,142] Trial 254 finished with value: 0.6665377906384343 and parameters: {'n_estimators': 753}. Best is trial 253 with value: 0.6665701097226138.\n",
      "[I 2023-12-06 15:14:05,973] Trial 255 finished with value: 0.6666437435305446 and parameters: {'n_estimators': 796}. Best is trial 255 with value: 0.6666437435305446.\n",
      "[I 2023-12-06 15:15:36,298] Trial 256 finished with value: 0.6665747558802952 and parameters: {'n_estimators': 789}. Best is trial 255 with value: 0.6666437435305446.\n",
      "[I 2023-12-06 15:17:07,884] Trial 257 finished with value: 0.6665747558802952 and parameters: {'n_estimators': 789}. Best is trial 255 with value: 0.6666437435305446.\n",
      "[I 2023-12-06 15:18:39,038] Trial 258 finished with value: 0.6666541226193601 and parameters: {'n_estimators': 797}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:20:11,502] Trial 259 finished with value: 0.6665701097226137 and parameters: {'n_estimators': 791}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:21:39,897] Trial 260 finished with value: 0.6665570220754509 and parameters: {'n_estimators': 773}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:23:11,040] Trial 261 finished with value: 0.6665690012476391 and parameters: {'n_estimators': 787}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:24:43,609] Trial 262 finished with value: 0.6665747558802952 and parameters: {'n_estimators': 789}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:26:10,057] Trial 263 finished with value: 0.6664933400089141 and parameters: {'n_estimators': 770}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:27:41,973] Trial 264 finished with value: 0.6665747558802951 and parameters: {'n_estimators': 789}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:29:12,840] Trial 265 finished with value: 0.6666322252841473 and parameters: {'n_estimators': 794}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:30:44,614] Trial 266 finished with value: 0.6665913366259114 and parameters: {'n_estimators': 792}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:32:12,341] Trial 267 finished with value: 0.6665676013752917 and parameters: {'n_estimators': 790}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:33:43,010] Trial 268 finished with value: 0.6665913366259114 and parameters: {'n_estimators': 792}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:35:14,982] Trial 269 finished with value: 0.6666154176985575 and parameters: {'n_estimators': 793}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:36:45,625] Trial 270 finished with value: 0.6665701097226138 and parameters: {'n_estimators': 791}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:38:16,875] Trial 271 finished with value: 0.666581581890312 and parameters: {'n_estimators': 788}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:39:48,219] Trial 272 finished with value: 0.6665676013752917 and parameters: {'n_estimators': 790}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:41:19,075] Trial 273 finished with value: 0.6665913366259113 and parameters: {'n_estimators': 792}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:42:49,216] Trial 274 finished with value: 0.666581581890312 and parameters: {'n_estimators': 788}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:44:21,146] Trial 275 finished with value: 0.6666322252841475 and parameters: {'n_estimators': 794}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:45:52,460] Trial 276 finished with value: 0.6665676013752917 and parameters: {'n_estimators': 790}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:47:24,641] Trial 277 finished with value: 0.6666159821088564 and parameters: {'n_estimators': 795}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:48:54,149] Trial 278 finished with value: 0.6665913366259115 and parameters: {'n_estimators': 792}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:50:23,604] Trial 279 finished with value: 0.666650917330982 and parameters: {'n_estimators': 798}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:51:55,596] Trial 280 finished with value: 0.6666159821088564 and parameters: {'n_estimators': 795}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:53:27,928] Trial 281 finished with value: 0.6666159821088564 and parameters: {'n_estimators': 795}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:55:04,281] Trial 282 finished with value: 0.6666168592228358 and parameters: {'n_estimators': 820}. Best is trial 258 with value: 0.6666541226193601.\n",
      "[I 2023-12-06 15:56:38,986] Trial 283 finished with value: 0.6666541937713887 and parameters: {'n_estimators': 822}. Best is trial 283 with value: 0.6666541937713887.\n",
      "[I 2023-12-06 15:58:11,692] Trial 284 finished with value: 0.6666278932470004 and parameters: {'n_estimators': 823}. Best is trial 283 with value: 0.6666541937713887.\n",
      "[I 2023-12-06 15:59:45,541] Trial 285 finished with value: 0.6666130455745751 and parameters: {'n_estimators': 819}. Best is trial 283 with value: 0.6666541937713887.\n",
      "[I 2023-12-06 16:01:21,172] Trial 286 finished with value: 0.6665732434696177 and parameters: {'n_estimators': 824}. Best is trial 283 with value: 0.6666541937713887.\n",
      "[I 2023-12-06 16:02:56,460] Trial 287 finished with value: 0.6665732434696177 and parameters: {'n_estimators': 824}. Best is trial 283 with value: 0.6666541937713887.\n",
      "[I 2023-12-06 16:04:31,176] Trial 288 finished with value: 0.6665763015050244 and parameters: {'n_estimators': 826}. Best is trial 283 with value: 0.6666541937713887.\n",
      "[I 2023-12-06 16:06:05,737] Trial 289 finished with value: 0.6666399437672264 and parameters: {'n_estimators': 816}. Best is trial 283 with value: 0.6666541937713887.\n",
      "[I 2023-12-06 16:07:38,152] Trial 290 finished with value: 0.6666674493839012 and parameters: {'n_estimators': 808}. Best is trial 290 with value: 0.6666674493839012.\n",
      "[I 2023-12-06 16:09:15,362] Trial 291 finished with value: 0.6666485138700786 and parameters: {'n_estimators': 840}. Best is trial 290 with value: 0.6666674493839012.\n",
      "[I 2023-12-06 16:10:48,914] Trial 292 finished with value: 0.666623435807256 and parameters: {'n_estimators': 812}. Best is trial 290 with value: 0.6666674493839012.\n",
      "[I 2023-12-06 16:12:26,014] Trial 293 finished with value: 0.666635989720782 and parameters: {'n_estimators': 842}. Best is trial 290 with value: 0.6666674493839012.\n",
      "[I 2023-12-06 16:14:03,205] Trial 294 finished with value: 0.6665699399612425 and parameters: {'n_estimators': 837}. Best is trial 290 with value: 0.6666674493839012.\n",
      "[I 2023-12-06 16:15:34,339] Trial 295 finished with value: 0.6666457180757865 and parameters: {'n_estimators': 813}. Best is trial 290 with value: 0.6666674493839012.\n",
      "[I 2023-12-06 16:17:06,629] Trial 296 finished with value: 0.6666369860447747 and parameters: {'n_estimators': 814}. Best is trial 290 with value: 0.6666674493839012.\n",
      "[I 2023-12-06 16:18:40,069] Trial 297 finished with value: 0.6666620531649835 and parameters: {'n_estimators': 807}. Best is trial 290 with value: 0.6666674493839012.\n",
      "[I 2023-12-06 16:20:13,986] Trial 298 finished with value: 0.6666412413782011 and parameters: {'n_estimators': 811}. Best is trial 290 with value: 0.6666674493839012.\n",
      "[I 2023-12-06 16:21:47,309] Trial 299 finished with value: 0.6666818615472077 and parameters: {'n_estimators': 809}. Best is trial 299 with value: 0.6666818615472077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6667\n",
      "\tBest params:\n",
      "\t\tn_estimators: 809\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.668049    0.674609    0.679188    0.685368   \n",
      "1                    TP  410.000000  406.000000  411.000000  399.000000   \n",
      "2                    TN  341.000000  345.000000  342.000000  338.000000   \n",
      "3                    FP   89.000000   79.000000   86.000000   97.000000   \n",
      "4                    FN   59.000000   69.000000   60.000000   65.000000   \n",
      "5              Accuracy    0.835373    0.835373    0.837597    0.819800   \n",
      "6             Precision    0.821643    0.837113    0.826962    0.804435   \n",
      "7           Sensitivity    0.874200    0.854737    0.872611    0.859914   \n",
      "8           Specificity    0.793000    0.813700    0.799100    0.777000   \n",
      "9              F1 score    0.847107    0.845833    0.849174    0.831250   \n",
      "10  F1 score (weighted)    0.834948    0.835248    0.837235    0.819363   \n",
      "11     F1 score (macro)    0.834397    0.834611    0.836635    0.818966   \n",
      "12    Balanced Accuracy    0.833612    0.834208    0.835838    0.818463   \n",
      "13                  MCC    0.670675    0.669431    0.674686    0.640028   \n",
      "14                  NPV    0.852500    0.833300    0.850700    0.838700   \n",
      "15              ROC_AUC    0.833612    0.834208    0.835838    0.818463   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.668521    0.695704  \n",
      "1   411.000000  399.000000  \n",
      "2   351.000000  351.000000  \n",
      "3    78.000000   80.000000  \n",
      "4    59.000000   69.000000  \n",
      "5     0.847608    0.834260  \n",
      "6     0.840491    0.832985  \n",
      "7     0.874468    0.852564  \n",
      "8     0.818200    0.814400  \n",
      "9     0.857143    0.842661  \n",
      "10    0.847393    0.834152  \n",
      "11    0.846927    0.833786  \n",
      "12    0.846325    0.833475  \n",
      "13    0.694616    0.667824  \n",
      "14    0.856100    0.835700  \n",
      "15    0.846325    0.833475  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_5_cat = np.where((y_pred_rf_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 16:23:29,481] Trial 300 finished with value: 0.6690108345736229 and parameters: {'n_estimators': 814}. Best is trial 300 with value: 0.6690108345736229.\n",
      "[I 2023-12-06 16:24:59,542] Trial 301 finished with value: 0.6690134433314722 and parameters: {'n_estimators': 812}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:26:27,717] Trial 302 finished with value: 0.669007648458291 and parameters: {'n_estimators': 816}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:27:57,872] Trial 303 finished with value: 0.6690134433314722 and parameters: {'n_estimators': 812}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:29:32,183] Trial 304 finished with value: 0.6689482005099413 and parameters: {'n_estimators': 856}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:31:06,656] Trial 305 finished with value: 0.6689043698384518 and parameters: {'n_estimators': 850}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:32:42,159] Trial 306 finished with value: 0.6689107563155215 and parameters: {'n_estimators': 851}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:34:17,217] Trial 307 finished with value: 0.6689482005099412 and parameters: {'n_estimators': 856}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:35:53,065] Trial 308 finished with value: 0.6689367353758916 and parameters: {'n_estimators': 852}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:37:27,777] Trial 309 finished with value: 0.6689367353758915 and parameters: {'n_estimators': 852}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:39:01,993] Trial 310 finished with value: 0.6689482005099412 and parameters: {'n_estimators': 856}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:40:38,279] Trial 311 finished with value: 0.668960209872477 and parameters: {'n_estimators': 855}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:42:13,343] Trial 312 finished with value: 0.6688825274469984 and parameters: {'n_estimators': 848}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:43:48,460] Trial 313 finished with value: 0.6689850115127454 and parameters: {'n_estimators': 863}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:45:26,360] Trial 314 finished with value: 0.6689769482018436 and parameters: {'n_estimators': 861}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:47:04,051] Trial 315 finished with value: 0.6689730060468296 and parameters: {'n_estimators': 860}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:48:39,274] Trial 316 finished with value: 0.6689712626596342 and parameters: {'n_estimators': 858}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:50:15,250] Trial 317 finished with value: 0.6689861662079005 and parameters: {'n_estimators': 862}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:51:50,803] Trial 318 finished with value: 0.6689730060468296 and parameters: {'n_estimators': 860}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:53:25,197] Trial 319 finished with value: 0.6689367353758915 and parameters: {'n_estimators': 852}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:55:03,054] Trial 320 finished with value: 0.6690051296788255 and parameters: {'n_estimators': 871}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:56:41,392] Trial 321 finished with value: 0.6689861662079005 and parameters: {'n_estimators': 862}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:58:19,595] Trial 322 finished with value: 0.6689861662079005 and parameters: {'n_estimators': 862}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 16:59:57,237] Trial 323 finished with value: 0.6689861662079005 and parameters: {'n_estimators': 862}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:01:34,856] Trial 324 finished with value: 0.6689769482018436 and parameters: {'n_estimators': 861}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:03:11,689] Trial 325 finished with value: 0.6689712626596342 and parameters: {'n_estimators': 858}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:04:49,964] Trial 326 finished with value: 0.6689525384182465 and parameters: {'n_estimators': 866}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:06:28,582] Trial 327 finished with value: 0.668988407664421 and parameters: {'n_estimators': 869}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:08:01,319] Trial 328 finished with value: 0.6689761788276986 and parameters: {'n_estimators': 870}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:09:39,893] Trial 329 finished with value: 0.6689761788276986 and parameters: {'n_estimators': 870}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:11:15,729] Trial 330 finished with value: 0.6690098600385386 and parameters: {'n_estimators': 872}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:12:54,657] Trial 331 finished with value: 0.6690098600385386 and parameters: {'n_estimators': 872}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:14:33,552] Trial 332 finished with value: 0.6689761788276986 and parameters: {'n_estimators': 870}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:16:14,063] Trial 333 finished with value: 0.6690032537197249 and parameters: {'n_estimators': 875}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:17:54,892] Trial 334 finished with value: 0.668991502068955 and parameters: {'n_estimators': 876}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:19:35,978] Trial 335 finished with value: 0.6689707016154584 and parameters: {'n_estimators': 880}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:21:12,790] Trial 336 finished with value: 0.6689915020689551 and parameters: {'n_estimators': 876}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:22:54,262] Trial 337 finished with value: 0.6690032537197249 and parameters: {'n_estimators': 875}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:24:34,845] Trial 338 finished with value: 0.6689844842665083 and parameters: {'n_estimators': 877}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:26:15,174] Trial 339 finished with value: 0.6689857401324191 and parameters: {'n_estimators': 899}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:27:59,235] Trial 340 finished with value: 0.6689857401324192 and parameters: {'n_estimators': 899}. Best is trial 301 with value: 0.6690134433314722.\n",
      "[I 2023-12-06 17:29:43,791] Trial 341 finished with value: 0.669013584883867 and parameters: {'n_estimators': 903}. Best is trial 341 with value: 0.669013584883867.\n",
      "[I 2023-12-06 17:31:26,702] Trial 342 finished with value: 0.6689786719735576 and parameters: {'n_estimators': 900}. Best is trial 341 with value: 0.669013584883867.\n",
      "[I 2023-12-06 17:33:09,702] Trial 343 finished with value: 0.6689844383527632 and parameters: {'n_estimators': 896}. Best is trial 341 with value: 0.669013584883867.\n",
      "[I 2023-12-06 17:34:52,843] Trial 344 finished with value: 0.669066550691477 and parameters: {'n_estimators': 906}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:36:32,472] Trial 345 finished with value: 0.6689824003046245 and parameters: {'n_estimators': 902}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:38:17,533] Trial 346 finished with value: 0.6690418097225151 and parameters: {'n_estimators': 905}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:39:59,621] Trial 347 finished with value: 0.6690092883428423 and parameters: {'n_estimators': 914}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:41:45,293] Trial 348 finished with value: 0.6690631355514689 and parameters: {'n_estimators': 922}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:43:29,646] Trial 349 finished with value: 0.6690646562637269 and parameters: {'n_estimators': 923}. Best is trial 344 with value: 0.669066550691477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6691\n",
      "\tBest params:\n",
      "\t\tn_estimators: 906\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.668049    0.674609    0.679188    0.685368   \n",
      "1                    TP  410.000000  406.000000  411.000000  399.000000   \n",
      "2                    TN  341.000000  345.000000  342.000000  338.000000   \n",
      "3                    FP   89.000000   79.000000   86.000000   97.000000   \n",
      "4                    FN   59.000000   69.000000   60.000000   65.000000   \n",
      "5              Accuracy    0.835373    0.835373    0.837597    0.819800   \n",
      "6             Precision    0.821643    0.837113    0.826962    0.804435   \n",
      "7           Sensitivity    0.874200    0.854737    0.872611    0.859914   \n",
      "8           Specificity    0.793000    0.813700    0.799100    0.777000   \n",
      "9              F1 score    0.847107    0.845833    0.849174    0.831250   \n",
      "10  F1 score (weighted)    0.834948    0.835248    0.837235    0.819363   \n",
      "11     F1 score (macro)    0.834397    0.834611    0.836635    0.818966   \n",
      "12    Balanced Accuracy    0.833612    0.834208    0.835838    0.818463   \n",
      "13                  MCC    0.670675    0.669431    0.674686    0.640028   \n",
      "14                  NPV    0.852500    0.833300    0.850700    0.838700   \n",
      "15              ROC_AUC    0.833612    0.834208    0.835838    0.818463   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.668521    0.695704    0.642760  \n",
      "1   411.000000  399.000000  395.000000  \n",
      "2   351.000000  351.000000  341.000000  \n",
      "3    78.000000   80.000000   86.000000  \n",
      "4    59.000000   69.000000   77.000000  \n",
      "5     0.847608    0.834260    0.818687  \n",
      "6     0.840491    0.832985    0.821206  \n",
      "7     0.874468    0.852564    0.836864  \n",
      "8     0.818200    0.814400    0.798600  \n",
      "9     0.857143    0.842661    0.828961  \n",
      "10    0.847393    0.834152    0.818578  \n",
      "11    0.846927    0.833786    0.818031  \n",
      "12    0.846325    0.833475    0.817730  \n",
      "13    0.694616    0.667824    0.636227  \n",
      "14    0.856100    0.835700    0.815800  \n",
      "15    0.846325    0.833475    0.817730  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_6_cat = np.where((y_pred_rf_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 17:45:27,328] Trial 350 finished with value: 0.664544865292609 and parameters: {'n_estimators': 926}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:47:14,865] Trial 351 finished with value: 0.6645501556492752 and parameters: {'n_estimators': 917}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:49:01,264] Trial 352 finished with value: 0.6645142523903476 and parameters: {'n_estimators': 911}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:50:50,378] Trial 353 finished with value: 0.6645507505329326 and parameters: {'n_estimators': 933}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:52:34,496] Trial 354 finished with value: 0.6645498758081366 and parameters: {'n_estimators': 891}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:54:18,511] Trial 355 finished with value: 0.6645530541973136 and parameters: {'n_estimators': 886}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:56:00,338] Trial 356 finished with value: 0.6645611111628681 and parameters: {'n_estimators': 915}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:57:42,446] Trial 357 finished with value: 0.6645628495552549 and parameters: {'n_estimators': 888}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 17:59:32,260] Trial 358 finished with value: 0.6645865055879311 and parameters: {'n_estimators': 936}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:01:17,256] Trial 359 finished with value: 0.6645196725660277 and parameters: {'n_estimators': 905}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:03:01,561] Trial 360 finished with value: 0.6645802515423507 and parameters: {'n_estimators': 889}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:04:42,737] Trial 361 finished with value: 0.6645432952223539 and parameters: {'n_estimators': 918}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:06:26,189] Trial 362 finished with value: 0.6645381326766833 and parameters: {'n_estimators': 880}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:08:11,713] Trial 363 finished with value: 0.6645227664424033 and parameters: {'n_estimators': 906}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:10:02,357] Trial 364 finished with value: 0.6646439351857227 and parameters: {'n_estimators': 945}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:11:43,366] Trial 365 finished with value: 0.6645576933645068 and parameters: {'n_estimators': 887}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:13:27,165] Trial 366 finished with value: 0.6645474496843585 and parameters: {'n_estimators': 924}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:15:10,755] Trial 367 finished with value: 0.6645794108415214 and parameters: {'n_estimators': 901}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:16:52,779] Trial 368 finished with value: 0.6645766885929955 and parameters: {'n_estimators': 876}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:18:30,470] Trial 369 finished with value: 0.6645665120144869 and parameters: {'n_estimators': 878}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:20:22,220] Trial 370 finished with value: 0.6647364429782474 and parameters: {'n_estimators': 962}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:22:05,932] Trial 371 finished with value: 0.6645196725660277 and parameters: {'n_estimators': 905}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:23:51,219] Trial 372 finished with value: 0.6645323433319452 and parameters: {'n_estimators': 927}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:25:28,613] Trial 373 finished with value: 0.6645665120144869 and parameters: {'n_estimators': 878}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:27:01,741] Trial 374 finished with value: 0.6645208658141772 and parameters: {'n_estimators': 838}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:28:45,347] Trial 375 finished with value: 0.6645487431449276 and parameters: {'n_estimators': 896}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:30:22,446] Trial 376 finished with value: 0.6645317152989373 and parameters: {'n_estimators': 837}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:31:59,151] Trial 377 finished with value: 0.6645861607959499 and parameters: {'n_estimators': 875}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:33:47,416] Trial 378 finished with value: 0.6645687485953935 and parameters: {'n_estimators': 934}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:35:32,492] Trial 379 finished with value: 0.66450602783753 and parameters: {'n_estimators': 909}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:36:10,545] Trial 380 finished with value: 0.6642468232263989 and parameters: {'n_estimators': 327}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:37:48,333] Trial 381 finished with value: 0.664531570895512 and parameters: {'n_estimators': 892}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:39:37,038] Trial 382 finished with value: 0.6646599695075246 and parameters: {'n_estimators': 947}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:41:18,567] Trial 383 finished with value: 0.6645861607959498 and parameters: {'n_estimators': 875}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:43:05,018] Trial 384 finished with value: 0.6645432952223539 and parameters: {'n_estimators': 918}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:44:40,806] Trial 385 finished with value: 0.6645301860045205 and parameters: {'n_estimators': 839}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:46:18,741] Trial 386 finished with value: 0.6645161286239041 and parameters: {'n_estimators': 894}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:47:55,282] Trial 387 finished with value: 0.6646280661931518 and parameters: {'n_estimators': 873}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:49:37,644] Trial 388 finished with value: 0.6645166575870285 and parameters: {'n_estimators': 910}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:51:18,955] Trial 389 finished with value: 0.6645102192909598 and parameters: {'n_estimators': 893}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:52:54,840] Trial 390 finished with value: 0.6645353187996186 and parameters: {'n_estimators': 841}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:54:35,740] Trial 391 finished with value: 0.6645987476194956 and parameters: {'n_estimators': 871}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:56:22,726] Trial 392 finished with value: 0.6645507505329326 and parameters: {'n_estimators': 933}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:58:04,069] Trial 393 finished with value: 0.6645538672722352 and parameters: {'n_estimators': 885}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 18:59:39,736] Trial 394 finished with value: 0.6645183774847376 and parameters: {'n_estimators': 835}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:01:25,187] Trial 395 finished with value: 0.66450602783753 and parameters: {'n_estimators': 909}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:03:06,045] Trial 396 finished with value: 0.66460234280527 and parameters: {'n_estimators': 869}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:04:51,604] Trial 397 finished with value: 0.6646921001853523 and parameters: {'n_estimators': 953}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:06:36,935] Trial 398 finished with value: 0.6645657229828431 and parameters: {'n_estimators': 921}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:08:15,566] Trial 399 finished with value: 0.6645498758081366 and parameters: {'n_estimators': 891}. Best is trial 344 with value: 0.669066550691477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6691\n",
      "\tBest params:\n",
      "\t\tn_estimators: 906\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.668049    0.674609    0.679188    0.685368   \n",
      "1                    TP  410.000000  406.000000  411.000000  399.000000   \n",
      "2                    TN  341.000000  345.000000  342.000000  338.000000   \n",
      "3                    FP   89.000000   79.000000   86.000000   97.000000   \n",
      "4                    FN   59.000000   69.000000   60.000000   65.000000   \n",
      "5              Accuracy    0.835373    0.835373    0.837597    0.819800   \n",
      "6             Precision    0.821643    0.837113    0.826962    0.804435   \n",
      "7           Sensitivity    0.874200    0.854737    0.872611    0.859914   \n",
      "8           Specificity    0.793000    0.813700    0.799100    0.777000   \n",
      "9              F1 score    0.847107    0.845833    0.849174    0.831250   \n",
      "10  F1 score (weighted)    0.834948    0.835248    0.837235    0.819363   \n",
      "11     F1 score (macro)    0.834397    0.834611    0.836635    0.818966   \n",
      "12    Balanced Accuracy    0.833612    0.834208    0.835838    0.818463   \n",
      "13                  MCC    0.670675    0.669431    0.674686    0.640028   \n",
      "14                  NPV    0.852500    0.833300    0.850700    0.838700   \n",
      "15              ROC_AUC    0.833612    0.834208    0.835838    0.818463   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.668521    0.695704    0.642760    0.641451  \n",
      "1   411.000000  399.000000  395.000000  405.000000  \n",
      "2   351.000000  351.000000  341.000000  324.000000  \n",
      "3    78.000000   80.000000   86.000000   94.000000  \n",
      "4    59.000000   69.000000   77.000000   76.000000  \n",
      "5     0.847608    0.834260    0.818687    0.810901  \n",
      "6     0.840491    0.832985    0.821206    0.811623  \n",
      "7     0.874468    0.852564    0.836864    0.841996  \n",
      "8     0.818200    0.814400    0.798600    0.775100  \n",
      "9     0.857143    0.842661    0.828961    0.826531  \n",
      "10    0.847393    0.834152    0.818578    0.810557  \n",
      "11    0.846927    0.833786    0.818031    0.809353  \n",
      "12    0.846325    0.833475    0.817730    0.808558  \n",
      "13    0.694616    0.667824    0.636227    0.619365  \n",
      "14    0.856100    0.835700    0.815800    0.810000  \n",
      "15    0.846325    0.833475    0.817730    0.808558  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_7_cat = np.where((y_pred_rf_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 19:10:05,442] Trial 400 finished with value: 0.6656976866745656 and parameters: {'n_estimators': 833}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:11:48,708] Trial 401 finished with value: 0.6655050777696655 and parameters: {'n_estimators': 876}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:13:34,890] Trial 402 finished with value: 0.6653927844181982 and parameters: {'n_estimators': 902}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:15:16,474] Trial 403 finished with value: 0.6655368416799232 and parameters: {'n_estimators': 863}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:17:02,903] Trial 404 finished with value: 0.6652765816101363 and parameters: {'n_estimators': 930}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:18:39,833] Trial 405 finished with value: 0.6656230913420123 and parameters: {'n_estimators': 842}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:20:23,817] Trial 406 finished with value: 0.6654891829510656 and parameters: {'n_estimators': 888}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:20:39,933] Trial 407 finished with value: 0.6628180485746452 and parameters: {'n_estimators': 123}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:22:32,909] Trial 408 finished with value: 0.6653016541788744 and parameters: {'n_estimators': 977}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:24:18,340] Trial 409 finished with value: 0.6653161414736359 and parameters: {'n_estimators': 909}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:25:58,361] Trial 410 finished with value: 0.6655404438783752 and parameters: {'n_estimators': 868}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:27:40,140] Trial 411 finished with value: 0.6654787542241669 and parameters: {'n_estimators': 891}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:28:30,500] Trial 412 finished with value: 0.6646706969884122 and parameters: {'n_estimators': 417}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:30:08,535] Trial 413 finished with value: 0.66563129215529 and parameters: {'n_estimators': 844}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:31:57,900] Trial 414 finished with value: 0.6652945230134469 and parameters: {'n_estimators': 919}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:33:41,852] Trial 415 finished with value: 0.6655041471794894 and parameters: {'n_estimators': 870}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:35:34,044] Trial 416 finished with value: 0.6653658374007082 and parameters: {'n_estimators': 945}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:37:18,348] Trial 417 finished with value: 0.6654184714319127 and parameters: {'n_estimators': 896}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:38:59,783] Trial 418 finished with value: 0.6655631580195065 and parameters: {'n_estimators': 861}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:40:39,086] Trial 419 finished with value: 0.6656976866745656 and parameters: {'n_estimators': 833}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:42:24,617] Trial 420 finished with value: 0.6653681344823352 and parameters: {'n_estimators': 906}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:44:10,060] Trial 421 finished with value: 0.6655074807237167 and parameters: {'n_estimators': 885}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:45:50,267] Trial 422 finished with value: 0.6655875365734103 and parameters: {'n_estimators': 850}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:47:38,981] Trial 423 finished with value: 0.6653144909483243 and parameters: {'n_estimators': 934}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:49:22,614] Trial 424 finished with value: 0.6654819592748729 and parameters: {'n_estimators': 871}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:51:09,519] Trial 425 finished with value: 0.6653004292238716 and parameters: {'n_estimators': 918}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:52:55,431] Trial 426 finished with value: 0.6655200314933464 and parameters: {'n_estimators': 887}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:54:34,666] Trial 427 finished with value: 0.6656997176146578 and parameters: {'n_estimators': 832}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:56:17,863] Trial 428 finished with value: 0.6655091316618615 and parameters: {'n_estimators': 866}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:57:57,861] Trial 429 finished with value: 0.6653681344823352 and parameters: {'n_estimators': 906}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 19:59:39,441] Trial 430 finished with value: 0.6656265643875197 and parameters: {'n_estimators': 852}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:01:33,252] Trial 431 finished with value: 0.6653748145188633 and parameters: {'n_estimators': 957}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:03:13,060] Trial 432 finished with value: 0.6655558790713533 and parameters: {'n_estimators': 882}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:05:01,705] Trial 433 finished with value: 0.6652954097864162 and parameters: {'n_estimators': 923}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:06:48,346] Trial 434 finished with value: 0.6653862845932441 and parameters: {'n_estimators': 899}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:08:29,906] Trial 435 finished with value: 0.6655011102909072 and parameters: {'n_estimators': 873}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:10:06,229] Trial 436 finished with value: 0.665606012557509 and parameters: {'n_estimators': 848}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:11:43,826] Trial 437 finished with value: 0.6657040077334309 and parameters: {'n_estimators': 827}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:13:26,657] Trial 438 finished with value: 0.6655558790713533 and parameters: {'n_estimators': 882}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:13:48,753] Trial 439 finished with value: 0.6639752799028205 and parameters: {'n_estimators': 172}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:15:38,034] Trial 440 finished with value: 0.6653624512848919 and parameters: {'n_estimators': 941}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:17:21,518] Trial 441 finished with value: 0.6653632577557642 and parameters: {'n_estimators': 904}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:19:00,486] Trial 442 finished with value: 0.6655368416799232 and parameters: {'n_estimators': 863}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:20:46,265] Trial 443 finished with value: 0.6653077772794176 and parameters: {'n_estimators': 922}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:22:26,881] Trial 444 finished with value: 0.66563129215529 and parameters: {'n_estimators': 844}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:24:13,268] Trial 445 finished with value: 0.6654466762284764 and parameters: {'n_estimators': 894}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:25:55,357] Trial 446 finished with value: 0.6655027069829272 and parameters: {'n_estimators': 864}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:27:39,882] Trial 447 finished with value: 0.6655481960530876 and parameters: {'n_estimators': 881}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:29:26,892] Trial 448 finished with value: 0.6652979774702221 and parameters: {'n_estimators': 910}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:31:04,464] Trial 449 finished with value: 0.66562959699026 and parameters: {'n_estimators': 837}. Best is trial 344 with value: 0.669066550691477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6691\n",
      "\tBest params:\n",
      "\t\tn_estimators: 906\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.668049    0.674609    0.679188    0.685368   \n",
      "1                    TP  410.000000  406.000000  411.000000  399.000000   \n",
      "2                    TN  341.000000  345.000000  342.000000  338.000000   \n",
      "3                    FP   89.000000   79.000000   86.000000   97.000000   \n",
      "4                    FN   59.000000   69.000000   60.000000   65.000000   \n",
      "5              Accuracy    0.835373    0.835373    0.837597    0.819800   \n",
      "6             Precision    0.821643    0.837113    0.826962    0.804435   \n",
      "7           Sensitivity    0.874200    0.854737    0.872611    0.859914   \n",
      "8           Specificity    0.793000    0.813700    0.799100    0.777000   \n",
      "9              F1 score    0.847107    0.845833    0.849174    0.831250   \n",
      "10  F1 score (weighted)    0.834948    0.835248    0.837235    0.819363   \n",
      "11     F1 score (macro)    0.834397    0.834611    0.836635    0.818966   \n",
      "12    Balanced Accuracy    0.833612    0.834208    0.835838    0.818463   \n",
      "13                  MCC    0.670675    0.669431    0.674686    0.640028   \n",
      "14                  NPV    0.852500    0.833300    0.850700    0.838700   \n",
      "15              ROC_AUC    0.833612    0.834208    0.835838    0.818463   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.668521    0.695704    0.642760    0.641451    0.680583  \n",
      "1   411.000000  399.000000  395.000000  405.000000  415.000000  \n",
      "2   351.000000  351.000000  341.000000  324.000000  350.000000  \n",
      "3    78.000000   80.000000   86.000000   94.000000   66.000000  \n",
      "4    59.000000   69.000000   77.000000   76.000000   68.000000  \n",
      "5     0.847608    0.834260    0.818687    0.810901    0.850945  \n",
      "6     0.840491    0.832985    0.821206    0.811623    0.862786  \n",
      "7     0.874468    0.852564    0.836864    0.841996    0.859213  \n",
      "8     0.818200    0.814400    0.798600    0.775100    0.841300  \n",
      "9     0.857143    0.842661    0.828961    0.826531    0.860996  \n",
      "10    0.847393    0.834152    0.818578    0.810557    0.850970  \n",
      "11    0.846927    0.833786    0.818031    0.809353    0.850162  \n",
      "12    0.846325    0.833475    0.817730    0.808558    0.850280  \n",
      "13    0.694616    0.667824    0.636227    0.619365    0.700333  \n",
      "14    0.856100    0.835700    0.815800    0.810000    0.837300  \n",
      "15    0.846325    0.833475    0.817730    0.808558    0.850280  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_8_cat = np.where((y_pred_rf_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 20:33:04,700] Trial 450 finished with value: 0.650252000680244 and parameters: {'n_estimators': 933}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:34:44,833] Trial 451 finished with value: 0.6503618913992169 and parameters: {'n_estimators': 857}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:36:27,342] Trial 452 finished with value: 0.650346504808007 and parameters: {'n_estimators': 888}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:38:09,362] Trial 453 finished with value: 0.6503514602024534 and parameters: {'n_estimators': 873}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:40:02,031] Trial 454 finished with value: 0.6502890334032394 and parameters: {'n_estimators': 966}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:41:38,126] Trial 455 finished with value: 0.6504121400637572 and parameters: {'n_estimators': 824}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:43:22,436] Trial 456 finished with value: 0.6503498778444857 and parameters: {'n_estimators': 908}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:45:06,984] Trial 457 finished with value: 0.6503266960680049 and parameters: {'n_estimators': 894}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:46:45,393] Trial 458 finished with value: 0.6502747343054874 and parameters: {'n_estimators': 852}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:48:26,675] Trial 459 finished with value: 0.6503221005482009 and parameters: {'n_estimators': 866}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:50:10,278] Trial 460 finished with value: 0.650233258269079 and parameters: {'n_estimators': 921}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:51:52,128] Trial 461 finished with value: 0.6503216863044283 and parameters: {'n_estimators': 889}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:53:40,190] Trial 462 finished with value: 0.6502603362809406 and parameters: {'n_estimators': 944}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:55:18,394] Trial 463 finished with value: 0.6503860308577953 and parameters: {'n_estimators': 839}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:57:00,678] Trial 464 finished with value: 0.6503714075327028 and parameters: {'n_estimators': 876}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 20:58:46,620] Trial 465 finished with value: 0.6503572049299825 and parameters: {'n_estimators': 905}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:00:26,970] Trial 466 finished with value: 0.650361891399217 and parameters: {'n_estimators': 857}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:02:01,498] Trial 467 finished with value: 0.6504200788027462 and parameters: {'n_estimators': 823}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:03:42,777] Trial 468 finished with value: 0.6503662993216971 and parameters: {'n_estimators': 877}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:05:30,025] Trial 469 finished with value: 0.6502480662702915 and parameters: {'n_estimators': 918}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:07:14,546] Trial 470 finished with value: 0.6503266960680049 and parameters: {'n_estimators': 894}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:08:53,577] Trial 471 finished with value: 0.6502747343054874 and parameters: {'n_estimators': 852}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:10:40,568] Trial 472 finished with value: 0.6502613540224476 and parameters: {'n_estimators': 928}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:12:22,434] Trial 473 finished with value: 0.6503731034791015 and parameters: {'n_estimators': 874}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:13:56,355] Trial 474 finished with value: 0.6504261623741617 and parameters: {'n_estimators': 829}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:15:41,708] Trial 475 finished with value: 0.6503686926264103 and parameters: {'n_estimators': 901}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:17:20,858] Trial 476 finished with value: 0.650390709917952 and parameters: {'n_estimators': 862}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:19:13,983] Trial 477 finished with value: 0.6501918433251971 and parameters: {'n_estimators': 989}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:21:02,753] Trial 478 finished with value: 0.6501768543325248 and parameters: {'n_estimators': 954}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:22:45,768] Trial 479 finished with value: 0.6503462504349183 and parameters: {'n_estimators': 884}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:24:18,754] Trial 480 finished with value: 0.6504002622865365 and parameters: {'n_estimators': 838}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:26:05,536] Trial 481 finished with value: 0.6502963492242848 and parameters: {'n_estimators': 911}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:27:44,980] Trial 482 finished with value: 0.6503005977214338 and parameters: {'n_estimators': 851}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:29:26,940] Trial 483 finished with value: 0.6503302180705548 and parameters: {'n_estimators': 896}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:31:12,928] Trial 484 finished with value: 0.6502482170411767 and parameters: {'n_estimators': 935}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:32:54,166] Trial 485 finished with value: 0.650371407532703 and parameters: {'n_estimators': 876}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:34:30,055] Trial 486 finished with value: 0.6505258953578797 and parameters: {'n_estimators': 820}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:36:08,962] Trial 487 finished with value: 0.6504056145134164 and parameters: {'n_estimators': 864}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:36:43,927] Trial 488 finished with value: 0.6495195497896209 and parameters: {'n_estimators': 286}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:38:28,170] Trial 489 finished with value: 0.6503572049299825 and parameters: {'n_estimators': 905}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:40:07,999] Trial 490 finished with value: 0.6503374601528948 and parameters: {'n_estimators': 885}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:41:52,604] Trial 491 finished with value: 0.650233258269079 and parameters: {'n_estimators': 921}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:43:25,396] Trial 492 finished with value: 0.6503148132181502 and parameters: {'n_estimators': 845}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:45:06,649] Trial 493 finished with value: 0.6503221005482009 and parameters: {'n_estimators': 866}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:46:50,316] Trial 494 finished with value: 0.6503317882063653 and parameters: {'n_estimators': 891}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:48:25,691] Trial 495 finished with value: 0.6505680280912044 and parameters: {'n_estimators': 817}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:50:12,618] Trial 496 finished with value: 0.6502633239999293 and parameters: {'n_estimators': 917}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:51:51,359] Trial 497 finished with value: 0.6503009191296301 and parameters: {'n_estimators': 846}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:53:33,038] Trial 498 finished with value: 0.6503599977327413 and parameters: {'n_estimators': 875}. Best is trial 344 with value: 0.669066550691477.\n",
      "[I 2023-12-06 21:55:17,633] Trial 499 finished with value: 0.6503293216388542 and parameters: {'n_estimators': 899}. Best is trial 344 with value: 0.669066550691477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6691\n",
      "\tBest params:\n",
      "\t\tn_estimators: 906\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.668049    0.674609    0.679188    0.685368   \n",
      "1                    TP  410.000000  406.000000  411.000000  399.000000   \n",
      "2                    TN  341.000000  345.000000  342.000000  338.000000   \n",
      "3                    FP   89.000000   79.000000   86.000000   97.000000   \n",
      "4                    FN   59.000000   69.000000   60.000000   65.000000   \n",
      "5              Accuracy    0.835373    0.835373    0.837597    0.819800   \n",
      "6             Precision    0.821643    0.837113    0.826962    0.804435   \n",
      "7           Sensitivity    0.874200    0.854737    0.872611    0.859914   \n",
      "8           Specificity    0.793000    0.813700    0.799100    0.777000   \n",
      "9              F1 score    0.847107    0.845833    0.849174    0.831250   \n",
      "10  F1 score (weighted)    0.834948    0.835248    0.837235    0.819363   \n",
      "11     F1 score (macro)    0.834397    0.834611    0.836635    0.818966   \n",
      "12    Balanced Accuracy    0.833612    0.834208    0.835838    0.818463   \n",
      "13                  MCC    0.670675    0.669431    0.674686    0.640028   \n",
      "14                  NPV    0.852500    0.833300    0.850700    0.838700   \n",
      "15              ROC_AUC    0.833612    0.834208    0.835838    0.818463   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.668521    0.695704    0.642760    0.641451    0.680583    0.679226  \n",
      "1   411.000000  399.000000  395.000000  405.000000  415.000000  417.000000  \n",
      "2   351.000000  351.000000  341.000000  324.000000  350.000000  340.000000  \n",
      "3    78.000000   80.000000   86.000000   94.000000   66.000000   75.000000  \n",
      "4    59.000000   69.000000   77.000000   76.000000   68.000000   67.000000  \n",
      "5     0.847608    0.834260    0.818687    0.810901    0.850945    0.842047  \n",
      "6     0.840491    0.832985    0.821206    0.811623    0.862786    0.847561  \n",
      "7     0.874468    0.852564    0.836864    0.841996    0.859213    0.861570  \n",
      "8     0.818200    0.814400    0.798600    0.775100    0.841300    0.819300  \n",
      "9     0.857143    0.842661    0.828961    0.826531    0.860996    0.854508  \n",
      "10    0.847393    0.834152    0.818578    0.810557    0.850970    0.841925  \n",
      "11    0.846927    0.833786    0.818031    0.809353    0.850162    0.840879  \n",
      "12    0.846325    0.833475    0.817730    0.808558    0.850280    0.840424  \n",
      "13    0.694616    0.667824    0.636227    0.619365    0.700333    0.681894  \n",
      "14    0.856100    0.835700    0.815800    0.810000    0.837300    0.835400  \n",
      "15    0.846325    0.833475    0.817730    0.808558    0.850280    0.840424  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_9_cat = np.where((y_pred_rf_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6691\n",
      "\tBest params:\n",
      "\t\tn_estimators: 906\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.681958</td>\n",
       "      <td>0.014563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>201.300000</td>\n",
       "      <td>7.717944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>172.300000</td>\n",
       "      <td>6.377913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>6.051630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>34.800000</td>\n",
       "      <td>4.516636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.831701</td>\n",
       "      <td>0.017220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.831510</td>\n",
       "      <td>0.024461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.852611</td>\n",
       "      <td>0.018602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.808820</td>\n",
       "      <td>0.025307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.841746</td>\n",
       "      <td>0.017514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.831563</td>\n",
       "      <td>0.017267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.830893</td>\n",
       "      <td>0.017254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.830723</td>\n",
       "      <td>0.017316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.662486</td>\n",
       "      <td>0.034350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.832010</td>\n",
       "      <td>0.020671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.830723</td>\n",
       "      <td>0.017316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.681958     0.014563\n",
       "1                    TP       201.300000     7.717944\n",
       "2                    TN       172.300000     6.377913\n",
       "3                    FP        40.800000     6.051630\n",
       "4                    FN        34.800000     4.516636\n",
       "5              Accuracy         0.831701     0.017220\n",
       "6             Precision         0.831510     0.024461\n",
       "7           Sensitivity         0.852611     0.018602\n",
       "8           Specificity         0.808820     0.025307\n",
       "9              F1 score         0.841746     0.017514\n",
       "10  F1 score (weighted)         0.831563     0.017267\n",
       "11     F1 score (macro)         0.830893     0.017254\n",
       "12    Balanced Accuracy         0.830723     0.017316\n",
       "13                  MCC         0.662486     0.034350\n",
       "14                  NPV         0.832010     0.020671\n",
       "15              ROC_AUC         0.830723     0.017316"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.668049</td>\n",
       "      <td>0.674609</td>\n",
       "      <td>0.679188</td>\n",
       "      <td>0.685368</td>\n",
       "      <td>0.668521</td>\n",
       "      <td>0.695704</td>\n",
       "      <td>0.642760</td>\n",
       "      <td>0.641451</td>\n",
       "      <td>0.680583</td>\n",
       "      <td>0.679226</td>\n",
       "      <td>0.671546</td>\n",
       "      <td>0.017444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>406.800000</td>\n",
       "      <td>7.315129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>342.300000</td>\n",
       "      <td>8.028422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>9.273618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>66.900000</td>\n",
       "      <td>6.419588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.835373</td>\n",
       "      <td>0.835373</td>\n",
       "      <td>0.837597</td>\n",
       "      <td>0.819800</td>\n",
       "      <td>0.847608</td>\n",
       "      <td>0.834260</td>\n",
       "      <td>0.818687</td>\n",
       "      <td>0.810901</td>\n",
       "      <td>0.850945</td>\n",
       "      <td>0.842047</td>\n",
       "      <td>0.833259</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.821643</td>\n",
       "      <td>0.837113</td>\n",
       "      <td>0.826962</td>\n",
       "      <td>0.804435</td>\n",
       "      <td>0.840491</td>\n",
       "      <td>0.832985</td>\n",
       "      <td>0.821206</td>\n",
       "      <td>0.811623</td>\n",
       "      <td>0.862786</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.830681</td>\n",
       "      <td>0.017307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.874200</td>\n",
       "      <td>0.854737</td>\n",
       "      <td>0.872611</td>\n",
       "      <td>0.859914</td>\n",
       "      <td>0.874468</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.836864</td>\n",
       "      <td>0.841996</td>\n",
       "      <td>0.859213</td>\n",
       "      <td>0.861570</td>\n",
       "      <td>0.858814</td>\n",
       "      <td>0.012916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.813700</td>\n",
       "      <td>0.799100</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.818200</td>\n",
       "      <td>0.814400</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.841300</td>\n",
       "      <td>0.819300</td>\n",
       "      <td>0.804970</td>\n",
       "      <td>0.020435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.847107</td>\n",
       "      <td>0.845833</td>\n",
       "      <td>0.849174</td>\n",
       "      <td>0.831250</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.842661</td>\n",
       "      <td>0.828961</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.860996</td>\n",
       "      <td>0.854508</td>\n",
       "      <td>0.844416</td>\n",
       "      <td>0.012045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.834948</td>\n",
       "      <td>0.835248</td>\n",
       "      <td>0.837235</td>\n",
       "      <td>0.819363</td>\n",
       "      <td>0.847393</td>\n",
       "      <td>0.834152</td>\n",
       "      <td>0.818578</td>\n",
       "      <td>0.810557</td>\n",
       "      <td>0.850970</td>\n",
       "      <td>0.841925</td>\n",
       "      <td>0.833037</td>\n",
       "      <td>0.013062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.834397</td>\n",
       "      <td>0.834611</td>\n",
       "      <td>0.836635</td>\n",
       "      <td>0.818966</td>\n",
       "      <td>0.846927</td>\n",
       "      <td>0.833786</td>\n",
       "      <td>0.818031</td>\n",
       "      <td>0.809353</td>\n",
       "      <td>0.850162</td>\n",
       "      <td>0.840879</td>\n",
       "      <td>0.832375</td>\n",
       "      <td>0.013103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.833612</td>\n",
       "      <td>0.834208</td>\n",
       "      <td>0.835838</td>\n",
       "      <td>0.818463</td>\n",
       "      <td>0.846325</td>\n",
       "      <td>0.833475</td>\n",
       "      <td>0.817730</td>\n",
       "      <td>0.808558</td>\n",
       "      <td>0.850280</td>\n",
       "      <td>0.840424</td>\n",
       "      <td>0.831891</td>\n",
       "      <td>0.013212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.670675</td>\n",
       "      <td>0.669431</td>\n",
       "      <td>0.674686</td>\n",
       "      <td>0.640028</td>\n",
       "      <td>0.694616</td>\n",
       "      <td>0.667824</td>\n",
       "      <td>0.636227</td>\n",
       "      <td>0.619365</td>\n",
       "      <td>0.700333</td>\n",
       "      <td>0.681894</td>\n",
       "      <td>0.665508</td>\n",
       "      <td>0.026026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>0.838700</td>\n",
       "      <td>0.856100</td>\n",
       "      <td>0.835700</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.837300</td>\n",
       "      <td>0.835400</td>\n",
       "      <td>0.836550</td>\n",
       "      <td>0.014870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.833612</td>\n",
       "      <td>0.834208</td>\n",
       "      <td>0.835838</td>\n",
       "      <td>0.818463</td>\n",
       "      <td>0.846325</td>\n",
       "      <td>0.833475</td>\n",
       "      <td>0.817730</td>\n",
       "      <td>0.808558</td>\n",
       "      <td>0.850280</td>\n",
       "      <td>0.840424</td>\n",
       "      <td>0.831891</td>\n",
       "      <td>0.013212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.668049    0.674609    0.679188    0.685368   \n",
       "1                    TP  410.000000  406.000000  411.000000  399.000000   \n",
       "2                    TN  341.000000  345.000000  342.000000  338.000000   \n",
       "3                    FP   89.000000   79.000000   86.000000   97.000000   \n",
       "4                    FN   59.000000   69.000000   60.000000   65.000000   \n",
       "5              Accuracy    0.835373    0.835373    0.837597    0.819800   \n",
       "6             Precision    0.821643    0.837113    0.826962    0.804435   \n",
       "7           Sensitivity    0.874200    0.854737    0.872611    0.859914   \n",
       "8           Specificity    0.793000    0.813700    0.799100    0.777000   \n",
       "9              F1 score    0.847107    0.845833    0.849174    0.831250   \n",
       "10  F1 score (weighted)    0.834948    0.835248    0.837235    0.819363   \n",
       "11     F1 score (macro)    0.834397    0.834611    0.836635    0.818966   \n",
       "12    Balanced Accuracy    0.833612    0.834208    0.835838    0.818463   \n",
       "13                  MCC    0.670675    0.669431    0.674686    0.640028   \n",
       "14                  NPV    0.852500    0.833300    0.850700    0.838700   \n",
       "15              ROC_AUC    0.833612    0.834208    0.835838    0.818463   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.668521    0.695704    0.642760    0.641451    0.680583    0.679226   \n",
       "1   411.000000  399.000000  395.000000  405.000000  415.000000  417.000000   \n",
       "2   351.000000  351.000000  341.000000  324.000000  350.000000  340.000000   \n",
       "3    78.000000   80.000000   86.000000   94.000000   66.000000   75.000000   \n",
       "4    59.000000   69.000000   77.000000   76.000000   68.000000   67.000000   \n",
       "5     0.847608    0.834260    0.818687    0.810901    0.850945    0.842047   \n",
       "6     0.840491    0.832985    0.821206    0.811623    0.862786    0.847561   \n",
       "7     0.874468    0.852564    0.836864    0.841996    0.859213    0.861570   \n",
       "8     0.818200    0.814400    0.798600    0.775100    0.841300    0.819300   \n",
       "9     0.857143    0.842661    0.828961    0.826531    0.860996    0.854508   \n",
       "10    0.847393    0.834152    0.818578    0.810557    0.850970    0.841925   \n",
       "11    0.846927    0.833786    0.818031    0.809353    0.850162    0.840879   \n",
       "12    0.846325    0.833475    0.817730    0.808558    0.850280    0.840424   \n",
       "13    0.694616    0.667824    0.636227    0.619365    0.700333    0.681894   \n",
       "14    0.856100    0.835700    0.815800    0.810000    0.837300    0.835400   \n",
       "15    0.846325    0.833475    0.817730    0.808558    0.850280    0.840424   \n",
       "\n",
       "           ave       std  \n",
       "0     0.671546  0.017444  \n",
       "1   406.800000  7.315129  \n",
       "2   342.300000  8.028422  \n",
       "3    83.000000  9.273618  \n",
       "4    66.900000  6.419588  \n",
       "5     0.833259  0.012987  \n",
       "6     0.830681  0.017307  \n",
       "7     0.858814  0.012916  \n",
       "8     0.804970  0.020435  \n",
       "9     0.844416  0.012045  \n",
       "10    0.833037  0.013062  \n",
       "11    0.832375  0.013103  \n",
       "12    0.831891  0.013212  \n",
       "13    0.665508  0.026026  \n",
       "14    0.836550  0.014870  \n",
       "15    0.831891  0.013212  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_rf0</th>\n",
       "      <th>y_pred_rf1</th>\n",
       "      <th>y_pred_rf2</th>\n",
       "      <th>y_pred_rf3</th>\n",
       "      <th>y_pred_rf4</th>\n",
       "      <th>y_pred_rf_ave</th>\n",
       "      <th>y_pred_rf_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4176702</td>\n",
       "      <td>0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>5.505710</td>\n",
       "      <td>5.550615</td>\n",
       "      <td>5.504201</td>\n",
       "      <td>5.511151</td>\n",
       "      <td>5.507655</td>\n",
       "      <td>5.679889</td>\n",
       "      <td>0.367114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL272401</td>\n",
       "      <td>1</td>\n",
       "      <td>7.26</td>\n",
       "      <td>6.738022</td>\n",
       "      <td>6.743419</td>\n",
       "      <td>6.736271</td>\n",
       "      <td>6.694661</td>\n",
       "      <td>6.803153</td>\n",
       "      <td>6.829254</td>\n",
       "      <td>0.195224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL118</td>\n",
       "      <td>2</td>\n",
       "      <td>5.93</td>\n",
       "      <td>5.752434</td>\n",
       "      <td>5.660371</td>\n",
       "      <td>5.654286</td>\n",
       "      <td>5.810624</td>\n",
       "      <td>5.741369</td>\n",
       "      <td>5.758181</td>\n",
       "      <td>0.093940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3655939</td>\n",
       "      <td>3</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.581242</td>\n",
       "      <td>6.486131</td>\n",
       "      <td>6.529855</td>\n",
       "      <td>6.499316</td>\n",
       "      <td>6.557200</td>\n",
       "      <td>6.488957</td>\n",
       "      <td>0.098847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3621537</td>\n",
       "      <td>4</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.820532</td>\n",
       "      <td>5.754759</td>\n",
       "      <td>5.696358</td>\n",
       "      <td>5.868528</td>\n",
       "      <td>5.837274</td>\n",
       "      <td>5.809575</td>\n",
       "      <td>0.064737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL2047606</td>\n",
       "      <td>4487</td>\n",
       "      <td>5.44</td>\n",
       "      <td>5.658968</td>\n",
       "      <td>5.570938</td>\n",
       "      <td>5.530684</td>\n",
       "      <td>5.561325</td>\n",
       "      <td>5.592130</td>\n",
       "      <td>5.559008</td>\n",
       "      <td>0.066083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL217781</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.48</td>\n",
       "      <td>6.593201</td>\n",
       "      <td>6.675044</td>\n",
       "      <td>6.627025</td>\n",
       "      <td>6.547151</td>\n",
       "      <td>6.658144</td>\n",
       "      <td>6.763427</td>\n",
       "      <td>0.323195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL2105763</td>\n",
       "      <td>4489</td>\n",
       "      <td>9.92</td>\n",
       "      <td>6.836382</td>\n",
       "      <td>6.969534</td>\n",
       "      <td>6.780518</td>\n",
       "      <td>6.903353</td>\n",
       "      <td>6.913074</td>\n",
       "      <td>7.387144</td>\n",
       "      <td>1.134305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL3415969</td>\n",
       "      <td>4490</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.930866</td>\n",
       "      <td>5.934840</td>\n",
       "      <td>6.453162</td>\n",
       "      <td>5.912963</td>\n",
       "      <td>5.907799</td>\n",
       "      <td>5.981605</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL467066</td>\n",
       "      <td>4491</td>\n",
       "      <td>7.55</td>\n",
       "      <td>7.204939</td>\n",
       "      <td>7.229489</td>\n",
       "      <td>7.181438</td>\n",
       "      <td>7.167230</td>\n",
       "      <td>7.250315</td>\n",
       "      <td>7.263902</td>\n",
       "      <td>0.130922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_rf0  y_pred_rf1  \\\n",
       "0         CHEMBL4176702            0     6.50    5.505710    5.550615   \n",
       "1          CHEMBL272401            1     7.26    6.738022    6.743419   \n",
       "2             CHEMBL118            2     5.93    5.752434    5.660371   \n",
       "3         CHEMBL3655939            3     6.28    6.581242    6.486131   \n",
       "4         CHEMBL3621537            4     5.88    5.820532    5.754759   \n",
       "...                 ...          ...      ...         ...         ...   \n",
       "4487      CHEMBL2047606         4487     5.44    5.658968    5.570938   \n",
       "4488       CHEMBL217781         4488     7.48    6.593201    6.675044   \n",
       "4489      CHEMBL2105763         4489     9.92    6.836382    6.969534   \n",
       "4490      CHEMBL3415969         4490     5.75    5.930866    5.934840   \n",
       "4491       CHEMBL467066         4491     7.55    7.204939    7.229489   \n",
       "\n",
       "      y_pred_rf2  y_pred_rf3  y_pred_rf4  y_pred_rf_ave  y_pred_rf_std  \n",
       "0       5.504201    5.511151    5.507655       5.679889       0.367114  \n",
       "1       6.736271    6.694661    6.803153       6.829254       0.195224  \n",
       "2       5.654286    5.810624    5.741369       5.758181       0.093940  \n",
       "3       6.529855    6.499316    6.557200       6.488957       0.098847  \n",
       "4       5.696358    5.868528    5.837274       5.809575       0.064737  \n",
       "...          ...         ...         ...            ...            ...  \n",
       "4487    5.530684    5.561325    5.592130       5.559008       0.066083  \n",
       "4488    6.627025    6.547151    6.658144       6.763427       0.323195  \n",
       "4489    6.780518    6.903353    6.913074       7.387144       1.134305  \n",
       "4490    6.453162    5.912963    5.907799       5.981605       0.220200  \n",
       "4491    7.181438    7.167230    7.250315       7.263902       0.130922  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=1121218, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where((y_pred_optimized_rf >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "rf_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.6807 with a standard deviation of 0.0249\n",
      "RF optimized model r2_score 0.6833 with a standard deviation of 0.0220\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf.joblib']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_reg, \"OUTPUT/rf_reg.joblib\")\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.669431     0.019571\n",
      "1                    TP       196.400000     9.731963\n",
      "2                    TN       170.800000     8.066391\n",
      "3                    FP        42.300000     5.558777\n",
      "4                    FN        39.700000     6.147267\n",
      "5              Accuracy         0.817453     0.022980\n",
      "6             Precision         0.822612     0.024171\n",
      "7           Sensitivity         0.831693     0.026979\n",
      "8           Specificity         0.801480     0.025525\n",
      "9              F1 score         0.827026     0.023660\n",
      "10  F1 score (weighted)         0.817385     0.022985\n",
      "11     F1 score (macro)         0.816685     0.023040\n",
      "12    Balanced Accuracy         0.816592     0.023082\n",
      "13                  MCC         0.633654     0.046163\n",
      "14                  NPV         0.811510     0.027257\n",
      "15              ROC_AUC         0.816592     0.023082\n",
      "CPU times: user 29.2 s, sys: 116 ms, total: 29.3 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=16,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=16,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=16,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 08:09:09,481] A new study created in memory with name: lgbmRegressor\n",
      "[I 2023-12-11 08:09:10,029] Trial 0 finished with value: 0.258581641115595 and parameters: {'n_estimators': 53, 'learning_rate': 0.0376459149114092, 'max_depth': 3, 'max_bin': 182, 'num_leaves': 733}. Best is trial 0 with value: 0.258581641115595.\n",
      "[I 2023-12-11 08:09:14,373] Trial 1 finished with value: 0.678998182328582 and parameters: {'n_estimators': 861, 'learning_rate': 0.08590689256589533, 'max_depth': 8, 'max_bin': 244, 'num_leaves': 333}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:09:15,520] Trial 2 finished with value: 0.6275799188655113 and parameters: {'n_estimators': 182, 'learning_rate': 0.1848083771432217, 'max_depth': 4, 'max_bin': 276, 'num_leaves': 575}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:09:17,670] Trial 3 finished with value: 0.5514356975470378 and parameters: {'n_estimators': 114, 'learning_rate': 0.02199349911976079, 'max_depth': 9, 'max_bin': 219, 'num_leaves': 654}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:09:21,475] Trial 4 finished with value: 0.6611360707919599 and parameters: {'n_estimators': 659, 'learning_rate': 0.06931104474647926, 'max_depth': 5, 'max_bin': 173, 'num_leaves': 589}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:09:25,305] Trial 5 finished with value: 0.6785622988499679 and parameters: {'n_estimators': 871, 'learning_rate': 0.09904487204050982, 'max_depth': 9, 'max_bin': 245, 'num_leaves': 304}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:09:38,557] Trial 6 finished with value: 0.650935133673447 and parameters: {'n_estimators': 883, 'learning_rate': 0.007399998887629662, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 680}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:09:41,343] Trial 7 finished with value: 0.6668483823685649 and parameters: {'n_estimators': 631, 'learning_rate': 0.1237830292976512, 'max_depth': 7, 'max_bin': 300, 'num_leaves': 633}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:09:47,012] Trial 8 finished with value: 0.659602835249155 and parameters: {'n_estimators': 816, 'learning_rate': 0.030622215360590094, 'max_depth': 6, 'max_bin': 281, 'num_leaves': 286}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:09:50,075] Trial 9 finished with value: 0.6758084371487196 and parameters: {'n_estimators': 731, 'learning_rate': 0.1641966915334686, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 362}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:09:53,587] Trial 10 finished with value: 0.6709663418206139 and parameters: {'n_estimators': 378, 'learning_rate': 0.0787052867712046, 'max_depth': 9, 'max_bin': 222, 'num_leaves': 110}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:09:56,736] Trial 11 finished with value: 0.6776217450580827 and parameters: {'n_estimators': 533, 'learning_rate': 0.11420571681164844, 'max_depth': 9, 'max_bin': 244, 'num_leaves': 215}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:00,747] Trial 12 finished with value: 0.6745427809020496 and parameters: {'n_estimators': 883, 'learning_rate': 0.09057281155256001, 'max_depth': 8, 'max_bin': 251, 'num_leaves': 456}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:03,468] Trial 13 finished with value: 0.6708914399006545 and parameters: {'n_estimators': 413, 'learning_rate': 0.13703074815244648, 'max_depth': 7, 'max_bin': 202, 'num_leaves': 466}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:08,565] Trial 14 finished with value: 0.6758371518459507 and parameters: {'n_estimators': 740, 'learning_rate': 0.05936923974014544, 'max_depth': 10, 'max_bin': 247, 'num_leaves': 170}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:12,750] Trial 15 finished with value: 0.6788852692747341 and parameters: {'n_estimators': 540, 'learning_rate': 0.10016812163293086, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 312}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:16,251] Trial 16 finished with value: 0.6685923610431279 and parameters: {'n_estimators': 258, 'learning_rate': 0.058098455804664745, 'max_depth': 12, 'max_bin': 201, 'num_leaves': 39}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:20,141] Trial 17 finished with value: 0.6763781142238058 and parameters: {'n_estimators': 546, 'learning_rate': 0.10439446829676245, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 427}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:22,038] Trial 18 finished with value: 0.6537932774609391 and parameters: {'n_estimators': 293, 'learning_rate': 0.141847688145256, 'max_depth': 5, 'max_bin': 260, 'num_leaves': 329}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:26,488] Trial 19 finished with value: 0.676984734804073 and parameters: {'n_estimators': 476, 'learning_rate': 0.08242198291239314, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 506}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:30,254] Trial 20 finished with value: 0.676575552844122 and parameters: {'n_estimators': 617, 'learning_rate': 0.10112642353997754, 'max_depth': 8, 'max_bin': 228, 'num_leaves': 231}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:34,239] Trial 21 finished with value: 0.677047500578178 and parameters: {'n_estimators': 786, 'learning_rate': 0.09680227982870909, 'max_depth': 10, 'max_bin': 237, 'num_leaves': 308}. Best is trial 1 with value: 0.678998182328582.\n",
      "[I 2023-12-11 08:10:38,285] Trial 22 finished with value: 0.680500083664402 and parameters: {'n_estimators': 804, 'learning_rate': 0.11392902976810214, 'max_depth': 10, 'max_bin': 261, 'num_leaves': 383}. Best is trial 22 with value: 0.680500083664402.\n",
      "[I 2023-12-11 08:10:41,837] Trial 23 finished with value: 0.675600528550698 and parameters: {'n_estimators': 689, 'learning_rate': 0.1213757259490082, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 395}. Best is trial 22 with value: 0.680500083664402.\n",
      "[I 2023-12-11 08:10:46,838] Trial 24 finished with value: 0.6807312114390953 and parameters: {'n_estimators': 801, 'learning_rate': 0.07896648668672877, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 521}. Best is trial 24 with value: 0.6807312114390953.\n",
      "[I 2023-12-11 08:10:51,752] Trial 25 finished with value: 0.6798751157321334 and parameters: {'n_estimators': 810, 'learning_rate': 0.07322742259648536, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 539}. Best is trial 24 with value: 0.6807312114390953.\n",
      "[I 2023-12-11 08:10:58,015] Trial 26 finished with value: 0.6774183540476597 and parameters: {'n_estimators': 785, 'learning_rate': 0.050736587919489255, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 524}. Best is trial 24 with value: 0.6807312114390953.\n",
      "[I 2023-12-11 08:11:03,127] Trial 27 finished with value: 0.6830470590203902 and parameters: {'n_estimators': 738, 'learning_rate': 0.06989099481408675, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 539}. Best is trial 27 with value: 0.6830470590203902.\n",
      "[I 2023-12-11 08:11:08,829] Trial 28 finished with value: 0.6821912978495295 and parameters: {'n_estimators': 731, 'learning_rate': 0.061181591141533806, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 473}. Best is trial 27 with value: 0.6830470590203902.\n",
      "[I 2023-12-11 08:11:17,210] Trial 29 finished with value: 0.6846226453618642 and parameters: {'n_estimators': 707, 'learning_rate': 0.0348258214153746, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 737}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:11:19,761] Trial 30 finished with value: 0.5904192414362481 and parameters: {'n_estimators': 587, 'learning_rate': 0.0458439457991403, 'max_depth': 3, 'max_bin': 288, 'num_leaves': 739}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:11:27,960] Trial 31 finished with value: 0.6820610787596137 and parameters: {'n_estimators': 721, 'learning_rate': 0.03370681592769129, 'max_depth': 12, 'max_bin': 298, 'num_leaves': 680}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:11:36,213] Trial 32 finished with value: 0.6837132447783238 and parameters: {'n_estimators': 721, 'learning_rate': 0.03776816548920936, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 694}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:11:44,309] Trial 33 finished with value: 0.6842765807111009 and parameters: {'n_estimators': 683, 'learning_rate': 0.041065205923100145, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 703}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:11:51,950] Trial 34 finished with value: 0.6810394964378964 and parameters: {'n_estimators': 674, 'learning_rate': 0.03887123940771186, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 709}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:12:00,882] Trial 35 finished with value: 0.6702160441459014 and parameters: {'n_estimators': 583, 'learning_rate': 0.01950501709482796, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 613}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:12:07,484] Trial 36 finished with value: 0.6804914698277931 and parameters: {'n_estimators': 673, 'learning_rate': 0.04695307231460104, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 576}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:12:18,256] Trial 37 finished with value: 0.09372727821424558 and parameters: {'n_estimators': 471, 'learning_rate': 0.0002612700340062571, 'max_depth': 10, 'max_bin': 290, 'num_leaves': 712}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:12:19,976] Trial 38 finished with value: 0.4682540452254197 and parameters: {'n_estimators': 52, 'learning_rate': 0.02207777195629291, 'max_depth': 12, 'max_bin': 298, 'num_leaves': 665}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:12:26,230] Trial 39 finished with value: 0.6807605704899783 and parameters: {'n_estimators': 631, 'learning_rate': 0.06474747783066473, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 745}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:12:35,563] Trial 40 finished with value: 0.684099539714542 and parameters: {'n_estimators': 852, 'learning_rate': 0.04024388252880676, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 607}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:12:44,722] Trial 41 finished with value: 0.6823454072440418 and parameters: {'n_estimators': 827, 'learning_rate': 0.040506535442507215, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 625}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:12:53,388] Trial 42 finished with value: 0.6745248755479973 and parameters: {'n_estimators': 853, 'learning_rate': 0.029050489507485865, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 698}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:13:00,182] Trial 43 finished with value: 0.678346961470933 and parameters: {'n_estimators': 750, 'learning_rate': 0.05116700097518902, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 648}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:13:14,551] Trial 44 finished with value: 0.6695871485300104 and parameters: {'n_estimators': 848, 'learning_rate': 0.01429144795528376, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 586}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:13:23,025] Trial 45 finished with value: 0.6755663605939839 and parameters: {'n_estimators': 765, 'learning_rate': 0.03417515842326865, 'max_depth': 10, 'max_bin': 294, 'num_leaves': 655}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:13:28,607] Trial 46 finished with value: 0.6755205908167796 and parameters: {'n_estimators': 709, 'learning_rate': 0.07137878160764739, 'max_depth': 9, 'max_bin': 283, 'num_leaves': 612}. Best is trial 29 with value: 0.6846226453618642.\n",
      "[I 2023-12-11 08:13:40,980] Trial 47 finished with value: 0.685270435340522 and parameters: {'n_estimators': 895, 'learning_rate': 0.026625727379818196, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 718}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:13:49,665] Trial 48 finished with value: 0.6131064526489565 and parameters: {'n_estimators': 898, 'learning_rate': 0.01066263121814116, 'max_depth': 6, 'max_bin': 254, 'num_leaves': 747}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:14:01,651] Trial 49 finished with value: 0.6820251002641065 and parameters: {'n_estimators': 896, 'learning_rate': 0.02256005197924928, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 698}. Best is trial 47 with value: 0.685270435340522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6853\n",
      "\tBest params:\n",
      "\t\tn_estimators: 895\n",
      "\t\tlearning_rate: 0.026625727379818196\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 253\n",
      "\t\tnum_leaves: 718\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.695789\n",
      "1                    TP  401.000000\n",
      "2                    TN  352.000000\n",
      "3                    FP   78.000000\n",
      "4                    FN   68.000000\n",
      "5              Accuracy    0.837597\n",
      "6             Precision    0.837161\n",
      "7           Sensitivity    0.855011\n",
      "8           Specificity    0.818600\n",
      "9              F1 score    0.845992\n",
      "10  F1 score (weighted)    0.837499\n",
      "11     F1 score (macro)    0.837113\n",
      "12    Balanced Accuracy    0.836808\n",
      "13                  MCC    0.674435\n",
      "14                  NPV    0.838100\n",
      "15              ROC_AUC    0.836808\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_0_cat = np.where((y_pred_lgbm_0>= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 08:14:14,440] Trial 50 finished with value: 0.6721625397286283 and parameters: {'n_estimators': 849, 'learning_rate': 0.029127260031190042, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 718}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:14:23,443] Trial 51 finished with value: 0.6762718927006469 and parameters: {'n_estimators': 645, 'learning_rate': 0.04312214427637836, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 674}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:14:31,224] Trial 52 finished with value: 0.672897685450248 and parameters: {'n_estimators': 770, 'learning_rate': 0.053308804559671255, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 565}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:14:40,478] Trial 53 finished with value: 0.6736647953198269 and parameters: {'n_estimators': 706, 'learning_rate': 0.03768260525287426, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 642}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:14:47,311] Trial 54 finished with value: 0.6713185493110391 and parameters: {'n_estimators': 605, 'learning_rate': 0.0589548198045895, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 681}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:14:57,748] Trial 55 finished with value: 0.6721977970139297 and parameters: {'n_estimators': 837, 'learning_rate': 0.025936497140519956, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 600}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:15:10,816] Trial 56 finished with value: 0.6636704708317563 and parameters: {'n_estimators': 869, 'learning_rate': 0.01571231072953285, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 725}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:15:19,433] Trial 57 finished with value: 0.6655307853588494 and parameters: {'n_estimators': 754, 'learning_rate': 0.03422800209500072, 'max_depth': 10, 'max_bin': 279, 'num_leaves': 622}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:15:22,360] Trial 58 finished with value: 0.6290592506391455 and parameters: {'n_estimators': 117, 'learning_rate': 0.04654105163017752, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 686}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:15:28,750] Trial 59 finished with value: 0.6672160426572894 and parameters: {'n_estimators': 691, 'learning_rate': 0.06781575141124636, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 553}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:15:34,930] Trial 60 finished with value: 0.6641861798590438 and parameters: {'n_estimators': 654, 'learning_rate': 0.05271067459707572, 'max_depth': 7, 'max_bin': 213, 'num_leaves': 642}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:15:43,727] Trial 61 finished with value: 0.675203302756915 and parameters: {'n_estimators': 829, 'learning_rate': 0.0401849734285058, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 630}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:15:53,025] Trial 62 finished with value: 0.6755317384853613 and parameters: {'n_estimators': 815, 'learning_rate': 0.04013668568067657, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 661}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:16:04,866] Trial 63 finished with value: 0.6735125448223249 and parameters: {'n_estimators': 785, 'learning_rate': 0.027350390805579522, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 728}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:16:11,588] Trial 64 finished with value: 0.6695850020773648 and parameters: {'n_estimators': 734, 'learning_rate': 0.05576581902813694, 'max_depth': 11, 'max_bin': 192, 'num_leaves': 600}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:16:21,583] Trial 65 finished with value: 0.6744766584935138 and parameters: {'n_estimators': 870, 'learning_rate': 0.04284746028820273, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 704}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:16:32,697] Trial 66 finished with value: 0.674985781980616 and parameters: {'n_estimators': 813, 'learning_rate': 0.0338159164626563, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 747}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:16:38,712] Trial 67 finished with value: 0.6649165636755913 and parameters: {'n_estimators': 782, 'learning_rate': 0.064242878621992, 'max_depth': 10, 'max_bin': 257, 'num_leaves': 496}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:16:51,961] Trial 68 finished with value: 0.6675662253339334 and parameters: {'n_estimators': 871, 'learning_rate': 0.018640323096148645, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 627}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:16:54,556] Trial 69 finished with value: 0.5040905001917592 and parameters: {'n_estimators': 506, 'learning_rate': 0.02476125894233136, 'max_depth': 3, 'max_bin': 294, 'num_leaves': 552}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:17:00,624] Trial 70 finished with value: 0.6662739394007677 and parameters: {'n_estimators': 351, 'learning_rate': 0.048469701599633014, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 264}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:17:08,457] Trial 71 finished with value: 0.674743509800634 and parameters: {'n_estimators': 731, 'learning_rate': 0.06066588895249307, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 455}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:17:15,750] Trial 72 finished with value: 0.6698369852630547 and parameters: {'n_estimators': 679, 'learning_rate': 0.0437773137145689, 'max_depth': 12, 'max_bin': 299, 'num_leaves': 488}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:17:21,794] Trial 73 finished with value: 0.6684015308225267 and parameters: {'n_estimators': 707, 'learning_rate': 0.07510466695511896, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 532}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:17:31,655] Trial 74 finished with value: 0.6746926865319045 and parameters: {'n_estimators': 754, 'learning_rate': 0.03161670004892442, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 416}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:17:40,318] Trial 75 finished with value: 0.6717216158446929 and parameters: {'n_estimators': 837, 'learning_rate': 0.03732034513465771, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 463}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:17:49,101] Trial 76 finished with value: 0.6714889707211281 and parameters: {'n_estimators': 790, 'learning_rate': 0.05286259261304875, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 662}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:17:52,363] Trial 77 finished with value: 0.6433290824547914 and parameters: {'n_estimators': 561, 'learning_rate': 0.08693144260670613, 'max_depth': 4, 'max_bin': 277, 'num_leaves': 589}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:17:58,995] Trial 78 finished with value: 0.6688505727203395 and parameters: {'n_estimators': 726, 'learning_rate': 0.05724502556209674, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 687}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:18:07,244] Trial 79 finished with value: 0.6716901775444719 and parameters: {'n_estimators': 825, 'learning_rate': 0.04781027639580996, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 509}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:18:12,675] Trial 80 finished with value: 0.6655256453593378 and parameters: {'n_estimators': 634, 'learning_rate': 0.06639500632246667, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 572}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:18:23,156] Trial 81 finished with value: 0.6748643754524359 and parameters: {'n_estimators': 707, 'learning_rate': 0.031559272167199844, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 718}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:18:32,343] Trial 82 finished with value: 0.6740208859246988 and parameters: {'n_estimators': 762, 'learning_rate': 0.03694818534766967, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 688}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:18:42,403] Trial 83 finished with value: 0.6648757583984991 and parameters: {'n_estimators': 662, 'learning_rate': 0.024442592505035634, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 667}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:18:51,153] Trial 84 finished with value: 0.6727305121046536 and parameters: {'n_estimators': 719, 'learning_rate': 0.040446055732228224, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 703}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:19:02,293] Trial 85 finished with value: 0.67574883759584 and parameters: {'n_estimators': 889, 'learning_rate': 0.028965851532930846, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 647}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:19:14,037] Trial 86 finished with value: 0.6653542278392317 and parameters: {'n_estimators': 801, 'learning_rate': 0.0178305504723097, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 728}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:19:21,754] Trial 87 finished with value: 0.6734736737390181 and parameters: {'n_estimators': 616, 'learning_rate': 0.04564759021598835, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 611}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:19:31,427] Trial 88 finished with value: 0.6715989014047723 and parameters: {'n_estimators': 686, 'learning_rate': 0.03517790085640853, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 356}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:19:39,666] Trial 89 finished with value: 0.6724962788534865 and parameters: {'n_estimators': 743, 'learning_rate': 0.04965137436532896, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 674}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:19:52,695] Trial 90 finished with value: 0.657894036810481 and parameters: {'n_estimators': 850, 'learning_rate': 0.011829917411907271, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 553}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:20:03,670] Trial 91 finished with value: 0.6740768387147635 and parameters: {'n_estimators': 890, 'learning_rate': 0.025419683832768804, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 708}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:20:16,308] Trial 92 finished with value: 0.676390289417247 and parameters: {'n_estimators': 888, 'learning_rate': 0.020758159711419914, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 732}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:20:25,876] Trial 93 finished with value: 0.6747262882504595 and parameters: {'n_estimators': 860, 'learning_rate': 0.030279725722467933, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 688}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:20:35,255] Trial 94 finished with value: 0.6042953999614847 and parameters: {'n_estimators': 774, 'learning_rate': 0.007607947740016038, 'max_depth': 8, 'max_bin': 236, 'num_leaves': 750}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:20:42,823] Trial 95 finished with value: 0.6728397647172439 and parameters: {'n_estimators': 803, 'learning_rate': 0.0420266484677672, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 636}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:20:52,591] Trial 96 finished with value: 0.6633105176896887 and parameters: {'n_estimators': 834, 'learning_rate': 0.022210606899603358, 'max_depth': 9, 'max_bin': 279, 'num_leaves': 700}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:21:01,194] Trial 97 finished with value: 0.6736759355598332 and parameters: {'n_estimators': 869, 'learning_rate': 0.03578293400235629, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 111}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:21:07,823] Trial 98 finished with value: 0.6717336528800003 and parameters: {'n_estimators': 669, 'learning_rate': 0.06150170229177876, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 655}. Best is trial 47 with value: 0.685270435340522.\n",
      "[I 2023-12-11 08:21:15,202] Trial 99 finished with value: 0.6730553558018019 and parameters: {'n_estimators': 898, 'learning_rate': 0.05400478326406105, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 607}. Best is trial 47 with value: 0.685270435340522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6853\n",
      "\tBest params:\n",
      "\t\tn_estimators: 895\n",
      "\t\tlearning_rate: 0.026625727379818196\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 253\n",
      "\t\tnum_leaves: 718\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.695789    0.706151\n",
      "1                    TP  401.000000  393.000000\n",
      "2                    TN  352.000000  345.000000\n",
      "3                    FP   78.000000   79.000000\n",
      "4                    FN   68.000000   82.000000\n",
      "5              Accuracy    0.837597    0.820912\n",
      "6             Precision    0.837161    0.832627\n",
      "7           Sensitivity    0.855011    0.827368\n",
      "8           Specificity    0.818600    0.813700\n",
      "9              F1 score    0.845992    0.829989\n",
      "10  F1 score (weighted)    0.837499    0.820944\n",
      "11     F1 score (macro)    0.837113    0.820400\n",
      "12    Balanced Accuracy    0.836808    0.820524\n",
      "13                  MCC    0.674435    0.640819\n",
      "14                  NPV    0.838100    0.808000\n",
      "15              ROC_AUC    0.836808    0.820524\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_1_cat = np.where((y_pred_lgbm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 08:21:25,750] Trial 100 finished with value: 0.6872659012355518 and parameters: {'n_estimators': 742, 'learning_rate': 0.04337819207210719, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 678}. Best is trial 100 with value: 0.6872659012355518.\n",
      "[I 2023-12-11 08:21:35,018] Trial 101 finished with value: 0.687396873430845 and parameters: {'n_estimators': 694, 'learning_rate': 0.03980544927022812, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 672}. Best is trial 101 with value: 0.687396873430845.\n",
      "[I 2023-12-11 08:21:44,072] Trial 102 finished with value: 0.6875906748834122 and parameters: {'n_estimators': 692, 'learning_rate': 0.040057039751297284, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 672}. Best is trial 102 with value: 0.6875906748834122.\n",
      "[I 2023-12-11 08:21:52,849] Trial 103 finished with value: 0.6886170785308964 and parameters: {'n_estimators': 695, 'learning_rate': 0.04408566912417059, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 626}. Best is trial 103 with value: 0.6886170785308964.\n",
      "[I 2023-12-11 08:22:01,065] Trial 104 finished with value: 0.6888686950083264 and parameters: {'n_estimators': 692, 'learning_rate': 0.044654668359762624, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 626}. Best is trial 104 with value: 0.6888686950083264.\n",
      "[I 2023-12-11 08:22:10,219] Trial 105 finished with value: 0.6865445979295871 and parameters: {'n_estimators': 648, 'learning_rate': 0.0441681597645675, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 671}. Best is trial 104 with value: 0.6888686950083264.\n",
      "[I 2023-12-11 08:22:18,352] Trial 106 finished with value: 0.688243847375428 and parameters: {'n_estimators': 595, 'learning_rate': 0.04542376046821604, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 669}. Best is trial 104 with value: 0.6888686950083264.\n",
      "[I 2023-12-11 08:22:26,137] Trial 107 finished with value: 0.6857036104688229 and parameters: {'n_estimators': 591, 'learning_rate': 0.045369977404636275, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 670}. Best is trial 104 with value: 0.6888686950083264.\n",
      "[I 2023-12-11 08:22:35,012] Trial 108 finished with value: 0.6888876533533985 and parameters: {'n_estimators': 586, 'learning_rate': 0.044286297717453134, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 671}. Best is trial 108 with value: 0.6888876533533985.\n",
      "[I 2023-12-11 08:22:39,744] Trial 109 finished with value: 0.6680770554472535 and parameters: {'n_estimators': 580, 'learning_rate': 0.049721378344354944, 'max_depth': 6, 'max_bin': 231, 'num_leaves': 672}. Best is trial 108 with value: 0.6888876533533985.\n",
      "[I 2023-12-11 08:22:47,865] Trial 110 finished with value: 0.6870946170386937 and parameters: {'n_estimators': 529, 'learning_rate': 0.04642754082346633, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 644}. Best is trial 108 with value: 0.6888876533533985.\n",
      "[I 2023-12-11 08:22:55,786] Trial 111 finished with value: 0.6846820925855276 and parameters: {'n_estimators': 523, 'learning_rate': 0.04380910775120836, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 640}. Best is trial 108 with value: 0.6888876533533985.\n",
      "[I 2023-12-11 08:23:03,714] Trial 112 finished with value: 0.6846677218910115 and parameters: {'n_estimators': 515, 'learning_rate': 0.047276934564587245, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 651}. Best is trial 108 with value: 0.6888876533533985.\n",
      "[I 2023-12-11 08:23:11,671] Trial 113 finished with value: 0.686211392517799 and parameters: {'n_estimators': 601, 'learning_rate': 0.043837419468096495, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 630}. Best is trial 108 with value: 0.6888876533533985.\n",
      "[I 2023-12-11 08:23:18,519] Trial 114 finished with value: 0.684497677309919 and parameters: {'n_estimators': 553, 'learning_rate': 0.05623287444008139, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 622}. Best is trial 108 with value: 0.6888876533533985.\n",
      "[I 2023-12-11 08:23:25,697] Trial 115 finished with value: 0.688485008624213 and parameters: {'n_estimators': 589, 'learning_rate': 0.05245071345174445, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 668}. Best is trial 108 with value: 0.6888876533533985.\n",
      "[I 2023-12-11 08:23:34,043] Trial 116 finished with value: 0.6886948073445494 and parameters: {'n_estimators': 601, 'learning_rate': 0.053067615178724305, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 665}. Best is trial 108 with value: 0.6888876533533985.\n",
      "[I 2023-12-11 08:23:42,294] Trial 117 finished with value: 0.691221149930741 and parameters: {'n_estimators': 609, 'learning_rate': 0.05349456089914068, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 590}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:23:49,622] Trial 118 finished with value: 0.6847248395885168 and parameters: {'n_estimators': 626, 'learning_rate': 0.05182636486575668, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 588}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:23:55,898] Trial 119 finished with value: 0.684049819810173 and parameters: {'n_estimators': 490, 'learning_rate': 0.057922681590465486, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 644}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:24:02,755] Trial 120 finished with value: 0.6871010868333693 and parameters: {'n_estimators': 636, 'learning_rate': 0.06261849218142647, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 660}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:24:10,241] Trial 121 finished with value: 0.68819822078566 and parameters: {'n_estimators': 643, 'learning_rate': 0.050715073104066574, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 661}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:24:16,976] Trial 122 finished with value: 0.6876055344711067 and parameters: {'n_estimators': 565, 'learning_rate': 0.054533719930015816, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 654}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:24:23,549] Trial 123 finished with value: 0.6868013623864497 and parameters: {'n_estimators': 579, 'learning_rate': 0.06363434433855543, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 658}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:24:31,313] Trial 124 finished with value: 0.6886428051113922 and parameters: {'n_estimators': 615, 'learning_rate': 0.053827524715175025, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 623}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:24:38,971] Trial 125 finished with value: 0.6880996118483804 and parameters: {'n_estimators': 567, 'learning_rate': 0.05525888247860292, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 600}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:24:45,827] Trial 126 finished with value: 0.684847277542892 and parameters: {'n_estimators': 564, 'learning_rate': 0.05482312322400783, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 587}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:24:52,136] Trial 127 finished with value: 0.6860689379524293 and parameters: {'n_estimators': 606, 'learning_rate': 0.0686226778209372, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 600}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:24:59,190] Trial 128 finished with value: 0.6815418884219155 and parameters: {'n_estimators': 436, 'learning_rate': 0.051177047575591945, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 622}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:25:08,710] Trial 129 finished with value: 0.6877402049229039 and parameters: {'n_estimators': 567, 'learning_rate': 0.05520897135457968, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 618}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:25:15,592] Trial 130 finished with value: 0.6876468402712452 and parameters: {'n_estimators': 568, 'learning_rate': 0.05949783658985267, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 572}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:25:22,252] Trial 131 finished with value: 0.6875744942616271 and parameters: {'n_estimators': 542, 'learning_rate': 0.05888612216824976, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 573}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:25:29,707] Trial 132 finished with value: 0.6834962939027139 and parameters: {'n_estimators': 616, 'learning_rate': 0.05522871308016575, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 631}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:25:37,103] Trial 133 finished with value: 0.6849862331491464 and parameters: {'n_estimators': 580, 'learning_rate': 0.05994653867249615, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 608}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:25:45,460] Trial 134 finished with value: 0.6866037865176919 and parameters: {'n_estimators': 568, 'learning_rate': 0.05030044410403995, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 622}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:25:52,684] Trial 135 finished with value: 0.6868825010755519 and parameters: {'n_estimators': 597, 'learning_rate': 0.06694319296098246, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 598}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:25:59,589] Trial 136 finished with value: 0.6825973268005707 and parameters: {'n_estimators': 545, 'learning_rate': 0.05412139933334033, 'max_depth': 11, 'max_bin': 234, 'num_leaves': 562}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:26:07,745] Trial 137 finished with value: 0.687446731338297 and parameters: {'n_estimators': 660, 'learning_rate': 0.049892278678544226, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 588}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:26:14,199] Trial 138 finished with value: 0.6829458008996392 and parameters: {'n_estimators': 642, 'learning_rate': 0.07450518163590471, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 616}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:26:20,404] Trial 139 finished with value: 0.6856032664171658 and parameters: {'n_estimators': 616, 'learning_rate': 0.062307271479227, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 573}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:26:28,345] Trial 140 finished with value: 0.6861473661549761 and parameters: {'n_estimators': 568, 'learning_rate': 0.05694700598206352, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 637}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:26:35,508] Trial 141 finished with value: 0.6878362927289663 and parameters: {'n_estimators': 543, 'learning_rate': 0.055804280635426536, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 582}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:26:41,062] Trial 142 finished with value: 0.6874964132410759 and parameters: {'n_estimators': 498, 'learning_rate': 0.07098387485269461, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 546}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:26:48,404] Trial 143 finished with value: 0.6856836199731442 and parameters: {'n_estimators': 595, 'learning_rate': 0.05206842797918766, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 653}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:26:56,433] Trial 144 finished with value: 0.684161301510533 and parameters: {'n_estimators': 539, 'learning_rate': 0.047864628487743595, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 598}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:27:02,891] Trial 145 finished with value: 0.686104022989783 and parameters: {'n_estimators': 626, 'learning_rate': 0.05896947778941471, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 686}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:27:10,223] Trial 146 finished with value: 0.6844922928280923 and parameters: {'n_estimators': 474, 'learning_rate': 0.054235752560238146, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 618}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:27:17,292] Trial 147 finished with value: 0.6872982323167948 and parameters: {'n_estimators': 556, 'learning_rate': 0.0642709100697825, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 578}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:27:26,375] Trial 148 finished with value: 0.686395501133548 and parameters: {'n_estimators': 608, 'learning_rate': 0.03792645937336822, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 521}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:27:33,889] Trial 149 finished with value: 0.6869372420151965 and parameters: {'n_estimators': 580, 'learning_rate': 0.04794825263559714, 'max_depth': 11, 'max_bin': 237, 'num_leaves': 640}. Best is trial 117 with value: 0.691221149930741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6912\n",
      "\tBest params:\n",
      "\t\tn_estimators: 609\n",
      "\t\tlearning_rate: 0.05349456089914068\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 222\n",
      "\t\tnum_leaves: 590\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.695789    0.706151    0.690541\n",
      "1                    TP  401.000000  393.000000  396.000000\n",
      "2                    TN  352.000000  345.000000  352.000000\n",
      "3                    FP   78.000000   79.000000   76.000000\n",
      "4                    FN   68.000000   82.000000   75.000000\n",
      "5              Accuracy    0.837597    0.820912    0.832036\n",
      "6             Precision    0.837161    0.832627    0.838983\n",
      "7           Sensitivity    0.855011    0.827368    0.840764\n",
      "8           Specificity    0.818600    0.813700    0.822400\n",
      "9              F1 score    0.845992    0.829989    0.839873\n",
      "10  F1 score (weighted)    0.837499    0.820944    0.832026\n",
      "11     F1 score (macro)    0.837113    0.820400    0.831632\n",
      "12    Balanced Accuracy    0.836808    0.820524    0.831597\n",
      "13                  MCC    0.674435    0.640819    0.663267\n",
      "14                  NPV    0.838100    0.808000    0.824400\n",
      "15              ROC_AUC    0.836808    0.820524    0.831597\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_2_cat = np.where((y_pred_lgbm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 08:27:42,333] Trial 150 finished with value: 0.6698344710730163 and parameters: {'n_estimators': 656, 'learning_rate': 0.05963349511560973, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 695}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:27:49,289] Trial 151 finished with value: 0.6739034074072394 and parameters: {'n_estimators': 539, 'learning_rate': 0.056856292848505754, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 568}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:27:55,802] Trial 152 finished with value: 0.6709750213803043 and parameters: {'n_estimators': 520, 'learning_rate': 0.06677116537389369, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 560}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:28:03,469] Trial 153 finished with value: 0.6711517205588902 and parameters: {'n_estimators': 551, 'learning_rate': 0.051373723857219285, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 579}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:28:10,554] Trial 154 finished with value: 0.6712501527009944 and parameters: {'n_estimators': 587, 'learning_rate': 0.05928530556252532, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 607}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:28:18,084] Trial 155 finished with value: 0.6678839975121058 and parameters: {'n_estimators': 449, 'learning_rate': 0.04091924459882097, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 656}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:28:23,740] Trial 156 finished with value: 0.6576087410723 and parameters: {'n_estimators': 630, 'learning_rate': 0.046994920431586255, 'max_depth': 7, 'max_bin': 255, 'num_leaves': 635}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:28:31,100] Trial 157 finished with value: 0.6728803408644641 and parameters: {'n_estimators': 566, 'learning_rate': 0.05514199056350716, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 596}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:28:38,851] Trial 158 finished with value: 0.6690779597235641 and parameters: {'n_estimators': 596, 'learning_rate': 0.0506036633907923, 'max_depth': 9, 'max_bin': 229, 'num_leaves': 542}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:28:44,919] Trial 159 finished with value: 0.6737606343223745 and parameters: {'n_estimators': 677, 'learning_rate': 0.0788361257127628, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 618}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:28:52,448] Trial 160 finished with value: 0.6730629465851734 and parameters: {'n_estimators': 534, 'learning_rate': 0.06289756791459916, 'max_depth': 11, 'max_bin': 235, 'num_leaves': 664}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:28:58,949] Trial 161 finished with value: 0.6733720093823241 and parameters: {'n_estimators': 504, 'learning_rate': 0.07081063146651559, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 543}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:29:05,516] Trial 162 finished with value: 0.6730562730092577 and parameters: {'n_estimators': 496, 'learning_rate': 0.0712403215601261, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 566}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:29:13,332] Trial 163 finished with value: 0.6719639465036537 and parameters: {'n_estimators': 552, 'learning_rate': 0.058700984836847256, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 649}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:29:21,424] Trial 164 finished with value: 0.6724123149890942 and parameters: {'n_estimators': 576, 'learning_rate': 0.04405599572194265, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 609}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:29:29,081] Trial 165 finished with value: 0.6760590439560411 and parameters: {'n_estimators': 615, 'learning_rate': 0.05374060230099032, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 586}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:29:35,330] Trial 166 finished with value: 0.6724270153965862 and parameters: {'n_estimators': 486, 'learning_rate': 0.06583631507096806, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 629}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:29:43,239] Trial 167 finished with value: 0.6724826258217373 and parameters: {'n_estimators': 520, 'learning_rate': 0.04940680912472883, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 684}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:29:50,554] Trial 168 finished with value: 0.6707468066727019 and parameters: {'n_estimators': 640, 'learning_rate': 0.06261824920906994, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 548}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:29:59,153] Trial 169 finished with value: 0.67145005808354 and parameters: {'n_estimators': 602, 'learning_rate': 0.04020235392073587, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 527}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:30:07,616] Trial 170 finished with value: 0.6710087900360189 and parameters: {'n_estimators': 567, 'learning_rate': 0.04604354598247481, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 580}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:30:15,528] Trial 171 finished with value: 0.6732213897319002 and parameters: {'n_estimators': 666, 'learning_rate': 0.05144802574809966, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 595}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:30:23,472] Trial 172 finished with value: 0.6706686813766737 and parameters: {'n_estimators': 699, 'learning_rate': 0.05788828776039549, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 587}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:30:32,381] Trial 173 finished with value: 0.6767213167784872 and parameters: {'n_estimators': 663, 'learning_rate': 0.0489538271393838, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 629}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:30:40,615] Trial 174 finished with value: 0.6750966604343042 and parameters: {'n_estimators': 651, 'learning_rate': 0.05353064349912772, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 612}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:30:44,604] Trial 175 finished with value: 0.6195491892074076 and parameters: {'n_estimators': 619, 'learning_rate': 0.04445491358250145, 'max_depth': 4, 'max_bin': 238, 'num_leaves': 662}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:30:53,242] Trial 176 finished with value: 0.6705869723985742 and parameters: {'n_estimators': 587, 'learning_rate': 0.03582615238757644, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 645}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:31:00,707] Trial 177 finished with value: 0.6754029709906006 and parameters: {'n_estimators': 687, 'learning_rate': 0.05672982573509701, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 679}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:31:08,340] Trial 178 finished with value: 0.6728059163792842 and parameters: {'n_estimators': 542, 'learning_rate': 0.04906348287473895, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 597}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:31:16,875] Trial 179 finished with value: 0.6718888005006383 and parameters: {'n_estimators': 639, 'learning_rate': 0.042187557571545645, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 567}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:31:24,071] Trial 180 finished with value: 0.6731820894629572 and parameters: {'n_estimators': 561, 'learning_rate': 0.06106792096395329, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 629}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:31:33,358] Trial 181 finished with value: 0.6712301862782165 and parameters: {'n_estimators': 694, 'learning_rate': 0.03773407658807295, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 670}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:31:38,207] Trial 182 finished with value: 0.6609164766570956 and parameters: {'n_estimators': 263, 'learning_rate': 0.052839913136983205, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 706}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:31:46,349] Trial 183 finished with value: 0.6716897937298776 and parameters: {'n_estimators': 673, 'learning_rate': 0.041359281362657326, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 670}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:31:55,059] Trial 184 finished with value: 0.6695129543950751 and parameters: {'n_estimators': 594, 'learning_rate': 0.032300893146097784, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 650}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:32:03,307] Trial 185 finished with value: 0.6745582210733583 and parameters: {'n_estimators': 718, 'learning_rate': 0.04598691506616056, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 686}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:32:07,040] Trial 186 finished with value: 0.6414066636415485 and parameters: {'n_estimators': 180, 'learning_rate': 0.03932124085327564, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 615}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:32:14,453] Trial 187 finished with value: 0.6728771305255501 and parameters: {'n_estimators': 626, 'learning_rate': 0.05519054762927281, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 580}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:32:21,764] Trial 188 finished with value: 0.6707073611123944 and parameters: {'n_estimators': 530, 'learning_rate': 0.04883476537922522, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 439}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:32:27,340] Trial 189 finished with value: 0.6693012959758629 and parameters: {'n_estimators': 651, 'learning_rate': 0.09280774942654742, 'max_depth': 11, 'max_bin': 257, 'num_leaves': 659}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:32:34,595] Trial 190 finished with value: 0.6728832938880001 and parameters: {'n_estimators': 576, 'learning_rate': 0.057423841006703454, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 555}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:32:41,751] Trial 191 finished with value: 0.6743680510919943 and parameters: {'n_estimators': 545, 'learning_rate': 0.0672769813820699, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 577}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:32:49,444] Trial 192 finished with value: 0.6721734735326879 and parameters: {'n_estimators': 559, 'learning_rate': 0.0639842329969456, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 597}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:32:56,616] Trial 193 finished with value: 0.6730598135615015 and parameters: {'n_estimators': 605, 'learning_rate': 0.060862648223614355, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 640}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:33:03,768] Trial 194 finished with value: 0.6693985776196785 and parameters: {'n_estimators': 509, 'learning_rate': 0.051542872770259825, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 570}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:33:10,440] Trial 195 finished with value: 0.6761529947651053 and parameters: {'n_estimators': 554, 'learning_rate': 0.0715076487582362, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 610}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:33:17,728] Trial 196 finished with value: 0.675209603181129 and parameters: {'n_estimators': 588, 'learning_rate': 0.06552413590733977, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 584}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:33:25,910] Trial 197 finished with value: 0.6707151642110842 and parameters: {'n_estimators': 575, 'learning_rate': 0.0474756853245597, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 694}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:33:32,674] Trial 198 finished with value: 0.672970564475762 and parameters: {'n_estimators': 681, 'learning_rate': 0.07457324062059216, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 622}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:33:39,796] Trial 199 finished with value: 0.6704246583524672 and parameters: {'n_estimators': 607, 'learning_rate': 0.05550893336776528, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 667}. Best is trial 117 with value: 0.691221149930741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6912\n",
      "\tBest params:\n",
      "\t\tn_estimators: 609\n",
      "\t\tlearning_rate: 0.05349456089914068\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 222\n",
      "\t\tnum_leaves: 590\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.695789    0.706151    0.690541    0.727531\n",
      "1                    TP  401.000000  393.000000  396.000000  399.000000\n",
      "2                    TN  352.000000  345.000000  352.000000  359.000000\n",
      "3                    FP   78.000000   79.000000   76.000000   76.000000\n",
      "4                    FN   68.000000   82.000000   75.000000   65.000000\n",
      "5              Accuracy    0.837597    0.820912    0.832036    0.843159\n",
      "6             Precision    0.837161    0.832627    0.838983    0.840000\n",
      "7           Sensitivity    0.855011    0.827368    0.840764    0.859914\n",
      "8           Specificity    0.818600    0.813700    0.822400    0.825300\n",
      "9              F1 score    0.845992    0.829989    0.839873    0.849840\n",
      "10  F1 score (weighted)    0.837499    0.820944    0.832026    0.843074\n",
      "11     F1 score (macro)    0.837113    0.820400    0.831632    0.842848\n",
      "12    Balanced Accuracy    0.836808    0.820524    0.831597    0.842601\n",
      "13                  MCC    0.674435    0.640819    0.663267    0.685949\n",
      "14                  NPV    0.838100    0.808000    0.824400    0.846700\n",
      "15              ROC_AUC    0.836808    0.820524    0.831597    0.842601\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_3_cat = np.where((y_pred_lgbm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 08:33:47,454] Trial 200 finished with value: 0.6860809555080868 and parameters: {'n_estimators': 560, 'learning_rate': 0.060369830612370744, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 646}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:33:55,789] Trial 201 finished with value: 0.6891009364784347 and parameters: {'n_estimators': 743, 'learning_rate': 0.043310440163862, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 184}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:34:04,553] Trial 202 finished with value: 0.6877340419234901 and parameters: {'n_estimators': 532, 'learning_rate': 0.04267200934563549, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 309}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:34:13,496] Trial 203 finished with value: 0.6870661706542387 and parameters: {'n_estimators': 716, 'learning_rate': 0.04040011328133441, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 179}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:34:22,171] Trial 204 finished with value: 0.6891377622019257 and parameters: {'n_estimators': 698, 'learning_rate': 0.04568552041761243, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 166}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:34:29,435] Trial 205 finished with value: 0.6872451655438587 and parameters: {'n_estimators': 526, 'learning_rate': 0.0451739816220169, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 156}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:34:36,604] Trial 206 finished with value: 0.6855491428293067 and parameters: {'n_estimators': 700, 'learning_rate': 0.050203209963399636, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 79}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:34:44,700] Trial 207 finished with value: 0.6898714218754776 and parameters: {'n_estimators': 732, 'learning_rate': 0.04385438994495046, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 247}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:34:54,371] Trial 208 finished with value: 0.6883899870708124 and parameters: {'n_estimators': 738, 'learning_rate': 0.035341351575193414, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 275}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:35:03,351] Trial 209 finished with value: 0.6840280285783494 and parameters: {'n_estimators': 751, 'learning_rate': 0.036171778945089536, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 268}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:35:12,638] Trial 210 finished with value: 0.6849611384198516 and parameters: {'n_estimators': 744, 'learning_rate': 0.033096268373643684, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 227}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:35:20,533] Trial 211 finished with value: 0.6839940502570061 and parameters: {'n_estimators': 729, 'learning_rate': 0.04189257892761231, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 193}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:35:29,354] Trial 212 finished with value: 0.6911882162830743 and parameters: {'n_estimators': 726, 'learning_rate': 0.04387111020434927, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 300}. Best is trial 117 with value: 0.691221149930741.\n",
      "[I 2023-12-11 08:35:38,009] Trial 213 finished with value: 0.691700956422012 and parameters: {'n_estimators': 767, 'learning_rate': 0.044691915080746415, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 319}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:35:45,950] Trial 214 finished with value: 0.6891619796354611 and parameters: {'n_estimators': 761, 'learning_rate': 0.04413935156526459, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 324}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:35:54,001] Trial 215 finished with value: 0.6893009401893275 and parameters: {'n_estimators': 764, 'learning_rate': 0.04644859080154764, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 301}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:36:02,786] Trial 216 finished with value: 0.6893475621087767 and parameters: {'n_estimators': 764, 'learning_rate': 0.04301837712192011, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 318}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:36:11,372] Trial 217 finished with value: 0.6873854763032494 and parameters: {'n_estimators': 764, 'learning_rate': 0.04445104397821835, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 332}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:36:20,435] Trial 218 finished with value: 0.6845454066100143 and parameters: {'n_estimators': 774, 'learning_rate': 0.035693265066919375, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 282}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:36:29,576] Trial 219 finished with value: 0.6890414098869597 and parameters: {'n_estimators': 789, 'learning_rate': 0.043567556205826974, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 304}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:36:38,759] Trial 220 finished with value: 0.684027532237584 and parameters: {'n_estimators': 778, 'learning_rate': 0.03821184321511762, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 290}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:36:46,896] Trial 221 finished with value: 0.6886410258951134 and parameters: {'n_estimators': 794, 'learning_rate': 0.04372613180765512, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 309}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:36:54,947] Trial 222 finished with value: 0.6846191772066463 and parameters: {'n_estimators': 756, 'learning_rate': 0.045978733411721345, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 320}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:37:03,010] Trial 223 finished with value: 0.6859577050862791 and parameters: {'n_estimators': 802, 'learning_rate': 0.043834990836536365, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 296}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:37:12,620] Trial 224 finished with value: 0.6851605092103299 and parameters: {'n_estimators': 788, 'learning_rate': 0.03154568702841498, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 352}. Best is trial 213 with value: 0.691700956422012.\n",
      "[I 2023-12-11 08:37:20,731] Trial 225 finished with value: 0.6931625399512271 and parameters: {'n_estimators': 764, 'learning_rate': 0.04742794787391275, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 250}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:37:28,922] Trial 226 finished with value: 0.6853754582908522 and parameters: {'n_estimators': 759, 'learning_rate': 0.0384104200199182, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 255}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:37:36,352] Trial 227 finished with value: 0.6898159552420703 and parameters: {'n_estimators': 734, 'learning_rate': 0.046859514901406926, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 319}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:37:43,700] Trial 228 finished with value: 0.6873592379947244 and parameters: {'n_estimators': 736, 'learning_rate': 0.04563982808855695, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 318}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:37:51,045] Trial 229 finished with value: 0.6864400216472852 and parameters: {'n_estimators': 731, 'learning_rate': 0.04274418583891084, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 251}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:37:58,246] Trial 230 finished with value: 0.6862250348473176 and parameters: {'n_estimators': 792, 'learning_rate': 0.04844618039949138, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 343}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:38:07,139] Trial 231 finished with value: 0.6918696225377341 and parameters: {'n_estimators': 769, 'learning_rate': 0.04781919616650389, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 376}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:38:15,528] Trial 232 finished with value: 0.6903086282918942 and parameters: {'n_estimators': 768, 'learning_rate': 0.04711360830188632, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 300}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:38:24,268] Trial 233 finished with value: 0.6887943990670282 and parameters: {'n_estimators': 761, 'learning_rate': 0.04108871315227432, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 280}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:38:32,857] Trial 234 finished with value: 0.6862342012098102 and parameters: {'n_estimators': 774, 'learning_rate': 0.039906984301898554, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 277}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:38:42,844] Trial 235 finished with value: 0.6856106019717656 and parameters: {'n_estimators': 743, 'learning_rate': 0.035318313355554756, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 372}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:38:52,501] Trial 236 finished with value: 0.6901696147550826 and parameters: {'n_estimators': 764, 'learning_rate': 0.0424444904387612, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 298}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:39:00,948] Trial 237 finished with value: 0.6850825890291915 and parameters: {'n_estimators': 766, 'learning_rate': 0.041897382974925096, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 299}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:39:11,056] Trial 238 finished with value: 0.6883027764524737 and parameters: {'n_estimators': 753, 'learning_rate': 0.03792727084565432, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 311}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:39:18,449] Trial 239 finished with value: 0.6865138591035191 and parameters: {'n_estimators': 784, 'learning_rate': 0.046838580752167076, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 323}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:39:29,226] Trial 240 finished with value: 0.6853661113529912 and parameters: {'n_estimators': 810, 'learning_rate': 0.030056272235351955, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 338}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:39:38,065] Trial 241 finished with value: 0.6856408636557214 and parameters: {'n_estimators': 762, 'learning_rate': 0.03685544513411842, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 310}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:39:46,742] Trial 242 finished with value: 0.687796343724731 and parameters: {'n_estimators': 747, 'learning_rate': 0.04213515759956244, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 278}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:39:55,784] Trial 243 finished with value: 0.6862717640268029 and parameters: {'n_estimators': 725, 'learning_rate': 0.0389578431264458, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 297}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:40:04,213] Trial 244 finished with value: 0.6861393030490167 and parameters: {'n_estimators': 752, 'learning_rate': 0.046108213560267605, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 260}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:40:14,313] Trial 245 finished with value: 0.6850490153851192 and parameters: {'n_estimators': 772, 'learning_rate': 0.034573263004411714, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 147}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:40:23,194] Trial 246 finished with value: 0.6896889687464158 and parameters: {'n_estimators': 791, 'learning_rate': 0.041548719931639465, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 312}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:40:30,821] Trial 247 finished with value: 0.6870457975536419 and parameters: {'n_estimators': 791, 'learning_rate': 0.043244963865092065, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 405}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:40:34,498] Trial 248 finished with value: 0.6769081506591639 and parameters: {'n_estimators': 816, 'learning_rate': 0.10754579261175706, 'max_depth': 6, 'max_bin': 266, 'num_leaves': 295}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:40:41,833] Trial 249 finished with value: 0.6866547790335847 and parameters: {'n_estimators': 710, 'learning_rate': 0.04817717697923922, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 324}. Best is trial 225 with value: 0.6931625399512271.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6932\n",
      "\tBest params:\n",
      "\t\tn_estimators: 764\n",
      "\t\tlearning_rate: 0.04742794787391275\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 266\n",
      "\t\tnum_leaves: 250\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.695789    0.706151    0.690541    0.727531   \n",
      "1                    TP  401.000000  393.000000  396.000000  399.000000   \n",
      "2                    TN  352.000000  345.000000  352.000000  359.000000   \n",
      "3                    FP   78.000000   79.000000   76.000000   76.000000   \n",
      "4                    FN   68.000000   82.000000   75.000000   65.000000   \n",
      "5              Accuracy    0.837597    0.820912    0.832036    0.843159   \n",
      "6             Precision    0.837161    0.832627    0.838983    0.840000   \n",
      "7           Sensitivity    0.855011    0.827368    0.840764    0.859914   \n",
      "8           Specificity    0.818600    0.813700    0.822400    0.825300   \n",
      "9              F1 score    0.845992    0.829989    0.839873    0.849840   \n",
      "10  F1 score (weighted)    0.837499    0.820944    0.832026    0.843074   \n",
      "11     F1 score (macro)    0.837113    0.820400    0.831632    0.842848   \n",
      "12    Balanced Accuracy    0.836808    0.820524    0.831597    0.842601   \n",
      "13                  MCC    0.674435    0.640819    0.663267    0.685949   \n",
      "14                  NPV    0.838100    0.808000    0.824400    0.846700   \n",
      "15              ROC_AUC    0.836808    0.820524    0.831597    0.842601   \n",
      "\n",
      "          Set4  \n",
      "0     0.688238  \n",
      "1   400.000000  \n",
      "2   340.000000  \n",
      "3    89.000000  \n",
      "4    70.000000  \n",
      "5     0.823137  \n",
      "6     0.817996  \n",
      "7     0.851064  \n",
      "8     0.792500  \n",
      "9     0.834202  \n",
      "10    0.822886  \n",
      "11    0.822345  \n",
      "12    0.821802  \n",
      "13    0.645432  \n",
      "14    0.829300  \n",
      "15    0.821802  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_4_cat = np.where((y_pred_lgbm_4 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 08:40:50,724] Trial 250 finished with value: 0.6752738799305706 and parameters: {'n_estimators': 734, 'learning_rate': 0.04339050554221033, 'max_depth': 8, 'max_bin': 263, 'num_leaves': 278}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:41:00,046] Trial 251 finished with value: 0.6872432579869727 and parameters: {'n_estimators': 798, 'learning_rate': 0.05021883726513868, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 235}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:41:09,123] Trial 252 finished with value: 0.6902594060804569 and parameters: {'n_estimators': 784, 'learning_rate': 0.047004682429963224, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 305}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:41:18,391] Trial 253 finished with value: 0.6892009540546838 and parameters: {'n_estimators': 773, 'learning_rate': 0.04592666777337722, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 332}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:41:27,579] Trial 254 finished with value: 0.6861810925492444 and parameters: {'n_estimators': 785, 'learning_rate': 0.04652229532276576, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 339}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:41:37,365] Trial 255 finished with value: 0.6844251126044627 and parameters: {'n_estimators': 770, 'learning_rate': 0.04068038069776986, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 304}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:41:46,321] Trial 256 finished with value: 0.6892299074637623 and parameters: {'n_estimators': 775, 'learning_rate': 0.04661261517546418, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 313}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:41:54,733] Trial 257 finished with value: 0.685179464603609 and parameters: {'n_estimators': 817, 'learning_rate': 0.048354134750394434, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 374}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:42:03,025] Trial 258 finished with value: 0.6876032920020007 and parameters: {'n_estimators': 767, 'learning_rate': 0.04456341299038562, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 332}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:42:12,441] Trial 259 finished with value: 0.6881743674523296 and parameters: {'n_estimators': 795, 'learning_rate': 0.05058225328593572, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 310}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:42:18,082] Trial 260 finished with value: 0.6513885620149654 and parameters: {'n_estimators': 779, 'learning_rate': 0.0409697339262803, 'max_depth': 5, 'max_bin': 275, 'num_leaves': 358}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:42:23,322] Trial 261 finished with value: 0.6828115211953742 and parameters: {'n_estimators': 757, 'learning_rate': 0.12979080936188594, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 202}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:42:33,624] Trial 262 finished with value: 0.6866181617660065 and parameters: {'n_estimators': 801, 'learning_rate': 0.04723120684169859, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 293}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:42:43,559] Trial 263 finished with value: 0.6854756577248857 and parameters: {'n_estimators': 781, 'learning_rate': 0.040057545998137586, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 323}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:42:49,472] Trial 264 finished with value: 0.6832204130427433 and parameters: {'n_estimators': 723, 'learning_rate': 0.08646891141098295, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 346}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:42:57,429] Trial 265 finished with value: 0.6853998291437418 and parameters: {'n_estimators': 747, 'learning_rate': 0.05151097919252221, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 306}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:43:06,907] Trial 266 finished with value: 0.6849096261080054 and parameters: {'n_estimators': 759, 'learning_rate': 0.04491274110613102, 'max_depth': 12, 'max_bin': 194, 'num_leaves': 138}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:43:18,400] Trial 267 finished with value: 0.6873717030651604 and parameters: {'n_estimators': 832, 'learning_rate': 0.02778389343542461, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 289}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:43:27,548] Trial 268 finished with value: 0.6896075005127212 and parameters: {'n_estimators': 778, 'learning_rate': 0.04797451404488095, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 326}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:43:31,675] Trial 269 finished with value: 0.675802376996328 and parameters: {'n_estimators': 765, 'learning_rate': 0.18631844116446855, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 239}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:43:40,622] Trial 270 finished with value: 0.6885838318708414 and parameters: {'n_estimators': 777, 'learning_rate': 0.048430928627849534, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 327}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:43:49,276] Trial 271 finished with value: 0.6879270713519837 and parameters: {'n_estimators': 741, 'learning_rate': 0.04984655398027115, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 388}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:44:04,222] Trial 272 finished with value: 0.6224009480058043 and parameters: {'n_estimators': 730, 'learning_rate': 0.005280107891652341, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 317}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:44:14,375] Trial 273 finished with value: 0.6893232426527807 and parameters: {'n_estimators': 807, 'learning_rate': 0.0394234439890127, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 288}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:44:25,555] Trial 274 finished with value: 0.6849986444594207 and parameters: {'n_estimators': 814, 'learning_rate': 0.03303332540657302, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 291}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:44:36,067] Trial 275 finished with value: 0.6894323650961159 and parameters: {'n_estimators': 784, 'learning_rate': 0.03938641391693753, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 268}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:44:46,691] Trial 276 finished with value: 0.6862764699834591 and parameters: {'n_estimators': 803, 'learning_rate': 0.03630484948362493, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 258}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:44:55,746] Trial 277 finished with value: 0.6834491745863183 and parameters: {'n_estimators': 779, 'learning_rate': 0.03964308083823315, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 276}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:45:05,945] Trial 278 finished with value: 0.6891656662442122 and parameters: {'n_estimators': 759, 'learning_rate': 0.04245543124842549, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 218}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:45:09,571] Trial 279 finished with value: 0.6752502109416625 and parameters: {'n_estimators': 789, 'learning_rate': 0.16726680320161585, 'max_depth': 10, 'max_bin': 181, 'num_leaves': 172}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:45:19,742] Trial 280 finished with value: 0.685381759946362 and parameters: {'n_estimators': 824, 'learning_rate': 0.038043230075529996, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 200}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:45:28,379] Trial 281 finished with value: 0.6870571860495109 and parameters: {'n_estimators': 712, 'learning_rate': 0.044947015137714944, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 242}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:45:38,988] Trial 282 finished with value: 0.685627396747914 and parameters: {'n_estimators': 751, 'learning_rate': 0.03276892474044541, 'max_depth': 12, 'max_bin': 197, 'num_leaves': 335}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:45:49,314] Trial 283 finished with value: 0.6894908080200504 and parameters: {'n_estimators': 773, 'learning_rate': 0.04227786783050659, 'max_depth': 12, 'max_bin': 179, 'num_leaves': 317}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:45:56,874] Trial 284 finished with value: 0.6769614071377583 and parameters: {'n_estimators': 766, 'learning_rate': 0.041817367300056224, 'max_depth': 8, 'max_bin': 182, 'num_leaves': 299}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:46:07,405] Trial 285 finished with value: 0.6889890569044581 and parameters: {'n_estimators': 779, 'learning_rate': 0.03786475989135661, 'max_depth': 12, 'max_bin': 178, 'num_leaves': 219}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:46:17,519] Trial 286 finished with value: 0.6836485019451741 and parameters: {'n_estimators': 806, 'learning_rate': 0.03794312967085378, 'max_depth': 12, 'max_bin': 177, 'num_leaves': 215}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:46:27,932] Trial 287 finished with value: 0.684231817684873 and parameters: {'n_estimators': 777, 'learning_rate': 0.03198459925295555, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 221}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:46:38,388] Trial 288 finished with value: 0.6871084949653977 and parameters: {'n_estimators': 790, 'learning_rate': 0.03621898180607689, 'max_depth': 12, 'max_bin': 167, 'num_leaves': 263}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:46:50,624] Trial 289 finished with value: 0.6863213783861928 and parameters: {'n_estimators': 748, 'learning_rate': 0.029209631844737106, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 319}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:47:00,815] Trial 290 finished with value: 0.6864673908296264 and parameters: {'n_estimators': 774, 'learning_rate': 0.04140908733912683, 'max_depth': 12, 'max_bin': 184, 'num_leaves': 249}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:47:06,409] Trial 291 finished with value: 0.6790540270597483 and parameters: {'n_estimators': 741, 'learning_rate': 0.09834775897086374, 'max_depth': 12, 'max_bin': 171, 'num_leaves': 186}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:47:16,160] Trial 292 finished with value: 0.6884792519374385 and parameters: {'n_estimators': 809, 'learning_rate': 0.046222293307929985, 'max_depth': 12, 'max_bin': 177, 'num_leaves': 350}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:47:23,538] Trial 293 finished with value: 0.6855602046919422 and parameters: {'n_estimators': 763, 'learning_rate': 0.07902176316565933, 'max_depth': 12, 'max_bin': 174, 'num_leaves': 206}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:47:32,105] Trial 294 finished with value: 0.683245447217696 and parameters: {'n_estimators': 790, 'learning_rate': 0.04753152715411565, 'max_depth': 12, 'max_bin': 156, 'num_leaves': 365}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:47:43,101] Trial 295 finished with value: 0.685351623925883 and parameters: {'n_estimators': 841, 'learning_rate': 0.03899081244283092, 'max_depth': 12, 'max_bin': 191, 'num_leaves': 306}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:47:52,117] Trial 296 finished with value: 0.6879919530582446 and parameters: {'n_estimators': 727, 'learning_rate': 0.04287708169475941, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 328}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:48:02,690] Trial 297 finished with value: 0.6863058058407195 and parameters: {'n_estimators': 773, 'learning_rate': 0.03493103355273164, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 226}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:48:09,465] Trial 298 finished with value: 0.6606040517173427 and parameters: {'n_estimators': 381, 'learning_rate': 0.02230079785897609, 'max_depth': 12, 'max_bin': 167, 'num_leaves': 268}. Best is trial 225 with value: 0.6931625399512271.\n",
      "[I 2023-12-11 08:48:18,186] Trial 299 finished with value: 0.6890093247959541 and parameters: {'n_estimators': 754, 'learning_rate': 0.051358441858836215, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 293}. Best is trial 225 with value: 0.6931625399512271.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6932\n",
      "\tBest params:\n",
      "\t\tn_estimators: 764\n",
      "\t\tlearning_rate: 0.04742794787391275\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 266\n",
      "\t\tnum_leaves: 250\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.695789    0.706151    0.690541    0.727531   \n",
      "1                    TP  401.000000  393.000000  396.000000  399.000000   \n",
      "2                    TN  352.000000  345.000000  352.000000  359.000000   \n",
      "3                    FP   78.000000   79.000000   76.000000   76.000000   \n",
      "4                    FN   68.000000   82.000000   75.000000   65.000000   \n",
      "5              Accuracy    0.837597    0.820912    0.832036    0.843159   \n",
      "6             Precision    0.837161    0.832627    0.838983    0.840000   \n",
      "7           Sensitivity    0.855011    0.827368    0.840764    0.859914   \n",
      "8           Specificity    0.818600    0.813700    0.822400    0.825300   \n",
      "9              F1 score    0.845992    0.829989    0.839873    0.849840   \n",
      "10  F1 score (weighted)    0.837499    0.820944    0.832026    0.843074   \n",
      "11     F1 score (macro)    0.837113    0.820400    0.831632    0.842848   \n",
      "12    Balanced Accuracy    0.836808    0.820524    0.831597    0.842601   \n",
      "13                  MCC    0.674435    0.640819    0.663267    0.685949   \n",
      "14                  NPV    0.838100    0.808000    0.824400    0.846700   \n",
      "15              ROC_AUC    0.836808    0.820524    0.831597    0.842601   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.688238    0.703452  \n",
      "1   400.000000  388.000000  \n",
      "2   340.000000  348.000000  \n",
      "3    89.000000   83.000000  \n",
      "4    70.000000   80.000000  \n",
      "5     0.823137    0.818687  \n",
      "6     0.817996    0.823779  \n",
      "7     0.851064    0.829060  \n",
      "8     0.792500    0.807400  \n",
      "9     0.834202    0.826411  \n",
      "10    0.822886    0.818660  \n",
      "11    0.822345    0.818328  \n",
      "12    0.821802    0.818242  \n",
      "13    0.645432    0.636674  \n",
      "14    0.829300    0.813100  \n",
      "15    0.821802    0.818242  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_5_cat = np.where((y_pred_lgbm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 08:48:26,590] Trial 300 finished with value: 0.694694964431036 and parameters: {'n_estimators': 754, 'learning_rate': 0.051454311577780354, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 293}. Best is trial 300 with value: 0.694694964431036.\n",
      "[I 2023-12-11 08:48:35,348] Trial 301 finished with value: 0.7007586898439598 and parameters: {'n_estimators': 731, 'learning_rate': 0.04707730020946589, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 312}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:48:40,350] Trial 302 finished with value: 0.6661991152844571 and parameters: {'n_estimators': 724, 'learning_rate': 0.048850921601139616, 'max_depth': 5, 'max_bin': 280, 'num_leaves': 338}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:48:48,304] Trial 303 finished with value: 0.6947626597521925 and parameters: {'n_estimators': 739, 'learning_rate': 0.052428797272437926, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 321}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:48:57,254] Trial 304 finished with value: 0.6961291095290223 and parameters: {'n_estimators': 713, 'learning_rate': 0.05207167248743902, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 317}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:49:04,368] Trial 305 finished with value: 0.692747609182283 and parameters: {'n_estimators': 714, 'learning_rate': 0.05405793542144131, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 321}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:49:11,961] Trial 306 finished with value: 0.6956930073352023 and parameters: {'n_estimators': 713, 'learning_rate': 0.05446177262841548, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 315}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:49:19,972] Trial 307 finished with value: 0.6984115733572925 and parameters: {'n_estimators': 710, 'learning_rate': 0.05372772524592471, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 315}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:49:27,482] Trial 308 finished with value: 0.6970793856873463 and parameters: {'n_estimators': 715, 'learning_rate': 0.055662488653192545, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 315}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:49:35,393] Trial 309 finished with value: 0.6935511955296908 and parameters: {'n_estimators': 706, 'learning_rate': 0.05381407383790323, 'max_depth': 9, 'max_bin': 212, 'num_leaves': 286}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:49:42,886] Trial 310 finished with value: 0.6958529834738887 and parameters: {'n_estimators': 707, 'learning_rate': 0.053929823341834875, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 285}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:49:50,303] Trial 311 finished with value: 0.6918431672106975 and parameters: {'n_estimators': 717, 'learning_rate': 0.054708266941736416, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 316}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:49:57,373] Trial 312 finished with value: 0.6965032263004705 and parameters: {'n_estimators': 720, 'learning_rate': 0.056703737699665316, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 282}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:50:04,869] Trial 313 finished with value: 0.6921286810920007 and parameters: {'n_estimators': 709, 'learning_rate': 0.05555752614472235, 'max_depth': 9, 'max_bin': 207, 'num_leaves': 287}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:50:11,573] Trial 314 finished with value: 0.694262232565303 and parameters: {'n_estimators': 715, 'learning_rate': 0.05629846138282759, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 297}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:50:18,606] Trial 315 finished with value: 0.6960897200162302 and parameters: {'n_estimators': 704, 'learning_rate': 0.05434347809715231, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 284}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:50:26,001] Trial 316 finished with value: 0.6942964950017801 and parameters: {'n_estimators': 711, 'learning_rate': 0.056467772216644535, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 289}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:50:32,846] Trial 317 finished with value: 0.6965228002204072 and parameters: {'n_estimators': 710, 'learning_rate': 0.058353535813931, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 283}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:50:39,461] Trial 318 finished with value: 0.6952076549435444 and parameters: {'n_estimators': 707, 'learning_rate': 0.056409799826175555, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 284}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:50:46,886] Trial 319 finished with value: 0.695716229039217 and parameters: {'n_estimators': 706, 'learning_rate': 0.05593908066447313, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 283}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:50:53,810] Trial 320 finished with value: 0.6941081717407792 and parameters: {'n_estimators': 703, 'learning_rate': 0.058024508308753386, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 279}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:51:00,378] Trial 321 finished with value: 0.6958050189227725 and parameters: {'n_estimators': 706, 'learning_rate': 0.05762497042976143, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 279}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:51:07,394] Trial 322 finished with value: 0.6979179310408217 and parameters: {'n_estimators': 705, 'learning_rate': 0.05829775486287654, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 279}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:51:14,655] Trial 323 finished with value: 0.6957199188300859 and parameters: {'n_estimators': 705, 'learning_rate': 0.058651320421139165, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 280}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:51:22,481] Trial 324 finished with value: 0.6973847784457525 and parameters: {'n_estimators': 704, 'learning_rate': 0.05817592583924525, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 280}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:51:29,738] Trial 325 finished with value: 0.6993942000646035 and parameters: {'n_estimators': 702, 'learning_rate': 0.058760668566504984, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 278}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:51:36,140] Trial 326 finished with value: 0.6948902443428201 and parameters: {'n_estimators': 700, 'learning_rate': 0.059462301812573656, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 276}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:51:43,550] Trial 327 finished with value: 0.6970104838029563 and parameters: {'n_estimators': 684, 'learning_rate': 0.060764386145198815, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 271}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:51:50,181] Trial 328 finished with value: 0.6950989624728015 and parameters: {'n_estimators': 695, 'learning_rate': 0.060214037831333236, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 272}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:51:57,336] Trial 329 finished with value: 0.6957623317568087 and parameters: {'n_estimators': 681, 'learning_rate': 0.06161500157255357, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 269}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:52:05,003] Trial 330 finished with value: 0.697758399716524 and parameters: {'n_estimators': 681, 'learning_rate': 0.06186074907242445, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 268}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:52:11,676] Trial 331 finished with value: 0.6924822646089848 and parameters: {'n_estimators': 682, 'learning_rate': 0.062018238739919934, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 263}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:52:18,526] Trial 332 finished with value: 0.6955817538837288 and parameters: {'n_estimators': 683, 'learning_rate': 0.06232603155426429, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 271}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:52:25,058] Trial 333 finished with value: 0.6964540953049887 and parameters: {'n_estimators': 687, 'learning_rate': 0.06387229005215993, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 270}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:52:32,129] Trial 334 finished with value: 0.6968835298947161 and parameters: {'n_estimators': 671, 'learning_rate': 0.06324395469981234, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 268}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:52:38,802] Trial 335 finished with value: 0.6995091465701433 and parameters: {'n_estimators': 680, 'learning_rate': 0.06408417113881928, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 265}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:52:45,355] Trial 336 finished with value: 0.6951562787898155 and parameters: {'n_estimators': 680, 'learning_rate': 0.06493037089062827, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 267}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:52:51,515] Trial 337 finished with value: 0.6947634346931857 and parameters: {'n_estimators': 675, 'learning_rate': 0.06808083544669624, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 260}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:52:58,111] Trial 338 finished with value: 0.6989017109967006 and parameters: {'n_estimators': 686, 'learning_rate': 0.06363715742027339, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 267}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:53:04,719] Trial 339 finished with value: 0.6978996139439031 and parameters: {'n_estimators': 678, 'learning_rate': 0.06428412285028207, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 248}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:53:11,540] Trial 340 finished with value: 0.7000916870094924 and parameters: {'n_estimators': 669, 'learning_rate': 0.06359241235517993, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 242}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:53:17,476] Trial 341 finished with value: 0.6911996459601949 and parameters: {'n_estimators': 663, 'learning_rate': 0.06315718389389573, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 241}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:53:24,530] Trial 342 finished with value: 0.6988106573667797 and parameters: {'n_estimators': 683, 'learning_rate': 0.0680864878503366, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 249}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:53:31,314] Trial 343 finished with value: 0.6937402272613659 and parameters: {'n_estimators': 667, 'learning_rate': 0.06732091827199368, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 251}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:53:37,911] Trial 344 finished with value: 0.6955079079545164 and parameters: {'n_estimators': 688, 'learning_rate': 0.0687432980375047, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 253}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:53:44,684] Trial 345 finished with value: 0.6921564429288974 and parameters: {'n_estimators': 672, 'learning_rate': 0.06421657838727836, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 241}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:53:50,951] Trial 346 finished with value: 0.6918544030046372 and parameters: {'n_estimators': 688, 'learning_rate': 0.06473996251114815, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 262}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:53:57,291] Trial 347 finished with value: 0.6953002406445292 and parameters: {'n_estimators': 655, 'learning_rate': 0.07082304726372404, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 258}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:54:04,429] Trial 348 finished with value: 0.6959598103262048 and parameters: {'n_estimators': 692, 'learning_rate': 0.06062852875793852, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 233}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:54:10,651] Trial 349 finished with value: 0.694902682341712 and parameters: {'n_estimators': 689, 'learning_rate': 0.061439035146119765, 'max_depth': 10, 'max_bin': 190, 'num_leaves': 243}. Best is trial 301 with value: 0.7007586898439598.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.700759\n",
      "\tBest params:\n",
      "\t\tn_estimators: 731\n",
      "\t\tlearning_rate: 0.04707730020946589\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 279\n",
      "\t\tnum_leaves: 312\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.695789    0.706151    0.690541    0.727531   \n",
      "1                    TP  401.000000  393.000000  396.000000  399.000000   \n",
      "2                    TN  352.000000  345.000000  352.000000  359.000000   \n",
      "3                    FP   78.000000   79.000000   76.000000   76.000000   \n",
      "4                    FN   68.000000   82.000000   75.000000   65.000000   \n",
      "5              Accuracy    0.837597    0.820912    0.832036    0.843159   \n",
      "6             Precision    0.837161    0.832627    0.838983    0.840000   \n",
      "7           Sensitivity    0.855011    0.827368    0.840764    0.859914   \n",
      "8           Specificity    0.818600    0.813700    0.822400    0.825300   \n",
      "9              F1 score    0.845992    0.829989    0.839873    0.849840   \n",
      "10  F1 score (weighted)    0.837499    0.820944    0.832026    0.843074   \n",
      "11     F1 score (macro)    0.837113    0.820400    0.831632    0.842848   \n",
      "12    Balanced Accuracy    0.836808    0.820524    0.831597    0.842601   \n",
      "13                  MCC    0.674435    0.640819    0.663267    0.685949   \n",
      "14                  NPV    0.838100    0.808000    0.824400    0.846700   \n",
      "15              ROC_AUC    0.836808    0.820524    0.831597    0.842601   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.688238    0.703452    0.640478  \n",
      "1   400.000000  388.000000  381.000000  \n",
      "2   340.000000  348.000000  357.000000  \n",
      "3    89.000000   83.000000   70.000000  \n",
      "4    70.000000   80.000000   91.000000  \n",
      "5     0.823137    0.818687    0.820912  \n",
      "6     0.817996    0.823779    0.844789  \n",
      "7     0.851064    0.829060    0.807203  \n",
      "8     0.792500    0.807400    0.836100  \n",
      "9     0.834202    0.826411    0.825569  \n",
      "10    0.822886    0.818660    0.821024  \n",
      "11    0.822345    0.818328    0.820784  \n",
      "12    0.821802    0.818242    0.821634  \n",
      "13    0.645432    0.636674    0.642466  \n",
      "14    0.829300    0.813100    0.796900  \n",
      "15    0.821802    0.818242    0.821634  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_6_cat = np.where((y_pred_lgbm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 08:54:17,427] Trial 350 finished with value: 0.6838639511934187 and parameters: {'n_estimators': 672, 'learning_rate': 0.06528852249026602, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 232}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:54:24,116] Trial 351 finished with value: 0.6858789591498551 and parameters: {'n_estimators': 688, 'learning_rate': 0.060708071897859456, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 272}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:54:31,022] Trial 352 finished with value: 0.6878612459020007 and parameters: {'n_estimators': 695, 'learning_rate': 0.06892361259398416, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 263}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:54:37,514] Trial 353 finished with value: 0.6825518410657722 and parameters: {'n_estimators': 660, 'learning_rate': 0.05995613528010262, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 235}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:54:43,956] Trial 354 finished with value: 0.6880012595063292 and parameters: {'n_estimators': 674, 'learning_rate': 0.07481262077288724, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 255}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:54:50,847] Trial 355 finished with value: 0.6849288145479397 and parameters: {'n_estimators': 698, 'learning_rate': 0.06478126821161166, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 277}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:54:56,735] Trial 356 finished with value: 0.6860048432120014 and parameters: {'n_estimators': 696, 'learning_rate': 0.061457801678952234, 'max_depth': 9, 'max_bin': 199, 'num_leaves': 275}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:55:03,025] Trial 357 finished with value: 0.6866947444855359 and parameters: {'n_estimators': 650, 'learning_rate': 0.06860816613689706, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 249}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:55:10,147] Trial 358 finished with value: 0.6859302043971247 and parameters: {'n_estimators': 672, 'learning_rate': 0.05890465705174286, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 263}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:55:16,461] Trial 359 finished with value: 0.6875050820414506 and parameters: {'n_estimators': 684, 'learning_rate': 0.06627000688673938, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 281}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:55:22,893] Trial 360 finished with value: 0.6810531300796996 and parameters: {'n_estimators': 704, 'learning_rate': 0.059084769165588906, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 237}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:55:28,602] Trial 361 finished with value: 0.6829460666673084 and parameters: {'n_estimators': 662, 'learning_rate': 0.0720823875804356, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 253}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:55:35,996] Trial 362 finished with value: 0.6851221032848238 and parameters: {'n_estimators': 714, 'learning_rate': 0.06272025613646412, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 269}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:55:42,510] Trial 363 finished with value: 0.6846051676346828 and parameters: {'n_estimators': 687, 'learning_rate': 0.05920865113663661, 'max_depth': 9, 'max_bin': 201, 'num_leaves': 277}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:55:49,443] Trial 364 finished with value: 0.6838246848733031 and parameters: {'n_estimators': 701, 'learning_rate': 0.0642930246041007, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 230}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:55:56,890] Trial 365 finished with value: 0.6825838546187637 and parameters: {'n_estimators': 676, 'learning_rate': 0.058755160558173126, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 285}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:56:02,894] Trial 366 finished with value: 0.6854127257584688 and parameters: {'n_estimators': 719, 'learning_rate': 0.06648381436864761, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 250}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:56:09,313] Trial 367 finished with value: 0.6832873048067262 and parameters: {'n_estimators': 648, 'learning_rate': 0.07199159924992397, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 263}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:56:15,766] Trial 368 finished with value: 0.6865079534594416 and parameters: {'n_estimators': 695, 'learning_rate': 0.06246083208915615, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 285}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:56:22,665] Trial 369 finished with value: 0.6840499986500512 and parameters: {'n_estimators': 718, 'learning_rate': 0.0575257138424893, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 267}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:56:28,250] Trial 370 finished with value: 0.6839259690329238 and parameters: {'n_estimators': 665, 'learning_rate': 0.07583316762881803, 'max_depth': 9, 'max_bin': 208, 'num_leaves': 244}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:56:33,959] Trial 371 finished with value: 0.682099273825053 and parameters: {'n_estimators': 702, 'learning_rate': 0.06805441007471756, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 283}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:56:40,744] Trial 372 finished with value: 0.6852337532095806 and parameters: {'n_estimators': 681, 'learning_rate': 0.062160920626137525, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 258}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:56:46,959] Trial 373 finished with value: 0.6838758608079887 and parameters: {'n_estimators': 717, 'learning_rate': 0.05865267774239586, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 229}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:56:53,975] Trial 374 finished with value: 0.6838122074339846 and parameters: {'n_estimators': 692, 'learning_rate': 0.05647754222510984, 'max_depth': 9, 'max_bin': 213, 'num_leaves': 274}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:57:00,929] Trial 375 finished with value: 0.689495096124192 and parameters: {'n_estimators': 675, 'learning_rate': 0.06506641407363847, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 289}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:57:06,867] Trial 376 finished with value: 0.6881674185857517 and parameters: {'n_estimators': 702, 'learning_rate': 0.08269220990359588, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 267}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:57:12,812] Trial 377 finished with value: 0.6820571005901573 and parameters: {'n_estimators': 725, 'learning_rate': 0.06908553923419475, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 245}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:57:19,333] Trial 378 finished with value: 0.6867163009320533 and parameters: {'n_estimators': 656, 'learning_rate': 0.06178363342585359, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 286}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:57:25,541] Trial 379 finished with value: 0.6833920748156073 and parameters: {'n_estimators': 682, 'learning_rate': 0.05734464963221725, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 297}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:57:32,703] Trial 380 finished with value: 0.6877186528559847 and parameters: {'n_estimators': 706, 'learning_rate': 0.056356079164230105, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 256}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:57:39,169] Trial 381 finished with value: 0.6861171404672166 and parameters: {'n_estimators': 642, 'learning_rate': 0.06319135742272082, 'max_depth': 10, 'max_bin': 194, 'num_leaves': 271}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:57:44,414] Trial 382 finished with value: 0.6849163753857198 and parameters: {'n_estimators': 718, 'learning_rate': 0.07156115610962623, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 227}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:57:50,425] Trial 383 finished with value: 0.6858594821248316 and parameters: {'n_estimators': 691, 'learning_rate': 0.06003967937553742, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 283}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:57:55,432] Trial 384 finished with value: 0.6861604998672538 and parameters: {'n_estimators': 666, 'learning_rate': 0.08967349744089119, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 253}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:02,229] Trial 385 finished with value: 0.6839565687164041 and parameters: {'n_estimators': 706, 'learning_rate': 0.067106320761798, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 297}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:08,992] Trial 386 finished with value: 0.681988952931685 and parameters: {'n_estimators': 725, 'learning_rate': 0.05377704765109336, 'max_depth': 9, 'max_bin': 203, 'num_leaves': 269}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:14,322] Trial 387 finished with value: 0.6786955465216381 and parameters: {'n_estimators': 683, 'learning_rate': 0.07649365257616762, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 210}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:21,734] Trial 388 finished with value: 0.6825914511998733 and parameters: {'n_estimators': 663, 'learning_rate': 0.05814254382520824, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 241}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:28,826] Trial 389 finished with value: 0.6881811011771639 and parameters: {'n_estimators': 693, 'learning_rate': 0.06266022527881, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 279}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:30,493] Trial 390 finished with value: 0.605630266489805 and parameters: {'n_estimators': 73, 'learning_rate': 0.05496420315930155, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 298}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:34,563] Trial 391 finished with value: 0.6790368650061593 and parameters: {'n_estimators': 310, 'learning_rate': 0.10259433149727237, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 258}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:41,054] Trial 392 finished with value: 0.6884012319292634 and parameters: {'n_estimators': 727, 'learning_rate': 0.06652537242500439, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 275}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:47,468] Trial 393 finished with value: 0.6832570520813228 and parameters: {'n_estimators': 707, 'learning_rate': 0.060472454335178345, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 290}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:53,134] Trial 394 finished with value: 0.6866083196390139 and parameters: {'n_estimators': 686, 'learning_rate': 0.07203613866694736, 'max_depth': 9, 'max_bin': 212, 'num_leaves': 246}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:58:56,340] Trial 395 finished with value: 0.6661380229769513 and parameters: {'n_estimators': 642, 'learning_rate': 0.1966439443877571, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 263}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:59:03,075] Trial 396 finished with value: 0.6861568760554501 and parameters: {'n_estimators': 673, 'learning_rate': 0.05486831639848158, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 303}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:59:09,969] Trial 397 finished with value: 0.6847307077608564 and parameters: {'n_estimators': 707, 'learning_rate': 0.0635813872952031, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 233}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:59:15,499] Trial 398 finished with value: 0.6852842853226664 and parameters: {'n_estimators': 725, 'learning_rate': 0.081683402104663, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 282}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:59:22,054] Trial 399 finished with value: 0.6824668196545411 and parameters: {'n_estimators': 702, 'learning_rate': 0.05867858182990373, 'max_depth': 10, 'max_bin': 192, 'num_leaves': 266}. Best is trial 301 with value: 0.7007586898439598.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7007587\n",
      "\tBest params:\n",
      "\t\tn_estimators: 731\n",
      "\t\tlearning_rate: 0.04707730020946589\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 279\n",
      "\t\tnum_leaves: 312\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.695789    0.706151    0.690541    0.727531   \n",
      "1                    TP  401.000000  393.000000  396.000000  399.000000   \n",
      "2                    TN  352.000000  345.000000  352.000000  359.000000   \n",
      "3                    FP   78.000000   79.000000   76.000000   76.000000   \n",
      "4                    FN   68.000000   82.000000   75.000000   65.000000   \n",
      "5              Accuracy    0.837597    0.820912    0.832036    0.843159   \n",
      "6             Precision    0.837161    0.832627    0.838983    0.840000   \n",
      "7           Sensitivity    0.855011    0.827368    0.840764    0.859914   \n",
      "8           Specificity    0.818600    0.813700    0.822400    0.825300   \n",
      "9              F1 score    0.845992    0.829989    0.839873    0.849840   \n",
      "10  F1 score (weighted)    0.837499    0.820944    0.832026    0.843074   \n",
      "11     F1 score (macro)    0.837113    0.820400    0.831632    0.842848   \n",
      "12    Balanced Accuracy    0.836808    0.820524    0.831597    0.842601   \n",
      "13                  MCC    0.674435    0.640819    0.663267    0.685949   \n",
      "14                  NPV    0.838100    0.808000    0.824400    0.846700   \n",
      "15              ROC_AUC    0.836808    0.820524    0.831597    0.842601   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.688238    0.703452    0.640478    0.665989  \n",
      "1   400.000000  388.000000  381.000000  391.000000  \n",
      "2   340.000000  348.000000  357.000000  337.000000  \n",
      "3    89.000000   83.000000   70.000000   81.000000  \n",
      "4    70.000000   80.000000   91.000000   90.000000  \n",
      "5     0.823137    0.818687    0.820912    0.809789  \n",
      "6     0.817996    0.823779    0.844789    0.828390  \n",
      "7     0.851064    0.829060    0.807203    0.812890  \n",
      "8     0.792500    0.807400    0.836100    0.806200  \n",
      "9     0.834202    0.826411    0.825569    0.820567  \n",
      "10    0.822886    0.818660    0.821024    0.809903  \n",
      "11    0.822345    0.818328    0.820784    0.809100  \n",
      "12    0.821802    0.818242    0.821634    0.809555  \n",
      "13    0.645432    0.636674    0.642466    0.618363  \n",
      "14    0.829300    0.813100    0.796900    0.789200  \n",
      "15    0.821802    0.818242    0.821634    0.809555  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_7_cat = np.where((y_pred_lgbm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 08:59:26,518] Trial 400 finished with value: 0.6700333861974241 and parameters: {'n_estimators': 658, 'learning_rate': 0.115038316542641, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 251}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:59:32,210] Trial 401 finished with value: 0.6754539518746397 and parameters: {'n_estimators': 678, 'learning_rate': 0.06936220981621219, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 217}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:59:38,431] Trial 402 finished with value: 0.6774414606014706 and parameters: {'n_estimators': 729, 'learning_rate': 0.05376024576482899, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 299}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:59:44,343] Trial 403 finished with value: 0.6778648262965451 and parameters: {'n_estimators': 693, 'learning_rate': 0.06518806983603181, 'max_depth': 9, 'max_bin': 198, 'num_leaves': 283}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:59:50,668] Trial 404 finished with value: 0.6814336430804403 and parameters: {'n_estimators': 714, 'learning_rate': 0.058332381255093706, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 268}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 08:59:54,883] Trial 405 finished with value: 0.6743087427111538 and parameters: {'n_estimators': 675, 'learning_rate': 0.14206306252443268, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 235}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:00:01,758] Trial 406 finished with value: 0.6786066891550497 and parameters: {'n_estimators': 695, 'learning_rate': 0.05270672863575848, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 290}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:00:06,385] Trial 407 finished with value: 0.6819541717394653 and parameters: {'n_estimators': 733, 'learning_rate': 0.0942519259429822, 'max_depth': 10, 'max_bin': 194, 'num_leaves': 255}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:00:13,030] Trial 408 finished with value: 0.682058093859166 and parameters: {'n_estimators': 712, 'learning_rate': 0.062009544964766704, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 275}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:00:19,957] Trial 409 finished with value: 0.6802428738896591 and parameters: {'n_estimators': 664, 'learning_rate': 0.05749646046640091, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 308}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:00:25,552] Trial 410 finished with value: 0.6749185948568314 and parameters: {'n_estimators': 633, 'learning_rate': 0.0738429755259437, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 245}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:00:31,158] Trial 411 finished with value: 0.6798848782506614 and parameters: {'n_estimators': 684, 'learning_rate': 0.06714422990590548, 'max_depth': 9, 'max_bin': 200, 'num_leaves': 46}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:00:37,968] Trial 412 finished with value: 0.6778731672407231 and parameters: {'n_estimators': 703, 'learning_rate': 0.06221711889262567, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 289}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:00:44,576] Trial 413 finished with value: 0.6776662572584085 and parameters: {'n_estimators': 649, 'learning_rate': 0.052952634914851714, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 262}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:00:50,789] Trial 414 finished with value: 0.6791575960168815 and parameters: {'n_estimators': 732, 'learning_rate': 0.06969194931522298, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 304}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:00:57,421] Trial 415 finished with value: 0.6806159562851918 and parameters: {'n_estimators': 692, 'learning_rate': 0.05791357577052006, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 274}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:01:03,374] Trial 416 finished with value: 0.6774513093192527 and parameters: {'n_estimators': 717, 'learning_rate': 0.06447783141219184, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 225}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:01:10,250] Trial 417 finished with value: 0.6783937592192473 and parameters: {'n_estimators': 675, 'learning_rate': 0.05318638478326022, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 257}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:01:16,487] Trial 418 finished with value: 0.6782233220924084 and parameters: {'n_estimators': 701, 'learning_rate': 0.06094995311540184, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 288}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:01:22,790] Trial 419 finished with value: 0.6738459495102546 and parameters: {'n_estimators': 656, 'learning_rate': 0.05625587256183907, 'max_depth': 9, 'max_bin': 205, 'num_leaves': 275}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:01:28,703] Trial 420 finished with value: 0.6790199201628021 and parameters: {'n_estimators': 719, 'learning_rate': 0.06640187902814398, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 245}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:01:33,452] Trial 421 finished with value: 0.6715585009902515 and parameters: {'n_estimators': 684, 'learning_rate': 0.07897557280412847, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 303}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:01:39,875] Trial 422 finished with value: 0.6791647020187539 and parameters: {'n_estimators': 734, 'learning_rate': 0.059341790005644546, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 269}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:01:46,193] Trial 423 finished with value: 0.6832862308770543 and parameters: {'n_estimators': 667, 'learning_rate': 0.07309197763471142, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 286}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:01:52,637] Trial 424 finished with value: 0.6798380510204408 and parameters: {'n_estimators': 703, 'learning_rate': 0.05229470666037975, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 259}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:01:57,195] Trial 425 finished with value: 0.6765450445185787 and parameters: {'n_estimators': 688, 'learning_rate': 0.10732211847215692, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 304}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:02:03,592] Trial 426 finished with value: 0.6777854342749448 and parameters: {'n_estimators': 713, 'learning_rate': 0.0630593313983274, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 245}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:02:09,744] Trial 427 finished with value: 0.6769475025355265 and parameters: {'n_estimators': 671, 'learning_rate': 0.05609692396036808, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 278}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:02:16,052] Trial 428 finished with value: 0.6818004710859317 and parameters: {'n_estimators': 734, 'learning_rate': 0.0689478860130615, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 233}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:02:22,864] Trial 429 finished with value: 0.6789077303385311 and parameters: {'n_estimators': 700, 'learning_rate': 0.061462788572577705, 'max_depth': 9, 'max_bin': 204, 'num_leaves': 290}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:02:27,914] Trial 430 finished with value: 0.674331511369143 and parameters: {'n_estimators': 686, 'learning_rate': 0.09586953364307511, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 264}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:02:38,183] Trial 431 finished with value: 0.6436007015556907 and parameters: {'n_estimators': 647, 'learning_rate': 0.01047651903687341, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 294}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:02:43,471] Trial 432 finished with value: 0.6790418148723892 and parameters: {'n_estimators': 720, 'learning_rate': 0.08788706980294725, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 212}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:02:49,018] Trial 433 finished with value: 0.6781682324723267 and parameters: {'n_estimators': 675, 'learning_rate': 0.09135100939278835, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 254}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:02:55,544] Trial 434 finished with value: 0.6791529290466438 and parameters: {'n_estimators': 705, 'learning_rate': 0.06504817235895477, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 274}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:03:02,894] Trial 435 finished with value: 0.6813241331628603 and parameters: {'n_estimators': 736, 'learning_rate': 0.052865566390566665, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 304}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:03:09,174] Trial 436 finished with value: 0.6783562771518536 and parameters: {'n_estimators': 661, 'learning_rate': 0.05756128409694237, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 238}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:03:15,156] Trial 437 finished with value: 0.6778522337842353 and parameters: {'n_estimators': 693, 'learning_rate': 0.0602318054028104, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 278}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:03:20,429] Trial 438 finished with value: 0.6772112791622498 and parameters: {'n_estimators': 719, 'learning_rate': 0.08374364167645952, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 264}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:03:26,284] Trial 439 finished with value: 0.6780159189818982 and parameters: {'n_estimators': 685, 'learning_rate': 0.06848551555232502, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 289}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:03:31,157] Trial 440 finished with value: 0.6703801939263887 and parameters: {'n_estimators': 708, 'learning_rate': 0.07721523738490198, 'max_depth': 7, 'max_bin': 217, 'num_leaves': 250}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:03:38,325] Trial 441 finished with value: 0.6741142846781806 and parameters: {'n_estimators': 632, 'learning_rate': 0.055670523206867016, 'max_depth': 9, 'max_bin': 205, 'num_leaves': 309}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:03:45,990] Trial 442 finished with value: 0.681549509150391 and parameters: {'n_estimators': 670, 'learning_rate': 0.05168299958847531, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 266}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:03:53,181] Trial 443 finished with value: 0.6777235836633482 and parameters: {'n_estimators': 723, 'learning_rate': 0.06414646235393301, 'max_depth': 10, 'max_bin': 190, 'num_leaves': 281}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:03:54,383] Trial 444 finished with value: 0.5392613823493225 and parameters: {'n_estimators': 163, 'learning_rate': 0.09852767696586345, 'max_depth': 3, 'max_bin': 213, 'num_leaves': 223}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:04:05,148] Trial 445 finished with value: 0.6727729247710091 and parameters: {'n_estimators': 696, 'learning_rate': 0.01979496865955356, 'max_depth': 11, 'max_bin': 202, 'num_leaves': 349}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:04:11,746] Trial 446 finished with value: 0.6808810483989542 and parameters: {'n_estimators': 649, 'learning_rate': 0.07271625188161254, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 431}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:04:17,529] Trial 447 finished with value: 0.6755838611391679 and parameters: {'n_estimators': 739, 'learning_rate': 0.06080960066223173, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 297}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:04:25,178] Trial 448 finished with value: 0.6816603925951766 and parameters: {'n_estimators': 682, 'learning_rate': 0.05711619441965681, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 254}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:04:32,227] Trial 449 finished with value: 0.6801270475364497 and parameters: {'n_estimators': 711, 'learning_rate': 0.05133341935864477, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 335}. Best is trial 301 with value: 0.7007586898439598.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.70075869\n",
      "\tBest params:\n",
      "\t\tn_estimators: 731\n",
      "\t\tlearning_rate: 0.04707730020946589\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 279\n",
      "\t\tnum_leaves: 312\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.695789    0.706151    0.690541    0.727531   \n",
      "1                    TP  401.000000  393.000000  396.000000  399.000000   \n",
      "2                    TN  352.000000  345.000000  352.000000  359.000000   \n",
      "3                    FP   78.000000   79.000000   76.000000   76.000000   \n",
      "4                    FN   68.000000   82.000000   75.000000   65.000000   \n",
      "5              Accuracy    0.837597    0.820912    0.832036    0.843159   \n",
      "6             Precision    0.837161    0.832627    0.838983    0.840000   \n",
      "7           Sensitivity    0.855011    0.827368    0.840764    0.859914   \n",
      "8           Specificity    0.818600    0.813700    0.822400    0.825300   \n",
      "9              F1 score    0.845992    0.829989    0.839873    0.849840   \n",
      "10  F1 score (weighted)    0.837499    0.820944    0.832026    0.843074   \n",
      "11     F1 score (macro)    0.837113    0.820400    0.831632    0.842848   \n",
      "12    Balanced Accuracy    0.836808    0.820524    0.831597    0.842601   \n",
      "13                  MCC    0.674435    0.640819    0.663267    0.685949   \n",
      "14                  NPV    0.838100    0.808000    0.824400    0.846700   \n",
      "15              ROC_AUC    0.836808    0.820524    0.831597    0.842601   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.688238    0.703452    0.640478    0.665989    0.680979  \n",
      "1   400.000000  388.000000  381.000000  391.000000  404.000000  \n",
      "2   340.000000  348.000000  357.000000  337.000000  354.000000  \n",
      "3    89.000000   83.000000   70.000000   81.000000   62.000000  \n",
      "4    70.000000   80.000000   91.000000   90.000000   79.000000  \n",
      "5     0.823137    0.818687    0.820912    0.809789    0.843159  \n",
      "6     0.817996    0.823779    0.844789    0.828390    0.866953  \n",
      "7     0.851064    0.829060    0.807203    0.812890    0.836439  \n",
      "8     0.792500    0.807400    0.836100    0.806200    0.851000  \n",
      "9     0.834202    0.826411    0.825569    0.820567    0.851423  \n",
      "10    0.822886    0.818660    0.821024    0.809903    0.843325  \n",
      "11    0.822345    0.818328    0.820784    0.809100    0.842672  \n",
      "12    0.821802    0.818242    0.821634    0.809555    0.843700  \n",
      "13    0.645432    0.636674    0.642466    0.618363    0.685951  \n",
      "14    0.829300    0.813100    0.796900    0.789200    0.817600  \n",
      "15    0.821802    0.818242    0.821634    0.809555    0.843700  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_8_cat = np.where((y_pred_lgbm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 09:04:39,928] Trial 450 finished with value: 0.6648738896652363 and parameters: {'n_estimators': 697, 'learning_rate': 0.06571120666060314, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 237}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:04:47,241] Trial 451 finished with value: 0.6672454704119997 and parameters: {'n_estimators': 672, 'learning_rate': 0.059273007543624814, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 273}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:04:54,218] Trial 452 finished with value: 0.6659788168166845 and parameters: {'n_estimators': 724, 'learning_rate': 0.05503099710583261, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 313}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:04:59,933] Trial 453 finished with value: 0.6564231991703153 and parameters: {'n_estimators': 707, 'learning_rate': 0.06263640211005368, 'max_depth': 6, 'max_bin': 202, 'num_leaves': 288}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:05:05,634] Trial 454 finished with value: 0.6619034458190602 and parameters: {'n_estimators': 654, 'learning_rate': 0.07082939323796184, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 263}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:05:11,564] Trial 455 finished with value: 0.6647551165501373 and parameters: {'n_estimators': 740, 'learning_rate': 0.06537354697307848, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 247}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:05:18,132] Trial 456 finished with value: 0.6644352900997833 and parameters: {'n_estimators': 687, 'learning_rate': 0.05492922230888068, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 279}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:05:25,298] Trial 457 finished with value: 0.6629594018592838 and parameters: {'n_estimators': 669, 'learning_rate': 0.05132797597024284, 'max_depth': 9, 'max_bin': 193, 'num_leaves': 301}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:05:31,939] Trial 458 finished with value: 0.6615108972555395 and parameters: {'n_estimators': 712, 'learning_rate': 0.05986095574694402, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 262}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:05:36,378] Trial 459 finished with value: 0.658708244711787 and parameters: {'n_estimators': 695, 'learning_rate': 0.1264999978435918, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 224}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:05:41,676] Trial 460 finished with value: 0.6615920811483836 and parameters: {'n_estimators': 728, 'learning_rate': 0.07593710771294129, 'max_depth': 9, 'max_bin': 205, 'num_leaves': 289}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:05:47,595] Trial 461 finished with value: 0.6603461407886704 and parameters: {'n_estimators': 682, 'learning_rate': 0.06847086619843339, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 310}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:05:51,381] Trial 462 finished with value: 0.6574877213998043 and parameters: {'n_estimators': 658, 'learning_rate': 0.1436844510346276, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 202}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:06:01,554] Trial 463 finished with value: 0.6547996912778055 and parameters: {'n_estimators': 701, 'learning_rate': 0.018577242654169684, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 274}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:06:07,950] Trial 464 finished with value: 0.6629126119164801 and parameters: {'n_estimators': 635, 'learning_rate': 0.05830477577133806, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 254}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:06:13,671] Trial 465 finished with value: 0.6642180306157972 and parameters: {'n_estimators': 718, 'learning_rate': 0.08190093704849347, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 243}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:06:19,513] Trial 466 finished with value: 0.6614997936969149 and parameters: {'n_estimators': 405, 'learning_rate': 0.06257037278212094, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 294}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:06:27,198] Trial 467 finished with value: 0.6642558321527344 and parameters: {'n_estimators': 686, 'learning_rate': 0.05137688982406309, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 272}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:06:36,523] Trial 468 finished with value: 0.6604285549361362 and parameters: {'n_estimators': 738, 'learning_rate': 0.024720958789019608, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 282}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:06:42,323] Trial 469 finished with value: 0.6631337079138502 and parameters: {'n_estimators': 698, 'learning_rate': 0.05607164057916224, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 261}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:06:48,311] Trial 470 finished with value: 0.6658135471538822 and parameters: {'n_estimators': 677, 'learning_rate': 0.06632284571971936, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 327}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:06:54,599] Trial 471 finished with value: 0.6606913886520602 and parameters: {'n_estimators': 718, 'learning_rate': 0.06072124237563182, 'max_depth': 9, 'max_bin': 218, 'num_leaves': 233}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:06:58,747] Trial 472 finished with value: 0.6541890305386382 and parameters: {'n_estimators': 662, 'learning_rate': 0.16603789857337853, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 303}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:07:03,238] Trial 473 finished with value: 0.6594932883382347 and parameters: {'n_estimators': 706, 'learning_rate': 0.10011075185973703, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 283}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:07:10,301] Trial 474 finished with value: 0.6672612579108996 and parameters: {'n_estimators': 733, 'learning_rate': 0.05477647954403483, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 267}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:07:15,384] Trial 475 finished with value: 0.6637267852643951 and parameters: {'n_estimators': 690, 'learning_rate': 0.11872972981766078, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 250}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:07:21,642] Trial 476 finished with value: 0.6624579021924234 and parameters: {'n_estimators': 457, 'learning_rate': 0.07074269379416236, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 316}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:07:27,862] Trial 477 finished with value: 0.6628202360204648 and parameters: {'n_estimators': 671, 'learning_rate': 0.06403600487996158, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 291}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:07:32,574] Trial 478 finished with value: 0.6617767399044743 and parameters: {'n_estimators': 713, 'learning_rate': 0.13476285469529317, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 272}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:07:37,647] Trial 479 finished with value: 0.6637357900446513 and parameters: {'n_estimators': 642, 'learning_rate': 0.08925843934340692, 'max_depth': 9, 'max_bin': 192, 'num_leaves': 214}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:07:47,911] Trial 480 finished with value: 0.652852603398584 and parameters: {'n_estimators': 742, 'learning_rate': 0.016892323705285137, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 238}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:07:53,639] Trial 481 finished with value: 0.667669540719504 and parameters: {'n_estimators': 695, 'learning_rate': 0.08533151391768257, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 298}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:08:00,585] Trial 482 finished with value: 0.6652187096737631 and parameters: {'n_estimators': 677, 'learning_rate': 0.051223477157373624, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 337}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:08:07,290] Trial 483 finished with value: 0.6691775944054591 and parameters: {'n_estimators': 722, 'learning_rate': 0.05863775477553331, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 254}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:08:13,902] Trial 484 finished with value: 0.6656530014979549 and parameters: {'n_estimators': 704, 'learning_rate': 0.061622796713060105, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 284}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:08:18,547] Trial 485 finished with value: 0.6628997368967642 and parameters: {'n_estimators': 657, 'learning_rate': 0.10657837569833914, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 264}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:08:30,162] Trial 486 finished with value: 0.6150620921958557 and parameters: {'n_estimators': 677, 'learning_rate': 0.007186431805400059, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 307}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:08:35,338] Trial 487 finished with value: 0.6590762157579471 and parameters: {'n_estimators': 722, 'learning_rate': 0.06773390764176551, 'max_depth': 8, 'max_bin': 204, 'num_leaves': 280}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:08:41,289] Trial 488 finished with value: 0.6615027869274345 and parameters: {'n_estimators': 695, 'learning_rate': 0.05665753784591017, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 243}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:08:48,166] Trial 489 finished with value: 0.6632719202472596 and parameters: {'n_estimators': 739, 'learning_rate': 0.052623038383137255, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 260}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:08:53,623] Trial 490 finished with value: 0.6611350312769775 and parameters: {'n_estimators': 711, 'learning_rate': 0.07950871878922788, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 295}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:08:59,657] Trial 491 finished with value: 0.6645309500513545 and parameters: {'n_estimators': 686, 'learning_rate': 0.07392813434925134, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 230}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:09:06,091] Trial 492 finished with value: 0.6639248298463155 and parameters: {'n_estimators': 665, 'learning_rate': 0.06338631476090267, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 313}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:09:11,822] Trial 493 finished with value: 0.6599197022873364 and parameters: {'n_estimators': 703, 'learning_rate': 0.058183332513394345, 'max_depth': 9, 'max_bin': 214, 'num_leaves': 272}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:09:18,274] Trial 494 finished with value: 0.6638931912087593 and parameters: {'n_estimators': 727, 'learning_rate': 0.0667087350447115, 'max_depth': 10, 'max_bin': 194, 'num_leaves': 286}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:09:24,537] Trial 495 finished with value: 0.6640611500775646 and parameters: {'n_estimators': 643, 'learning_rate': 0.051365233301049605, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 408}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:09:31,388] Trial 496 finished with value: 0.6633416762824037 and parameters: {'n_estimators': 688, 'learning_rate': 0.05529616568190495, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 254}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:09:37,546] Trial 497 finished with value: 0.6608835127610249 and parameters: {'n_estimators': 662, 'learning_rate': 0.06072369375521213, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 326}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:09:43,857] Trial 498 finished with value: 0.6644114881437686 and parameters: {'n_estimators': 709, 'learning_rate': 0.07057130172663281, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 270}. Best is trial 301 with value: 0.7007586898439598.\n",
      "[I 2023-12-11 09:09:47,914] Trial 499 finished with value: 0.6618350968705117 and parameters: {'n_estimators': 745, 'learning_rate': 0.12075392141247021, 'max_depth': 9, 'max_bin': 198, 'num_leaves': 361}. Best is trial 301 with value: 0.7007586898439598.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.700758690\n",
      "\tBest params:\n",
      "\t\tn_estimators: 731\n",
      "\t\tlearning_rate: 0.04707730020946589\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 279\n",
      "\t\tnum_leaves: 312\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.695789    0.706151    0.690541    0.727531   \n",
      "1                    TP  401.000000  393.000000  396.000000  399.000000   \n",
      "2                    TN  352.000000  345.000000  352.000000  359.000000   \n",
      "3                    FP   78.000000   79.000000   76.000000   76.000000   \n",
      "4                    FN   68.000000   82.000000   75.000000   65.000000   \n",
      "5              Accuracy    0.837597    0.820912    0.832036    0.843159   \n",
      "6             Precision    0.837161    0.832627    0.838983    0.840000   \n",
      "7           Sensitivity    0.855011    0.827368    0.840764    0.859914   \n",
      "8           Specificity    0.818600    0.813700    0.822400    0.825300   \n",
      "9              F1 score    0.845992    0.829989    0.839873    0.849840   \n",
      "10  F1 score (weighted)    0.837499    0.820944    0.832026    0.843074   \n",
      "11     F1 score (macro)    0.837113    0.820400    0.831632    0.842848   \n",
      "12    Balanced Accuracy    0.836808    0.820524    0.831597    0.842601   \n",
      "13                  MCC    0.674435    0.640819    0.663267    0.685949   \n",
      "14                  NPV    0.838100    0.808000    0.824400    0.846700   \n",
      "15              ROC_AUC    0.836808    0.820524    0.831597    0.842601   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.688238    0.703452    0.640478    0.665989    0.680979    0.699497  \n",
      "1   400.000000  388.000000  381.000000  391.000000  404.000000  403.000000  \n",
      "2   340.000000  348.000000  357.000000  337.000000  354.000000  350.000000  \n",
      "3    89.000000   83.000000   70.000000   81.000000   62.000000   65.000000  \n",
      "4    70.000000   80.000000   91.000000   90.000000   79.000000   81.000000  \n",
      "5     0.823137    0.818687    0.820912    0.809789    0.843159    0.837597  \n",
      "6     0.817996    0.823779    0.844789    0.828390    0.866953    0.861111  \n",
      "7     0.851064    0.829060    0.807203    0.812890    0.836439    0.832645  \n",
      "8     0.792500    0.807400    0.836100    0.806200    0.851000    0.843400  \n",
      "9     0.834202    0.826411    0.825569    0.820567    0.851423    0.846639  \n",
      "10    0.822886    0.818660    0.821024    0.809903    0.843325    0.837768  \n",
      "11    0.822345    0.818328    0.820784    0.809100    0.842672    0.837031  \n",
      "12    0.821802    0.818242    0.821634    0.809555    0.843700    0.838009  \n",
      "13    0.645432    0.636674    0.642466    0.618363    0.685951    0.674596  \n",
      "14    0.829300    0.813100    0.796900    0.789200    0.817600    0.812100  \n",
      "15    0.821802    0.818242    0.821634    0.809555    0.843700    0.838009  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_9_cat = np.where((y_pred_lgbm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                           np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.702515     0.015722\n",
      "1                    TP       197.600000     7.763161\n",
      "2                    TN       173.000000     7.859884\n",
      "3                    FP        40.100000     4.954235\n",
      "4                    FN        38.500000     4.743416\n",
      "5              Accuracy         0.825024     0.015305\n",
      "6             Precision         0.831354     0.019142\n",
      "7           Sensitivity         0.836934     0.019348\n",
      "8           Specificity         0.811790     0.022643\n",
      "9              F1 score         0.833981     0.015303\n",
      "10  F1 score (weighted)         0.824980     0.015334\n",
      "11     F1 score (macro)         0.824331     0.015511\n",
      "12    Balanced Accuracy         0.824363     0.015456\n",
      "13                  MCC         0.649024     0.030705\n",
      "14                  NPV         0.817970     0.021125\n",
      "15              ROC_AUC         0.824363     0.015456\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.695789</td>\n",
       "      <td>0.706151</td>\n",
       "      <td>0.690541</td>\n",
       "      <td>0.727531</td>\n",
       "      <td>0.688238</td>\n",
       "      <td>0.703452</td>\n",
       "      <td>0.640478</td>\n",
       "      <td>0.665989</td>\n",
       "      <td>0.680979</td>\n",
       "      <td>0.699497</td>\n",
       "      <td>0.689864</td>\n",
       "      <td>0.023776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>395.600000</td>\n",
       "      <td>7.336363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>349.400000</td>\n",
       "      <td>7.058486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>75.900000</td>\n",
       "      <td>8.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>78.100000</td>\n",
       "      <td>8.723531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.837597</td>\n",
       "      <td>0.820912</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.843159</td>\n",
       "      <td>0.823137</td>\n",
       "      <td>0.818687</td>\n",
       "      <td>0.820912</td>\n",
       "      <td>0.809789</td>\n",
       "      <td>0.843159</td>\n",
       "      <td>0.837597</td>\n",
       "      <td>0.828699</td>\n",
       "      <td>0.011536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.837161</td>\n",
       "      <td>0.832627</td>\n",
       "      <td>0.838983</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.817996</td>\n",
       "      <td>0.823779</td>\n",
       "      <td>0.844789</td>\n",
       "      <td>0.828390</td>\n",
       "      <td>0.866953</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.839179</td>\n",
       "      <td>0.015398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.855011</td>\n",
       "      <td>0.827368</td>\n",
       "      <td>0.840764</td>\n",
       "      <td>0.859914</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.829060</td>\n",
       "      <td>0.807203</td>\n",
       "      <td>0.812890</td>\n",
       "      <td>0.836439</td>\n",
       "      <td>0.832645</td>\n",
       "      <td>0.835236</td>\n",
       "      <td>0.017239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.813700</td>\n",
       "      <td>0.822400</td>\n",
       "      <td>0.825300</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.836100</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>0.843400</td>\n",
       "      <td>0.821660</td>\n",
       "      <td>0.018016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.845992</td>\n",
       "      <td>0.829989</td>\n",
       "      <td>0.839873</td>\n",
       "      <td>0.849840</td>\n",
       "      <td>0.834202</td>\n",
       "      <td>0.826411</td>\n",
       "      <td>0.825569</td>\n",
       "      <td>0.820567</td>\n",
       "      <td>0.851423</td>\n",
       "      <td>0.846639</td>\n",
       "      <td>0.837050</td>\n",
       "      <td>0.011179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.837499</td>\n",
       "      <td>0.820944</td>\n",
       "      <td>0.832026</td>\n",
       "      <td>0.843074</td>\n",
       "      <td>0.822886</td>\n",
       "      <td>0.818660</td>\n",
       "      <td>0.821024</td>\n",
       "      <td>0.809903</td>\n",
       "      <td>0.843325</td>\n",
       "      <td>0.837768</td>\n",
       "      <td>0.828711</td>\n",
       "      <td>0.011538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.837113</td>\n",
       "      <td>0.820400</td>\n",
       "      <td>0.831632</td>\n",
       "      <td>0.842848</td>\n",
       "      <td>0.822345</td>\n",
       "      <td>0.818328</td>\n",
       "      <td>0.820784</td>\n",
       "      <td>0.809100</td>\n",
       "      <td>0.842672</td>\n",
       "      <td>0.837031</td>\n",
       "      <td>0.828225</td>\n",
       "      <td>0.011574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.836808</td>\n",
       "      <td>0.820524</td>\n",
       "      <td>0.831597</td>\n",
       "      <td>0.842601</td>\n",
       "      <td>0.821802</td>\n",
       "      <td>0.818242</td>\n",
       "      <td>0.821634</td>\n",
       "      <td>0.809555</td>\n",
       "      <td>0.843700</td>\n",
       "      <td>0.838009</td>\n",
       "      <td>0.828447</td>\n",
       "      <td>0.011636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.674435</td>\n",
       "      <td>0.640819</td>\n",
       "      <td>0.663267</td>\n",
       "      <td>0.685949</td>\n",
       "      <td>0.645432</td>\n",
       "      <td>0.636674</td>\n",
       "      <td>0.642466</td>\n",
       "      <td>0.618363</td>\n",
       "      <td>0.685951</td>\n",
       "      <td>0.674596</td>\n",
       "      <td>0.656795</td>\n",
       "      <td>0.023194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.838100</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.824400</td>\n",
       "      <td>0.846700</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>0.813100</td>\n",
       "      <td>0.796900</td>\n",
       "      <td>0.789200</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.812100</td>\n",
       "      <td>0.817540</td>\n",
       "      <td>0.017726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.836808</td>\n",
       "      <td>0.820524</td>\n",
       "      <td>0.831597</td>\n",
       "      <td>0.842601</td>\n",
       "      <td>0.821802</td>\n",
       "      <td>0.818242</td>\n",
       "      <td>0.821634</td>\n",
       "      <td>0.809555</td>\n",
       "      <td>0.843700</td>\n",
       "      <td>0.838009</td>\n",
       "      <td>0.828447</td>\n",
       "      <td>0.011636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.695789    0.706151    0.690541    0.727531   \n",
       "1                    TP  401.000000  393.000000  396.000000  399.000000   \n",
       "2                    TN  352.000000  345.000000  352.000000  359.000000   \n",
       "3                    FP   78.000000   79.000000   76.000000   76.000000   \n",
       "4                    FN   68.000000   82.000000   75.000000   65.000000   \n",
       "5              Accuracy    0.837597    0.820912    0.832036    0.843159   \n",
       "6             Precision    0.837161    0.832627    0.838983    0.840000   \n",
       "7           Sensitivity    0.855011    0.827368    0.840764    0.859914   \n",
       "8           Specificity    0.818600    0.813700    0.822400    0.825300   \n",
       "9              F1 score    0.845992    0.829989    0.839873    0.849840   \n",
       "10  F1 score (weighted)    0.837499    0.820944    0.832026    0.843074   \n",
       "11     F1 score (macro)    0.837113    0.820400    0.831632    0.842848   \n",
       "12    Balanced Accuracy    0.836808    0.820524    0.831597    0.842601   \n",
       "13                  MCC    0.674435    0.640819    0.663267    0.685949   \n",
       "14                  NPV    0.838100    0.808000    0.824400    0.846700   \n",
       "15              ROC_AUC    0.836808    0.820524    0.831597    0.842601   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.688238    0.703452    0.640478    0.665989    0.680979    0.699497   \n",
       "1   400.000000  388.000000  381.000000  391.000000  404.000000  403.000000   \n",
       "2   340.000000  348.000000  357.000000  337.000000  354.000000  350.000000   \n",
       "3    89.000000   83.000000   70.000000   81.000000   62.000000   65.000000   \n",
       "4    70.000000   80.000000   91.000000   90.000000   79.000000   81.000000   \n",
       "5     0.823137    0.818687    0.820912    0.809789    0.843159    0.837597   \n",
       "6     0.817996    0.823779    0.844789    0.828390    0.866953    0.861111   \n",
       "7     0.851064    0.829060    0.807203    0.812890    0.836439    0.832645   \n",
       "8     0.792500    0.807400    0.836100    0.806200    0.851000    0.843400   \n",
       "9     0.834202    0.826411    0.825569    0.820567    0.851423    0.846639   \n",
       "10    0.822886    0.818660    0.821024    0.809903    0.843325    0.837768   \n",
       "11    0.822345    0.818328    0.820784    0.809100    0.842672    0.837031   \n",
       "12    0.821802    0.818242    0.821634    0.809555    0.843700    0.838009   \n",
       "13    0.645432    0.636674    0.642466    0.618363    0.685951    0.674596   \n",
       "14    0.829300    0.813100    0.796900    0.789200    0.817600    0.812100   \n",
       "15    0.821802    0.818242    0.821634    0.809555    0.843700    0.838009   \n",
       "\n",
       "           ave       std  \n",
       "0     0.689864  0.023776  \n",
       "1   395.600000  7.336363  \n",
       "2   349.400000  7.058486  \n",
       "3    75.900000  8.225300  \n",
       "4    78.100000  8.723531  \n",
       "5     0.828699  0.011536  \n",
       "6     0.839179  0.015398  \n",
       "7     0.835236  0.017239  \n",
       "8     0.821660  0.018016  \n",
       "9     0.837050  0.011179  \n",
       "10    0.828711  0.011538  \n",
       "11    0.828225  0.011574  \n",
       "12    0.828447  0.011636  \n",
       "13    0.656795  0.023194  \n",
       "14    0.817540  0.017726  \n",
       "15    0.828447  0.011636  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_lgbm0</th>\n",
       "      <th>y_pred_lgbm1</th>\n",
       "      <th>y_pred_lgbm2</th>\n",
       "      <th>y_pred_lgbm3</th>\n",
       "      <th>y_pred_lgbm4</th>\n",
       "      <th>y_pred_lgbm_ave</th>\n",
       "      <th>y_pred_lgbm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4176702</td>\n",
       "      <td>0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>5.565255</td>\n",
       "      <td>5.705026</td>\n",
       "      <td>5.937393</td>\n",
       "      <td>5.626906</td>\n",
       "      <td>5.809386</td>\n",
       "      <td>5.857328</td>\n",
       "      <td>0.311805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL272401</td>\n",
       "      <td>1</td>\n",
       "      <td>7.26</td>\n",
       "      <td>7.053297</td>\n",
       "      <td>6.882651</td>\n",
       "      <td>7.022504</td>\n",
       "      <td>6.761797</td>\n",
       "      <td>7.306279</td>\n",
       "      <td>7.047755</td>\n",
       "      <td>0.192173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL118</td>\n",
       "      <td>2</td>\n",
       "      <td>5.93</td>\n",
       "      <td>5.167096</td>\n",
       "      <td>5.212734</td>\n",
       "      <td>5.329301</td>\n",
       "      <td>5.382758</td>\n",
       "      <td>5.554621</td>\n",
       "      <td>5.429418</td>\n",
       "      <td>0.256328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3655939</td>\n",
       "      <td>3</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.741445</td>\n",
       "      <td>6.797126</td>\n",
       "      <td>6.670764</td>\n",
       "      <td>6.576688</td>\n",
       "      <td>6.699691</td>\n",
       "      <td>6.627619</td>\n",
       "      <td>0.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3621537</td>\n",
       "      <td>4</td>\n",
       "      <td>5.88</td>\n",
       "      <td>6.284534</td>\n",
       "      <td>6.195729</td>\n",
       "      <td>5.996763</td>\n",
       "      <td>6.128940</td>\n",
       "      <td>6.046171</td>\n",
       "      <td>6.088689</td>\n",
       "      <td>0.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL2047606</td>\n",
       "      <td>4487</td>\n",
       "      <td>5.44</td>\n",
       "      <td>5.663850</td>\n",
       "      <td>5.636298</td>\n",
       "      <td>5.548059</td>\n",
       "      <td>5.630025</td>\n",
       "      <td>5.582576</td>\n",
       "      <td>5.583468</td>\n",
       "      <td>0.074451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL217781</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.48</td>\n",
       "      <td>6.800631</td>\n",
       "      <td>6.576407</td>\n",
       "      <td>6.743852</td>\n",
       "      <td>6.419486</td>\n",
       "      <td>6.726066</td>\n",
       "      <td>6.791074</td>\n",
       "      <td>0.332998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL2105763</td>\n",
       "      <td>4489</td>\n",
       "      <td>9.92</td>\n",
       "      <td>6.810849</td>\n",
       "      <td>7.434572</td>\n",
       "      <td>7.158416</td>\n",
       "      <td>7.082478</td>\n",
       "      <td>7.098783</td>\n",
       "      <td>7.584183</td>\n",
       "      <td>1.060273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL3415969</td>\n",
       "      <td>4490</td>\n",
       "      <td>5.75</td>\n",
       "      <td>6.012308</td>\n",
       "      <td>5.822582</td>\n",
       "      <td>6.470055</td>\n",
       "      <td>6.119471</td>\n",
       "      <td>5.913145</td>\n",
       "      <td>6.014594</td>\n",
       "      <td>0.236509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL467066</td>\n",
       "      <td>4491</td>\n",
       "      <td>7.55</td>\n",
       "      <td>7.185057</td>\n",
       "      <td>7.421386</td>\n",
       "      <td>7.305435</td>\n",
       "      <td>7.235651</td>\n",
       "      <td>7.402284</td>\n",
       "      <td>7.349969</td>\n",
       "      <td>0.122541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_lgbm0  y_pred_lgbm1  \\\n",
       "0         CHEMBL4176702            0     6.50      5.565255      5.705026   \n",
       "1          CHEMBL272401            1     7.26      7.053297      6.882651   \n",
       "2             CHEMBL118            2     5.93      5.167096      5.212734   \n",
       "3         CHEMBL3655939            3     6.28      6.741445      6.797126   \n",
       "4         CHEMBL3621537            4     5.88      6.284534      6.195729   \n",
       "...                 ...          ...      ...           ...           ...   \n",
       "4487      CHEMBL2047606         4487     5.44      5.663850      5.636298   \n",
       "4488       CHEMBL217781         4488     7.48      6.800631      6.576407   \n",
       "4489      CHEMBL2105763         4489     9.92      6.810849      7.434572   \n",
       "4490      CHEMBL3415969         4490     5.75      6.012308      5.822582   \n",
       "4491       CHEMBL467066         4491     7.55      7.185057      7.421386   \n",
       "\n",
       "      y_pred_lgbm2  y_pred_lgbm3  y_pred_lgbm4  y_pred_lgbm_ave  \\\n",
       "0         5.937393      5.626906      5.809386         5.857328   \n",
       "1         7.022504      6.761797      7.306279         7.047755   \n",
       "2         5.329301      5.382758      5.554621         5.429418   \n",
       "3         6.670764      6.576688      6.699691         6.627619   \n",
       "4         5.996763      6.128940      6.046171         6.088689   \n",
       "...            ...           ...           ...              ...   \n",
       "4487      5.548059      5.630025      5.582576         5.583468   \n",
       "4488      6.743852      6.419486      6.726066         6.791074   \n",
       "4489      7.158416      7.082478      7.098783         7.584183   \n",
       "4490      6.470055      6.119471      5.913145         6.014594   \n",
       "4491      7.305435      7.235651      7.402284         7.349969   \n",
       "\n",
       "      y_pred_lgbm_std  \n",
       "0            0.311805  \n",
       "1            0.192173  \n",
       "2            0.256328  \n",
       "3            0.169400  \n",
       "4            0.132440  \n",
       "...               ...  \n",
       "4487         0.074451  \n",
       "4488         0.332998  \n",
       "4489         1.060273  \n",
       "4490         0.236509  \n",
       "4491         0.122541  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where((y_pred_optimized_lgbm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "lgbm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "537f1446-e8d9-4b66-8ee1-3b983d42520c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.705614</td>\n",
       "      <td>0.035978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.831698</td>\n",
       "      <td>0.019330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.844526</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.833358</td>\n",
       "      <td>0.026756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.829984</td>\n",
       "      <td>0.028459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.838577</td>\n",
       "      <td>0.020226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.831724</td>\n",
       "      <td>0.019373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.831146</td>\n",
       "      <td>0.019333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.831673</td>\n",
       "      <td>0.019537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.663079</td>\n",
       "      <td>0.038654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.818288</td>\n",
       "      <td>0.025867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.831673</td>\n",
       "      <td>0.019537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.705614     0.035978\n",
       "1              Accuracy         0.831698     0.019330\n",
       "2             Precision         0.844526     0.025668\n",
       "3           Sensitivity         0.833358     0.026756\n",
       "4           Specificity         0.829984     0.028459\n",
       "5              F1 score         0.838577     0.020226\n",
       "6   F1 score (weighted)         0.831724     0.019373\n",
       "7      F1 score (macro)         0.831146     0.019333\n",
       "8     Balanced Accuracy         0.831673     0.019537\n",
       "9                   MCC         0.663079     0.038654\n",
       "10                  NPV         0.818288     0.025867\n",
       "11              ROC_AUC         0.831673     0.019537"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAG8CAYAAADaV3/fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACJIElEQVR4nO3deXhU1f0/8PedJRshhBggCQESDFBAEaxfrYIC9qtt0UpRRLG0omgpINrKEhYRqQQSUBQRqD/1KypFEQSxaK24QN0ed62IgoFE2UIyZCNkm+X+/riZydxlZu4syWRu3q/n8ZGZuXPnnMwk9zPnfM7nCKIoiiAiIiIyMFO0G0BERETU1hjwEBERkeEx4CEiIiLDY8BDREREhseAh4iIiAyPAQ8REREZHgMeIiIiMjwGPERERGR4DHiIiIjI8BjwBHDvvfdCEARcf/31cDqd0W4OERERhaBTBTxTp06FIAgQBAEWiwV9+/bFjBkzUFVVpXl8QUEBnnzySTzxxBP46KOPMH36dNUxe/fuxfjx45GZmYkuXbpg+PDh+Mc//tHWXUFTUxNmz56N9PR0dOnSBddddx2OHTvm9zkOhwP33XcfcnNzkZiYiP79++Nvf/sbXC6X5xhRFPHAAw8gKysLiYmJGDNmDL799lvP46WlpZ6fofK/bdu2tVl/iYiIwiJ2Irfeeqv461//Wjx58qR49OhR8d///rfYu3dv8eabb1Yd+8QTT4jdu3cXP/roI1EURfHQoUNi3759xfnz58uOKygoEO+77z7xgw8+EIuLi8W1a9eKJpNJfPXVV9u0L3/+85/F3r17i3v27BG/+OILcezYseIFF1wgOhwOn89Zvny5eM4554i7d+8WS0pKxG3btonJycnio48+6jmmsLBQ7Nq1q/jyyy+L33zzjXjTTTeJmZmZYm1trSiKouhwOMSTJ0/K/lu2bJnYpUsX8cyZM23aZyIiolB1uoBn/PjxsvvuvfdeMS0tTXbftm3bxIyMDPHLL7+U3f/jjz+KeXl5YlFRkd/XGTdunHjbbbdFosmaqqurRavVKr744oue+44fPy6aTCbxjTfe8Pm8a665Rrz99ttl911//fXilClTRFEURZfLJWZkZIiFhYWexxsbG8Vu3bqJf//7332ed/jw4arzEhERdSSdakpL6ciRI3jjjTdgtVpl90+cOBEnT57E8OHDZff37dsXP/zwA+bPn+/3vDU1NUhLS/N7zNChQ5GcnOzzv6FDh/p87ueffw673Y6rr77ac19WVhbOO+88fPjhhz6fN2rUKLz99ts4dOgQAODrr7/G+++/j3HjxgEASkpKUFZWJjtvfHw8Ro8e7fO8n3/+Ob766itMmzbNb3+JiIiiyRLtBrS33bt3Izk5GU6nE42NjQCANWvWROz827dvx6effoonnnjC73Gvv/467Ha7z8eVQZi3srIyxMXFoXv37rL7e/XqhbKyMp/Py8/PR01NDX72s5/BbDbD6XSioKAAkydP9pzXfR7leX/88UfNcz799NMYPHgwLrvsMp+vS0REFG1RD3gOHDiAV199FSUlJaiqqsLcuXNx8cUXA5CSbF988UV8+eWXKC8vR1JSEs4//3zccsstAUdQfBk7diw2btyI+vp6PPXUUzh06BBmz54dkb7s3bsXU6dOxZNPPul3hAYA+vXrF5HX9CaKIgRB8Pn41q1bsXnzZmzZsgVDhw7FV199hb/85S/IysrCrbfe6jlOeQ5f521oaMCWLVuwZMmSyHWCiIioDUR9SqupqQk5OTm4/fbbVY81NzejpKQEN9xwA4qKijBnzhycPHkSq1atCvn1unTpgry8PAwbNgyPPfYYmpqasGzZsnC6AADYt28ffvvb32LNmjX44x//GPD4cKa0MjIy0NzcrFpdVl5erhqd8TZv3jwsWLAAN998M84//3z84Q9/wF//+lesXLnSc14AqlEiX+fdvn076uvrdfWXiIgomqI+wjNixAiMGDFC87GkpCTV6MFtt92GRYsWwWazIT09PezXX7p0KX7zm99gxowZyMrKCukce/fuxbXXXouioiL86U9/0vWccKa0fv7zn8NqtWLPnj2YNGkSAODkyZPYv3+/32Cwvr4eJpM8xjWbzZ5l6bm5ucjIyMCePXs870lzczP27duHoqIi1fmefvppXHfddejRo4fvjhIREXUAUQ94glVfXw9BEJCUlOTzGLvdrgomfAUQY8aMwdChQ7FixQo8/vjjQbdn7969uOaaa3DPPffghhtu8IyOxMXF+Z12C2dKq1u3bpg2bRrmzJmDc845B2lpaZg7dy7OP/98/O///q/nuF/+8peYMGEC7rrrLgDAb3/7WxQUFKBv374YOnQovvzyS6xZs8YzuiYIAv7yl79gxYoVGDBgAAYMGIAVK1YgKSkJt9xyi6wNxcXF+M9//oPXX3895H4QERG1l5gKeJqbm7FlyxaMHDnSb8Czc+dObN++3XN75MiRuOeee3wef++99+K2225Dfn4++vTpE1SbNm3ahPr6eqxcudIzNQQAo0ePxt69e4M6VzAeeeQRWCwWTJo0CQ0NDfjlL3+JTZs2wWw2e445fPgwbDab5/a6deuwZMkSzJw5E+Xl5cjKysL06dNx//33e46ZP38+GhoaMHPmTFRVVeGSSy7Bm2++ia5du8pe///+7//Qu3dv2YouIiKijkoQRVGMdiPcJk2aJEta9uZwOLBmzRqcPn0aS5cuDWqERxAEJCYmoqqqCg6Ho03aHi2CICA9PR02mw0d6K2MCPYtNhm5b4Cx+8e+xSYj981isahWJId8roicpY05HA488sgjqKiowP333+832AGk6SutKSyHw+E3byYWuVdP2e12w33Q2bfYZOS+AcbuH/sWm4zct0iK+iqtQNzBTllZGZYsWaKaWiEiIiIKJOojPI2NjbJl0OXl5SgtLUVycjK6d++ONWvWoKSkBPn5+XC5XKiurgYAJCcnw2KJevOJiIgoBkQ9Yjh8+LCsDs5zzz0HQEr6vfHGG/HZZ58BgGo7h6VLlwYs7kdEREQEdICAZ+jQoXjppZd8Pu7vMSIiIiI9OnwODxEREVG4GPAQERGR4THgISIiIsNjwENERESGx4CHiIiIDI8BDxERERkeAx4iIiIyPAY8REREZHgMeIiIiMjwGPAQERGR4THgISIiIsNjwENERESGx4CHiIiIDI8BDxERERkeAx4iIiIyPAY8REREZHgMeIiIiMjwGPAQERGR4THgISIiIsNjwENERESGx4CHiIiIDI8BDxERERkeAx4iIiIyPAY8REREZHgMeIiIiMjwGPAQERGR4THgISIiIsNjwENERESGx4CHiIiIDI8BDxERERkeAx4iIiIyPEu0G3DgwAG8+uqrKCkpQVVVFebOnYuLL77Y8/jHH3+Mt956C0eOHMGZM2ewatUq5OTkRK/BREREFHOiPsLT1NSEnJwc3H777T4fHzRoEG655ZZ2bhkREREZRdRHeEaMGIERI0b4fPyKK64AAJSXl+s+p91uh91u99wWBAGJiYkQBAGCIITe2A7I3R+j9Qtg32KVkfsGGLt/7Fts6gx9i4SoBzxtYefOndi+fbvndm5uLoqKipCenh7FVrWtjIyMaDehzbBvscnIfQOM3T/2LTYZuW+RYMiAZ8KECbj22ms9t90Ros1mk438GIEgCMjIyEBZWRlEUYx2cyKKfYtNRu4bYOz+sW+xych9s1qtERusMGTAY7VaYbVaVfeLomi4D4Mb+xab2LfYZeT+sW+xyYh9i2R/op60TERERNTWGPAQERGR4UV9SquxsRFlZWWe2+Xl5SgtLUVycjLS09NRV1cHm82GyspKAMCJEycAAKmpqUhNTY1Gk4mIiCjGRD3gOXz4MJYtW+a5/dxzzwEARo8ejVmzZuGzzz7Dhg0bPI8/+uijAICJEydi0qRJ7dpWIiIiik1RD3iGDh2Kl156yefjY8aMwZgxY9qvQURERGQ4zOEhIiIiw2PAQ0RERIbHgIeIiIgMjwEPERERGR4DHiIiIjI8BjxERERkeAx4iIiIyPAY8BAREZHhMeAhIiIiw2PAQ0RERIbHgIeIiIgMjwEPERERGR4DHiIiIjI8BjxERERkeAx4iIiIyPAY8BAREZHhMeAhIiIiw2PAQ0RERIbHgIeIiIgMjwEPERERGR4DHiIiIjI8S7BP+Pbbb/HFF1/g4MGDqKysRHNzM7p27Yrs7Gycd955uPTSS5GSktIWbSUiIiIKie6AZ+/evdi1axdOnDiBhIQE9OvXD/3790dcXBzq6urw008/4ZNPPsFzzz2HSy+9FDfddBN69OjRlm0nIiIi0kVXwJOfn4/y8nJcfvnlmDVrFvr37w+TST0bVldXh08++QT79u3DX//6V9x11134xS9+EfFGExEREQVDV8Bz4YUX4re//S2SkpL8HpecnIwrr7wSV155JQ4cOIC6urqINJKIiIgoHLoCnptuuinoEw8ZMiTo5xARERG1Ba7SIiIiIsPTNcJz4MCBoE7K0R0iIiLqSHQFPMuWLQvqpFu3bg2pMURERERtQfey9KSkJFx66aU4//zzIQhCW7aJiIiozYm1VXBtLASqK4HUNJhmLISQkhrtZlEb0RXwzJw5E3v37sXbb7+Nr7/+GmPHjsWYMWOQnp4edgMOHDiAV199FSUlJaiqqsLcuXNx8cUXex4XRRHbtm3D22+/jbq6OgwYMADTpk1Dnz59wn5tIiLqvFwbC4Hi76QbtlNwbVwJc35RdBtFbUZXwDN69GiMHj0ap06dwjvvvIO3334b27dvx9ChQ/HLX/4SF198MSyWoIs2AwCampqQk5ODsWPH4uGHH1Y9vmvXLrz22muYOXMmMjMzsWPHDixfvhyPPvooEhMTQ3pNIiIiVFf6v02GElSU0qtXL0yePBk33XQTvvrqK7zzzjt4/PHHkZCQgIkTJ2LcuHFBN2DEiBEYMWKE5mOiKOL111/HhAkTcMkllwAAZs2ahTvvvBPvv/8+rrrqKs3n2e122O12z21BEJCYmAhBEAw3Hefuj9H6BbBvscrIfQOM3b9O17fUNMB2SnY7FvveGd63SAhpWMZkMuHCCy/EwIEDsXv3brzyyis4cOBASAGPP+Xl5aiursYFF1zguc9qtWLIkCE4ePCgz4Bn586d2L59u+d2bm4uioqKIjIF11FlZGREuwlthn2LTUbuG2Ds/nWWvjmXrYWtYB6clTaY09KRvng1zKlpUWxdeIz8vkVCSAHPV199hXfffRefffYZ4uLicOWVV+Lqq6+OdNtQXV0NAOjWrZvs/m7dusFms/l83oQJE3Dttdd6brsjRJvNJhv5MQJBEJCRkYGysjKIohjt5kQU+xabjNw3wNj965R9u3c5BAAuAOUNTUDDyWg1MWRGft+sVmvEBit0Bzzl5eV45513sG/fPlRWVmLIkCGYPn06fvGLXyAuLi4ijfFFOaQV6A21Wq2wWq2q+0VRNNyHwY19i03sW+wycv/Yt9hkxL5Fsj+66/B89913SEtLw+jRozF27Fj06tUrYo3wJTU1FYA00tO9e3fP/bW1tapRHyIiokjhknXj0V1pOTExEX379sWPP/6ITZs2+TxWEATMnz8/Io3r2bMnUlNT8d///he5ubkAAIfDgQMHDuD3v/99RF6DiIhIiUvWjUdXwOOePzt69GjAY4PNqG5sbERZWZnndnl5OUpLS5GcnIz09HSMGzcOO3fuRGZmJjIyMrBz507Ex8dj1KhRQb0OERGRblyybji6Ap7169e3WQMOHz4s27riueeeAyDV/pk1axbGjx+P5uZmPPXUUzh79izy8vKwePFi1uAhIqK2o7FknWJbaNUCI2jo0KF46aWXfD4uCAImTZqESZMmtWOriIioMzPNWAjXxpWyHB6KbWEHPCdOnMBPP/2ElJQUDB482JCFj4io42AyKbUHISWVOTsGozvgeeONN/DBBx/AYrHg8ssvx5VXXonNmzdj9+7dnmVjeXl5WLJkCRISEtqswUTUuTGZlIhCoSvg2bdvH5555hn06NEDCQkJeOKJJ1BRUYHXXnsNv/zlL9GvXz+UlJTg3Xffxe7duzFx4sS2bjcRdVZMJiWiEOgKeN58801ceumluOeeeyAIAl555RVs3boV1113HSZPnuw5LikpCR999BEDHiJqO0wmJaIQmPQcdOLECVxxxRWe/JyxY8fC5XLh/PPPlx03bNgwv1s+EBGFyzRjIZA3GEjvBeQNZjIpEemia4Snvr4eKSkpnttdu3YFII3oeEtKSkJjY2MEm0dEJMdkUiIKha4RHiIiIqJYpnuV1rfffovTp08DaN3M69tvv0VFRYXnmJMnY2+XWSIiIjI+3QHPli1bVPdt3rw5oo0hIqLOR1lbSZgyE+LmDay1RBGlK+BZunRpW7eDiIg6KWVtJbFwPtDY4LnNWksUCboCniFDhrR1O4iIqLNS1lJqbvL/OFEImLRMRETRpaylFBfv/3GiEOga4XG5XNi3bx969erlGe0RRRGrVq2SHZeUlIRZs2bBZGIcRURE+ig36hSmzIK4eT037qSI0hXwfPHFF/h//+//oaiodQ5VFEV88cUXSE1NhcUinaampgYjRozAqFGj2qa1RERkOJq1lTpAzg43qjUWXQHP3r17cckll6Bv376qx/Lz89G/f38AwHPPPYcPP/yQAQ8REcU8blRrLLrmng4fPoyLLroo4HGDBw9GSUlJ2I0iIiKKFLG2Co7C+TgxbTwchfMh1lbreyI3qjUUXSM8NTU1SE9Pl90nCAJ+85vfIDU11XNf165dUVtbG9EGEhFRxxQrUz7ukRonAOC4/pEablRrKLpGeKxWq2qPLEEQMHXqVKSltX4AGhsbPfk8RERkbJ4pH9spoPg7KfG4IwpxpIYb1RqLruikV69eOHToEIYPH+73uEOHDqFXr16RaBcREXV0sTLlE+JIDTeqNRZdIzzDhw/Hnj17UFNT4/OY6upq7NmzBxdeeGHEGkdERB1Ycor/2x2Ee6TGnNGbIzWdmK4RnmuuuQbvvPMOlixZgilTpmD48OGIi4sDADQ3N+PLL7/07Ks1bty4tmstERHFlI6Q5yOkpMKyYBUyMzNx8uRJiKLYIdpF7UtXwNOtWzfMnz8fq1evxsMPPwyTyYSUFCmSr62thcvl8hzjvp+IiAyurtb/bXTcpd0dtV3UdnRnGA8cOBBr167FW2+9hW+++QY2mw0A0LdvXwwbNgy//OUvkZSU1GYNJSKituesOg1H4Xx9Ix96cmP85PmItVVwrVsOHCuV7sjOgWn2kvYZaYmV/COKmKCWVCUlJeG6667Ddddd11btISKiKLKtmK975EO5JYRmboyfoMi1sRAo/aH1sdIf2m+khUvOO52gN7266667UFpaqvnYTz/9hLvuuivcNhERURDE2io4C+bAOeMG6b+COfqL6yk4K23yO/yMfLhXMZlXPglzfpHmyIzfpd1a5y4tDrntweCS884n6KI5FRUVcDgcmo/Z7XZUVFSE3SgiItIvkiMl5rR0OMuOt95RVwvnwjtDTuz1u7Q7OUU+ygIADjtc6x6EefHDwTU8SFxy3vlEdFvzU6dOITExMZKnJCKiQLRGSkLMSUlfvLp15CMhEWhsaP/Cgi05PWJtFZxF+XAuvBPOovx2Gfkh49K9eei+ffs8t5966ilVYNPc3Iwff/wRQ4YMiWwLiYjIP2U+ivu+EJhT02BZsAqiKEojO40NrQ9GOrFXY1WXN66kokjSFfA0NzfL9sg6e/Ys7Ha77Bir1YrLLrsMkyZNimwLiYjIL9OMhXCte1C+2slPToruGjRBJPaGtOJKK1BreS6AsFdSKfvpXLY2qOeTsegKeK6++mpcffXVAIBZs2Zhzpw5yMnJact2ERGRTkJKalA5L1ojJ6YZC+BctxxHj/8IiCKQnQNh6j0QN6/3vwrL+5xB5hF5VnlV2oD6OiApGUhLb32dMFdSKftpK5gH3Ls8qHOQcQSdtLx+/fq2aIdfDQ0N2Lp1Kz755BPU1NQgNzcXU6dORV5eXru3hYgo5mmMnLjWLVcFLOLm9fqnkELII/KXOCzWVgEOB2CxSncEGLXS0yZnpQ1CgKewArNxhby1eU1NDSoqKtDc3Kx6LNJ5PH//+99x9OhR3HXXXUhLS8N//vMfPPjgg3jkkUdku7UTUSv+4SaftEZOSovVxwUzhaS14iqM2jaqESOLJfjPr6Kf5rR0uPS8LvOGDCnogKeqqgqPP/449u/f7/OYrVu3htUob83Nzfj4448xf/58TyA1adIkfPrpp3jzzTdx8803q55jt9tlOUaCICAxMRGCIEAQAsX3scXdH6P1C2DfwuXU+MNtWbCqzV7PzcjvG9Cx+ifWVMHpVfjPPHORrqDAPHMRnBtWtE4lVdqk0RSl1DT9/VQeFp8gtSfUn5PGKFSw5/L0s7oSQmoa0hevRkWj+kt6pF+3vXWkz2SkRbJPQQc8Tz/9NEpKSvD73/8e/fr1g9VqjVhjtDidTrhcLtXrxMXF4fvvv9d8zs6dO7F9+3bP7dzcXBQVFSE9Pb1N2xpNGRkZ0W5Cm2HfQnOirhZOr9vmulpkZma22espGfl9AwL3z1l1GrYV8+GstMGclo70xath9jHiEcyx3k6tuQ9Or6DW/NRD6LX66cCNz8wE1j6PU/OmofnA1/KVWF6sJgE9EuN1teVEQ73s8waXC1i1ACY//fHX71M9M9DsNToT1zMDvYL9/Lb001ugT2VEXjdKjP47Fy5BFEUxmCdMmzYNU6ZMwdixY9uqTSr33XcfLBYL7r77bqSmpuL999/H+vXrkZGRgbVr1Vn3vkZ4bDabanVZrBMEARkZGSgrK0OQb2WHx76Fx1HotUUAAOQNbrcRHqO+b4D//slGXOpq5YGEn59/KO+VWFMFZ/40wOH1Ny29FyyFT+nui2PBHdqrpLzp/Nyo+qDjHP76LdZWy0ehWhKa9Y5iKen9XHpeN8hRs2gy8u+c1WqN2GBFSDk855xzTkReXK+77roLGzduxJ///GeYTCbk5uZi5MiRKCkp0TzearVqjjyJomi4D4Mb+xab2rJvWvsctefP0Yjvmzsv6kRdLZzJKaq8KOfGlb4v+tWVvn8eGtMogX52zo0r5cEOAKSmBfcz97Us3FtpsRQYBcgDk33eqivlbfPVH3/97toN5vwiOIvygcoKKXisrIBzw4qwcmoCfi5bXlf5nFhgyN+5CPYn6IDn0ksvxRdffIFhw4ZFrBGBZGRkYNmyZWhsbERDQwO6d++ORx55BD179my3NhDFGpbOjzx3Qqs0dXNcndDqL8nX37RQKMuvla9lsQa9ikmYMhNi4XyfU1oApMDFdkq2fN07GV6YMhPi5g2tgfXC1VLg4x34+epPKLutt+y11dFHXajj0RXwHDlyxPPvSy+9FE888QRcLhcuuugiJCcnq47v379/5FroJSEhAQkJCairq8PXX3+NKVOmtMnrEBFpClQIz9eISXwChCmzfJ5W167jSsrXyskLOggQN2/wHeykdAfqz8iTmSttqlVMsoDJExTp609Iu6077Fw5RSHRFfAsXKj+EP773//Gv//9b83jI7lKCwC++uorAEBWVhbKysrw/PPPIysrC2PGjIno6xAR+RVgRMJzAS8tlk/pNDVKBfy8LtLBlg1QHi9MmaW7KKBP/kakmhoAp2IRd5VNml7y1tykOqfe0UU9x5lmLIQr/3ZV4KUHKy2TN10Bz4wZM9q6HX7V19fjhRdewOnTp5GcnIxLLrkEkydPhsUSchkhIqKgeaaAmpuBuDjVqI37Au5ceKd6pKe0WLbruKrey+LpMBU84TPoUY2sLP8rkJMH08LVPp8TMKjyl8PT1KhxQo18CuVdYdTe0SKkpErFB70Dnvo6Xc/1VWmZNao6J10RQ7RHUi677DJcdtllUW0DEZFsCqixQTVq46EVSChyYVSjK40Nmjkynoux8niH3bODua/n+Cqi57ngV9qkHdGTkoGq04AYqCyf1g/FJZ0jOSX0kaZAkpLlU29J6lQKTT4qLbO4YOfEIRIiIr10bmap2swTonyEoiUwUQVF1ZWaIz9ITvG9s7jWc9wXcOXUz5GD0qonh0NexTg7B8LdSyEuuzu0oCc5BeaVTwb/PL3S0uVTaWk6lyn7qrQc5qakFJuCDng2bNjg8zGTyYSkpCTk5eXh4osv5pQTERmLztVUys08nUX5qlVLphkLpWDGe+QiNU1z5MdzTEKiFKx45wdpPcd9++wZ+f0ul9QOi1V1vNC1K0SzGXB4BTwWa0ufy6Geu/Ki8XMIN0fJ+/iQkro1npe+eDXKG5rC3pSUwhOtKcWgI5Jvv/0W9fX1qK+vh8lkQteuXXHmzBm4XC4kJSUBAF577TVkZWVh6dKlSE1NjXSbiYiiwn0BNXvV4dGjNfenCYiLhzBlFoSUVJgKnpDvFl5pUwcp3hz2lrhDACxmIDu39aKudQHXW2g1OQWuxX9W1/WxWKRl5soCh2YLkNUHOFoKQASOlsB1/CcIXbtKm5AeK20Z0WoJkvxMG3kuft6J3j6WwPvLV9LinRQtCIJUxbnhZMgBFEVGtKYUgw545syZg4ceegh33nknfvGLX8BkMsHlcuGjjz7CP/7xD9x7771wOp146KGH8MILL0Q94ZmIKBS+voVaFqxCZmYmTp48qbsomq/cH0+Ss3dxPX+8p8UcDs+GmsKUmRBXzmtJNBaApiaItdXSiI6WzGwgPqF1as3h0H7txgZpas5slgc8fXKBsmPwBDRNjRAL50HMzpFPlXnzMW0ku/gpjm+rCyNrVEVZlKYUgw54nnvuOfz2t7+VJRGbTCaMHDkSNTU1ePbZZ/Hggw9i/Pjx+Oc//xnRxhIRhSKUIfSIXmwD/YEP9Q9+y/PEzRu8VlWJwNEj0giGxay9KajZIuuLc+Gdvl/jWKk82ElIhGn2ErjmTZUf19gAHDno+zy+po189d3fVB2iNy1CERClKUVTsE84fPgwsrOzNR/r06cPSktLAQA5OTk4c8bP0CwRURDE2io4i/LhXHgnnEX50giGTp7gxXbKs7IpIB1Bie42Kf+g19XKnxPqH3z387SChupKIDtX+3nHSuCccYP0X8EcKSlar+QUKbCIi1c/pjWiZLECeYN9Txsp++59vMbPzf0zDuk9pQ7BNGMhkDcYSO/l/7MR6dcN9gmJiYn49ttvNR/bv38/EhMTAQDNzc2efxMRhSusC1woIyrKi61GUKK3TZ4/8GnpgGCSRkO8nmOasVCdSKwlPkE6zmyR/l1pk6bDErtot3/iVOn1lNyJzw677ykoQGpzdo76vACEBaulJOoA7TUVPQ1zy/SdFtXFz+t404yF8tdoWboPgCutYph7StG88km/n41IC3pKa9SoUdi1axdEUcSll16Kbt26oaamBh9++CH++c9/Yty4cQCk7Sh69+4d8QYTUScVzgUuyCF0sbaqJUemJQjJztH+FqqzTfJcHZvqOUJKKpCT53vjUUC68N+1BNj+jJTg63RI01iVFVLw4y0+oXUVmGyZuQBYLOrk5LpaQBDkhQUFoaVmT7Vmgq+pd19g3VbtIotuXbsFvJgp82nco2ae/CJlDR73z5grrShIQQc8t9xyC6qqqvDKK6/glVdekT02cuRITJ48GQAwcOBADB8+PBJtJCIK6wIX7KocadWQ18hH2bHItEkrIGp5jny38dPq3JvGBmDd37QrINub5bfdgYZy2weI0oiNclTHnbhcfbr1vm5p+vJk/FVrTtV5Di/K3CnVKJLWz4srrUgHQQxx7/Vjx47hwIEDqKurQ3JyMoYMGeIzt6ejqKiogF3vMs0YIQhC0CtGYgX7Fpvaqm9aIw1CSmqbJK9qjlpYrJ7pHXNjPZzJKZr7Wfl7bVU9noREze0kVMcFIpjkIzkWqzRi9NMRddDTJxcoP9m6oqtPDkx/WSatxvIOhHIGSKNB3u3IG6xK3Ja9L+5coLpary00VgY8h6zvyp99Wg9pKjDM95e/c7HJarWiR48eETlXyJUBs7OzO3yAQ0TG4WspcZssXfa1NURLQOAEAByHuHl9UK+lNSqhdfHWDBR8UQY77rYWf6edWHzymNeUlijt5K4VQBwrASDI79MYoQq4xDvYqUjlz54rryhCgk5aJiLqUNogeVWVLBvgtfWvINP37dsTRCir1ZstLSMvVum/PrmA2c+fcWWujp8+QNlmZUVnILQ8GR3J396UScwAuBqLIkLXCM9NN92EgoIC5OXl4aabbvJ7rCAIePHFFyPSOCKigNogeVVWBbn0B+1aNl6vpXeUyec+Wb5Ge7Jz5dNMfXLVW1b4ahsAWOPkOT/KxGSvPvjcq8u9vUSIeTLB5tooR4xUNYI60Gos1gKKLboCnokTJyItTfqluOGGGyAIQoBnEBG1j0AX1FAvSp6VVQVz5EFHfALM3c+Rby2hd5TJ1z5ZtlNSDo3FIh2TmCTl2biTkQVBmnqaeo/8+coVX57GC8C5PwOamoCjR1rvF0VpdZdXX9DUJAUVqgTnFjl5YU0Rhl3VuAOvxuKu67FFV8Bz4403ev49adKkNmsMEVGwAl1Qw74oKUc+unZD1tO75AmiPi7KsmArOUU9beSttBg+p7xEUdqS4umH4TRbWndhd/oY3YmLl4K1/Gn++9bUKA+IlNqxKJwvHXo1FmsBxZSIbmd+4MABbNu2DUuXLo3kaYmIQhfiRUkWrHhTjDD4q9mjWmLt/xUDN+poiY6WozX3xt9GpIHEJ3SI0YoOve9VBx59IrWIBjy1tbU4cOBAJE9JRBSeEC9Kqk0tW5Z6m2cuUh/nPeXVsqEnAH3BVVqPlqmpCC4ndjql/B6tmj1+Ca3tcDjgvGsS0KUrkJYetfyUjpwn06FHn0glogEPEVFbCfXCF/JFSWNkx5xfpM5h9DeC5K8on1tKqjTVpWdFVTB0LWkXpEDO2TJCZW9ujbucDlk152jlp3TkPJkOPfpEKgx4iCgmhHrhC/mi5Csvp6YKp9bcB0d5mXRfcorPESRZsJWcIuXLOJ3y1zlWql392GeHNFZa6WWxAE5Xa90eUWxNjPaVtOwWrZ3KmSdDEcI6PEQdQDg7gXca7Xzh87Wjs3PjSjQf+NpTFwZOp1Szx2QCEhIhTJnlOYdsk8TFDwO5A7Vfa/YSfXV/AH3BjrLgoMXSsjHn/6lqCermFcipNk1dPN3zmY34ZznIOj5EvjDgIeoAwtoJvLNo9wufj8BCGWid+ElaWu5ySSupNq2Vn8UrAEBTozRC4y2zj/R+JyVLQY+y2KBfPqIX5d5a2bmtu1Jb44I4P1qDJe+pQI3l9e7PrN7Pst7AyFfgSRQsXb9Zc+fO1XWyhoaGwAcRkRqH7QNOkwhTZkIsnC9NvcTFy0ZS9J4jmNeHw9E6zWQ7BVf+NGl/qq6KKSzlFFXpD3AW5Uvt3bxBWm7uKz8nIREwm+X5NmaLdH9iktQWfyM6OXnSlJjy/MrneC+t75mpf7VXzgCYZi/Rt2Go+zOr87Osd4qSeTIUKbpGeJKTk9G1a9eA//Xs2RNDhgxp6zYTGQ+H7QOODIibN8hHUjavD/ocvoi1VXAt/rPsuZ5aN27u/alEIG7IBdLqqoREaI4EFX8Hcdk90vH+kpETk9R1fpwOqZ8N9YrARZCCIQjSqEvOAGDiVHXApcX789RQH/h4N+8VZ140t95wv4bez3IEgnxOBVMwdI3wPPDAA23cDKLOjctbob7glRZDrK32vcRb6wKpvK/SJi3PVu7krdjVG46WIEOPulr0Wv8Cjt7zB6Cywvdxyg09tVRXAecO0l7JpUwitlhagyeHQ7r9+HLfryOYgHN6qD9PvlaOxcWrX9NHECLbekPxmdX9WY5ADZuOvIKLOh6u0iLqADhsD/UF0GGXX8D0XCCVx9TXtQYl3ver/q2RC2M2t7TDAdkoTmoanFWnWyojh0uUAoRHl6qnmeLi5UGYcs+sSpvvIE0wQVj6GEy9+6oe8gQklTbp55OUDKSlQ5gyC2LhPPk5/QQhvj6zej/LEQnyORVMQdAV8NhsNqSnpwd98srKSs8eXERE/phmLJTyZLyngLwuYKYZC6X9ptxTTQ6HfAQI6ouo36BARjEtJZgUm26apJioZTPOE7f9Vj1VlTMAKDsmf71AS8jj4qX2l59UPCBAWLBaEYAozlNf5/u83dMgbl4Pp0Yuk9+AZMX/g/mph9DcsuS+LUcaIxLks9IxBUFXDs8999yDZ555BmVlZQGPdTgc+OijjzBv3jy88847YTeQiDoHISVVSsL1lprmydNwrZwnBRQOu/Rf6Q+qHB3ZMvD8IiAt+C9qAACz4k+j6JJyh5oapZEY5SooCMDE24CMbKmAn8UqbcwZaAn5Ob2k/6sqIovS6ExSF/Vz0npIq5aSkn2ft+5MSLlMQkoqeq1+GpbCp1pXdXVgXMFFwdA1wnPffffh2WefxRtvvIG8vDwMHToUubm56NatG6xWK+rq6nDq1CkcOnQIX3/9NRobGzFu3Dhce+21bd1+IooRelZQtU63VAD1Z4FKm5RM7GuURjGFoXwNzWkai1Uq9AdIOTx1terzK6ePAvcOeGhR4MOUTvwo5RhpnbG2CqjSmKKprZYCuZRU3zlEOnNxAunI2zoAnAqm4OgKeAYPHozCwkJ8+eWX2LNnD/71r3+huVn5DQfo2bMnfvWrX+Gqq65C9+7dI95YIopdehJM3RcwZ1G+vukoxRSG8jXEzeulBGVFXop58cOtF3NXS9Jvc1PrvyNB71YRPraAcG0s1E5Idq8WyxkgjW5UVwbeviLEqZ5A71lHD4iIvAWVtDxixAiMGDECDocDpaWlqKqqQnNzM7p27Yrs7Gzm6xCRb8EkmPp7zNfqI1+v4SPPQ7U5qNYqpWgKNCrz02Gg/yCYFq6Ga84f1Y8nJErTXimp0kagC+8MPigJ8J5xlRTFkpBWaVksFuTl5QU+MAKcTie2bduG9957D9XV1ejevTvGjBmD66+/HiYTC0UTATHyTTuYBFN/m252PwfmlU96boq1VXCtW95SgE8xFZWc0rKE2yrdzs5pDZKUF/OOFOwAgTcedbla83PMFql+j7fGhtapO++gZPF0mAqe0Pf5CPSeRXmVVEx87qnD6PARw65du7Bnzx5MmzYNjzzyCKZMmYJXX30Vb7zxRrSbRtRhxMLWFMEkmMqOVRa4a0lE9iQz598hVUR22CFbyZSQKBXlcz/msHsK6Ym1VeqCfx2KoPp5CQ88Lt1WftE7ctD3dhSVNr/bQAQS8D3TUWSwLYsDxsLnnjqODl+H59ChQ7joootw4YUXApDyhN5//30cPnzY53Psdjvs9tb5c0EQkJiYCEEQICj3sYlx7v4YrV8A+xYUjW/akTi3WFMFp9cyb/PMRQG/Qfvqm9CtO0wLVul6Xe9jxdpqODesaE1krrLBWTAHOHlUY3WTl8YG6Rhv7kKE/rZ76BBEaXpI+fNesAqOwvnyqTj36rGERHXO09kzQJ9czW0gtD4fyvcu0HtmnrlIem+8Px+K8zo1pr0sOj8HAQXxueffk9gUyT51+IDnZz/7Gfbs2YMTJ04gKysLpaWlOHjwIG699Vafz9m5cye2b9/uuZ2bm4uioqKQagnFioyMjGg3oc2wb4Gd6pmBZq+LWlzPDPTKzAz/vGvug9PrYmV+6iH0Wv20rueG0jdn1WnYVsyHs9IGc1o60hevBhJ6whYXh+YztdJy8MYG4LSfCsd+CA1nIRaH9tx2V/wdhMcfhCkh0fPz6D5rESpNAuzWONXSeHNKKpwOhzyQczqRtWwtTk77HcTG1i0l4tLS/X4+lO+d1vtiTk0DMjOBtc/77caJulp4b35hrqtFZgQ+m0Bon3v+Pem8BFEMVCgiukRRxAsvvIBdu3bBZDLB5XLh5ptvxoQJE3w+x9cIj81mk91vBIIgICMjA2VlZejgb2XQ2Df9PKMgQYzE6OFYcId8dCC9FyyFT/l9Tjh9U41e5A2W/u9jJZPilaEuIOjefwpSPktttf/tIDq6+ATfo1oJiVK+knfAY7HC8vcdcCy7W17JuU8uLEsfkz1drKmCa+NKmOpq4UpOgcnrM6T1vugdpQnnuYEE87nn35PYZLVaIzZY0eFHeD788EO89957uPvuu9GnTx+UlpZi06ZNnuRlLVarFVarVXW/KIqG+zC4sW+xKWJ969pNtTomIufVSFrVe96Q+hZqEmxCorS3U8EceUAjiq0BgMUi5f/EcsDjbwrPvSLLvcM7AGTnSO/ByWPyY4+WSsGsV6Kvc+NKoPi7ltGY43BuWNH6mdJ4X/S+t1pbSETs9zmEzz3/nsSWSPYnIknLzc3NOH78OFyRrGHRYvPmzRg/fjxGjhyJvn374oorrsA111yDV155JeKvRURy7V7JVisJNlANGcEEYcFq6Zu9v1Gt6srW/vhK8o0GQZCWxMfF6znY90Np6TDNXiJ/v2Yv8XGwqE709Rds6t0BXavFiurXXEVF0RJ0wPOvf/1Llh9z5MgRzJgxA/feey/uuece2Gy2iDawqalJtfzcZDIZLool6pgi83umd6WOVoAlTJkp1d7xeXKXVGAQkFZl+VJXK21PAQCZfULrSFs492cwrfx/QN/+0rYRCYmt20f0yZUf2ydHvWrNYm39WfkKLtzL07W4Axs/QQ23cCAjCDrgeeedd9ClS+v+Lv/4xz+QnJyMW2+9FaIoYseOHRFt4M9//nPs2LEDX3zxBcrLy/HJJ59g9+7d+J//+Z+Ivg5RZ+UvGInUsl+959G6YIubHtOuOOyt+Ds47xwPHD3i+5jGBs/r42hpSP0ImzUOqlGa6srWn09lhdTOtHRpquamO6QRIDenU3sPrepKuDau9B1Ieo/8KAOmlsDGHdSYM3qrghqO0pARBD2ua7PZ0Lt3bwBAQ0MDDhw4gL/85S+45JJLkJycjK1bt0a0gbfffju2bt2Kp556CjU1NUhLS8NVV12FiRMnRvR1iDorv9VyldMcxd/DmT9Nmj6JYMVeb8picjhW4vNYxTN1HhfssRGk2nQUUj99/XweXy7fgPTET+qAxWGXAjnbKWm3+Zw81XvjveeUWFutyqlxH2NZsAqZmZk4efIkR9HJcIIOeOx2O8xmMwCpRo4oijj//PMBAD169EB1dXVEG5iYmIipU6di6tSpET0vEbVQXmwrK6RaNdWVGsX5RGkUorICrsXTpUrGeircBlFlWRmA+c1bMQKtekB1tXAd/1F7L7GEJGmKyh0oeT+/ZZ8tf1s8cMNN6qyCntJKT0/Hd99Jf4w+/fRT5OTkICkpCQBQW1vr+TcRxQhl8FF/tnX6qbEBPgMOrymiQFNdnhyQtHRphKKlAKDmFIxq9KcTjjQ0NkAsnO/jsXrP9BJyfGzxE2AEra0qHxN1ZEEHPJdffjlefvll5Ofn46233sLll1/ueezw4cMRKyhFRO1DmZCqyhHRs6Kp5QIr1lbBUTgfJ6aNh6Nwvudi6hlVSOshBUqVFVKgtHi6+sIb4s7eMcVi9Z+IDfje28vr/WlddaYow6FnBI3bMVAnE/SU1vXXXw+z2YyDBw/i4osvxm9+8xvPY0ePHsUll1wS0QYSUdtSTnE4i/LltWqyc6Sgp9IGVJ3WTiBW7EDuruWimlrR2NfJPVLkmSJLTgFyBki7gbdBqYuos1hhKnpaXTNIKS5ee0orrbUIm/u985WXoynKG34SRUvQAY8gCPjd736n+Vh+fn647SGidqS127RWoTh3fo5z4Z3yXByTSbowu/eoqlSUpVBeTP3tAO4V/CBvMND3XHkRvViU1U/az8s7SHQ64Zo3VZ6MrGS2QFiwGuKmtVLStsMJWMxAdq5mMBNUXk4wu9YTGUjI1bfq6+tx6NAhnDlzBiNGjEByssZSSSLq0Hyt0PJ58VReLN2jEO5pKuUKorpaiLXVnoBJFkzV1WqPYADS48kp4XWuIyg/oR4RE12B05KsVph694XTYpG2iwCk/7fs9h4OrYAWaMnt2Vgo7X2VnBLcKjw/tIJqLmunaAgp4Nm+fTt27dqF5mZpieXKlSuRnJyMv/3tbxg2bJjPESAi6mCCnN5QXixRaZMHLU2NUm6K+yLf2ADXugelKTHFBU82DaMMflLT1G0xmWJviivUHdntdimnqQ2mn3yNBgWcjgyR37IHRO0o6IDn3//+N7Zv346rr74aI0aMQGFhoeexCy+8EJ988gkDHqJYoRyxSU5pXZKu8W08YL6PKEI1fOE9LeWdq+Mr+Gm537VxpbxtnWmxltMh9T/I6aewRlPaKreHOUPUQQQd8Lzxxhu49tprMWXKFNXeWe6CVUQUG1QjNg6H/Nv44ukwFTzh86Lpef6Rg/pHX5SJyknJwNkz0miI0yndP+eP0nSZNa61WF+gasuxytfIVXUlTAtXt74/ySmAwyHlUfkIZlzrlrcGmLZTcK17EObFD+trR1vl9rRzzhCn0MiXoJell5eX44ILLtB8LDExEfX19WE3iojCo7fWinLLAFWhwcYGv8uWhZRUmGYsUG98KegsFujO/WlqVO+D1dykXZk4lsXFyW9bLED/QYBZ47tndaU0HejO4Sk7JgUzGsvJ3e+3Ksn7WKnupvnbWiIc7b0Pl2rZfUvpA0fhfDg5utSpBT3Ck5SUhJqaGs3HysvLkZJigERDohgXct6E1iqqABcJ18ZCef5NQiKsWX1hP3JQfbB3fk+nIbTUbhRagxc394hafALgVD5m979Kzet9kb3fobayjbaWaPfKzn5KH9gK5gH3Lm+/tlCHEvQIz3nnnYddu3ahsbHRc58gCHA6ndizZ4/P0R8iakdB5k14RggqbeqCeIGmIJTnTk6RllB7M1uknb+tigJ5nYIo5TaJLt/Tfl26to6CKIsI+uL9vvh6f/3tkq5spY+ikTHHz+fVqSybQJ1K0AHPTTfdBJvNhnvvvRfPPfccACmvZ9GiRSgrK+OmnkQdgfKPfoCgRbZbt+iSlpfrnYLQeC1XrWIUWBCAUyd8Vw/u7Fp2R/e7XYSb1vuiXMJvsUjHzF4S8KXdwa4r/w5plVbZ8ZiuwCybQlOUSTB7FW2kzifoKa2MjAw8+OCDePbZZ/Hvf/8bAPCf//wHQ4cOxezZs5Gezg8UUbT5qrXik8YojXnlkz4PlyWGuisj19W2JtaeVlQQDnV5dmeQkCh7f4QpM6V9tJqb1CNCJhNMC1fBtbEQrpXzfL+32bm6p5F8TofFaL6Lv53h0xevRnkDg+7OKqQ6PNnZ2Vi8eDHsdjvOnDmD5ORkxCmT8YioXbiOl0IszJcukHHxEBashql33+DyJoJcSaPa0TxvMMwrn5SmxcLMJTGU+AQpGds74LNYgOxcKUDUWEUkbt7guyBjXLxmfpYq2Vy1y70fvgIbA1Rg9g5+BEGAOTUNaOBK4s4q6Cktb1arFWlpaQx2iKJILMyXLpAul7TL9rK7g86/EKbMlIb/TS1/Eg5/D+fsm+A6/pP2E5QXSffWElqJyp2OIP0cExIhLHxIPUWVMwDmxQ97Vsaplkxr5Zm4z7dgtXZ+VpBTmH6Ptca1y2oqovYW9AjP9u3bAx7DPB6idqTMixFdQVWzFWurIK6cJy0N99wpSsFT4Txg3Vb1k5JT5CNCZ8/43wizEwt6erG+Tn47IRHCgiKIhfkQl81W78GVmiafBouLhzBlVsjty1q2FuUNTRFbpUXUUQQd8Gzbti3gMQx4iNqR1q7ailEAf8XYXBsL5cGOt+am1ueeLgeqqyCtOlIcp1xu3amJgKs1YDSt29qyo3mVKvdGqjItf2+QkCR/P+12iA/cA81S0y35P66NK1uf09gAcfN6QGfAy2kf6iyCDni2blV/26urq8Mnn3yC119/HQsWLIhIw4g6O70VY4UFqyEuu1te30YxTaHK+/CuoOwvOVUU4Xr0AeBoif/G6qwzaEh9+gMVJ7WTjL1G33zVRlLlQyk3YFXW5/GWnKL9HsZowjFRWworh8ctOTkZV155JUaNGoVnnnkmEqck6lTE2io4C+bAOeMG6b+COdI2Ad4VY30sEzb17gvTQ5ukpbhpPaQLZktOjSeXR6MYm+d8/vI9RBE4Whq4A6aI/Cnp+PrkttQTipPq5eQMAMzm1hwqJdGrxpEyv8n9nijfm6Rk6b3UU4/H/d4pl6UbYad5ogiL6F+pvLw87N+/P5KnJOoUXBsLpaq6DntrhV3ltgB+vrV7piXS0lu3a/AOkrSCmupKiLVVQFOgZbo6cjmaDbYFhJaERJjvXwvL0seQ9cw/pWTkulr/2zd06946gqMMiNzvifK9aanJEzDxWLGcnYj8C2lZui+lpaVISEiI5CmJOgc9UxB6Vt4oz3PkIJxF+RCmzIK4cq48V+dMjTSKdPRIcG3tpIQFqz3/tv3tXn3L71PTgNJi+X0mE9B/kCdY8ZnUrCwVEJ8AZPbRXs7uY1l6OBtpchNOMpqgA559+/ap7rPb7fjpp5/w7rvv4vLLL49Iw4g6Fa09rLJzpJotelf3aJ3H5QKKv5OSWDP7yPdmamr0v1cTtYqLh6l3X8/N5pIAP7eWoAZNTeqii/0HyVbQ+dprSisQ8hlw+KijFPKeamE+l6gjCjrg2bBhg+b9VqsVl19+Of7whz+E3SiizsY0Y6G0M7Z7eiQ7B6bZS4L+Ru25SB45KJ9CYRJreJqb4MyfBqSkSgnagXZx73suzPlFcM64QfGA4AlcA42gBLPpps9RonCSmZkITQYTdMDz+OOPq+6zWq1ITU2NRHuIYkKkh/uFlFSYFz8cdruElFSYZiyAa/Gf5Uub3dNhylEkN4sVgMjl5f5UVoRfa8hikZcDiNAIis/gKMgK2hF7LlEHFHTA06NHj7ZoB1FM6cjD/a6NhfJgRxAA9xRMXLzGBp4CzBtfBgBuDREKrZ+pO6cmO0c+bZiZLf2MqysjOoLiKwBXjvwIU2a2vn6AQF016uhwQKytZh4PxaxOspaUKMLaabjfvaTZufBO+TLzYNomilItF6fDU4lXpk9O69LpSpuUHGuJ6HqG2NInV/0z8kdrOXrLaIhp9pLWnbvzBgNmS2upAWVuTxgjKJ4AXFHCwD3y497GQty8QVepA/dzYbHIVg7G6g7qRIDOEZ5Zs2ZBEPRVFhMEAevWrQurUUQdnnK4PzlF9zdnN2fVaTgK5/t9jq+RJL9TaloJ0N4SkqSkWvc2BNPm+N4xuzOKT5Dq2IQ8fdWap6OcanIuvFN+qMUqvV9Bjr6o6A3Agw3UmcdDBqIr4BkyZIjugIcoVoSTh6OcKoDDEfQUl23FfJ/P8bRNWayuZZk5HI7WqRLFc2Vtq6tVbztRW9X6b/c2BNwHq1VpMZCZrf6ZmC3qqscWi7T6zXtpv7/NlJXBaE6e532TTScGO02qN98m2Lwc5vGQgege4SHqqEINXMLJwwn4zV3HN2Gncldsr+f4HHFpWWaucuQgnAVzpH+31GnBjbcDfy8M2A5UVwJn6wIf11k47EC5xl5S3c+RRn68c3Kyc9XHNTdpB6/VldLzcwbIaul4hDGaoneD0mA3Mg1641OiDqwTT9QbX2cpHBZy4BLJ4foQvgmb09LhLDuu/Zxg2+JyyS/ELTkaupyp4eosJa1l53W1EO5aAnHzepjrauFMTpECgpXz1Me2jMRJAYNir6y8wTCvfFL9HOVnqK5WCqR1/O7qXcIezFL3UI4n6shCDnjq6+tx4sQJNGuUlB8yZEhYjVKaNWsWKirUQ+5XX3017rjjjoi+lpF05JVEERVq4BLB4fpQvgmnL16NE0vv0X6OVpVdXzuah6utzhsLUroDzY3qaT+t3TQaGyBuWqtO6NbKmWoZifN8Jrz5+HxqTkU2Nhj7d5eoHQUd8DidTjz55JPYt28fXFqrE6C9o3o4Vq5cKXutn376CcuXL8ell14a0dcxnM6ScBhi4OJ3ya5780WtMv4aQvkmbE5Ng2XBKoii+urqadvpcqC6yk9QIkDXXlekzTufyVu37kB6T3UBx2OlgMMOJwDgOFwbV8qXbytXXh05qF7x5ePz6f0Zci68Ux6EGfV3l6gdBR3wvPbaa/j8888xY8YMrF+/HtOmTYPZbMbbb7+N+vp63HbbbRFvZEqKfOffV155Bb169fI5kmS322G3t/7hEQQBiYmJEATBcMnX7v5o9ksjEIil/vvtmxfzzEVwbljhCVzMMxfp6qfQrTtMC1Z5bjsK5UnEHrZTcC2eDvOK/xexKcFAfXO3zXHXJEDU/mIhcQc7AtA3FzCZW3N46moB7ykz0i+9JywLVsk/E1qqK2Hq1h2m+9YAgPp4l0sKXBISpSDaz+dTrKmC03uEx1s7/e7q/Z2LRexbbIpknwRR6+ulH3PnzsWVV16JX//615g8eTJWrlyJ/v37AwAKCgqQm5uLW265JWINVHI4HJg+fTquueYaXH/99ZrHvPTSS9i+fbvndm5uLoqKOt9wsLO6EraCeXBW2mBOS0f64tUwc5WFTyemjZfn1CjEDbkAvVY/HdHXdFadhm3FfJ/v0dHfXgK4nLrOpWxf84+HUX7vVIjK6RoKqNeGrYjrd67qdwgOB5oPfdt6oDUOcQMGe9439/HN3++XvW/mjN7IenqX39c8NW8amg987bktJCbB1K07f3eJIiToEZ5Tp04hJyfHE3V5j6RcddVVeOaZZ9o04Pnkk09w9uxZjBkzxucxEyZMwLXXXuu57W6rzWaTtdcIBEFARkYGysrKNKdGcO9yCABcAMobmoAGjdUnHVTAvkWYMzkFgO+Ap7m8DCdPBv75yb6pu7/RK0aGBEFAj3grTtzxO8/UhbPsOE5MGy8fSYqLU+eXBGif61gpXIXzdT+P1E6teQAW9+if1++QWFsNYcMKiD8elhKb7c1oPvA1Tiy9R3Y8FCM9zuSUgJ8dR3mZ7LbYpSuE5X8P+3dXz+fRrb1/59oT+xabrFYr0tPTI3KuoAOehIQEOBwOCIKA5ORkVFRUYNCgQQCAuLg41NW17fLWd999F8OHD0damu9vO1arFVarVXW/KIqG+zC4daS+RXp1WHv1TZbTk5wCnDwqz51JTdNsh7K/yho5zg0rZPk97uNPuC+a3hob4NywQtoPa2OhVCSwqak1Vcff9FZL+xjs+BCfIL03ylo6WiptcNVU+tyuwblAsViiulL22dBKYg/4GdaYgo7E5965caVsqlb5edTi/TtntNWeHelvZaQZsW+R7E/QAU9WVhbKy8sBAAMHDsRrr72GwYMHw2KxYNeuXcjKyopY45QqKirw3//+F3Pnzm2z16DwtffqsEj9QVYmHou11bpWXqmWHVsUwbYi4TRgVePqSvUx8YlSZWQ/v/ue9jHY8SIAJkH6uQWzGq2+TvNz7NmYVRmoKqabQklib7OaN2EuXug0qz3J8IIOeC677DKcOHECADBp0iQsXboUM2fOlE5msWDOnDmRbaGXd999F926dcOFF17YZq9BEdDOq8PC+YPsL1jSfdEK1D9l7oWy4KDqfKfV5wwUxMQnABBbiw9SCxFwBfiGmN5LSi72rqyclKz5OVZtzAoAFmtEgpM2q3kTbvmFzrLakwwv6IDnV7/6leffubm5WLNmDT799FMIgoBhw4a12QiPy+XC3r17MXr0aJjN5jZ5DYqQ9i5HH8Yf5Ih8e1X2NztHqtXi65v62TP+zxdKEcAuXaW+eBcfJH3cn0/vgCetJWdA+TnW+mzl5EV9isdf4B72yBG3lyCDCLvScnp6On7zm99Eoi1+ffPNN7DZbBg7dmybvxaFp93L0SenqDbydAs43RWBb6/ClJkQC+e3bsY59R6Yevf1/YS2qGpcWcH9sAISALNJGs0RRel2Vl/p/aitlpaOJyUDaenSe7rpsdbpyeyc1s+192ctITFin+9wpmb9Be7hjhxxewkyiqADngULFmDs2LEYOXIkkpOT26JNmi644AK89NJL7fZ6FLqol6N3OlsLCHpvnqk1guPn26tYWwXXuuVSQTlAuujNXqK6CImbN7S+hnszTq19lNwXC1VdCRYPbDOCCTCbWwoCioDTe4m/CFSWq6aoTIsfli7w3qNlPx6Gq2AOkJIK5A6AuaHes7VEpEZ3whptbMNpp6j/PhNFSNABj8lkwv/93//hueeew//8z/9g7NixGDZsmCELHlH70fx22627vicri7SdPKqueOumuBD4+/aqmiIq/UH7IuTnYqN1EUN2jvy8fXKAijImG7eF7ucAJpN66we35ib57cYG7e0gRFfrKFreYGQ9vQsnT56M7IqYcIKWIKadjLbqikivoAOeFStW4MSJE3jnnXfw3nvv4aOPPkJaWhpGjx6NMWPGICMjoy3aSQanFRgIMxbi1Jr7pPok/v4wa+1l5IvqQuDngqV1wdG6z9/FRnl8aTGE+x5p3YAyIUm6X3nhpchISZXyqXx9PuLi1YGmu7yAr+eUFktFKiM8whNOrkww005cdUWdlSmUJ2VlZWHKlCnYuHEj8vPzMWDAAPzzn//EPffcg6VLl0a6jdQZaHy7dW5cKVWebdn527VxpeZTTTMWAnmDpdU2eYOlERRvCYmex5QXAs8ff63X0LrgaNynfH3VJqDeHHZpA0q3smPSaI+PfekkHD0Nh2nGQiBnAFQ/x4RECAtWS58Pb3W1EKbMUt/v5rBLFbn9fCZDbqevz1EA7mkn88onYc4v8h+EcdUVdVJhJS2bTCZceOGFuPDCC/H9999j7dq1+P777yPVNupMtL7d6vzDrKd+js8LgI/XEGurpGRWiwVwOAGLGcjO1bwIScXoFnimCdwbSrqL1Lnyp8mn2Fqms/RtGAEwvycMdbUARCmw9P45JiTCVPAEhJRUiAVPwLV4uioPy1TwhPQ5qrQB9XVSQnNttfy9jMVcGa66ok4qrICnoaEBH3zwAfbu3YsffvgBcXFxGDlyZKTaRp2I1pC8akWMzj/MQV04fPzxV+Xv5AzweU6xtkoqRqeRHC2kpAI5ef4LDQYjPgHomQk01Es/K1+5SiRJTdOunZOcIqu3hOQU1e7kWp8jZ1G+/L2MwWCBq66oswop4Nm/fz/effddfPLJJ2hubkZeXh7uuOMOjBw5EklJSZFuI3UCWhcX88xFMD/1EJq9cnjC4TpeCrEwv3X5+ILV2lsA1FYBpcXyJ/v5Jq95QVWNFFkjE5w0NQJHS6UgKjmFdXdkWpadW6xAl66ty8uX36s+VBmo6Bz1cH9ezHW1nhyeWMNVV9RZBR3wzJo1CzabDd26dcPVV1+NsWPHIjs7uy3aRp2ckJKKXqufjthqGLEwXz5tUTgPpnVbtb/FK4OTMzVSFeO6WvU0ma9EZmiMFGlJ6Q40NwaxSktkoKOpZdm50wn0yYU5v0j7vdSonaN31ENISYVlwSpkZmZqfi65Aoqo4wo64MnJycFtt92GCy+8ECZTSDnPRG1Oq4aOaiWU123PharSpr31Q1OjbENQz75K7oubt5ZNKp350wJvIwFIeSHM04msIwelYEf587dYYSp4AoDYWqupJTCJxKgHV0ARdVxBBzzz5s1ri3YQRZRWDR0IigA9Lr71+HXLgxs1Kf4ervnTFDtvC9JUk/v19BBM/ndAp9C4XFLgEZ8gv79lGwhZLk4kAxOugCLqsMLeWoKoQ9K80IhSgCHAk8Pj4R4J0k1UBDst91ks+kZ1PE9hsNOmnA5pqXdlBXC2Dij9Ac4ZN0A1ohapwCRKK6A4lUYUGAMeajfB/FEWa6vg3FiIE17JoUH9AdcqHCeKAETg3MFtN81QXRl4c1AKjsUi33/MYpXu05Pz5HBKwU5Vpf/gMkKBSbRWQBl5Ko3BHEUKAx5qN8H8UXYfK9WqOQ7X4umeuil6mGYshGvdg9LIjTJpVfFtXqyt8tpvSaFPfyA+Xr0vly+JXYDq07raSDpl52ruPi+rnQNIIzkOh2I6UfQ94maxSoFOBAOTQCug2uzibeCpNCMHc9S+mHVMYRFrq+Asyodz4Z1wFuVDrK32fXAwf5SVj7n3ONLZHtfKeYDFAuG+NeqKuYpv866NhVJSsjezBcgZANNfHvBUsDUVPOG7+q5b+YmAbSQdLBZPxWFh6t1SIFNdCZQWS4EsIL0fisrEptlLpPvcu5z7k5OnrzJxBPmt7B0OrWX2RmHgYI7aF0d4KGT+Cu5pCia/QWtKqrQYzoV3+v1mrPw2KBbOl48CaCxJ1hwBaMnPEVJS5d/KHcq8HQVl4EShMbf+aRI3PabexDV/GpCTJ/scyN4nvwTPc9tdG128DV1MkJWhKUIY8FDI/BXc0xLMH2XTjIXqKQuHXfrD5y+wUr6+cim6wyGN/ngHTfV12o04VqoO6pS4yip4JpOUM+zv59bUKP1nO6U9WuOwe0ZI3J8DWbCrJJikndPT0qObA9JGF28jFxM0dDBH7UpXwDNr1iwIgv4NDB9//PGQG0QxxE/BPS3B/FEWUlJhXvH/WistK7dR8BVYKS8oyt2wtYKmpGSfAY1r3XL1Y165H/jdH4GHFoF1dILgd6PUIHl/DlSfCQEwCa1VtXv3DekllHk35pmLgMzMkM7Fi3fwjBzMUfvSFfAMGTJEFvDs378f1dXVGDRoELp164aamhocPHgQ3bt3x9ChQ9ussdTBKIMLremiAPwlcXpXWnYUzte1h5HygiJMmQWxcJ52QOO+QHZJllbyKDmVCbAtsnNak2hfeU6qvcPKx20nO0f6/7HSlilFr+AyOaW1gGBdreKJIuASPZuBQsdFU+vzqJwmdW5YAax9PqSu8OJNFD26R3jc/vOf/+DgwYN47LHHkJ6e7rm/oqICy5cvx5AhQyLfSuqQPMFFZQVQfxZISpbtFK6H3hUYwpSZUj6Oex+sKbNUxwBaO6dXQfSVd+MOmspPaj/uazsL7+DGdgoIYvSTdIiLB7L6am7jIdZWywJaOBzyQDghUdpjTO+IoILW55FJs0TGEHQOzyuvvIIbb7xRFuwAQI8ePTBx4kTs2LEDY8aMiVT7qANzBxeeEv6NDUBlhSdo0bUEV8fFRKypkicfNzZA3LQWTq+lysKUmVJyq9dWEqbZS6TXVy43t1g9SatibVX4icYR2Oer01MsEfcVMLs/c57P1k+H5Qckp8C88kn1ruZ1tQET3gFofx6ZNEtkCEEvSz916pTPHdG7dOmC8vLysBtFMcZH0KJrCa7y4lFdqVre7ty4Uj0ldaxUdm6xcL408uKwS/+V/iAtX1bueg4Bwn2PeJYiuzYWhtRlijDFEvFA5Q48ny1lPlDL58k0Y2HrkvWEROnzo2cpuMbybtm5cgYATgdOTBsPR+F8/2UYiKhDCTrg6dGjB9555x3Nx95++2306NEj7EZRjPFVA0TH6I3nYuJeieO1+sbf81SUq7EA7aKDEKXRoZaLqTogauG1zxZFUEIikKb4G2GxqnK/AgbLys+EyeSpxQO0jgSZVz4pJaV787P1hyq4ca/qA2BauFrK3Sr5Ac6y45Gto0NEbS7ogOd3v/sdPv30UyxcuBC7d+/G+++/j927d2PhwoX47LPPMH78+LZoJ3VgsouE10VHTzE0T86Ncoqh9Ac4FtyBU/OmAV1T5I8lJLYmsrpZ49QN85W7c7Sk9WKqCogEKfhyOrWfS2EQpOBDWQagZUNPGWVA07L7uWdERflZ6j/IdwFB5ev5KkMARaBksUijht5BF/N5iGJW0Dk87vycF198Ec8/37pSITU1FdOnT8fYsWMj1jiKDVqJwp68noRE6SLXUv/EJ+VFyOEAbKfQbDslfdN2bwDZkhwNQLq/JbEVTY1SICPjI7dGtemnAKT3BM7USOfR2mKCIkBsXQ3nTi72tTRbmTfTsvu5a/F06XnJKbL33+9nS1l2QDni4wvzeYgMJaTCg2PGjMHo0aNx4sQJnDlzBl27dkVWVlZQtXrIGAIu4wWkBOIZC1T1R2Tfxv3UwkFdbWsiqldyNPIGS9/EAWl6SovFKo0eVdrgMwCyWGBauAquOVMDd1gQmKTslwBdNYlakot98awAPHJQnqfT2NCaj+P1/vuVli4vO5CW7vtYbxrBjbtdZq9NbYkoNoS8l5YgCOjduzd+9rOfoXfv3gx2OinNXAuN6QjX4j/7z8nwdxHylRPkPc3h65t2Th7MRU9L0xO+ZPaR2qfnQs1gx78+OdLIizWudVdzLQFGRjyjhv0H+T6otFhX0rDPKdcQniekpMKyYBWynt4Fy4JV3LWbKIaENMJz/PhxbNu2DQcOHMCZM2dQUFCA/v37Y9u2bRg8eDDOO++8SLeTOio9w/4uV8AtKGQFA5NbcnbqahHXMwPOO+ZKt31Nc7TU/pFqAtmk6bGWaTRhykxpZEgrVye9Z+t0WKBd0KlVXBzQ3Kz92NESICERvdY+j9NxSeqCkV4lAfSQfS6Uu9U77Lp2zg612B+LBBIZS9ABT2lpKe6//34kJiZiyJAh+OijjzyPNTY2Ys+ePQx4OhM/w/6q6Qjl87xoXVwEQUCvzEycPHkSoij6Pm91JcQz1dKqrJbChLjpDuCZRyE+MFv79XPyYF78sDQlN+fW4PttVGazPGG7VzaEGQukSsXuYLYkQFXpxgaUz7kNpuV/l3Kx3CvwWmoj+RoV8VW3yf25EGurpU1DQygoSEQU9JTWP/7xD/Tr1w+PPfYYZs+WX0zy8vJw+PBhH88kI/I17K85HZGQGPS0gjef501Ng7iypTChezRJq3YP0Lp8efYSAGAdHiXl6rRqmyzYMc1YqKuytNhQD+fi6fLaSBaL3ymgQEvRhZRUaRsPbz6mxgLV8SGizifoEZ6DBw9i9uzZiI+Ph0vx7b1bt26orq6OVNuoHeiqhuyHv2F/rY0S3ed2HS+VVty4t4oIYnNHrfO65vxRX4Nbli97dOYRAj0J2E2N6q0WsnPk22v4Ok+AaUwVnXWb9Gy+qXfLEiLqPIIOeERRhMVHIuLZs2dhtVrDbhS1n7a8MPgLhsTCfPlWEQ/MhjPvZzoDrpaLq8sFHCuFq2COvgblDPBcIGWBXmfVPV3Kd/Kbv6RYdVVdCdPC1aqgQzxT63uTVrdAS7h1LPnWnVfTQerlhPuFgogiJ+iAp1+/fvjkk08wYsQI1WNfffUV+vfvH5GGeausrMTmzZvx1Vdfobm5GZmZmZgxY0abvFano3FhCPaPtOt4qRTABDNao6qMLHqmMbQuaLLXEAGIiqXKepw8Clf+7YDDCV0rsoyuvg5ISJJGcbRGaARB2sTz+I+t96WmaedbpaQC67bCeed4aP5sExIDTmPqHb3RJcx6OZEKVDjSRNRxBB3wjBs3DmvXrkV8fDyuuOIKAIDNZsP+/fvx7rvv4t57741oA+vq6rBkyRIMHToUixYtQkpKit/9vChIGheGYP9Iq0ZrCucB67bKj1FcQGCN0960syUAE2ur4NxYiBMt9U5wtCTwJp8WCwDBd+HAcDcJNRp3TRtfRFF6n/IG6w9CLGbtCtfJKQEDhkiuigo3eIpYoNJBRpqIKISA57LLLkNZWRm2bduGf/3rXwCAhx9+GGazGZMmTcJFF10U0Qbu2rUL55xzDmbOnOm5r2fPnhF9jc5MMx+mZe8gj0B/pJWjNY0NEGurZRc45QUEfXKBijL1Bbflm7j7eCmF9ri+zlisXF4eaaU/SFOBC1frG+HIzpXn97i1c0XisIOnSAUqrMxM1GGEVIfn+uuvx+jRo/H111+juroaKSkpuOCCC9pk49DPPvsMF1xwAdasWYMDBw4gLS0NV199Nf73f//X53Psdjvs9tZv+YIgIDExEYIgGK5Aors/ofZL6NYdpgWrZPe5NP5I+z1/XLwq0HBtXAmL93mVF4yGelgefwlibTWcG1a01t9paoJzxg3adXO8p0oEE5B2DnC2peZOSqrG1hIUEaU/qN9PH8x33w/XhhUw1VbDWVvtqYdknrkotn73/PwOBPM7Z565qPXznZrW4X8O4f496cjYt9gUyT4Johhc6dgDBw6gf//+SEhIUD3W2NiII0eOYMiQIRFr4O9//3sAwDXXXINLL70UxcXF2LRpE/70pz9h9OjRms956aWXsH37ds/t3NxcFBVx3lwvZ3UlbAXz4Ky0wZyWjvTFq2H28820+cfDODXrZlkeiDmjN7Ke3uW5fWreNDQf+NpzO27IBei1+mnZeZTHeLP2HwTHyaMQW6alhNQ0oKEeQnIKzN3T4PipFGJjfUj9pcCU76fRBfs7QEQdX9ABz0033YSCggLk5eWpHjty5AgWLlyIrVu3ajwzNJMnT8a5556L5cuXe+77v//7Pxw+fBgFBQWaz/E1wmOz2WT3G4EgCMjIyEBZWRmCfCsjSlVRN2+wbERANpLj/qarmCJxLLhD/q0aAExmCP0HwtRyvOp1qH14bfap9d65iTVVUuXrulq4klM875uRdJTfubbAvsUmI/fNarUiPV3n/ncBhDSl5YvD4YDJFPL2XJq6d++O7Oxs2X3Z2dn4+OOPfT7HarVqLo8XRdFwHwa3aPdNKxdI1p6u3bwq5lbJgh8poVSUtg5QiPvZeXDdu7y1f0z6bH/xCbJNO52L/iTb6dw7oHFuXCnLvXJuWGHYVUnR/p1rS+xbbDJi3yLZH10BT319PerrW6cLqqurYbPZZMc0Nzdj3759SE1NjVjjAGDQoEE4ceKE7L4TJ060Sb4QhS6YJFGtFTBwODQTjpsPHQCW39u6JYEyt4J8a9m3Cj8d9r33lZvZDDhdUC0pT+shVaf2XuHmFfyoVi9xVRIRdVC6Ap7XXntNlhOzevVqn8dOmDAh/FZ5ueaaa7BkyRLs2LEDl112GYqLi/H222/jT3/6U0Rfh8Ljq26J1v1au57D5SOKd9g9SbPm/CL5JqFVp+X1eFQUic4dmZ6qx3qZTED/QZ73wPnn6wM/xxoHODVWuKWlS8GoryBT+V5yVRIRdVC6Ap4LLrgACQkJEEUR//jHP/DrX/9aNadmtVrRt2/fiCYsA9L+XHPnzsWWLVvw8ssvo2fPnrj11ltx+eWXR/R1jCJalV191S3RHM3R2vU8kJYLq/dIknPhnf5He/rkxM7KrbCCHUVgFxcPVFd6dpEPuPdVfIK0mko5wtZSLNC17kHfz1UENO6A1NxSPyms4oFERBGkK+AZOHAgBg4cCABoamrCL3/5S6Sltd83t5///Of4+c9/3m6vF8uiVtnV11SGxv2erQn87aauVFfrqe3jCeqqTvt/zplaafm6AGkEw24HXE7/z+mITCYpIPIOigRB2hoiLR3ClFnSBp+ny6WfiWLKSbX3lZLTKY3kVFa03peQCFPBE1KwrMytslilQEejoJ+QkgrLglXI9NrlnoioIwg6afnGG29si3ZQpEQrh8LXVEZyivz+6tOekQdXS4JrK0GqlpzZB4AIHC2FZ+SisUHabDQ5RboA6ykwWN0SEImQclCEyCbUtxsR6hEgQQDS0ltH8PKL4Jx9k/q5pcUQ7ntEtuO5tIO5vBqyv41eVe9tTp5hE5GJyLiCvgI8++yzeOyxxzQfe+yxx/D888+H3SgKgzJnop1yKEwzFkpbEKT3AvIG+57KcDikPbPyp0n/zhkgjRgAAEQpZyc+Hub71wLpioraLaMWIVdT9pvv04Fptdvl8uw9Bkh7jWn+XBx2iJvXwzRjgfRZqK4EzIrvOdk5nqlC88onYc4vkk2D6n5viYg6sKBHeD777DPccMMNmo9dcMEF2LFjB/7whz+E3TAKTUQ3YNRBrK2Ca91y4FipdEd2DvC7KdJoTHNTwGTk1mDHS8sGpjhT01bNNo7qSmlj1Qfu9n+M91QnII12dT/HM0rkTyT3uCIiipagA57Kykqfe1n16NEDp08HyKugNta+OROujYXy/JDSH4CHFus/gdZGn3W1cOXf4XsT0M4qrYe0w7n3SE5qmrR5qz/ukR1vokva8iG/SKqLVJSva4Wd0YoIElHnEfSUVkJCgqoGj5vNZtMs+Eftx/NN3nbKM+XhvqA5F94JZ8Ec6b+Fd8JZlA+xtjq8F9SbI5TWQ6rWq5xOcTNbWkd7GhtCCHaMt4eMSlo6TAVPqKeXlJu3emtZaaU5tdny3ml9ZvzdT0QUi4Ie4RkwYAB2796Nyy67DBZL69MdDgdee+01DBo0KKINpCBpJC2rdip3s52SlhxbLCF9ixdrqzSrI6skJMK0+CGpHZU2aZTC3iytDnKzWOTF7XzyVVunE6wGqq7Unl7S2LwVgCcgElJSpanOxdNVo0Pu88qUFktL/llEkIgMJOgRnhtuuAHHjh3DnDlzsGvXLrz33nt45ZVXMGfOHBw7dgwTJ05si3aSXspv8nW10vJvX46V6voWLxslahkZcm0sVFxoBc2aL8KC1VKeT/F30tLnxgb1SI+uYAdSReBoMZkC17RpSz4S0IUFGoVALVZZ8rGQkqo9OqR1Xodd+jwoR9lYRJCIYlhIIzzz58/H008/jS1btnju79WrF+bPn6+5qSi1H1nSst7l2958fIvXLCCoPDa9p7R6yLuei9kCoWsKRHdSs5u/aRh/nF7LqS1WaZSovVZfuVzSa4aaW2S2SHWAgq1N41U5WfPh3n3hzBkgz6XKzlEd5yv5WPaZqa6U989PzZ1Yxxwlos4lpM1Dhw8fjnXr1uHkyZOora1FSkoKMjMzI902CoGqErF3wGMyAX3Plf5dVytdyBwO+YXS17d4rekNX7V3vAMep6Ptcj+ikdQczms6HaEFTP0HyQIVrQu1afaSkFfnyT4zRfny1VwGrrkTtSKdRBQVYe2WnpmZyUCnI1MGJIoLJ4CWqSn1hVJ5UVUVELSdknJH+vQHTh6V7nM4IEy9B+Lyv8ov6qXFkdsnKtb5CnYs1taCiyeOSiNXFjOQnQthykzZKipZkOp1oY7Exbq9yxpEFXOUiDoVXQHPgQMH0L9/fyQkJODAgQMBj4/0floUGj0XL1/THKpE55wBUu0W7+mjEz9JK6/cF/HSH6SKvjl58lEC2UXenQPDAEjGXXBR472QjbrYTqlrF0XwQq235o4hpoO40SlRp6Ir4Fm2bBkKCgqQl5eHZcuWBTx+69atYTeMwue+eLkvTq6V8/RfnJQX0bpa7QVSylwc915ZyhVBHl4nMJlb9oiK0QrIkdbyM1eNrlVql4HwiMKFWjUdlD8NyMmTPlvdurd7e0LRqUaziEhfwLN06VJkZ2d7/k2xJVCugta3ddW337pa7arJys0/U9OkYCo5JXDCtEkAHDG4mWdbaQlcVKNrCYny47JzpGX8lRVA/Vmg0gZnUX5ERll0j9woA2KH3bPKz7RgVVhtaC+sIE3UuegKeLynqDhdFYN81VlpuaBpBURBrfZKSJQCnJbz6a7P01mDnfgEKUCQ1SGyto4wKN+vpGQpyFEEIc6ifGn0p7EBqKyAa/H01h3OQ6Q7kVcZELsxD4aIOqiwkpYpRigvTu46K76WlysK3KlWeyklJcO88knPTWdRvr7l8BazatfumCQIwSVla9UcysnzvTt5yxYQKsr3rbEh/JVGOhN5PQFxabE8R4t5METUQekKeDZs2KD7hIIgYMaMGSE3iCJPmDITYuF8r808vS7O/paXe9/W+jbvVmWTjRjp/pafnSv9/1hJbAc+6b2kjU6VQZ7JpP55a7FYZPkjunNLtN6XcEdYdCbytuaHaa/yIyLqaHQFPN9++63sdn19Perr62EymdC1a1ecOXMGLpcLSUlJ6NKlS5s0lEInbt7ge8TFM63l+6KlfBxHDkkF9DwvILaOGC2eDmRk+w+QzBagT25r7ZhYCnbyBstXoAFARRlMDz8L1+I/y3/O/Qep6xxpMVtUP389ozR+t4sIUbCJvMyDIaJYIYhicAVSiouL8fDDD+P3v/89LrvsMphMJrhcLnz44YfYvHkz5s6d22GrLVdUVMBuN9YO3IIgIDMzEydPnoSvt9K58E5FACIAaemAewqlpQihMGWmFBz5SVgVa6vgmjMVfkctcgZIW1b4K7CXMwDmxQ9rtK2DM5vluTduCYnywCMhUdrKAWiZ+vnBd2CnPGfeYN1BhNYIS7SXh+v5TMYyI/ePfYtNRu6b1WpFjx49InKuoHN4nn/+efz2t7/FqFGjPPeZTCaMGjUK1dXVePbZZ/Hggw9GpHEUIaqpD1EKeABZgqpYOL/1ou0jYdW1sRABp2iOlSDg7uWlP8A54wZp2ieWaAU7gHoELSnZE3h4pn58LdV3Kla6VVfqXi3FERYiIn2CvtocOXIEffr00Xysb9++KC0tDbdNFGGmGQu1i9VpJL3KFH8P5/TfwTn7JriO/9T6vEAcDn3bJzjsoe+p1dGdPSPbbBWAtJJNi3JD1NS01tVSATZ1JSIifYIOeBITE/HNN99oPvbNN98gMTFR8zGKJlGq2+KtZUPIgM9zuYDGBoiF8/QvN5eJ4u7i0dTUqA5YfP28LRb1LuYaq6W0dqwnIiJ9gp7SuuKKK/Dqq6/C6XRi1KhRSE1NRXV1Nd577z28/vrruPbaa9uinRQG18ZC+eiNYJLqt6SkShdbh2IHcq3VP40N6qRcPYyy9DxcpcUQ7nsEYuE89c/Q3gwAMC1c7XtpenWl/OfPzS6JiIISdMAzefJk1NTUYPfu3di9e7fsscsvvxyTJ0+OWONIH2fVaTgK50tBTH2dVKguLb0170M5WiC6pCq9lRVS8OPNYoFw130Ql92j3vIhmGBHEKQCez0ygKMl/o+NT9CuTdPRZPUFTpcHaKvQEkQqpvQcdoib18NU8IQ6l8fl8owCuQMYVZ0b93/eFAUko52sTETUkQUd8JjNZsyaNQsTJkzA/v37UVdXh+TkZAwdOhS9e/duizZSALYV8+VLpd2Vd90XUH91dDSCGrFwfvj7W4mi1A6zRb2CSbkpl705NoKeyorAbUxLl/5TLl0HPAUdNYOelsfd3MnIflexKQpIcrSHiMi3kCstZ2VlISsrK5JtoRA5fW0u6V4JJYpSQNGlqzQCFGikxm8isdYOon7U1ar31VKOgLhcXoFEkOePGAHI+5k0GuUrqNEzwtUysuavCrHPvca0cnyUwap7G4/qSvm5uaUDEZFfIa0Jttvt2LNnDx599FEsX74cJ0+eBAB8+umnOHUqhmqqGITZvcRcyb1ayumQLuJp6VJtGHeCrHJTSjflhqCCyWuVV5DBiFZydHaO1AbNJenRqSEhPLAO5vwiCAsfUk/z6WGxehKO3aMzpqKn1cnIbsoVW/EJmkX+TDMWys9R8IS0jUeOotYVt3QgIvIr6BGe2tpaLFu2DMeOHfMkLDc0SN9UP/30U3z99de44447It5Q8i198WqcWHqPlMNTWeH7QMUeWWJtNVz50xS5IRojLN3PkYKTYAsEtiREC1Pvgbh5vdS+s2da6/RY4zrGNFbOAJh69wUAmHr3hfOcHvr6arEAEIDsHJhmL1Hl0PitkaNcvJbZJ6g6O8FWRCYi6uyCDng2b96M+vp6rFy5Ev369cMtt9zieWzo0KHYtWtXRBtIgZlT02BZsAqiKMI5/XfqERo3xSiAkJIqjbZ4b31gNksjQt7cI0jBBjwt2yqIm9ZKwUFttTy4cthb8nsaEa2RHcTFwzR7ify+QHuHublXn1kswScMn1Es7w9yuT8LDhIRBSfosfsvvvgCkyZNQv/+/SEI8q+p55xzDk6fPh2xxlEINMuKC9IohmIUQKytAk4elR+qrNeTkAjTjIXSc31NgQVSWiwl8WoVI3Q4ojcdE58A08onVcGKz77GxbdMwymGZxT5M7rq5Wht0EpERG0m6ICnoaHB574WDocDLl+jC9Q+NPJiTA8/C/Pih1UXdtfGQvWUUmIXKWckLV266Ccle6r8mgqeAPrkhtAoP6M3DjtQUxXCOVuEkm/j5nR4fiaeICV/mrSCSvlzsVhbdpt3QdUfRbCip0qyeeYixA25AEjrIf2cK20sJkhE1IaCntLq2bMnDh06hPPOO0/1WHFxccRXbr300kvYvn277L5u3brhySefjOjrGIZi1M17ukW5PxO0Vnc11kvLoYvypceVS9wryrReFIiPDz0fJ5wl8OE81+GAsyi/JR+mUHspeSAtI2AyGlWSlYSUVPRa/TSO3vMHKe9K+XMmIqKICjrgGTVqFHbt2oU+ffrgwgsvBCDt1FpcXIx//etfmDBhQsQb2adPHyxZ0ppnYYq1DSfbk1lROdnc+hbLLuq2U9rTNolJ0lRXabH8/patDTSXZqf3lP7fERKQg+UegfG1rNtkAvoP8uQjqSSnqPN3lDlA/qardARHREQUvqADnvHjx+PgwYN46KGH0KVLFwBAQUEBzpw5g+HDh2PcuHERb6TJZEJqamrEz2tIXbrKAw+HA67jP0LcvAE4clB+bFIy0NQkHyVpqJcCI2W+TXIKXOuWa79mXS2QkR18UnN7EwTtHKdKmzSFp9X+/oNadzv3U1vHW1ArqIIJjoiIKGRBBzwWiwULFy7Ehx9+iC+++AI1NTXo2rUrfv7zn+Oyyy5rk9GXsrIyTJ8+HRaLBQMGDMDkyZPRq1cvn8fb7XbY7a0XJUEQkJiYCEEQVInWsc7dH0+/UlLlS9OdDqlystbIjHv1lffxScm+RxmOlWrf39ggJT9brPp2SW8PgkkeyOUMkP5/rFTdxqrTMN2zFK7n1wNVNuCs1/YcU2ZJ03vu4GXJI9JxLbfNMxepPlNCt+4wLVjlv3ktz7HMWgzH+gK/54s1qs+kwRi5f+xbbOoMfYvIuURRc1mPpubmZjz44IO48cYbMWzYsIg1wp8vv/wSTU1NyMrKQnV1NXbs2IHjx49jzZo16Nq1q+ZzlHk/ubm5KCrqHHkRp/56K5oPfSu/02QGXE7Z7bifnYf0xathK5iH5gNfex6KG3IBAMjuAwBzRm84T1d4NrrssKxxiBswGN3vWoSqx1fAWWmDOS0dosMO+6EDPp8WN+QC9Fr9tOr+U/OmqX4+yuOcVadhWzHf81rpi1fDzJEaIqIOJagRnri4OPz0008wm81t1R6VESNGeP7dt29fDBw4ELNnz8a+fft87sw+YcIE2WPuCNFms8lGfoxAEARkZGSgrKwMoijCoZWIHBcnH+HpPxCue5ejvKEJ4h1zgQ0rPCMMzjvmSscs+pPsOc7kFCAxCSjRyGPpSOzNaD55DKfWPCCNlqSkwgXAscB/Mczm8jJPxXBvjvKygMc5Clv3MnOWHceJpffAomOEx/t9MxIj9w0wdv/Yt9hk5L5ZrVakp/vYTSBIQU9pDRw4EMXFxRg6dGhEGhCshIQE9O3bV/Pi5Ga1WmG1WlX3i6JouA+Dm6dvGnsvCQtWS5WOvXJKPD+Hrt20K/kWPKGZh6KuzNwBVZ0Gqk7DuWFFa98CFRNMTdP+bGjk2KiO00g81vs56xSfSYMycv/Yt9hkxL5Fsj9BJ9z84Q9/wFtvvYV9+/ahsbH9V+XY7XYcP34c3bt3b/fXjgVaey+ZeveFOb8I5pVPSvtF6agKLKSkwjRjgXTBr65srSWj3MMpXBZ1YBoxpcWe4n/ClFnaq9K89sDSovp5ah3HIoJERB1e0CM89913HxwOBzZs2IANGzYgPj5elVT07LPPRqyBzz33HC666CKkp6ejpqYGL7/8MhoaGjB69OiIvYax+I+GlbV43Jtdah63+M+t01q2U3BtXClfgaTcsTsUoT7folh+7+vctlOA7ZQ0wqXatd0KU9HTfgNAPVs4cF8rIqKOL+iA55JLLmnXTPDKykqsXbsWtbW1SElJwYABA1BQUOCz2nNnp6y1oyxkp/W4acYCacm5exVWdo70f+XKLsXmo86i/NCK9Vms0vJw5Z5dwcjOlYKeSpu0LL65SbpfEIBuadJ93sGUu9ii9/RUTl7we2Bp4L5WREQdX9ABz6xZs9qiHT795S9/adfXi3mBCtkpb5cWw5V/hzw4KP1Be6rpTA2cC+/0jGJ4RjaUtWn8EQSYip6WnhdKsNRSCNDXyJSbKhhraTNHYoiIOifdOTzNzc14//338corr+Cdd95BbW1wuztTO1Hmj1RXyvdoUj7usOsPVpoaW/eHWjwdAGDOL4Kp6GlZRWe/RBGu/Nul6ahQ9uXSWedJmXsjTJmpCnYiMbpDRESxQVcdnsrKSixduhTl5eWe+5KSkrBw4UIMHDiwTRsYSRUVFYZclp6ZmYmTJ09KGfq+KgLnDZZXDA6Ug5MzQLtIn1KfXJj+skw9YqOnCGFConZBRD1a+qOXasQnyOdHmvJ9MxIj9w0wdv/Yt9hk5L5ZrdaIpbDo+rr84osvorKyEjfccAMWLFiAW2+9FRaLBU899VREGkGR48kn0Rjp8X7cvPJJjRVXghSoZPQGftQ5TXW0xJPM7D2i4skD8ifUYAcIfs8p7llFRNSp6ZqH+OabbzBhwgRMnDgRgFQMMCMjA0VFRaiuruY+Vx2RMkH3TA2cM26Q/p2dA9PsJZo5LUJKKpx/+p16z6mcAdqbZwJAySGpPo/73O6aPRtXAiWHAKdT+3nhSE6Rb/kQaIqKe1YREXVqugKe6upqDBkyRHaf+3ZNTQ0DnihzVp2Wqv16XfxlwUxdrXw0pfQHz+otzWkd7z2o3GqrgfgE7R3RnU4AztZzz7+9NT9HNbwqSHt4VdqgWkIfnwDEJwJnagABQFw8eixbi4onH5GOr2/d4woOh9/VaEpMWCYi6tx0BTwulwtxcXGy+9y3nW3x7Z2CYlsxX37xXzxdqjmTmgbTwtVwrZynucQc0K7Lo9p4E5BvMBqI0+F7NCjvZzDnF0mjTd5TZhYrzI+/JDtUEAQkZGbCPGMhnBtXSgnL7tVWK+dp9seb3ppDRERkfLqXpZ84cUK2E7rL5fLcr9S/f/8INI30cir3z2pskP5rGfnQ3FahZUpHqy4P5hQAD/kYAUnrAZw9oz3S44/XcnIAUo6Pd1DkJ+fH6Z0Q7Q7oMrIDTlEFqklERESdh+6AZ/369Zr3r1u3TnXf1q1bQ28RBc2clg5n2XHtB6srpVGedQ/KCgt6Ag+NZF7zoKFw5g3WrpOTkipNkXkTBI2pK4W4eE+wIdZWSfe5a/205BQpR2TMMxcBmZnqNrpHq/IG+5+iYqIyERG10BXwzJgxo63bQWFIX7waJ6aN1171lJomrcxa/LD2k7U2x6ytknJkLFYAolRjJ7EL0FgPHCtRb+kgivJju3QFaqrklZSTkj3/dG0slI/uWCwARNVWFs4NK4C1z2uPUNXVSivN/GGiMhERtdAV8IwZM6aNm0HhMKemqfeJUk4h+aCVzCvV8fEKSHIGSP/3VxnZnY+TM0DK0VHWvUlLb/23xsiLa2OhOmArLZYCuYQkdcK0juCFicpEROQW9NYS1EEpRzP6D9KZr6IxFaWx/QT0Jvu2PNdvsKE18qI13eSwt07V5QyQRoKCCF64xxUREbkx4DEI88xF0hRQkKMZmknLyoDEYZeWhMsIUgBiNmuOvPgLNnyOKsmmrQTIgjE9U1hEREQ+MOAxiJBHM5TLzUt/gHDfoxCX/1W+bDwpWVpJpVjiLduqIoyRF2UQBIdiaTvzb4iIKAwMeDoh2WqoytPyBx0OiJvXS9tOKHJwtAKqUAItX/VxvM/jDqTMdbVwJqcw/4aIiMLCgKcTkk1jaXEvZW+jhF899XGElFRYFqwy7IZ4RETUvhjwdEaB6tG4l7K3VcIv6+MQEVE707VbOhmMv3yY+IS2nz5Svj7zc4iIqI0x4OmETDMWSlWK03tJy71zBkj/zhsM04r/1+b7TcleP28w83OIiKjNcUqrE4p2fZpovz4REXU+DHhIhjuMExGREXFKi2Q8K6hsp4Di76SVWkRERDGOAQ/JVdr83yYiIopBDHhITrmFhGpLCSIiotjDgIfkkpL93yYiIopBDHhILi3d/20iIqIYxICHZFgjh4iIjIjL0kmGNXKIiMiIOMJDREREhseAh4iIiAyPAQ8REREZXswFPDt37sSkSZOwadOmaDeFiIiIYkRMBTzFxcV466230K9fv2g3hYiIiGJIzAQ8jY2NWLduHaZPn44uXbpEuzlEREQUQ2JmWfpTTz2FESNGYNiwYdixY4ffY+12O+x2u+e2IAhITEyEIAgQBKGtm9qu3P0xWr8A9i1WGblvgLH7x77Fps7Qt0iIiYDngw8+QElJCVau1Ldz986dO7F9+3bP7dzcXBQVFSE93bhVgzMyMqLdhDbDvsUmI/cNMHb/2LfYZOS+RUKHD3hsNhs2bdqExYsXIy4uTtdzJkyYgGuvvdZz2x0h2mw22ciPEQiCgIyMDJSVlUEUxWg3J6LYt9hk5L4Bxu4f+xabjNw3q9UascGKDh/wHDlyBDU1NViwYIHnPpfLhe+++w5vvPEGtmzZApNJnopktVphtVpV5xJF0XAfBjf2LTaxb7HLyP1j32KTEfsWyf50+IDn/PPPx0MPPSS7b+PGjcjKysL48eNVwQ4RERGRUocPeBITE9G3b1/ZffHx8ejatavqfiIiIiItHB4hIiIiw+vwIzxaHnjggWg3gYiIiGIIR3iIiIjI8BjwEBERkeEx4CEiIiLDY8BDREREhseAh4iIiAyPAQ8REREZHgMeIiIiMjwGPERERGR4DHiIiIjI8BjwEBERkeEx4CEiIiLDY8BDREREhseAh4iIiAyPAQ8REREZHgMeIiIiMjwGPERERGR4DHiIiIjI8BjwEBERkeEx4CEiIiLDY8BDREREhseAh4iIiAyPAQ8REREZHgMeIiIiMjwGPERERGR4DHiIiIjI8BjwEBERkeEx4CEiIiLDY8BDREREhseAh4iIiAyPAQ8REREZniXaDQjkzTffxJtvvomKigoAQHZ2NiZOnIgRI0ZEuWVEREQUKzp8wJOWloZbbrkFGRkZAIB9+/Zh1apVWLVqFfr06RPl1hEREVEs6PABz0UXXSS7PXnyZLz55pv44YcfGPAQERGRLh0+4PHmcrnw0UcfoampCQMHDvR5nN1uh91u99wWBAGJiYmwWGKqu7oIggAAsFqtEEUxyq2JLPYtNhm5b4Cx+8e+xSYj9y2S121BjIGfzk8//YTFixfDbrcjISEBd999Ny688EKfx7/00kvYvn275/bIkSNxzz33tEdTiYiIKMLsdjusVmtY54iJVVpZWVlYvXo1CgoKcPXVV2P9+vU4duyYz+MnTJiATZs2ef6bMmUK1q5di4aGhnZsdftoaGhAfn4++xZj2LfYZeT+sW+xyeh9W7t2rWzWJlQxEfBYLBZkZGTg3HPPxS233IKcnBy8/vrrPo+3Wq1ISkry/JeYmIgPPvjAcEN9ACCKIkpKSti3GMO+xS4j9499i01G79sHH3wQkXPFRMCjJIpiRKI9IiIi6hw6fMCzZcsWfPfddygvL8dPP/2EF154Ad9++y0uv/zyaDeNiIiIYkSHX7ZUU1ODxx9/HFVVVUhKSkK/fv2wePFiDBs2TPc5rFYrJk6cGHbCU0fEvsUm9i12Gbl/7FtsYt/0iYlVWkRERETh6PBTWkREREThYsBDREREhseAh4iIiAyPAQ8REREZXodfpRWON998E2+++SYqKioAANnZ2Zg4cSJGjBgR5ZZF1s6dO/HCCy9g3LhxmDp1arSbEzbl1iAA0K1bNzz55JNRalFkVVZWYvPmzfjqq6/Q3NyMzMxMzJgxA/37949208Iya9Ysz++at6uvvhp33HFHFFoUOU6nE9u2bcN7772H6upqdO/eHWPGjMH1118Pkyn2vzc2NDRg69at+OSTT1BTU4Pc3FxMnToVeXl50W5aUA4cOIBXX30VJSUlqKqqwty5c3HxxRd7HhdFEdu2bcPbb7+Nuro6DBgwANOmTYuZjagD9e/jjz/GW2+9hSNHjuDMmTNYtWoVcnJyotfgIPjrm8PhwIsvvogvv/wS5eXlSEpKwvnnn49bbrkFaWlpul/D0AFPWloabrnlFmRkZAAA9u3bh1WrVmHVqlUx8wEPpLi4GG+99Rb69esX7aZEVJ8+fbBkyRLPbSNcVACgrq4OS5YswdChQ7Fo0SKkpKTg1KlTSEpKinbTwrZy5Uq4XC7P7Z9++gnLly/HpZdeGsVWRcauXbuwZ88ezJo1C9nZ2Thy5Ag2bNiApKQkjBs3LtrNC9vf//53HD16FHfddRfS0tLwn//8Bw8++CAeeeSRoC4o0dbU1IScnByMHTsWDz/8sOrxXbt24bXXXsPMmTORmZmJHTt2YPny5Xj00UeRmJgYhRYHJ1D/mpqaMGjQIPziF7/AE088EYUWhs5f35qbm1FSUoIbbrgBOTk5qKurw7PPPotVq1ahsLBQ92sYOuC56KKLZLcnT56MN998Ez/88IMhAp7GxkasW7cO06dPx44dO6LdnIgymUxITU2NdjMibteuXTjnnHMwc+ZMz309e/aMYosiJyUlRXb7lVdeQa9evTBkyJAotShyDh06hIsuusizaXHPnj3x/vvv4/Dhw1FuWfiam5vx8ccfY/78+Z73atKkSfj000/x5ptv4uabb45yC/UbMWKEzxF8URTx+uuvY8KECbjkkksASKOSd955J95//31cddVV7dnUkPjrHwBcccUVAIDy8vL2alLE+OtbUlKS7AswANx2221YtGgRbDYb0tPTdb2GMb426+ByufDBBx+gqakJAwcOjHZzIuKpp57CiBEjgirCGCvKysowffp0zJo1C48++ihOnToV7SZFxGeffYb+/ftjzZo1uOOOOzB//ny89dZb0W5WxDkcDrz33nsYO3YsBEGIdnPC9rOf/Qz79+/HiRMnAAClpaU4ePCgIabHnU4nXC6XqrBbXFwcvv/++yi1KvLKy8tRXV2NCy64wHOf1WrFkCFDcPDgwSi2jEJRX18PQRCCGh039AgPIA2rL168GHa7HQkJCZg7dy6ys7Oj3aywffDBBygpKcHKlSuj3ZSIGzBgAGbNmoWsrCxUV1djx44duO+++7BmzRp07do12s0LS3l5Ofbs2YNrrrkGEyZMQHFxMZ555hlYrVaMHj062s2LmE8++QRnz57FmDFjot2UiBg/fjzq6+vx17/+FSaTCS6XCzfffDNGjRoV7aaFLTExEQMHDsTLL7+M3r17IzU1Fe+//z6Ki4s96QBGUF1dDUDKB/TWrVs32Gy2KLSIQtXc3IwtW7Zg5MiRDHi8ZWVlYfXq1Th79iw+/vhjrF+/HsuWLYvpoMdms2HTpk1YvHgx4uLiot2ciPP+1ty3b18MHDgQs2fPxr59+3DttddGsWXhc7lcOPfcc3HLLbcAAHJzc3H06FG8+eabhgp43n33XQwfPjym8j/8+fDDD/Hee+/h7rvvRp8+fVBaWopNmzZ5kpdj3V133YWNGzfiz3/+M0wmE3JzczFy5EiUlJREu2kRpxxx5GYDscXhcODRRx+FKIpBL4YwfMBjsVg831LOPfdcHD58GK+//jr+9Kc/RblloTty5AhqamqwYMECz30ulwvfffcd3njjDWzZssUwSb4AkJCQgL59++LkyZPRbkrYunfvrgq2s7Oz8fHHH0epRZFXUVGB//73v5g7d260mxIxmzdvxvjx4zFy5EgAUiBeUVGBV155xRABT0ZGBpYtW4bGxkY0NDSge/fueOSRRwyTXwbAkxPoXmXnVltbqxr1oY7J4XDgkUceQUVFBe6///6gF3sYPuBREkURdrs92s0Iy/nnn4+HHnpIdt/GjRuRlZWF8ePHGyrYAQC73Y7jx49j8ODB0W5K2AYNGuTJA3E7ceIEevToEaUWRd67776Lbt26eRJ8jaCpqUn1e2UymQw3OpCQkICEhATU1dXh66+/xpQpU6LdpIjp2bMnUlNT8d///he5ubkApAvogQMH8Pvf/z7KraNA3MFOWVkZli5dGlJ6g6EDni1btmDEiBE455xz0NjYiA8++ADffvstFi9eHO2mhSUxMRF9+/aV3RcfH4+uXbuq7o9Fzz33HC666CKkp6ejpqYGL7/8MhoaGgwx5XPNNddgyZIl2LFjBy677DIUFxfj7bffjukRR28ulwt79+7F6NGjYTabo92ciPn5z3+OHTt2ID09HdnZ2SgtLcXu3bsxduzYaDctIr766isAUgpAWVkZnn/+eWRlZcXc6FVjYyPKyso8t8vLy1FaWork5GSkp6dj3Lhx2LlzJzIzM5GRkYGdO3ciPj4+ZnKxAvWvrq4ONpsNlZWVAOD5cpWamtrhV73661v37t2xZs0alJSUID8/Hy6Xy5OTlZycDItFXyhj6N3SN27ciP3796OqqgpJSUno168fxo8fb8hVTQ888ABycnIMUXjw0UcfxXfffYfa2lqkpKRgwIABuPnmm2M678rb559/ji1btqCsrAw9e/bENddcg//93/+NdrMi4uuvv0ZBQQEeffRRZGVlRbs5EaMszJeWloaRI0di4sSJuv/YdmQffvghXnjhBZw+fRrJycm45JJLMHny5JirD/Xtt99i2bJlqvtHjx6NWbNmeQoPvvXWWzh79izy8vIwbdq0mPmiGKh/e/fuxYYNG1SPT5w4EZMmTWqPJobMX99uvPFG3HXXXZrPW7p0KYYOHarrNQwd8BAREREBnagODxEREXVeDHiIiIjI8BjwEBERkeEx4CEiIiLDY8BDREREhseAh4iIiAyPAQ8REREZHgMeIiIiMrzYLxFKRBGhtxJrMJVNY8H69etx4MABrF+/PtpNIaI2xICHiAAAy5cvl91++eWX8e233+L++++X3W+ULT6IqHNhwENEAICBAwfKbqekpEAQBNX9Sk1NTYiPj2/LphERhY0BDxHp9sADD+DMmTOYNm0atmzZgtLSUlx00UX4y1/+gkmTJmluUjhr1iwMGTIEs2bN8txXXV2Nl156CV988YVnM84xY8bg+uuv97vL+qpVq1BaWorHH38cJpM8BXHRokVwOp0oKioCALzxxhv46KOPcPz4cTQ1NaFnz5644oorcM011/jd8LO8vBx33XUXZs6cqdotXKuPJ0+exEsvvYRvvvkG9fX16NWrF371q1/h17/+tecYl8uFnTt34j//+Q9sNhusVivS09Nx5ZVXYty4cb5/4EQUMQx4iCgoVVVVWLduHcaPH4/JkydDEISgnl9dXY2FCxfCZDJh4sSJ6NWrFw4dOoQdO3agoqICM2fO9PncK6+8EqtWrcL+/fsxbNgwz/3Hjx9HcXExbrvtNs99p06dwsiRI9GzZ09YLBb8+OOP2LFjB44fP+73NYJx7Ngx3HfffUhPT8cf//hHpKam4quvvsIzzzyDM2fO4MYbbwQAvPrqq9i2bRuuv/56DBkyBA6HAydOnMDZs2cj0g4iCowBDxEFpa6uDvfeey/OO++8kJ7/0ksv4ezZs1izZg3S09MBAOeffz7i4uLw/PPP47rrrvOZJzRixAh069YNe/fulQU87777LiwWC0aNGuW579Zbb/X82+VyYfDgwejatSs2bNiAP/7xj0hOTg6p/d6effZZJCYm4m9/+xuSkpIAAMOGDYPD4cArr7yC3/zmN0hOTsb333+Pvn37ykaGhg8fHvbrE5F+XJZOREHp0qVLyMEOAHzxxRcYOnQounfvDqfT6flvxIgRAIADBw74fK7ZbMbll1+Ojz/+GPX19QCkYOa9997DRRddhK5du3qOLSkpQVFREW6//XbcfPPNmDx5Mh5//HG4XC6cPHky5Pa7NTc3Y//+/fif//kfxMfHq/pit9vxww8/AADy8vLw448/4qmnnsJXX33laTsRtR+O8BBRULp37x7W82tqavD5559j8uTJmo/X1tb6ff6VV16J3bt344MPPsBVV12Fr776ClVVVRg7dqznGJvNhvvvvx9ZWVmYOnUqevbsCavViuLiYjz99NNobm4Oqw+ANNLldDrxxhtv4I033tA85syZMwCACRMmICEhAe+99x727NkDk8mEwYMH4/e//z3OPffcsNtCRIEx4CGioPjK2bFarXA4HKr73Rd9t65du6Jfv364+eabNc8TKKDKzs5GXl4e9u7di6uuugp79+5F9+7dccEFF3iO+eSTT9DU1IS5c+eiR48envtLS0v9nhsA4uLiAAB2u91vP7p06QKTyYQrrrgCv/rVrzTP1bNnTwDSyNS1116La6+9FmfPnsU333yDF154AQUFBdi4cSNXuRG1AwY8RBQRPXr0wI8//ii7b//+/WhsbJTdd+GFF+LLL79Er169Qs6jGTNmDJ566il8//33+Pzzz3HNNdfIVm25gzKr1eq5TxRFvP322wHP3a1bN1itVlVfPv30U9nt+Ph4DB06FCUlJejXr5/flV/eunTpgl/84heorKzEpk2bUFFRwdpGRO2AAQ8RRcQVV1yBrVu3YuvWrRgyZAiOHTuGN954w5PM63bTTTfhm2++wZIlS/Cb3/wGWVlZaG5uRkVFBb788kvceeedOOecc/y+1qhRo/Dcc89h7dq1sNvtquXjw4YNg8Viwdq1a3HdddfBbrfjzTff1LUqShAEXH755Xj33XeRkZGBfv36obi4GO+//77q2Ntuuw1LlizB/fffj6uvvho9evRAQ0MDysrK8Pnnn2Pp0qUAgMLCQvTt2xf9+/dHSkoKbDYbXnvtNfTo0QMZGRkB20RE4WPAQ0QRcd1116G+vh579+7FP//5T+Tl5eGvf/0rVq9eLTuue/fuWLlyJV5++WW8+uqrOH36NBITE9GzZ08MHz4cXbp0CfhaSUlJuPjii/H+++9j0KBByMrKkj3eu3dvzJkzBy+++CIeeughdO3aFaNGjcK1116LFStWBDz/H//4RwDArl270NjYiPPOOw8LFiyQ1RICpOm1oqIivPzyy3jxxRdRU1ODLl26IDMz05OEDQDnnXcePv74Y7z99ttoaGhAamoqhg0bhhtuuEH3yBARhUcQRVGMdiOIiIiI2hKXpRMREZHhMeAhIiIiw2PAQ0RERIbHgIeIiIgMjwEPERERGR4DHiIiIjI8BjxERERkeAx4iIiIyPAY8BAREZHhMeAhIiIiw2PAQ0RERIb3/wESGuRFMYYLsQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.6675 with a standard deviation of 0.0331\n",
      "LightGBM optimized model r2_score 0.7069 with a standard deviation of 0.0324\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm.joblib']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"OUTPUT/lgbm_reg.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm.joblib\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.661845     0.017506\n",
      "1                    TP       191.900000     6.244108\n",
      "2                    TN       171.600000     7.705698\n",
      "3                    FP        41.500000     5.317685\n",
      "4                    FN        44.200000     5.921711\n",
      "5              Accuracy         0.809217     0.019867\n",
      "6             Precision         0.822236     0.021820\n",
      "7           Sensitivity         0.813034     0.021449\n",
      "8           Specificity         0.805260     0.023597\n",
      "9              F1 score         0.817460     0.018476\n",
      "10  F1 score (weighted)         0.809300     0.019807\n",
      "11     F1 score (macro)         0.808713     0.020071\n",
      "12    Balanced Accuracy         0.809154     0.019750\n",
      "13                  MCC         0.617816     0.039725\n",
      "14                  NPV         0.795090     0.027717\n",
      "15              ROC_AUC         0.809154     0.019750\n",
      "CPU times: user 30min 56s, sys: 1.41 s, total: 30min 57s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=16, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=16, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 09:12:31,194] A new study created in memory with name: XGBRegressor\n",
      "[I 2023-12-11 09:12:46,863] Trial 0 finished with value: 0.685164478341162 and parameters: {'n_estimators': 887, 'eta': 0.08752799414668098, 'max_depth': 9, 'alpha': 0.5839, 'lambda': 20.43525878904361, 'max_bin': 457}. Best is trial 0 with value: 0.685164478341162.\n",
      "[I 2023-12-11 09:13:01,924] Trial 1 finished with value: 0.6877969683144108 and parameters: {'n_estimators': 698, 'eta': 0.07135989592336833, 'max_depth': 7, 'alpha': 0.8193, 'lambda': 21.411145306778998, 'max_bin': 479}. Best is trial 1 with value: 0.6877969683144108.\n",
      "[I 2023-12-11 09:13:04,268] Trial 2 finished with value: 0.33691829221926034 and parameters: {'n_estimators': 87, 'eta': 0.03510633241492115, 'max_depth': 9, 'alpha': 0.36860000000000004, 'lambda': 30.942179430007346, 'max_bin': 288}. Best is trial 1 with value: 0.6877969683144108.\n",
      "[I 2023-12-11 09:13:14,433] Trial 3 finished with value: 0.6557777849539653 and parameters: {'n_estimators': 370, 'eta': 0.029907533952022485, 'max_depth': 8, 'alpha': 0.257, 'lambda': 24.143316514428257, 'max_bin': 269}. Best is trial 1 with value: 0.6877969683144108.\n",
      "[I 2023-12-11 09:13:20,436] Trial 4 finished with value: 0.6480627369488019 and parameters: {'n_estimators': 149, 'eta': 0.04163211672116495, 'max_depth': 12, 'alpha': 0.5298, 'lambda': 16.63809465223446, 'max_bin': 494}. Best is trial 1 with value: 0.6877969683144108.\n",
      "[I 2023-12-11 09:13:35,798] Trial 5 finished with value: 0.6828385415492766 and parameters: {'n_estimators': 667, 'eta': 0.05074705803267841, 'max_depth': 7, 'alpha': 0.1144, 'lambda': 22.19570490862099, 'max_bin': 278}. Best is trial 1 with value: 0.6877969683144108.\n",
      "[I 2023-12-11 09:13:50,957] Trial 6 finished with value: 0.6883575071365173 and parameters: {'n_estimators': 655, 'eta': 0.08160308035367828, 'max_depth': 9, 'alpha': 0.6737000000000001, 'lambda': 18.93984488790502, 'max_bin': 422}. Best is trial 6 with value: 0.6883575071365173.\n",
      "[I 2023-12-11 09:14:07,558] Trial 7 finished with value: 0.6825585840720814 and parameters: {'n_estimators': 861, 'eta': 0.03827139380812326, 'max_depth': 6, 'alpha': 0.8525, 'lambda': 11.368020624826253, 'max_bin': 314}. Best is trial 6 with value: 0.6883575071365173.\n",
      "[I 2023-12-11 09:14:10,757] Trial 8 finished with value: 0.41400283033070534 and parameters: {'n_estimators': 106, 'eta': 0.025502774978474026, 'max_depth': 9, 'alpha': 0.26880000000000004, 'lambda': 2.0538111776548265, 'max_bin': 435}. Best is trial 6 with value: 0.6883575071365173.\n",
      "[I 2023-12-11 09:14:32,688] Trial 9 finished with value: 0.689976528016069 and parameters: {'n_estimators': 696, 'eta': 0.035236961681013555, 'max_depth': 10, 'alpha': 0.4748, 'lambda': 5.649795919949797, 'max_bin': 319}. Best is trial 9 with value: 0.689976528016069.\n",
      "[I 2023-12-11 09:14:35,987] Trial 10 finished with value: -20.61976353076833 and parameters: {'n_estimators': 440, 'eta': 0.00041073716488148476, 'max_depth': 12, 'alpha': 0.0001, 'lambda': 39.24015210389571, 'max_bin': 333}. Best is trial 9 with value: 0.689976528016069.\n",
      "[I 2023-12-11 09:14:48,302] Trial 11 finished with value: 0.6759037622694551 and parameters: {'n_estimators': 626, 'eta': 0.09754028566081584, 'max_depth': 11, 'alpha': 0.7140000000000001, 'lambda': 4.805295469792907, 'max_bin': 382}. Best is trial 9 with value: 0.689976528016069.\n",
      "[I 2023-12-11 09:15:04,195] Trial 12 finished with value: 0.6861154762062166 and parameters: {'n_estimators': 563, 'eta': 0.06313782221577142, 'max_depth': 10, 'alpha': 0.6516000000000001, 'lambda': 8.51712292625001, 'max_bin': 406}. Best is trial 9 with value: 0.689976528016069.\n",
      "[I 2023-12-11 09:15:22,648] Trial 13 finished with value: 0.6917193822605282 and parameters: {'n_estimators': 778, 'eta': 0.07837339133306301, 'max_depth': 10, 'alpha': 0.9936, 'lambda': 13.240374702972765, 'max_bin': 355}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:15:42,955] Trial 14 finished with value: 0.6864877861051168 and parameters: {'n_estimators': 764, 'eta': 0.06030091653693663, 'max_depth': 11, 'alpha': 1.0, 'lambda': 12.562650154471896, 'max_bin': 346}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:15:57,924] Trial 15 finished with value: 0.6870273645266198 and parameters: {'n_estimators': 792, 'eta': 0.07841703128814402, 'max_depth': 10, 'alpha': 0.3987, 'lambda': 6.507091817285462, 'max_bin': 361}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:16:06,606] Trial 16 finished with value: 0.6722612898516196 and parameters: {'n_estimators': 558, 'eta': 0.05432519574818198, 'max_depth': 5, 'alpha': 0.9804, 'lambda': 1.4616175523297654, 'max_bin': 314}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:16:18,535] Trial 17 finished with value: 0.685880060592542 and parameters: {'n_estimators': 310, 'eta': 0.06905390709767205, 'max_depth': 11, 'alpha': 0.4646, 'lambda': 11.531512859513606, 'max_bin': 384}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:16:32,030] Trial 18 finished with value: 0.6871988949165242 and parameters: {'n_estimators': 772, 'eta': 0.09962453767239071, 'max_depth': 10, 'alpha': 0.8307, 'lambda': 14.506107343506857, 'max_bin': 254}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:16:45,315] Trial 19 finished with value: 0.6836287950849603 and parameters: {'n_estimators': 498, 'eta': 0.0482388030725544, 'max_depth': 8, 'alpha': 0.7485, 'lambda': 7.987717908149608, 'max_bin': 307}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:16:54,053] Trial 20 finished with value: 0.681359974741644 and parameters: {'n_estimators': 244, 'eta': 0.07156843201028917, 'max_depth': 10, 'alpha': 0.2725, 'lambda': 4.478168352316393, 'max_bin': 350}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:17:08,578] Trial 21 finished with value: 0.6855232997540355 and parameters: {'n_estimators': 707, 'eta': 0.0810806980194571, 'max_depth': 8, 'alpha': 0.621, 'lambda': 16.88853590872278, 'max_bin': 411}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:17:21,117] Trial 22 finished with value: 0.6871624035557568 and parameters: {'n_estimators': 608, 'eta': 0.08845260539573013, 'max_depth': 9, 'alpha': 0.47250000000000003, 'lambda': 10.083349193306478, 'max_bin': 412}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:17:36,386] Trial 23 finished with value: 0.68861323977827 and parameters: {'n_estimators': 814, 'eta': 0.0802107394798098, 'max_depth': 11, 'alpha': 0.8784000000000001, 'lambda': 14.640483336966925, 'max_bin': 375}. Best is trial 13 with value: 0.6917193822605282.\n",
      "[I 2023-12-11 09:17:56,120] Trial 24 finished with value: 0.6918738799948488 and parameters: {'n_estimators': 808, 'eta': 0.059694207062120336, 'max_depth': 11, 'alpha': 0.9304, 'lambda': 14.033742098601945, 'max_bin': 379}. Best is trial 24 with value: 0.6918738799948488.\n",
      "[I 2023-12-11 09:18:15,631] Trial 25 finished with value: 0.6906100154208593 and parameters: {'n_estimators': 900, 'eta': 0.05785677909665264, 'max_depth': 12, 'alpha': 0.9063, 'lambda': 8.601070433061604, 'max_bin': 328}. Best is trial 24 with value: 0.6918738799948488.\n",
      "[I 2023-12-11 09:18:35,145] Trial 26 finished with value: 0.6888660085313543 and parameters: {'n_estimators': 898, 'eta': 0.06042292664367907, 'max_depth': 12, 'alpha': 0.9214, 'lambda': 9.927287959642364, 'max_bin': 360}. Best is trial 24 with value: 0.6918738799948488.\n",
      "[I 2023-12-11 09:18:54,293] Trial 27 finished with value: 0.6932640036953461 and parameters: {'n_estimators': 832, 'eta': 0.06473323099536027, 'max_depth': 12, 'alpha': 0.9414, 'lambda': 13.803022641898428, 'max_bin': 337}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:19:13,980] Trial 28 finished with value: 0.6897418055657372 and parameters: {'n_estimators': 828, 'eta': 0.06642056200680663, 'max_depth': 11, 'alpha': 0.7811, 'lambda': 14.348535944738954, 'max_bin': 391}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:19:32,890] Trial 29 finished with value: 0.6897429761720154 and parameters: {'n_estimators': 749, 'eta': 0.07424988580455683, 'max_depth': 12, 'alpha': 0.9497000000000001, 'lambda': 17.61421956540329, 'max_bin': 366}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:19:46,965] Trial 30 finished with value: 0.6859791789295117 and parameters: {'n_estimators': 848, 'eta': 0.08859722229110097, 'max_depth': 11, 'alpha': 0.9123, 'lambda': 12.494822723937654, 'max_bin': 445}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:20:06,965] Trial 31 finished with value: 0.6882901290450104 and parameters: {'n_estimators': 897, 'eta': 0.05793255167631876, 'max_depth': 12, 'alpha': 0.9989, 'lambda': 9.2041387162933, 'max_bin': 332}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:20:25,216] Trial 32 finished with value: 0.6891407154026582 and parameters: {'n_estimators': 815, 'eta': 0.06731218925571512, 'max_depth': 12, 'alpha': 0.879, 'lambda': 14.137791129944864, 'max_bin': 343}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:20:42,501] Trial 33 finished with value: 0.6870677745110015 and parameters: {'n_estimators': 736, 'eta': 0.06387053390813738, 'max_depth': 11, 'alpha': 0.8176, 'lambda': 8.566092812716338, 'max_bin': 303}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:21:03,950] Trial 34 finished with value: 0.6911118819325471 and parameters: {'n_estimators': 850, 'eta': 0.0726673309606417, 'max_depth': 12, 'alpha': 0.9276000000000001, 'lambda': 20.374276338266267, 'max_bin': 328}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:21:21,296] Trial 35 finished with value: 0.6894120364265669 and parameters: {'n_estimators': 727, 'eta': 0.07632245614951155, 'max_depth': 11, 'alpha': 0.786, 'lambda': 20.122056134294265, 'max_bin': 291}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:21:43,584] Trial 36 finished with value: 0.6886355059118408 and parameters: {'n_estimators': 837, 'eta': 0.07041938986215102, 'max_depth': 12, 'alpha': 0.5635, 'lambda': 21.835170710224908, 'max_bin': 394}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:22:03,112] Trial 37 finished with value: 0.6901031156260787 and parameters: {'n_estimators': 778, 'eta': 0.07278758445671409, 'max_depth': 10, 'alpha': 0.9407000000000001, 'lambda': 24.51340811085209, 'max_bin': 366}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:22:20,768] Trial 38 finished with value: 0.6897912166598185 and parameters: {'n_estimators': 670, 'eta': 0.08455766735573786, 'max_depth': 12, 'alpha': 0.7233, 'lambda': 18.630128509751092, 'max_bin': 462}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:22:38,284] Trial 39 finished with value: 0.6900703636119168 and parameters: {'n_estimators': 853, 'eta': 0.0667842353819212, 'max_depth': 7, 'alpha': 0.8470000000000001, 'lambda': 15.83903197407033, 'max_bin': 294}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:22:56,133] Trial 40 finished with value: 0.690014431870993 and parameters: {'n_estimators': 597, 'eta': 0.07578785828078108, 'max_depth': 11, 'alpha': 0.9655, 'lambda': 18.716811053997258, 'max_bin': 351}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:23:18,596] Trial 41 finished with value: 0.6886052767264864 and parameters: {'n_estimators': 869, 'eta': 0.05566036870724517, 'max_depth': 12, 'alpha': 0.8979, 'lambda': 12.037017169060103, 'max_bin': 327}. Best is trial 27 with value: 0.6932640036953461.\n",
      "[I 2023-12-11 09:23:46,351] Trial 42 finished with value: 0.6950106459270369 and parameters: {'n_estimators': 900, 'eta': 0.04873926601611921, 'max_depth': 12, 'alpha': 0.9139, 'lambda': 15.88205462774893, 'max_bin': 336}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:24:13,036] Trial 43 finished with value: 0.688337702273463 and parameters: {'n_estimators': 799, 'eta': 0.04851094196181824, 'max_depth': 12, 'alpha': 0.7864, 'lambda': 16.062266795349096, 'max_bin': 337}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:24:33,954] Trial 44 finished with value: 0.6896353255266878 and parameters: {'n_estimators': 854, 'eta': 0.06357810215858378, 'max_depth': 11, 'alpha': 0.8657, 'lambda': 20.93365005494815, 'max_bin': 353}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:24:57,062] Trial 45 finished with value: 0.6910632157245316 and parameters: {'n_estimators': 691, 'eta': 0.04443023693899734, 'max_depth': 10, 'alpha': 0.9568000000000001, 'lambda': 23.72905282521807, 'max_bin': 279}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:25:20,800] Trial 46 finished with value: 0.6920124716106211 and parameters: {'n_estimators': 747, 'eta': 0.05151467894140132, 'max_depth': 12, 'alpha': 0.6950000000000001, 'lambda': 17.185240187666494, 'max_bin': 375}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:25:41,740] Trial 47 finished with value: 0.6922862717583254 and parameters: {'n_estimators': 640, 'eta': 0.05344324764104754, 'max_depth': 11, 'alpha': 0.6761, 'lambda': 17.273688560099618, 'max_bin': 376}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:25:54,180] Trial 48 finished with value: 0.6767469623903504 and parameters: {'n_estimators': 642, 'eta': 0.05257346118592484, 'max_depth': 6, 'alpha': 0.6718000000000001, 'lambda': 17.44064448744973, 'max_bin': 373}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:26:12,678] Trial 49 finished with value: 0.688862070480694 and parameters: {'n_estimators': 510, 'eta': 0.04357394246842862, 'max_depth': 11, 'alpha': 0.6014, 'lambda': 15.478974203553657, 'max_bin': 423}. Best is trial 42 with value: 0.6950106459270369.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6950\n",
      "\tBest params:\n",
      "\t\tn_estimators: 900\n",
      "\t\teta: 0.04873926601611921\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9139\n",
      "\t\tlambda: 15.88205462774893\n",
      "\t\tmax_bin: 336\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.681005\n",
      "1                    TP  406.000000\n",
      "2                    TN  353.000000\n",
      "3                    FP   77.000000\n",
      "4                    FN   63.000000\n",
      "5              Accuracy    0.844271\n",
      "6             Precision    0.840580\n",
      "7           Sensitivity    0.865672\n",
      "8           Specificity    0.820900\n",
      "9              F1 score    0.852941\n",
      "10  F1 score (weighted)    0.844128\n",
      "11     F1 score (macro)    0.843728\n",
      "12    Balanced Accuracy    0.843301\n",
      "13                  MCC    0.687868\n",
      "14                  NPV    0.848600\n",
      "15              ROC_AUC    0.843301\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "y_pred_xgb_0_cat = np.where((y_pred_xgb_0 >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 09:26:39,296] Trial 50 finished with value: 0.6870468097487868 and parameters: {'n_estimators': 720, 'eta': 0.051822423764699914, 'max_depth': 12, 'alpha': 0.5429, 'lambda': 13.51954489941021, 'max_bin': 402}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:27:02,766] Trial 51 finished with value: 0.6820498085517771 and parameters: {'n_estimators': 764, 'eta': 0.05591524561038364, 'max_depth': 11, 'alpha': 0.7190000000000001, 'lambda': 13.30062668930453, 'max_bin': 384}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:27:14,888] Trial 52 finished with value: 0.6798546306956098 and parameters: {'n_estimators': 400, 'eta': 0.04797619936349773, 'max_depth': 9, 'alpha': 0.8167000000000001, 'lambda': 11.133198220518537, 'max_bin': 369}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:27:38,724] Trial 53 finished with value: 0.6882350695758063 and parameters: {'n_estimators': 799, 'eta': 0.038526101573035446, 'max_depth': 10, 'alpha': 0.6254000000000001, 'lambda': 16.10299392324831, 'max_bin': 358}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:27:59,190] Trial 54 finished with value: 0.6817418262823006 and parameters: {'n_estimators': 687, 'eta': 0.053556966070385334, 'max_depth': 11, 'alpha': 0.39740000000000003, 'lambda': 12.796957556853101, 'max_bin': 395}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:28:19,412] Trial 55 finished with value: 0.6826453636814025 and parameters: {'n_estimators': 769, 'eta': 0.05928189836056413, 'max_depth': 12, 'alpha': 0.9985, 'lambda': 17.828950301382353, 'max_bin': 341}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:28:35,713] Trial 56 finished with value: 0.683518890230423 and parameters: {'n_estimators': 552, 'eta': 0.06270347121302661, 'max_depth': 10, 'alpha': 0.7449, 'lambda': 10.99921469664385, 'max_bin': 376}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:28:57,567] Trial 57 finished with value: 0.6863951600232777 and parameters: {'n_estimators': 740, 'eta': 0.05085350989647445, 'max_depth': 11, 'alpha': 0.6847000000000001, 'lambda': 15.824161272035573, 'max_bin': 421}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:29:18,471] Trial 58 finished with value: 0.6733456430897021 and parameters: {'n_estimators': 876, 'eta': 0.0610118199772372, 'max_depth': 12, 'alpha': 0.0765, 'lambda': 14.44803215164535, 'max_bin': 383}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:29:42,964] Trial 59 finished with value: 0.6877695498149322 and parameters: {'n_estimators': 796, 'eta': 0.04701107287511502, 'max_depth': 11, 'alpha': 0.5116, 'lambda': 19.459958793302334, 'max_bin': 317}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:29:49,059] Trial 60 finished with value: 0.6589479751062747 and parameters: {'n_estimators': 184, 'eta': 0.05581459509676796, 'max_depth': 9, 'alpha': 0.8582000000000001, 'lambda': 17.123493307243915, 'max_bin': 357}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:29:51,016] Trial 61 finished with value: 0.49098227766222446 and parameters: {'n_estimators': 51, 'eta': 0.06952337617075552, 'max_depth': 12, 'alpha': 0.9304, 'lambda': 20.074055697578146, 'max_bin': 324}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:30:09,709] Trial 62 finished with value: 0.681740411592066 and parameters: {'n_estimators': 839, 'eta': 0.0652314922309632, 'max_depth': 12, 'alpha': 0.8957, 'lambda': 18.918227248384703, 'max_bin': 344}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:30:26,901] Trial 63 finished with value: 0.6834418702155516 and parameters: {'n_estimators': 823, 'eta': 0.07081822156315849, 'max_depth': 12, 'alpha': 0.9637, 'lambda': 13.326604987451315, 'max_bin': 310}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:30:50,631] Trial 64 finished with value: 0.6820011948621885 and parameters: {'n_estimators': 874, 'eta': 0.05885611537982435, 'max_depth': 12, 'alpha': 0.3154, 'lambda': 14.809254879331611, 'max_bin': 496}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:31:08,048] Trial 65 finished with value: 0.6811474133546401 and parameters: {'n_estimators': 815, 'eta': 0.076957480251361, 'max_depth': 11, 'alpha': 0.9259000000000001, 'lambda': 16.854338457019775, 'max_bin': 338}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:31:30,857] Trial 66 finished with value: 0.6851597648761434 and parameters: {'n_estimators': 757, 'eta': 0.050923292324244425, 'max_depth': 12, 'alpha': 0.8259000000000001, 'lambda': 12.058304792244083, 'max_bin': 323}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:31:49,011] Trial 67 finished with value: 0.6860716534961406 and parameters: {'n_estimators': 877, 'eta': 0.06767666819761892, 'max_depth': 11, 'alpha': 0.9656, 'lambda': 20.906484240322424, 'max_bin': 362}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:32:05,195] Trial 68 finished with value: 0.6828448409735633 and parameters: {'n_estimators': 664, 'eta': 0.07295898280485857, 'max_depth': 10, 'alpha': 0.7802, 'lambda': 15.115964907044088, 'max_bin': 377}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:32:24,713] Trial 69 finished with value: 0.6813798312726081 and parameters: {'n_estimators': 717, 'eta': 0.06069472198697887, 'max_depth': 12, 'alpha': 0.8822000000000001, 'lambda': 17.997817429811615, 'max_bin': 298}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:32:44,569] Trial 70 finished with value: 0.683563940633905 and parameters: {'n_estimators': 785, 'eta': 0.06316789437701467, 'max_depth': 11, 'alpha': 0.93, 'lambda': 10.610921849335606, 'max_bin': 349}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:33:10,205] Trial 71 finished with value: 0.6874232871275485 and parameters: {'n_estimators': 694, 'eta': 0.04086157932831872, 'max_depth': 10, 'alpha': 0.9795, 'lambda': 23.093508519029868, 'max_bin': 281}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:33:31,510] Trial 72 finished with value: 0.6847746783276147 and parameters: {'n_estimators': 634, 'eta': 0.045529230813363075, 'max_depth': 9, 'alpha': 0.9500000000000001, 'lambda': 25.46857370746306, 'max_bin': 263}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:33:51,727] Trial 73 finished with value: 0.6861777272065231 and parameters: {'n_estimators': 594, 'eta': 0.04219952699613317, 'max_depth': 10, 'alpha': 0.9992000000000001, 'lambda': 22.185247972154144, 'max_bin': 281}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:34:10,714] Trial 74 finished with value: 0.6840165693994573 and parameters: {'n_estimators': 682, 'eta': 0.04447970218253957, 'max_depth': 8, 'alpha': 0.8459, 'lambda': 19.670244345313954, 'max_bin': 332}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:34:32,478] Trial 75 finished with value: 0.6865841533506549 and parameters: {'n_estimators': 841, 'eta': 0.04971843738779053, 'max_depth': 10, 'alpha': 0.8818, 'lambda': 16.717009891547193, 'max_bin': 270}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:34:57,261] Trial 76 finished with value: 0.6821965104626776 and parameters: {'n_estimators': 730, 'eta': 0.05390409347717587, 'max_depth': 12, 'alpha': 0.8047000000000001, 'lambda': 18.25150316645638, 'max_bin': 368}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:35:16,258] Trial 77 finished with value: 0.682923973908729 and parameters: {'n_estimators': 816, 'eta': 0.05709863251914373, 'max_depth': 11, 'alpha': 0.9161, 'lambda': 13.593690446972408, 'max_bin': 400}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:35:34,417] Trial 78 finished with value: 0.6830495493018927 and parameters: {'n_estimators': 757, 'eta': 0.06591449510485185, 'max_depth': 12, 'alpha': 0.9427000000000001, 'lambda': 14.976344714978588, 'max_bin': 304}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:36:01,088] Trial 79 finished with value: 0.6866659606620975 and parameters: {'n_estimators': 703, 'eta': 0.03445187073057341, 'max_depth': 11, 'alpha': 0.7576, 'lambda': 12.052916917549375, 'max_bin': 387}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:36:29,019] Trial 80 finished with value: 0.6832751269523251 and parameters: {'n_estimators': 785, 'eta': 0.04537976198867684, 'max_depth': 12, 'alpha': 0.9703, 'lambda': 21.379869380266754, 'max_bin': 352}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:36:47,818] Trial 81 finished with value: 0.6797844904818675 and parameters: {'n_estimators': 900, 'eta': 0.05771463152067871, 'max_depth': 12, 'alpha': 0.9149, 'lambda': 9.685495975704107, 'max_bin': 332}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:37:07,825] Trial 82 finished with value: 0.6823764148352744 and parameters: {'n_estimators': 855, 'eta': 0.05169724591734576, 'max_depth': 12, 'alpha': 0.8497, 'lambda': 6.854378183937985, 'max_bin': 320}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:37:27,479] Trial 83 finished with value: 0.6862778057298842 and parameters: {'n_estimators': 899, 'eta': 0.05479183590351462, 'max_depth': 12, 'alpha': 0.901, 'lambda': 15.972382354126681, 'max_bin': 327}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:37:50,359] Trial 84 finished with value: 0.6859874845356946 and parameters: {'n_estimators': 867, 'eta': 0.047331185085420904, 'max_depth': 11, 'alpha': 0.9413, 'lambda': 13.867010432789213, 'max_bin': 313}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:38:10,199] Trial 85 finished with value: 0.6827037187088667 and parameters: {'n_estimators': 838, 'eta': 0.06847650922098578, 'max_depth': 12, 'alpha': 0.875, 'lambda': 12.943167772527113, 'max_bin': 250}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:38:31,075] Trial 86 finished with value: 0.6822083970333427 and parameters: {'n_estimators': 803, 'eta': 0.06115636008786794, 'max_depth': 12, 'alpha': 0.9811000000000001, 'lambda': 11.281110493478215, 'max_bin': 363}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:38:43,774] Trial 87 finished with value: 0.6708404873594791 and parameters: {'n_estimators': 744, 'eta': 0.05792136209804483, 'max_depth': 5, 'alpha': 0.6906, 'lambda': 17.208605894582025, 'max_bin': 482}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:39:02,622] Trial 88 finished with value: 0.683299641296591 and parameters: {'n_estimators': 881, 'eta': 0.053790682413151616, 'max_depth': 11, 'alpha': 0.9036000000000001, 'lambda': 9.146150545718939, 'max_bin': 378}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:39:20,080] Trial 89 finished with value: 0.6841394183826801 and parameters: {'n_estimators': 453, 'eta': 0.04967507286934007, 'max_depth': 10, 'alpha': 0.2074, 'lambda': 19.285353506527137, 'max_bin': 371}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:39:37,312] Trial 90 finished with value: 0.6813806017383497 and parameters: {'n_estimators': 857, 'eta': 0.06502606888552402, 'max_depth': 9, 'alpha': 0.6404000000000001, 'lambda': 15.070999366136055, 'max_bin': 338}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:39:53,979] Trial 91 finished with value: 0.6823766523956016 and parameters: {'n_estimators': 770, 'eta': 0.07946121855412956, 'max_depth': 10, 'alpha': 0.9566, 'lambda': 24.81748468332903, 'max_bin': 355}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:40:10,024] Trial 92 finished with value: 0.6818004317347082 and parameters: {'n_estimators': 782, 'eta': 0.07030271907843638, 'max_depth': 10, 'alpha': 0.4541, 'lambda': 20.33236769768879, 'max_bin': 389}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:40:23,313] Trial 93 finished with value: 0.6815455528361574 and parameters: {'n_estimators': 336, 'eta': 0.08278470498363695, 'max_depth': 11, 'alpha': 0.5834, 'lambda': 23.664413367789866, 'max_bin': 345}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:40:39,703] Trial 94 finished with value: 0.6816734612742059 and parameters: {'n_estimators': 825, 'eta': 0.07420915771422162, 'max_depth': 7, 'alpha': 0.9833000000000001, 'lambda': 25.81047558885375, 'max_bin': 366}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:40:55,102] Trial 95 finished with value: 0.6820701020766482 and parameters: {'n_estimators': 808, 'eta': 0.07279699447925622, 'max_depth': 10, 'alpha': 0.9428000000000001, 'lambda': 16.56341012251335, 'max_bin': 382}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:41:15,569] Trial 96 finished with value: 0.6791342189823648 and parameters: {'n_estimators': 832, 'eta': 0.06208065154122071, 'max_depth': 12, 'alpha': 0.8435, 'lambda': 12.241385836469949, 'max_bin': 348}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:41:30,976] Trial 97 finished with value: 0.685394006625924 and parameters: {'n_estimators': 745, 'eta': 0.0773920148737424, 'max_depth': 8, 'alpha': 0.889, 'lambda': 18.194047076215817, 'max_bin': 328}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:41:48,103] Trial 98 finished with value: 0.6875488712870252 and parameters: {'n_estimators': 651, 'eta': 0.05654132366051245, 'max_depth': 9, 'alpha': 0.9194, 'lambda': 10.658821467980239, 'max_bin': 359}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:42:08,618] Trial 99 finished with value: 0.6830589707819881 and parameters: {'n_estimators': 779, 'eta': 0.05913615012814199, 'max_depth': 12, 'alpha': 0.8034, 'lambda': 15.666499111898133, 'max_bin': 399}. Best is trial 42 with value: 0.6950106459270369.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6950\n",
      "\tBest params:\n",
      "\t\tn_estimators: 900\n",
      "\t\teta: 0.04873926601611921\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9139\n",
      "\t\tlambda: 15.88205462774893\n",
      "\t\tmax_bin: 336\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.681005    0.720551\n",
      "1                    TP  406.000000  405.000000\n",
      "2                    TN  353.000000  341.000000\n",
      "3                    FP   77.000000   83.000000\n",
      "4                    FN   63.000000   70.000000\n",
      "5              Accuracy    0.844271    0.829811\n",
      "6             Precision    0.840580    0.829918\n",
      "7           Sensitivity    0.865672    0.852632\n",
      "8           Specificity    0.820900    0.804200\n",
      "9              F1 score    0.852941    0.841121\n",
      "10  F1 score (weighted)    0.844128    0.829635\n",
      "11     F1 score (macro)    0.843728    0.828944\n",
      "12    Balanced Accuracy    0.843301    0.828438\n",
      "13                  MCC    0.687868    0.658238\n",
      "14                  NPV    0.848600    0.829700\n",
      "15              ROC_AUC    0.843301    0.828438\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_1_cat = np.where((y_pred_xgb_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 09:42:31,533] Trial 100 finished with value: 0.6887663515931131 and parameters: {'n_estimators': 724, 'eta': 0.067634759316528, 'max_depth': 11, 'alpha': 0.8631000000000001, 'lambda': 14.403601923105843, 'max_bin': 372}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:42:46,996] Trial 101 finished with value: 0.6836247618703425 and parameters: {'n_estimators': 887, 'eta': 0.06477992450501352, 'max_depth': 6, 'alpha': 0.9453, 'lambda': 16.12969013840523, 'max_bin': 270}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:43:03,596] Trial 102 finished with value: 0.6874775531618645 and parameters: {'n_estimators': 849, 'eta': 0.07570869852074036, 'max_depth': 7, 'alpha': 0.9024000000000001, 'lambda': 13.851741965322134, 'max_bin': 286}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:43:24,517] Trial 103 finished with value: 0.6927667692009856 and parameters: {'n_estimators': 868, 'eta': 0.07218091310642459, 'max_depth': 10, 'alpha': 0.9856, 'lambda': 17.265884390838007, 'max_bin': 259}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:43:41,903] Trial 104 finished with value: 0.6870842751633466 and parameters: {'n_estimators': 864, 'eta': 0.07233730517762615, 'max_depth': 10, 'alpha': 0.9956, 'lambda': 17.790986959134564, 'max_bin': 260}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:43:59,727] Trial 105 finished with value: 0.6861583058822541 and parameters: {'n_estimators': 614, 'eta': 0.06919928249957971, 'max_depth': 10, 'alpha': 0.9609000000000001, 'lambda': 18.656391005408018, 'max_bin': 274}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:44:21,859] Trial 106 finished with value: 0.6916983400723307 and parameters: {'n_estimators': 799, 'eta': 0.0525530949709949, 'max_depth': 10, 'alpha': 0.9304, 'lambda': 12.999634013251066, 'max_bin': 339}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:44:45,221] Trial 107 finished with value: 0.6920735822776845 and parameters: {'n_estimators': 798, 'eta': 0.05255641497086787, 'max_depth': 10, 'alpha': 0.9832000000000001, 'lambda': 12.932558631545596, 'max_bin': 259}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:45:08,262] Trial 108 finished with value: 0.692028691958294 and parameters: {'n_estimators': 802, 'eta': 0.05269784769801675, 'max_depth': 10, 'alpha': 0.9829, 'lambda': 12.821492735771015, 'max_bin': 261}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:45:29,583] Trial 109 finished with value: 0.6898180647432903 and parameters: {'n_estimators': 805, 'eta': 0.05256264498803942, 'max_depth': 9, 'alpha': 0.9831000000000001, 'lambda': 13.022779246750433, 'max_bin': 262}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:45:52,157] Trial 110 finished with value: 0.6943349553657518 and parameters: {'n_estimators': 830, 'eta': 0.04910850421367145, 'max_depth': 10, 'alpha': 0.9259000000000001, 'lambda': 11.737634297595607, 'max_bin': 266}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:46:16,538] Trial 111 finished with value: 0.6941961567792541 and parameters: {'n_estimators': 823, 'eta': 0.049047467641724955, 'max_depth': 10, 'alpha': 0.9279000000000001, 'lambda': 11.539672371605183, 'max_bin': 253}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:46:40,661] Trial 112 finished with value: 0.6934612238204176 and parameters: {'n_estimators': 827, 'eta': 0.04962377490460049, 'max_depth': 10, 'alpha': 0.9720000000000001, 'lambda': 11.562195301569439, 'max_bin': 255}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:47:05,463] Trial 113 finished with value: 0.6915206820544555 and parameters: {'n_estimators': 825, 'eta': 0.048619549457867305, 'max_depth': 10, 'alpha': 0.9683, 'lambda': 11.341299736203778, 'max_bin': 253}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:47:28,766] Trial 114 finished with value: 0.6912055778548386 and parameters: {'n_estimators': 832, 'eta': 0.05062795329885048, 'max_depth': 10, 'alpha': 0.9837, 'lambda': 10.328035808122365, 'max_bin': 257}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:47:49,125] Trial 115 finished with value: 0.6880756040743037 and parameters: {'n_estimators': 762, 'eta': 0.05552708679180639, 'max_depth': 10, 'alpha': 0.9968, 'lambda': 11.632296056586883, 'max_bin': 266}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:48:13,327] Trial 116 finished with value: 0.6914322755214484 and parameters: {'n_estimators': 885, 'eta': 0.04790692505113044, 'max_depth': 10, 'alpha': 0.9583, 'lambda': 14.208751544959373, 'max_bin': 257}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:48:34,977] Trial 117 finished with value: 0.6882184495082804 and parameters: {'n_estimators': 820, 'eta': 0.04685915270226418, 'max_depth': 9, 'alpha': 0.9262, 'lambda': 12.31810707282818, 'max_bin': 274}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:48:58,784] Trial 118 finished with value: 0.6914095020942415 and parameters: {'n_estimators': 792, 'eta': 0.04997213009191015, 'max_depth': 10, 'alpha': 0.8728, 'lambda': 13.746547609419247, 'max_bin': 251}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:49:20,594] Trial 119 finished with value: 0.691275345520369 and parameters: {'n_estimators': 868, 'eta': 0.055134133168792704, 'max_depth': 10, 'alpha': 0.9678, 'lambda': 14.987329948097278, 'max_bin': 265}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:49:35,845] Trial 120 finished with value: 0.6859795249291057 and parameters: {'n_estimators': 526, 'eta': 0.05303154201174701, 'max_depth': 9, 'alpha': 0.9092, 'lambda': 13.037536653422364, 'max_bin': 257}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:49:59,468] Trial 121 finished with value: 0.6898645562304431 and parameters: {'n_estimators': 848, 'eta': 0.05210684723269442, 'max_depth': 10, 'alpha': 0.9272, 'lambda': 11.671375483230207, 'max_bin': 274}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:50:21,585] Trial 122 finished with value: 0.6948348122469141 and parameters: {'n_estimators': 802, 'eta': 0.045598389361868386, 'max_depth': 10, 'alpha': 0.9398000000000001, 'lambda': 12.571608480893406, 'max_bin': 269}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:50:45,252] Trial 123 finished with value: 0.6948754916144664 and parameters: {'n_estimators': 810, 'eta': 0.0458640805312685, 'max_depth': 10, 'alpha': 0.9806, 'lambda': 12.64246534216388, 'max_bin': 268}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:51:09,179] Trial 124 finished with value: 0.6901526781136819 and parameters: {'n_estimators': 806, 'eta': 0.04570542301111537, 'max_depth': 10, 'alpha': 0.9482, 'lambda': 9.778713528470206, 'max_bin': 285}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:51:35,417] Trial 125 finished with value: 0.6922309074164275 and parameters: {'n_estimators': 843, 'eta': 0.04337179704706558, 'max_depth': 10, 'alpha': 0.8947, 'lambda': 12.410180571451882, 'max_bin': 267}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:52:00,273] Trial 126 finished with value: 0.694733807319422 and parameters: {'n_estimators': 839, 'eta': 0.04236482667556174, 'max_depth': 10, 'alpha': 0.8866, 'lambda': 10.773176288849918, 'max_bin': 267}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:52:25,189] Trial 127 finished with value: 0.6946128711954913 and parameters: {'n_estimators': 834, 'eta': 0.04237846801306591, 'max_depth': 10, 'alpha': 0.8913000000000001, 'lambda': 10.521761471434282, 'max_bin': 268}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:52:51,505] Trial 128 finished with value: 0.6928541798164415 and parameters: {'n_estimators': 841, 'eta': 0.04271694365702026, 'max_depth': 10, 'alpha': 0.8302, 'lambda': 10.340680436279918, 'max_bin': 267}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:53:15,334] Trial 129 finished with value: 0.6936917186056772 and parameters: {'n_estimators': 868, 'eta': 0.04269688571047088, 'max_depth': 10, 'alpha': 0.8863000000000001, 'lambda': 10.71968409413354, 'max_bin': 268}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:53:42,466] Trial 130 finished with value: 0.6923363254824746 and parameters: {'n_estimators': 885, 'eta': 0.0401773748192262, 'max_depth': 10, 'alpha': 0.8428, 'lambda': 10.135495434655242, 'max_bin': 293}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:54:10,816] Trial 131 finished with value: 0.6947902656638546 and parameters: {'n_estimators': 878, 'eta': 0.040980272030413115, 'max_depth': 10, 'alpha': 0.8324, 'lambda': 10.668708278188413, 'max_bin': 295}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:54:36,013] Trial 132 finished with value: 0.6933479714810853 and parameters: {'n_estimators': 881, 'eta': 0.04064879420407915, 'max_depth': 10, 'alpha': 0.8287, 'lambda': 10.600169705213728, 'max_bin': 292}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:55:01,863] Trial 133 finished with value: 0.6934467086276884 and parameters: {'n_estimators': 868, 'eta': 0.0423522881152765, 'max_depth': 10, 'alpha': 0.8092, 'lambda': 8.225396614951656, 'max_bin': 275}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:55:26,560] Trial 134 finished with value: 0.6910175820289991 and parameters: {'n_estimators': 862, 'eta': 0.043153015157572916, 'max_depth': 9, 'alpha': 0.7888000000000001, 'lambda': 8.974420229999287, 'max_bin': 299}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:55:53,758] Trial 135 finished with value: 0.6950036043271043 and parameters: {'n_estimators': 900, 'eta': 0.03890455660945997, 'max_depth': 10, 'alpha': 0.8639, 'lambda': 8.210365610549546, 'max_bin': 278}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:56:16,686] Trial 136 finished with value: 0.6930519616748303 and parameters: {'n_estimators': 900, 'eta': 0.039058392890072055, 'max_depth': 10, 'alpha': 0.8652000000000001, 'lambda': 8.123784549467326, 'max_bin': 289}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:56:41,719] Trial 137 finished with value: 0.6921054693518387 and parameters: {'n_estimators': 881, 'eta': 0.04167402046667436, 'max_depth': 10, 'alpha': 0.8161, 'lambda': 7.618050963889162, 'max_bin': 281}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:57:06,644] Trial 138 finished with value: 0.6935269120808119 and parameters: {'n_estimators': 861, 'eta': 0.03790134736499251, 'max_depth': 9, 'alpha': 0.8756, 'lambda': 9.51693287414565, 'max_bin': 277}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:57:31,084] Trial 139 finished with value: 0.6919085030845094 and parameters: {'n_estimators': 865, 'eta': 0.036207673580279584, 'max_depth': 9, 'alpha': 0.8539, 'lambda': 9.584854250187888, 'max_bin': 276}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:57:57,861] Trial 140 finished with value: 0.694285173222284 and parameters: {'n_estimators': 883, 'eta': 0.03729987940238792, 'max_depth': 10, 'alpha': 0.8851, 'lambda': 9.122218831435752, 'max_bin': 270}. Best is trial 42 with value: 0.6950106459270369.\n",
      "[I 2023-12-11 09:58:26,911] Trial 141 finished with value: 0.6959547969799948 and parameters: {'n_estimators': 886, 'eta': 0.03696319715961291, 'max_depth': 10, 'alpha': 0.887, 'lambda': 8.704210740196038, 'max_bin': 271}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 09:58:52,645] Trial 142 finished with value: 0.693346021654005 and parameters: {'n_estimators': 854, 'eta': 0.037666846559645265, 'max_depth': 10, 'alpha': 0.8841, 'lambda': 8.767111885206715, 'max_bin': 271}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 09:59:21,829] Trial 143 finished with value: 0.694595052503406 and parameters: {'n_estimators': 900, 'eta': 0.03360225038436243, 'max_depth': 10, 'alpha': 0.8702000000000001, 'lambda': 7.5664684711432075, 'max_bin': 284}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 09:59:50,234] Trial 144 finished with value: 0.694545168847146 and parameters: {'n_estimators': 896, 'eta': 0.03448032745281041, 'max_depth': 10, 'alpha': 0.8839, 'lambda': 9.561461966779504, 'max_bin': 284}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:00:19,523] Trial 145 finished with value: 0.6912042271156664 and parameters: {'n_estimators': 900, 'eta': 0.03347721029919371, 'max_depth': 10, 'alpha': 0.8731, 'lambda': 9.326623125751272, 'max_bin': 280}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:00:46,578] Trial 146 finished with value: 0.6945736736712592 and parameters: {'n_estimators': 889, 'eta': 0.03707311457686917, 'max_depth': 10, 'alpha': 0.892, 'lambda': 7.030054355338345, 'max_bin': 285}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:01:14,137] Trial 147 finished with value: 0.6950683960993214 and parameters: {'n_estimators': 887, 'eta': 0.03156221883460962, 'max_depth': 10, 'alpha': 0.9007000000000001, 'lambda': 7.448205787965727, 'max_bin': 284}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:01:41,006] Trial 148 finished with value: 0.6953662105603339 and parameters: {'n_estimators': 889, 'eta': 0.03315318432813035, 'max_depth': 10, 'alpha': 0.9069, 'lambda': 7.440675212719225, 'max_bin': 286}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:02:09,549] Trial 149 finished with value: 0.6943841281103048 and parameters: {'n_estimators': 884, 'eta': 0.032352048322121595, 'max_depth': 10, 'alpha': 0.9029, 'lambda': 7.152373367237766, 'max_bin': 286}. Best is trial 141 with value: 0.6959547969799948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6960\n",
      "\tBest params:\n",
      "\t\tn_estimators: 886\n",
      "\t\teta: 0.03696319715961291\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.887\n",
      "\t\tlambda: 8.704210740196038\n",
      "\t\tmax_bin: 271\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.681005    0.720551    0.717203\n",
      "1                    TP  406.000000  405.000000  405.000000\n",
      "2                    TN  353.000000  341.000000  346.000000\n",
      "3                    FP   77.000000   83.000000   82.000000\n",
      "4                    FN   63.000000   70.000000   66.000000\n",
      "5              Accuracy    0.844271    0.829811    0.835373\n",
      "6             Precision    0.840580    0.829918    0.831622\n",
      "7           Sensitivity    0.865672    0.852632    0.859873\n",
      "8           Specificity    0.820900    0.804200    0.808400\n",
      "9              F1 score    0.852941    0.841121    0.845511\n",
      "10  F1 score (weighted)    0.844128    0.829635    0.835180\n",
      "11     F1 score (macro)    0.843728    0.828944    0.834661\n",
      "12    Balanced Accuracy    0.843301    0.828438    0.834142\n",
      "13                  MCC    0.687868    0.658238    0.669854\n",
      "14                  NPV    0.848600    0.829700    0.839800\n",
      "15              ROC_AUC    0.843301    0.828438    0.834142\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_2_cat = np.where((y_pred_xgb_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 10:02:42,976] Trial 150 finished with value: 0.6847163155196014 and parameters: {'n_estimators': 899, 'eta': 0.03181335395216623, 'max_depth': 10, 'alpha': 0.9028, 'lambda': 7.145941364849205, 'max_bin': 284}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:03:07,849] Trial 151 finished with value: 0.6814745175493585 and parameters: {'n_estimators': 884, 'eta': 0.029891275433820943, 'max_depth': 10, 'alpha': 0.8524, 'lambda': 5.603943388868274, 'max_bin': 295}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:03:34,601] Trial 152 finished with value: 0.685376553495093 and parameters: {'n_estimators': 889, 'eta': 0.0353654417739092, 'max_depth': 10, 'alpha': 0.9076000000000001, 'lambda': 6.674241129464964, 'max_bin': 289}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:03:59,046] Trial 153 finished with value: 0.6861919137951835 and parameters: {'n_estimators': 883, 'eta': 0.03674410518623947, 'max_depth': 10, 'alpha': 0.8906000000000001, 'lambda': 7.3449018979548715, 'max_bin': 284}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:04:22,136] Trial 154 finished with value: 0.6873521348427434 and parameters: {'n_estimators': 852, 'eta': 0.039273251951128435, 'max_depth': 10, 'alpha': 0.8406, 'lambda': 7.98101391583128, 'max_bin': 287}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:04:47,315] Trial 155 finished with value: 0.6831560999866959 and parameters: {'n_estimators': 900, 'eta': 0.03352373490053448, 'max_depth': 10, 'alpha': 0.9098, 'lambda': 5.631368723493688, 'max_bin': 299}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:04:56,509] Trial 156 finished with value: 0.6522701415717889 and parameters: {'n_estimators': 258, 'eta': 0.030260594574129118, 'max_depth': 10, 'alpha': 0.8647, 'lambda': 8.500388156920993, 'max_bin': 279}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:05:20,089] Trial 157 finished with value: 0.6860017508557943 and parameters: {'n_estimators': 878, 'eta': 0.03616234511054528, 'max_depth': 10, 'alpha': 0.8908, 'lambda': 6.51967303089926, 'max_bin': 270}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:05:44,818] Trial 158 finished with value: 0.6823196738870912 and parameters: {'n_estimators': 841, 'eta': 0.038666943351244126, 'max_depth': 10, 'alpha': 0.861, 'lambda': 7.704664333280588, 'max_bin': 307}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:06:11,768] Trial 159 finished with value: 0.6824124545069915 and parameters: {'n_estimators': 874, 'eta': 0.033717613057804796, 'max_depth': 10, 'alpha': 0.9203, 'lambda': 8.692723454415429, 'max_bin': 279}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:06:34,310] Trial 160 finished with value: 0.6854270424820296 and parameters: {'n_estimators': 854, 'eta': 0.040050349984726716, 'max_depth': 10, 'alpha': 0.892, 'lambda': 7.352067676670702, 'max_bin': 271}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:06:55,170] Trial 161 finished with value: 0.6841087055629502 and parameters: {'n_estimators': 886, 'eta': 0.045200095927367866, 'max_depth': 10, 'alpha': 0.9359000000000001, 'lambda': 5.954340332653722, 'max_bin': 263}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:07:21,364] Trial 162 finished with value: 0.6838555958050796 and parameters: {'n_estimators': 900, 'eta': 0.036956355199394636, 'max_depth': 10, 'alpha': 0.9161, 'lambda': 9.105520261508827, 'max_bin': 285}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:07:48,807] Trial 163 finished with value: 0.6829033318129494 and parameters: {'n_estimators': 830, 'eta': 0.031223264418726044, 'max_depth': 10, 'alpha': 0.9347000000000001, 'lambda': 10.031071959877876, 'max_bin': 295}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:08:17,599] Trial 164 finished with value: 0.6809995683311734 and parameters: {'n_estimators': 857, 'eta': 0.0274077045162545, 'max_depth': 10, 'alpha': 0.8706, 'lambda': 8.107641523021913, 'max_bin': 274}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:08:45,104] Trial 165 finished with value: 0.686884694586096 and parameters: {'n_estimators': 873, 'eta': 0.03496143321610263, 'max_depth': 10, 'alpha': 0.9118, 'lambda': 11.214805556344114, 'max_bin': 265}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:09:10,228] Trial 166 finished with value: 0.6833039324987441 and parameters: {'n_estimators': 841, 'eta': 0.03206784827990721, 'max_depth': 9, 'alpha': 0.9529000000000001, 'lambda': 6.338095863603982, 'max_bin': 291}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:09:32,997] Trial 167 finished with value: 0.6826232214938954 and parameters: {'n_estimators': 820, 'eta': 0.038208222150596306, 'max_depth': 10, 'alpha': 0.8381000000000001, 'lambda': 4.737725458606825, 'max_bin': 280}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:09:55,786] Trial 168 finished with value: 0.6830861633563613 and parameters: {'n_estimators': 880, 'eta': 0.04065308948304556, 'max_depth': 10, 'alpha': 0.8835000000000001, 'lambda': 9.68978965323004, 'max_bin': 272}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:10:20,907] Trial 169 finished with value: 0.6849578655935404 and parameters: {'n_estimators': 851, 'eta': 0.035306393491441844, 'max_depth': 10, 'alpha': 0.9289000000000001, 'lambda': 7.25103078573272, 'max_bin': 302}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:10:43,368] Trial 170 finished with value: 0.6832173635148309 and parameters: {'n_estimators': 869, 'eta': 0.044198034169950884, 'max_depth': 10, 'alpha': 0.897, 'lambda': 10.832541175767435, 'max_bin': 283}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:11:08,694] Trial 171 finished with value: 0.6836843399781254 and parameters: {'n_estimators': 900, 'eta': 0.04152298063098481, 'max_depth': 10, 'alpha': 0.8611000000000001, 'lambda': 10.937092973913805, 'max_bin': 265}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:11:31,336] Trial 172 finished with value: 0.6838030670357622 and parameters: {'n_estimators': 870, 'eta': 0.04632187782578174, 'max_depth': 10, 'alpha': 0.887, 'lambda': 8.390885262605389, 'max_bin': 268}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:11:53,936] Trial 173 finished with value: 0.6823170820355303 and parameters: {'n_estimators': 836, 'eta': 0.04430140014818017, 'max_depth': 10, 'alpha': 0.9059, 'lambda': 10.201632779454815, 'max_bin': 277}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:12:20,878] Trial 174 finished with value: 0.6830467482368927 and parameters: {'n_estimators': 885, 'eta': 0.028700885349957343, 'max_depth': 10, 'alpha': 0.9404, 'lambda': 11.872630471299779, 'max_bin': 251}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:12:46,430] Trial 175 finished with value: 0.6822942885842606 and parameters: {'n_estimators': 855, 'eta': 0.03380487357699806, 'max_depth': 10, 'alpha': 0.8280000000000001, 'lambda': 9.085514613530316, 'max_bin': 261}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:13:06,777] Trial 176 finished with value: 0.6820922873499573 and parameters: {'n_estimators': 868, 'eta': 0.041837507002086674, 'max_depth': 8, 'alpha': 0.8746, 'lambda': 10.764776960061912, 'max_bin': 288}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:13:31,750] Trial 177 finished with value: 0.6825787676157147 and parameters: {'n_estimators': 825, 'eta': 0.032546330984530254, 'max_depth': 10, 'alpha': 0.8500000000000001, 'lambda': 9.749887672250471, 'max_bin': 270}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:13:57,185] Trial 178 finished with value: 0.6855679544179158 and parameters: {'n_estimators': 887, 'eta': 0.03657852628459155, 'max_depth': 10, 'alpha': 0.9182, 'lambda': 7.626329453474963, 'max_bin': 277}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:14:21,231] Trial 179 finished with value: 0.6820995661364531 and parameters: {'n_estimators': 841, 'eta': 0.039878050360229234, 'max_depth': 11, 'alpha': 0.8952, 'lambda': 8.737365937595518, 'max_bin': 265}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:14:39,734] Trial 180 finished with value: 0.6794293268102962 and parameters: {'n_estimators': 864, 'eta': 0.04657411657930395, 'max_depth': 10, 'alpha': 0.873, 'lambda': 6.75675636199526, 'max_bin': 283}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:15:00,294] Trial 181 finished with value: 0.6813922838213262 and parameters: {'n_estimators': 862, 'eta': 0.038267165987390425, 'max_depth': 9, 'alpha': 0.8753000000000001, 'lambda': 9.758250277845354, 'max_bin': 276}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:15:22,293] Trial 182 finished with value: 0.6831707313981552 and parameters: {'n_estimators': 885, 'eta': 0.037873579226758734, 'max_depth': 9, 'alpha': 0.9163, 'lambda': 9.313874049279939, 'max_bin': 272}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:15:36,737] Trial 183 finished with value: 0.6755330835689162 and parameters: {'n_estimators': 410, 'eta': 0.035221206602913974, 'max_depth': 10, 'alpha': 0.9534, 'lambda': 11.470986398658816, 'max_bin': 255}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:15:59,992] Trial 184 finished with value: 0.681574064429455 and parameters: {'n_estimators': 900, 'eta': 0.042702188729368595, 'max_depth': 10, 'alpha': 0.8881, 'lambda': 10.554632719951309, 'max_bin': 280}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:16:26,562] Trial 185 finished with value: 0.6829870179787054 and parameters: {'n_estimators': 815, 'eta': 0.0318354588255312, 'max_depth': 10, 'alpha': 0.8586, 'lambda': 8.164515365981046, 'max_bin': 290}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:16:49,325] Trial 186 finished with value: 0.6858286793556461 and parameters: {'n_estimators': 853, 'eta': 0.04792278820105068, 'max_depth': 10, 'alpha': 0.9321, 'lambda': 11.828064375917744, 'max_bin': 268}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:17:12,827] Trial 187 finished with value: 0.6856692533511666 and parameters: {'n_estimators': 871, 'eta': 0.044116954892221755, 'max_depth': 10, 'alpha': 0.9116000000000001, 'lambda': 5.221019329291444, 'max_bin': 296}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:17:36,663] Trial 188 finished with value: 0.6794094665522261 and parameters: {'n_estimators': 842, 'eta': 0.03961532210020385, 'max_depth': 9, 'alpha': 0.8331000000000001, 'lambda': 6.408450777971519, 'max_bin': 276}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:18:04,115] Trial 189 finished with value: 0.6812210139916128 and parameters: {'n_estimators': 883, 'eta': 0.027403025390167438, 'max_depth': 10, 'alpha': 0.8892, 'lambda': 9.084109749603272, 'max_bin': 262}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:18:27,669] Trial 190 finished with value: 0.6818556486108707 and parameters: {'n_estimators': 862, 'eta': 0.03704225416240475, 'max_depth': 11, 'alpha': 0.7952, 'lambda': 4.3187238721740915, 'max_bin': 286}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:18:51,759] Trial 191 finished with value: 0.686212707945349 and parameters: {'n_estimators': 827, 'eta': 0.04787903375061581, 'max_depth': 10, 'alpha': 0.9621000000000001, 'lambda': 12.295076464365325, 'max_bin': 254}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:19:15,903] Trial 192 finished with value: 0.6791701441862902 and parameters: {'n_estimators': 817, 'eta': 0.03386834837671291, 'max_depth': 8, 'alpha': 0.1748, 'lambda': 11.269014623597956, 'max_bin': 260}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:19:37,146] Trial 193 finished with value: 0.6840640337906012 and parameters: {'n_estimators': 900, 'eta': 0.04950849676793695, 'max_depth': 10, 'alpha': 0.9443, 'lambda': 10.118634648268452, 'max_bin': 269}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:19:59,076] Trial 194 finished with value: 0.6851995936316123 and parameters: {'n_estimators': 874, 'eta': 0.045479255361338136, 'max_depth': 10, 'alpha': 0.9062, 'lambda': 10.884634537039144, 'max_bin': 258}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:20:22,705] Trial 195 finished with value: 0.6850182265842546 and parameters: {'n_estimators': 835, 'eta': 0.04136190256309101, 'max_depth': 10, 'alpha': 0.8765000000000001, 'lambda': 11.907240453144317, 'max_bin': 273}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:20:51,627] Trial 196 finished with value: 0.6822375197371711 and parameters: {'n_estimators': 855, 'eta': 0.030704527917708103, 'max_depth': 10, 'alpha': 0.9288000000000001, 'lambda': 7.6181100697254145, 'max_bin': 265}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:20:55,617] Trial 197 finished with value: 0.5883316828453508 and parameters: {'n_estimators': 116, 'eta': 0.03825160504021817, 'max_depth': 10, 'alpha': 0.9646, 'lambda': 8.48625162897494, 'max_bin': 250}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:21:22,236] Trial 198 finished with value: 0.6841818123277816 and parameters: {'n_estimators': 883, 'eta': 0.03553964659985594, 'max_depth': 10, 'alpha': 0.8558, 'lambda': 9.711440602049677, 'max_bin': 281}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:21:44,710] Trial 199 finished with value: 0.6846001015250007 and parameters: {'n_estimators': 784, 'eta': 0.043541641666852544, 'max_depth': 10, 'alpha': 0.8994000000000001, 'lambda': 11.191986979239658, 'max_bin': 276}. Best is trial 141 with value: 0.6959547969799948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6960\n",
      "\tBest params:\n",
      "\t\tn_estimators: 886\n",
      "\t\teta: 0.03696319715961291\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.887\n",
      "\t\tlambda: 8.704210740196038\n",
      "\t\tmax_bin: 271\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.681005    0.720551    0.717203    0.721099\n",
      "1                    TP  406.000000  405.000000  405.000000  405.000000\n",
      "2                    TN  353.000000  341.000000  346.000000  353.000000\n",
      "3                    FP   77.000000   83.000000   82.000000   82.000000\n",
      "4                    FN   63.000000   70.000000   66.000000   59.000000\n",
      "5              Accuracy    0.844271    0.829811    0.835373    0.843159\n",
      "6             Precision    0.840580    0.829918    0.831622    0.831622\n",
      "7           Sensitivity    0.865672    0.852632    0.859873    0.872845\n",
      "8           Specificity    0.820900    0.804200    0.808400    0.811500\n",
      "9              F1 score    0.852941    0.841121    0.845511    0.851735\n",
      "10  F1 score (weighted)    0.844128    0.829635    0.835180    0.842926\n",
      "11     F1 score (macro)    0.843728    0.828944    0.834661    0.842633\n",
      "12    Balanced Accuracy    0.843301    0.828438    0.834142    0.842170\n",
      "13                  MCC    0.687868    0.658238    0.669854    0.686376\n",
      "14                  NPV    0.848600    0.829700    0.839800    0.856800\n",
      "15              ROC_AUC    0.843301    0.828438    0.834142    0.842170\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_3_cat = np.where((y_pred_xgb_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 10:22:11,936] Trial 200 finished with value: 0.6928979456260301 and parameters: {'n_estimators': 815, 'eta': 0.04637685733783568, 'max_depth': 10, 'alpha': 0.9442, 'lambda': 12.524112704148669, 'max_bin': 290}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:22:36,286] Trial 201 finished with value: 0.6938871183606989 and parameters: {'n_estimators': 866, 'eta': 0.042338087437428625, 'max_depth': 10, 'alpha': 0.812, 'lambda': 8.187203116275633, 'max_bin': 274}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:22:56,442] Trial 202 finished with value: 0.6936367727136132 and parameters: {'n_estimators': 867, 'eta': 0.04033545495525575, 'max_depth': 10, 'alpha': 0.8253, 'lambda': 7.172177114257997, 'max_bin': 271}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:23:18,085] Trial 203 finished with value: 0.6933570788446183 and parameters: {'n_estimators': 868, 'eta': 0.04263693443612798, 'max_depth': 10, 'alpha': 0.7618, 'lambda': 6.936399879186221, 'max_bin': 271}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:23:41,559] Trial 204 finished with value: 0.692230893409392 and parameters: {'n_estimators': 888, 'eta': 0.04002922452758812, 'max_depth': 10, 'alpha': 0.8330000000000001, 'lambda': 8.209916336232578, 'max_bin': 281}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:24:06,746] Trial 205 finished with value: 0.6935620403516541 and parameters: {'n_estimators': 852, 'eta': 0.037270514972417214, 'max_depth': 10, 'alpha': 0.8464, 'lambda': 7.528344908049701, 'max_bin': 276}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:24:27,804] Trial 206 finished with value: 0.6922122336659866 and parameters: {'n_estimators': 853, 'eta': 0.041558959118049556, 'max_depth': 10, 'alpha': 0.8082, 'lambda': 6.13946066174222, 'max_bin': 268}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:24:51,909] Trial 207 finished with value: 0.6925365797498048 and parameters: {'n_estimators': 900, 'eta': 0.033217521884420315, 'max_depth': 10, 'alpha': 0.7695000000000001, 'lambda': 7.0379550558631525, 'max_bin': 285}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:25:16,807] Trial 208 finished with value: 0.6906070158989585 and parameters: {'n_estimators': 877, 'eta': 0.03579925549146, 'max_depth': 10, 'alpha': 0.8243, 'lambda': 7.785758089977019, 'max_bin': 274}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:25:40,745] Trial 209 finished with value: 0.692463304205847 and parameters: {'n_estimators': 848, 'eta': 0.03942687175103994, 'max_depth': 10, 'alpha': 0.8614, 'lambda': 8.998379698359114, 'max_bin': 265}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:26:03,234] Trial 210 finished with value: 0.6874329610454044 and parameters: {'n_estimators': 883, 'eta': 0.037132030977266604, 'max_depth': 10, 'alpha': 0.0159, 'lambda': 7.737619356270273, 'max_bin': 278}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:26:30,313] Trial 211 finished with value: 0.6954001850683236 and parameters: {'n_estimators': 864, 'eta': 0.03840497148246538, 'max_depth': 10, 'alpha': 0.8529, 'lambda': 9.242298619471327, 'max_bin': 277}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:26:54,263] Trial 212 finished with value: 0.6935070069516909 and parameters: {'n_estimators': 870, 'eta': 0.04067454848980638, 'max_depth': 10, 'alpha': 0.8486, 'lambda': 8.864798062544292, 'max_bin': 284}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:27:16,739] Trial 213 finished with value: 0.6921541330224213 and parameters: {'n_estimators': 839, 'eta': 0.04482627143468575, 'max_depth': 10, 'alpha': 0.8178000000000001, 'lambda': 7.081485208484808, 'max_bin': 273}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:27:44,045] Trial 214 finished with value: 0.6918387944513625 and parameters: {'n_estimators': 887, 'eta': 0.03489806905042466, 'max_depth': 10, 'alpha': 0.8500000000000001, 'lambda': 10.46353342824399, 'max_bin': 292}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:28:06,419] Trial 215 finished with value: 0.6927964555604431 and parameters: {'n_estimators': 858, 'eta': 0.038728965149826704, 'max_depth': 10, 'alpha': 0.883, 'lambda': 8.319011737703873, 'max_bin': 268}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:28:30,591] Trial 216 finished with value: 0.6908867350636432 and parameters: {'n_estimators': 900, 'eta': 0.04326512723777823, 'max_depth': 10, 'alpha': 0.8679, 'lambda': 6.422343384743328, 'max_bin': 279}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:28:58,555] Trial 217 finished with value: 0.6929385234373104 and parameters: {'n_estimators': 872, 'eta': 0.03237597283355996, 'max_depth': 10, 'alpha': 0.9007000000000001, 'lambda': 9.501542506907173, 'max_bin': 287}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:29:25,431] Trial 218 finished with value: 0.6919112268199117 and parameters: {'n_estimators': 842, 'eta': 0.024505187108396345, 'max_depth': 10, 'alpha': 0.8340000000000001, 'lambda': 5.647999081913261, 'max_bin': 262}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:29:47,732] Trial 219 finished with value: 0.6920198093056866 and parameters: {'n_estimators': 863, 'eta': 0.04133498743996826, 'max_depth': 10, 'alpha': 0.9162, 'lambda': 7.5688450910096785, 'max_bin': 275}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:30:13,428] Trial 220 finished with value: 0.6938407567231168 and parameters: {'n_estimators': 884, 'eta': 0.03717712177203472, 'max_depth': 10, 'alpha': 0.8824000000000001, 'lambda': 10.294443737246592, 'max_bin': 270}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:30:37,253] Trial 221 finished with value: 0.6926319120107916 and parameters: {'n_estimators': 885, 'eta': 0.03661389477664401, 'max_depth': 10, 'alpha': 0.8816, 'lambda': 10.274409926287138, 'max_bin': 270}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:31:01,795] Trial 222 finished with value: 0.6910386399328763 and parameters: {'n_estimators': 878, 'eta': 0.03848578150732109, 'max_depth': 10, 'alpha': 0.8459, 'lambda': 8.757702667732586, 'max_bin': 283}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:31:28,036] Trial 223 finished with value: 0.6928500327118318 and parameters: {'n_estimators': 887, 'eta': 0.03470498897033342, 'max_depth': 10, 'alpha': 0.8954000000000001, 'lambda': 10.014033706163412, 'max_bin': 457}. Best is trial 141 with value: 0.6959547969799948.\n",
      "[I 2023-12-11 10:31:51,561] Trial 224 finished with value: 0.6964865595264497 and parameters: {'n_estimators': 850, 'eta': 0.0451693044496368, 'max_depth': 10, 'alpha': 0.8724000000000001, 'lambda': 10.712172814327872, 'max_bin': 265}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:32:14,201] Trial 225 finished with value: 0.6947487817875981 and parameters: {'n_estimators': 829, 'eta': 0.04571650954629198, 'max_depth': 10, 'alpha': 0.8703000000000001, 'lambda': 11.003728169113984, 'max_bin': 263}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:32:34,718] Trial 226 finished with value: 0.692544947128182 and parameters: {'n_estimators': 817, 'eta': 0.04691574093703852, 'max_depth': 10, 'alpha': 0.8744000000000001, 'lambda': 10.966104523632806, 'max_bin': 257}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:32:55,332] Trial 227 finished with value: 0.6907131602765326 and parameters: {'n_estimators': 831, 'eta': 0.04906210637060348, 'max_depth': 10, 'alpha': 0.9195000000000001, 'lambda': 11.942199886906081, 'max_bin': 264}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:33:17,951] Trial 228 finished with value: 0.6928720752595248 and parameters: {'n_estimators': 898, 'eta': 0.04493780090779847, 'max_depth': 10, 'alpha': 0.896, 'lambda': 10.583693376280984, 'max_bin': 261}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:33:39,655] Trial 229 finished with value: 0.6938004490077005 and parameters: {'n_estimators': 808, 'eta': 0.046446681697473804, 'max_depth': 10, 'alpha': 0.8679, 'lambda': 13.279923478909403, 'max_bin': 266}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:34:00,740] Trial 230 finished with value: 0.6934437809383065 and parameters: {'n_estimators': 798, 'eta': 0.04843172180164607, 'max_depth': 10, 'alpha': 0.8708, 'lambda': 13.297617027056189, 'max_bin': 256}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:34:24,742] Trial 231 finished with value: 0.6911321713419041 and parameters: {'n_estimators': 829, 'eta': 0.04570942788733904, 'max_depth': 10, 'alpha': 0.8892, 'lambda': 12.286780073332094, 'max_bin': 267}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:34:46,710] Trial 232 finished with value: 0.6927897870157277 and parameters: {'n_estimators': 846, 'eta': 0.044268272299206435, 'max_depth': 10, 'alpha': 0.861, 'lambda': 11.418420770885682, 'max_bin': 262}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:35:07,912] Trial 233 finished with value: 0.6949149271220902 and parameters: {'n_estimators': 805, 'eta': 0.04703733306840478, 'max_depth': 10, 'alpha': 0.904, 'lambda': 13.705550945847566, 'max_bin': 268}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:35:28,844] Trial 234 finished with value: 0.6936980397932793 and parameters: {'n_estimators': 797, 'eta': 0.04810956211588582, 'max_depth': 10, 'alpha': 0.9137000000000001, 'lambda': 13.038424491117949, 'max_bin': 272}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:35:49,469] Trial 235 finished with value: 0.6946658439136117 and parameters: {'n_estimators': 782, 'eta': 0.05073536968269997, 'max_depth': 10, 'alpha': 0.9223, 'lambda': 14.041920886767636, 'max_bin': 265}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:36:09,287] Trial 236 finished with value: 0.6915582408146246 and parameters: {'n_estimators': 775, 'eta': 0.05111280558877709, 'max_depth': 10, 'alpha': 0.9372, 'lambda': 13.819127819396899, 'max_bin': 259}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:36:30,120] Trial 237 finished with value: 0.6948909447813529 and parameters: {'n_estimators': 789, 'eta': 0.050057171316560664, 'max_depth': 10, 'alpha': 0.9217000000000001, 'lambda': 15.004110194919503, 'max_bin': 278}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:36:50,682] Trial 238 finished with value: 0.694495668760385 and parameters: {'n_estimators': 794, 'eta': 0.049763016013826836, 'max_depth': 10, 'alpha': 0.9315, 'lambda': 12.446907697833062, 'max_bin': 278}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:37:10,990] Trial 239 finished with value: 0.6929139164439219 and parameters: {'n_estimators': 785, 'eta': 0.05061857795083366, 'max_depth': 10, 'alpha': 0.9261, 'lambda': 14.9371061064345, 'max_bin': 282}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:37:32,328] Trial 240 finished with value: 0.6925206098914753 and parameters: {'n_estimators': 773, 'eta': 0.05108390827996797, 'max_depth': 10, 'alpha': 0.9465, 'lambda': 14.730775172370306, 'max_bin': 288}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:37:52,524] Trial 241 finished with value: 0.6931632826880149 and parameters: {'n_estimators': 807, 'eta': 0.049311683286911746, 'max_depth': 10, 'alpha': 0.908, 'lambda': 14.454049753524153, 'max_bin': 280}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:38:13,834] Trial 242 finished with value: 0.6923266542686533 and parameters: {'n_estimators': 786, 'eta': 0.0478254888304206, 'max_depth': 10, 'alpha': 0.9303, 'lambda': 13.86063960339758, 'max_bin': 278}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:38:34,407] Trial 243 finished with value: 0.6926138865079845 and parameters: {'n_estimators': 813, 'eta': 0.04972156099137686, 'max_depth': 10, 'alpha': 0.33740000000000003, 'lambda': 12.608400529194997, 'max_bin': 273}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:38:57,610] Trial 244 finished with value: 0.6948995759832179 and parameters: {'n_estimators': 797, 'eta': 0.04647223782547813, 'max_depth': 10, 'alpha': 0.9056000000000001, 'lambda': 15.65584261751421, 'max_bin': 294}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:39:20,591] Trial 245 finished with value: 0.6951624074680568 and parameters: {'n_estimators': 764, 'eta': 0.04726850586428279, 'max_depth': 10, 'alpha': 0.9072, 'lambda': 16.092440548183752, 'max_bin': 294}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:39:45,032] Trial 246 finished with value: 0.6962533958347906 and parameters: {'n_estimators': 765, 'eta': 0.04584443750753311, 'max_depth': 10, 'alpha': 0.9083, 'lambda': 15.78798272604757, 'max_bin': 299}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:40:07,068] Trial 247 finished with value: 0.6947786987944025 and parameters: {'n_estimators': 773, 'eta': 0.04719071905657027, 'max_depth': 10, 'alpha': 0.9081, 'lambda': 14.318119806148324, 'max_bin': 307}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:40:29,020] Trial 248 finished with value: 0.6911043710719378 and parameters: {'n_estimators': 760, 'eta': 0.04740674435808546, 'max_depth': 10, 'alpha': 0.9077000000000001, 'lambda': 15.95623107324565, 'max_bin': 306}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:40:51,552] Trial 249 finished with value: 0.6920109039575321 and parameters: {'n_estimators': 737, 'eta': 0.04654944688200627, 'max_depth': 10, 'alpha': 0.9049, 'lambda': 15.664958104059183, 'max_bin': 298}. Best is trial 224 with value: 0.6964865595264497.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6965\n",
      "\tBest params:\n",
      "\t\tn_estimators: 850\n",
      "\t\teta: 0.0451693044496368\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.8724000000000001\n",
      "\t\tlambda: 10.712172814327872\n",
      "\t\tmax_bin: 265\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681005    0.720551    0.717203    0.721099   \n",
      "1                    TP  406.000000  405.000000  405.000000  405.000000   \n",
      "2                    TN  353.000000  341.000000  346.000000  353.000000   \n",
      "3                    FP   77.000000   83.000000   82.000000   82.000000   \n",
      "4                    FN   63.000000   70.000000   66.000000   59.000000   \n",
      "5              Accuracy    0.844271    0.829811    0.835373    0.843159   \n",
      "6             Precision    0.840580    0.829918    0.831622    0.831622   \n",
      "7           Sensitivity    0.865672    0.852632    0.859873    0.872845   \n",
      "8           Specificity    0.820900    0.804200    0.808400    0.811500   \n",
      "9              F1 score    0.852941    0.841121    0.845511    0.851735   \n",
      "10  F1 score (weighted)    0.844128    0.829635    0.835180    0.842926   \n",
      "11     F1 score (macro)    0.843728    0.828944    0.834661    0.842633   \n",
      "12    Balanced Accuracy    0.843301    0.828438    0.834142    0.842170   \n",
      "13                  MCC    0.687868    0.658238    0.669854    0.686376   \n",
      "14                  NPV    0.848600    0.829700    0.839800    0.856800   \n",
      "15              ROC_AUC    0.843301    0.828438    0.834142    0.842170   \n",
      "\n",
      "          Set4  \n",
      "0     0.702278  \n",
      "1   406.000000  \n",
      "2   348.000000  \n",
      "3    81.000000  \n",
      "4    64.000000  \n",
      "5     0.838710  \n",
      "6     0.833676  \n",
      "7     0.863830  \n",
      "8     0.811200  \n",
      "9     0.848485  \n",
      "10    0.838512  \n",
      "11    0.838036  \n",
      "12    0.837509  \n",
      "13    0.676675  \n",
      "14    0.844700  \n",
      "15    0.837509  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_4_cat = np.where((y_pred_xgb_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c6c1fb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_xgb_4_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 10:41:20,533] Trial 250 finished with value: 0.6932114871082182 and parameters: {'n_estimators': 757, 'eta': 0.04377621372871987, 'max_depth': 10, 'alpha': 0.9181, 'lambda': 15.46880314581436, 'max_bin': 300}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:41:47,232] Trial 251 finished with value: 0.6930812871156461 and parameters: {'n_estimators': 787, 'eta': 0.04475145288574193, 'max_depth': 10, 'alpha': 0.9011, 'lambda': 16.471825337198567, 'max_bin': 294}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:42:13,772] Trial 252 finished with value: 0.6946897116728098 and parameters: {'n_estimators': 768, 'eta': 0.045626263630608115, 'max_depth': 10, 'alpha': 0.9528000000000001, 'lambda': 15.102003228466844, 'max_bin': 303}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:42:41,478] Trial 253 finished with value: 0.6953655516050524 and parameters: {'n_estimators': 767, 'eta': 0.0465848557439525, 'max_depth': 11, 'alpha': 0.9523, 'lambda': 15.175101491944966, 'max_bin': 303}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:43:09,104] Trial 254 finished with value: 0.6963608485426129 and parameters: {'n_estimators': 753, 'eta': 0.046376257023322535, 'max_depth': 11, 'alpha': 0.9693, 'lambda': 14.644546942453461, 'max_bin': 303}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:43:33,686] Trial 255 finished with value: 0.6927602673375667 and parameters: {'n_estimators': 752, 'eta': 0.046038583385502305, 'max_depth': 11, 'alpha': 0.9568000000000001, 'lambda': 14.059830992721542, 'max_bin': 312}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:43:58,851] Trial 256 finished with value: 0.6933018570584568 and parameters: {'n_estimators': 743, 'eta': 0.04602512145837144, 'max_depth': 11, 'alpha': 0.9755, 'lambda': 15.239629798042927, 'max_bin': 310}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:44:24,881] Trial 257 finished with value: 0.693650871680645 and parameters: {'n_estimators': 760, 'eta': 0.047749637896036895, 'max_depth': 11, 'alpha': 0.9511000000000001, 'lambda': 14.915361208425702, 'max_bin': 306}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:44:50,661] Trial 258 finished with value: 0.6930556235512843 and parameters: {'n_estimators': 772, 'eta': 0.044748196677592326, 'max_depth': 11, 'alpha': 0.9679000000000001, 'lambda': 15.764887168173813, 'max_bin': 302}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:45:17,100] Trial 259 finished with value: 0.6932720732041704 and parameters: {'n_estimators': 720, 'eta': 0.04690699113255044, 'max_depth': 11, 'alpha': 0.9481, 'lambda': 16.510774831271682, 'max_bin': 319}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:45:42,401] Trial 260 finished with value: 0.6909867726543774 and parameters: {'n_estimators': 759, 'eta': 0.043589601803504865, 'max_depth': 10, 'alpha': 0.43160000000000004, 'lambda': 14.518135335089934, 'max_bin': 301}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:46:09,475] Trial 261 finished with value: 0.6956940922255593 and parameters: {'n_estimators': 780, 'eta': 0.04570487881084898, 'max_depth': 11, 'alpha': 0.9999, 'lambda': 14.160551415945026, 'max_bin': 295}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:46:36,144] Trial 262 finished with value: 0.6939739795271161 and parameters: {'n_estimators': 779, 'eta': 0.0447778198729877, 'max_depth': 11, 'alpha': 0.9958, 'lambda': 14.34201916431236, 'max_bin': 296}. Best is trial 224 with value: 0.6964865595264497.\n",
      "[I 2023-12-11 10:47:02,726] Trial 263 finished with value: 0.6969738566806953 and parameters: {'n_estimators': 771, 'eta': 0.048204182410429154, 'max_depth': 11, 'alpha': 0.9790000000000001, 'lambda': 15.312773447806503, 'max_bin': 306}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:47:29,856] Trial 264 finished with value: 0.6950019307959889 and parameters: {'n_estimators': 770, 'eta': 0.048302816975797, 'max_depth': 11, 'alpha': 0.9827, 'lambda': 15.429081331296356, 'max_bin': 309}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:47:56,182] Trial 265 finished with value: 0.695412960472933 and parameters: {'n_estimators': 778, 'eta': 0.04832039571135376, 'max_depth': 11, 'alpha': 0.9912000000000001, 'lambda': 15.198379029032365, 'max_bin': 314}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:48:24,021] Trial 266 finished with value: 0.6958406639098282 and parameters: {'n_estimators': 767, 'eta': 0.04822655126071457, 'max_depth': 11, 'alpha': 0.9997, 'lambda': 15.589951157193623, 'max_bin': 313}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:48:49,203] Trial 267 finished with value: 0.6938288165428643 and parameters: {'n_estimators': 732, 'eta': 0.0492086417179437, 'max_depth': 11, 'alpha': 0.9977, 'lambda': 16.498299162011456, 'max_bin': 309}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:49:13,737] Trial 268 finished with value: 0.6952175504609389 and parameters: {'n_estimators': 741, 'eta': 0.04812041251749221, 'max_depth': 11, 'alpha': 0.9997, 'lambda': 15.778683898975524, 'max_bin': 315}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:49:40,291] Trial 269 finished with value: 0.6949516496836864 and parameters: {'n_estimators': 751, 'eta': 0.047944815493273715, 'max_depth': 11, 'alpha': 0.9993000000000001, 'lambda': 15.915793354476758, 'max_bin': 318}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:50:06,767] Trial 270 finished with value: 0.6961191771637371 and parameters: {'n_estimators': 751, 'eta': 0.04845093842428123, 'max_depth': 11, 'alpha': 0.9825, 'lambda': 15.617580362307763, 'max_bin': 317}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:50:31,252] Trial 271 finished with value: 0.6927144924117009 and parameters: {'n_estimators': 705, 'eta': 0.05160174832613746, 'max_depth': 11, 'alpha': 0.9802000000000001, 'lambda': 17.263237528099207, 'max_bin': 315}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:50:57,082] Trial 272 finished with value: 0.6959958412632228 and parameters: {'n_estimators': 745, 'eta': 0.04884162222544935, 'max_depth': 11, 'alpha': 0.9933000000000001, 'lambda': 15.719216120677384, 'max_bin': 319}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:51:21,923] Trial 273 finished with value: 0.6948109363940214 and parameters: {'n_estimators': 734, 'eta': 0.04824871680052932, 'max_depth': 11, 'alpha': 0.9981000000000001, 'lambda': 15.7330014499312, 'max_bin': 323}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:51:45,269] Trial 274 finished with value: 0.6929748610438453 and parameters: {'n_estimators': 750, 'eta': 0.05334261801843311, 'max_depth': 11, 'alpha': 0.9767, 'lambda': 16.301745287978566, 'max_bin': 317}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:52:10,351] Trial 275 finished with value: 0.6965692690376969 and parameters: {'n_estimators': 754, 'eta': 0.050173928280732055, 'max_depth': 11, 'alpha': 0.9991000000000001, 'lambda': 15.455500170434133, 'max_bin': 314}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:52:33,265] Trial 276 finished with value: 0.6937725018678746 and parameters: {'n_estimators': 716, 'eta': 0.05103198491344794, 'max_depth': 11, 'alpha': 0.9932000000000001, 'lambda': 15.540753545172329, 'max_bin': 314}. Best is trial 263 with value: 0.6969738566806953.\n",
      "[I 2023-12-11 10:52:58,103] Trial 277 finished with value: 0.6976462307205417 and parameters: {'n_estimators': 749, 'eta': 0.04902999970853683, 'max_depth': 11, 'alpha': 0.9995, 'lambda': 16.605903062060605, 'max_bin': 321}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:53:21,428] Trial 278 finished with value: 0.6942318689042767 and parameters: {'n_estimators': 747, 'eta': 0.04953181589810993, 'max_depth': 11, 'alpha': 0.9848, 'lambda': 16.91361518080489, 'max_bin': 320}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:53:45,016] Trial 279 finished with value: 0.6947081943474454 and parameters: {'n_estimators': 725, 'eta': 0.052860202476108446, 'max_depth': 11, 'alpha': 0.9985, 'lambda': 16.0674017653798, 'max_bin': 312}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:54:10,258] Trial 280 finished with value: 0.6925163695494478 and parameters: {'n_estimators': 741, 'eta': 0.049065173929879635, 'max_depth': 11, 'alpha': 0.9725, 'lambda': 15.294106425179036, 'max_bin': 326}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:54:37,873] Trial 281 finished with value: 0.6953762280304707 and parameters: {'n_estimators': 764, 'eta': 0.05161110319336143, 'max_depth': 11, 'alpha': 0.9745, 'lambda': 17.248848041325306, 'max_bin': 317}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:55:05,372] Trial 282 finished with value: 0.6937521489176195 and parameters: {'n_estimators': 763, 'eta': 0.053867662451980755, 'max_depth': 11, 'alpha': 0.9753000000000001, 'lambda': 17.507857579911146, 'max_bin': 322}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:55:31,240] Trial 283 finished with value: 0.6916931139840198 and parameters: {'n_estimators': 749, 'eta': 0.04788490574860048, 'max_depth': 11, 'alpha': 0.9942000000000001, 'lambda': 16.74615384737393, 'max_bin': 315}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:55:57,398] Trial 284 finished with value: 0.6959672887747023 and parameters: {'n_estimators': 732, 'eta': 0.051479468257057084, 'max_depth': 11, 'alpha': 0.9690000000000001, 'lambda': 16.307862031854743, 'max_bin': 331}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:56:22,975] Trial 285 finished with value: 0.6945161083778826 and parameters: {'n_estimators': 699, 'eta': 0.051353171495383244, 'max_depth': 11, 'alpha': 0.9747, 'lambda': 16.94963249258643, 'max_bin': 331}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:56:48,483] Trial 286 finished with value: 0.696456223486708 and parameters: {'n_estimators': 731, 'eta': 0.05406984526896636, 'max_depth': 11, 'alpha': 0.9973000000000001, 'lambda': 17.54118716056017, 'max_bin': 318}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:57:13,804] Trial 287 finished with value: 0.6934243208682042 and parameters: {'n_estimators': 723, 'eta': 0.05494262922296907, 'max_depth': 11, 'alpha': 0.9968, 'lambda': 17.754256149022396, 'max_bin': 316}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:57:38,973] Trial 288 finished with value: 0.6964306225776389 and parameters: {'n_estimators': 734, 'eta': 0.05250079663904394, 'max_depth': 11, 'alpha': 0.9681000000000001, 'lambda': 16.416730669011233, 'max_bin': 324}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:58:03,547] Trial 289 finished with value: 0.694923229207844 and parameters: {'n_estimators': 708, 'eta': 0.053977457218426554, 'max_depth': 11, 'alpha': 0.9663, 'lambda': 18.078297387198983, 'max_bin': 324}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:58:29,171] Trial 290 finished with value: 0.6955888707154331 and parameters: {'n_estimators': 730, 'eta': 0.05189208392549005, 'max_depth': 11, 'alpha': 0.9732000000000001, 'lambda': 16.55141952761492, 'max_bin': 330}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:58:53,512] Trial 291 finished with value: 0.6941834833767765 and parameters: {'n_estimators': 736, 'eta': 0.05635081351768373, 'max_depth': 11, 'alpha': 0.9633, 'lambda': 16.93681304050431, 'max_bin': 335}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:59:18,602] Trial 292 finished with value: 0.6959374669730949 and parameters: {'n_estimators': 728, 'eta': 0.05228084687790896, 'max_depth': 11, 'alpha': 0.9662000000000001, 'lambda': 16.496916934073738, 'max_bin': 329}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 10:59:43,666] Trial 293 finished with value: 0.693477863750647 and parameters: {'n_estimators': 666, 'eta': 0.05223161543828133, 'max_depth': 11, 'alpha': 0.9686, 'lambda': 18.228392885020096, 'max_bin': 328}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 11:00:07,791] Trial 294 finished with value: 0.6921931748705615 and parameters: {'n_estimators': 682, 'eta': 0.05327809383779262, 'max_depth': 11, 'alpha': 0.9620000000000001, 'lambda': 16.295382917425805, 'max_bin': 329}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 11:00:32,649] Trial 295 finished with value: 0.6947838174148685 and parameters: {'n_estimators': 730, 'eta': 0.05140709902429195, 'max_depth': 11, 'alpha': 0.9742000000000001, 'lambda': 17.45019969262792, 'max_bin': 321}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 11:00:57,108] Trial 296 finished with value: 0.6954706284851532 and parameters: {'n_estimators': 715, 'eta': 0.05228782138756321, 'max_depth': 11, 'alpha': 0.9992000000000001, 'lambda': 16.43893113799739, 'max_bin': 334}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 11:01:21,501] Trial 297 finished with value: 0.6956502535871678 and parameters: {'n_estimators': 714, 'eta': 0.05631295252297993, 'max_depth': 11, 'alpha': 0.9983000000000001, 'lambda': 17.193895711272045, 'max_bin': 323}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 11:01:47,521] Trial 298 finished with value: 0.6961606337789068 and parameters: {'n_estimators': 713, 'eta': 0.05616177115581712, 'max_depth': 11, 'alpha': 0.995, 'lambda': 16.623798155554475, 'max_bin': 332}. Best is trial 277 with value: 0.6976462307205417.\n",
      "[I 2023-12-11 11:02:13,140] Trial 299 finished with value: 0.6957730680459775 and parameters: {'n_estimators': 705, 'eta': 0.056665981077466884, 'max_depth': 11, 'alpha': 0.9972000000000001, 'lambda': 19.012822957238576, 'max_bin': 334}. Best is trial 277 with value: 0.6976462307205417.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tn_estimators: 749\n",
      "\t\teta: 0.04902999970853683\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.9995\n",
      "\t\tlambda: 16.605903062060605\n",
      "\t\tmax_bin: 321\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681005    0.720551    0.717203    0.721099   \n",
      "1                    TP  406.000000  405.000000  405.000000  405.000000   \n",
      "2                    TN  353.000000  341.000000  346.000000  353.000000   \n",
      "3                    FP   77.000000   83.000000   82.000000   82.000000   \n",
      "4                    FN   63.000000   70.000000   66.000000   59.000000   \n",
      "5              Accuracy    0.844271    0.829811    0.835373    0.843159   \n",
      "6             Precision    0.840580    0.829918    0.831622    0.831622   \n",
      "7           Sensitivity    0.865672    0.852632    0.859873    0.872845   \n",
      "8           Specificity    0.820900    0.804200    0.808400    0.811500   \n",
      "9              F1 score    0.852941    0.841121    0.845511    0.851735   \n",
      "10  F1 score (weighted)    0.844128    0.829635    0.835180    0.842926   \n",
      "11     F1 score (macro)    0.843728    0.828944    0.834661    0.842633   \n",
      "12    Balanced Accuracy    0.843301    0.828438    0.834142    0.842170   \n",
      "13                  MCC    0.687868    0.658238    0.669854    0.686376   \n",
      "14                  NPV    0.848600    0.829700    0.839800    0.856800   \n",
      "15              ROC_AUC    0.843301    0.828438    0.834142    0.842170   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.702278    0.708199  \n",
      "1   406.000000  393.000000  \n",
      "2   348.000000  353.000000  \n",
      "3    81.000000   78.000000  \n",
      "4    64.000000   75.000000  \n",
      "5     0.838710    0.829811  \n",
      "6     0.833676    0.834395  \n",
      "7     0.863830    0.839744  \n",
      "8     0.811200    0.819000  \n",
      "9     0.848485    0.837061  \n",
      "10    0.838512    0.829786  \n",
      "11    0.838036    0.829473  \n",
      "12    0.837509    0.829385  \n",
      "13    0.676675    0.658965  \n",
      "14    0.844700    0.824800  \n",
      "15    0.837509    0.829385  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "\n",
    "y_pred_xgb_5_cat = np.where((y_pred_xgb_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 11:02:41,888] Trial 300 finished with value: 0.7104506251644054 and parameters: {'n_estimators': 708, 'eta': 0.05760042797200709, 'max_depth': 11, 'alpha': 0.9816, 'lambda': 18.890830013982413, 'max_bin': 340}. Best is trial 300 with value: 0.7104506251644054.\n",
      "[I 2023-12-11 11:03:08,708] Trial 301 finished with value: 0.7107383063531328 and parameters: {'n_estimators': 686, 'eta': 0.058168925372234524, 'max_depth': 11, 'alpha': 0.9999, 'lambda': 19.394176683760232, 'max_bin': 341}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:03:33,068] Trial 302 finished with value: 0.7056706903022418 and parameters: {'n_estimators': 694, 'eta': 0.057309280784771065, 'max_depth': 11, 'alpha': 0.9983000000000001, 'lambda': 18.78221222463725, 'max_bin': 337}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:03:57,505] Trial 303 finished with value: 0.705031696620104 and parameters: {'n_estimators': 689, 'eta': 0.05593723523532048, 'max_depth': 11, 'alpha': 0.9993000000000001, 'lambda': 19.160941669808093, 'max_bin': 340}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:04:20,714] Trial 304 finished with value: 0.7086947545056554 and parameters: {'n_estimators': 677, 'eta': 0.05746408076318556, 'max_depth': 11, 'alpha': 0.9974000000000001, 'lambda': 19.068651227856407, 'max_bin': 342}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:04:43,929] Trial 305 finished with value: 0.7063453426974371 and parameters: {'n_estimators': 692, 'eta': 0.05769590306405998, 'max_depth': 11, 'alpha': 0.9814, 'lambda': 19.083516774695852, 'max_bin': 340}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:05:06,576] Trial 306 finished with value: 0.7082164032089013 and parameters: {'n_estimators': 683, 'eta': 0.05812836195612035, 'max_depth': 11, 'alpha': 0.9804, 'lambda': 19.315917468818842, 'max_bin': 342}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:05:31,368] Trial 307 finished with value: 0.7070985574594328 and parameters: {'n_estimators': 687, 'eta': 0.05836188984643615, 'max_depth': 11, 'alpha': 0.9843000000000001, 'lambda': 19.14479535787832, 'max_bin': 346}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:05:53,872] Trial 308 finished with value: 0.7051949779650879 and parameters: {'n_estimators': 692, 'eta': 0.05930258704554089, 'max_depth': 11, 'alpha': 0.9836, 'lambda': 19.154040738801267, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:06:17,313] Trial 309 finished with value: 0.7064771742995067 and parameters: {'n_estimators': 685, 'eta': 0.05941227854393256, 'max_depth': 11, 'alpha': 0.9795, 'lambda': 19.312045579552088, 'max_bin': 342}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:06:41,382] Trial 310 finished with value: 0.7085833763483647 and parameters: {'n_estimators': 681, 'eta': 0.058979433158450834, 'max_depth': 11, 'alpha': 0.9750000000000001, 'lambda': 19.575872087530566, 'max_bin': 346}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:07:05,215] Trial 311 finished with value: 0.7070029983137427 and parameters: {'n_estimators': 678, 'eta': 0.057252249824395175, 'max_depth': 11, 'alpha': 0.9692000000000001, 'lambda': 19.386103636754303, 'max_bin': 343}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:07:29,407] Trial 312 finished with value: 0.7068661993148002 and parameters: {'n_estimators': 681, 'eta': 0.05905530825550643, 'max_depth': 11, 'alpha': 0.9776, 'lambda': 19.609512949369652, 'max_bin': 347}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:07:53,261] Trial 313 finished with value: 0.7067238886514414 and parameters: {'n_estimators': 681, 'eta': 0.05911325142887542, 'max_depth': 11, 'alpha': 0.9762000000000001, 'lambda': 19.64195488947954, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:08:16,223] Trial 314 finished with value: 0.7050614643926456 and parameters: {'n_estimators': 677, 'eta': 0.06043068026872235, 'max_depth': 11, 'alpha': 0.9771000000000001, 'lambda': 19.76557662478726, 'max_bin': 343}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:08:39,429] Trial 315 finished with value: 0.7069650945406591 and parameters: {'n_estimators': 675, 'eta': 0.05943737581162572, 'max_depth': 11, 'alpha': 0.9568000000000001, 'lambda': 19.884302354608913, 'max_bin': 341}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:09:01,382] Trial 316 finished with value: 0.7070065831773086 and parameters: {'n_estimators': 677, 'eta': 0.05853881438660908, 'max_depth': 11, 'alpha': 0.9545, 'lambda': 19.579819503122618, 'max_bin': 342}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:09:24,428] Trial 317 finished with value: 0.7068079791327936 and parameters: {'n_estimators': 651, 'eta': 0.061370334404835726, 'max_depth': 11, 'alpha': 0.9648, 'lambda': 19.648148123705504, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:09:46,608] Trial 318 finished with value: 0.7097687995210789 and parameters: {'n_estimators': 652, 'eta': 0.05935781554991047, 'max_depth': 11, 'alpha': 0.9576, 'lambda': 19.709889197280003, 'max_bin': 342}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:10:09,635] Trial 319 finished with value: 0.7101883406966722 and parameters: {'n_estimators': 651, 'eta': 0.0592308546357756, 'max_depth': 11, 'alpha': 0.9578000000000001, 'lambda': 19.638055618336704, 'max_bin': 343}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:10:33,446] Trial 320 finished with value: 0.7087259641588861 and parameters: {'n_estimators': 649, 'eta': 0.05948079448936441, 'max_depth': 11, 'alpha': 0.9539000000000001, 'lambda': 19.756363950464415, 'max_bin': 343}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:10:56,323] Trial 321 finished with value: 0.708034515532993 and parameters: {'n_estimators': 653, 'eta': 0.05953018738124562, 'max_depth': 11, 'alpha': 0.9498000000000001, 'lambda': 19.901079734261298, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:11:21,166] Trial 322 finished with value: 0.7085420220194629 and parameters: {'n_estimators': 653, 'eta': 0.05956968865587323, 'max_depth': 11, 'alpha': 0.9510000000000001, 'lambda': 19.751590072793938, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:11:44,363] Trial 323 finished with value: 0.7071042856327665 and parameters: {'n_estimators': 649, 'eta': 0.05887890937766796, 'max_depth': 11, 'alpha': 0.9507000000000001, 'lambda': 19.87581519467713, 'max_bin': 345}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:12:09,117] Trial 324 finished with value: 0.7090373414946656 and parameters: {'n_estimators': 642, 'eta': 0.06001359815017581, 'max_depth': 11, 'alpha': 0.9508000000000001, 'lambda': 19.772774061866215, 'max_bin': 345}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:12:29,939] Trial 325 finished with value: 0.7100640634951709 and parameters: {'n_estimators': 645, 'eta': 0.059574301604246996, 'max_depth': 11, 'alpha': 0.9507000000000001, 'lambda': 19.81007166367856, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:12:53,158] Trial 326 finished with value: 0.7068196931731062 and parameters: {'n_estimators': 647, 'eta': 0.05986571464020587, 'max_depth': 11, 'alpha': 0.9495, 'lambda': 19.876168726124064, 'max_bin': 346}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:13:16,704] Trial 327 finished with value: 0.7067596820440922 and parameters: {'n_estimators': 649, 'eta': 0.05992656879270063, 'max_depth': 11, 'alpha': 0.9484, 'lambda': 20.33631774122215, 'max_bin': 348}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:13:41,107] Trial 328 finished with value: 0.7075416121321394 and parameters: {'n_estimators': 648, 'eta': 0.061970777711183894, 'max_depth': 11, 'alpha': 0.9523, 'lambda': 20.83842322471047, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:14:02,248] Trial 329 finished with value: 0.7067828827184497 and parameters: {'n_estimators': 648, 'eta': 0.06188831290102772, 'max_depth': 11, 'alpha': 0.9488000000000001, 'lambda': 20.564829165500015, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:14:24,224] Trial 330 finished with value: 0.7084923384106688 and parameters: {'n_estimators': 646, 'eta': 0.06203119265447856, 'max_depth': 11, 'alpha': 0.9579000000000001, 'lambda': 20.510980657328, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:14:46,521] Trial 331 finished with value: 0.7080645080050031 and parameters: {'n_estimators': 627, 'eta': 0.062047267795483765, 'max_depth': 11, 'alpha': 0.9451, 'lambda': 20.501908798136323, 'max_bin': 352}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:15:09,083] Trial 332 finished with value: 0.7078904129489008 and parameters: {'n_estimators': 627, 'eta': 0.06207772837660331, 'max_depth': 11, 'alpha': 0.9448000000000001, 'lambda': 20.560621021523794, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:15:31,253] Trial 333 finished with value: 0.7070452856963406 and parameters: {'n_estimators': 622, 'eta': 0.06260547675489028, 'max_depth': 11, 'alpha': 0.9451, 'lambda': 20.715421933644844, 'max_bin': 355}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:15:52,175] Trial 334 finished with value: 0.7071772482257199 and parameters: {'n_estimators': 611, 'eta': 0.06283612167358828, 'max_depth': 11, 'alpha': 0.9456, 'lambda': 21.12410944517732, 'max_bin': 355}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:16:14,471] Trial 335 finished with value: 0.7073888063856888 and parameters: {'n_estimators': 624, 'eta': 0.06351500154659746, 'max_depth': 11, 'alpha': 0.9478000000000001, 'lambda': 21.00137977295807, 'max_bin': 357}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:16:35,458] Trial 336 finished with value: 0.7075313288177886 and parameters: {'n_estimators': 621, 'eta': 0.0634373182963177, 'max_depth': 11, 'alpha': 0.9398000000000001, 'lambda': 21.125373130792703, 'max_bin': 354}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:16:59,373] Trial 337 finished with value: 0.7087284461361333 and parameters: {'n_estimators': 626, 'eta': 0.06291901115226295, 'max_depth': 11, 'alpha': 0.9425, 'lambda': 20.697008044200985, 'max_bin': 357}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:17:22,601] Trial 338 finished with value: 0.7036044289273601 and parameters: {'n_estimators': 619, 'eta': 0.0656776006610178, 'max_depth': 11, 'alpha': 0.9396, 'lambda': 21.033335770532712, 'max_bin': 355}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:17:44,739] Trial 339 finished with value: 0.7072866531695652 and parameters: {'n_estimators': 595, 'eta': 0.06371666628934691, 'max_depth': 11, 'alpha': 0.9499000000000001, 'lambda': 21.536375171041186, 'max_bin': 361}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:18:06,671] Trial 340 finished with value: 0.7041969354052544 and parameters: {'n_estimators': 590, 'eta': 0.0630972755077361, 'max_depth': 11, 'alpha': 0.9399000000000001, 'lambda': 21.737246544430285, 'max_bin': 360}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:18:30,704] Trial 341 finished with value: 0.7085751043057985 and parameters: {'n_estimators': 628, 'eta': 0.06325932631886191, 'max_depth': 11, 'alpha': 0.9419000000000001, 'lambda': 21.011133211147705, 'max_bin': 354}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:18:51,892] Trial 342 finished with value: 0.7069321934037607 and parameters: {'n_estimators': 630, 'eta': 0.06281806725528569, 'max_depth': 11, 'alpha': 0.9369000000000001, 'lambda': 21.20593799396025, 'max_bin': 356}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:19:16,128] Trial 343 finished with value: 0.7053899906990309 and parameters: {'n_estimators': 601, 'eta': 0.06343152068296175, 'max_depth': 11, 'alpha': 0.9452, 'lambda': 20.709116425451562, 'max_bin': 353}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:19:38,107] Trial 344 finished with value: 0.7062289878837427 and parameters: {'n_estimators': 623, 'eta': 0.06468545767477163, 'max_depth': 11, 'alpha': 0.9380000000000001, 'lambda': 21.448056259991443, 'max_bin': 363}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:20:00,206] Trial 345 finished with value: 0.7084677203559697 and parameters: {'n_estimators': 579, 'eta': 0.062026024499013996, 'max_depth': 11, 'alpha': 0.9479000000000001, 'lambda': 20.626580309524076, 'max_bin': 352}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:20:24,017] Trial 346 finished with value: 0.7048994068676967 and parameters: {'n_estimators': 591, 'eta': 0.061687473603051365, 'max_depth': 11, 'alpha': 0.9488000000000001, 'lambda': 22.310617297015114, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:20:48,105] Trial 347 finished with value: 0.706599261038339 and parameters: {'n_estimators': 636, 'eta': 0.06476460019525777, 'max_depth': 11, 'alpha': 0.9315, 'lambda': 20.6002861513121, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:21:09,527] Trial 348 finished with value: 0.7034676229945607 and parameters: {'n_estimators': 609, 'eta': 0.06100939395263663, 'max_depth': 11, 'alpha': 0.9540000000000001, 'lambda': 21.233100045291955, 'max_bin': 360}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:21:31,284] Trial 349 finished with value: 0.7061212118815573 and parameters: {'n_estimators': 575, 'eta': 0.06674144512298098, 'max_depth': 11, 'alpha': 0.9338000000000001, 'lambda': 20.434722661160095, 'max_bin': 353}. Best is trial 301 with value: 0.7107383063531328.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7107\n",
      "\tBest params:\n",
      "\t\tn_estimators: 686\n",
      "\t\teta: 0.058168925372234524\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.9999\n",
      "\t\tlambda: 19.394176683760232\n",
      "\t\tmax_bin: 341\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681005    0.720551    0.717203    0.721099   \n",
      "1                    TP  406.000000  405.000000  405.000000  405.000000   \n",
      "2                    TN  353.000000  341.000000  346.000000  353.000000   \n",
      "3                    FP   77.000000   83.000000   82.000000   82.000000   \n",
      "4                    FN   63.000000   70.000000   66.000000   59.000000   \n",
      "5              Accuracy    0.844271    0.829811    0.835373    0.843159   \n",
      "6             Precision    0.840580    0.829918    0.831622    0.831622   \n",
      "7           Sensitivity    0.865672    0.852632    0.859873    0.872845   \n",
      "8           Specificity    0.820900    0.804200    0.808400    0.811500   \n",
      "9              F1 score    0.852941    0.841121    0.845511    0.851735   \n",
      "10  F1 score (weighted)    0.844128    0.829635    0.835180    0.842926   \n",
      "11     F1 score (macro)    0.843728    0.828944    0.834661    0.842633   \n",
      "12    Balanced Accuracy    0.843301    0.828438    0.834142    0.842170   \n",
      "13                  MCC    0.687868    0.658238    0.669854    0.686376   \n",
      "14                  NPV    0.848600    0.829700    0.839800    0.856800   \n",
      "15              ROC_AUC    0.843301    0.828438    0.834142    0.842170   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.702278    0.708199    0.649452  \n",
      "1   406.000000  393.000000  395.000000  \n",
      "2   348.000000  353.000000  345.000000  \n",
      "3    81.000000   78.000000   82.000000  \n",
      "4    64.000000   75.000000   77.000000  \n",
      "5     0.838710    0.829811    0.823137  \n",
      "6     0.833676    0.834395    0.828092  \n",
      "7     0.863830    0.839744    0.836864  \n",
      "8     0.811200    0.819000    0.808000  \n",
      "9     0.848485    0.837061    0.832455  \n",
      "10    0.838512    0.829786    0.823082  \n",
      "11    0.838036    0.829473    0.822588  \n",
      "12    0.837509    0.829385    0.822413  \n",
      "13    0.676675    0.658965    0.645227  \n",
      "14    0.844700    0.824800    0.817500  \n",
      "15    0.837509    0.829385    0.822413  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_6_cat = np.where((y_pred_xgb_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 11:21:55,681] Trial 350 finished with value: 0.6923780132045267 and parameters: {'n_estimators': 661, 'eta': 0.06339256539582927, 'max_depth': 11, 'alpha': 0.9555, 'lambda': 21.88987280917009, 'max_bin': 367}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:22:18,870] Trial 351 finished with value: 0.6906750701439718 and parameters: {'n_estimators': 636, 'eta': 0.06118103445274438, 'max_depth': 11, 'alpha': 0.9522, 'lambda': 20.31047760388058, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:22:40,945] Trial 352 finished with value: 0.6907865817382197 and parameters: {'n_estimators': 663, 'eta': 0.06412693896870941, 'max_depth': 11, 'alpha': 0.9313, 'lambda': 21.980173488119362, 'max_bin': 356}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:23:02,402] Trial 353 finished with value: 0.692309764551698 and parameters: {'n_estimators': 574, 'eta': 0.06205234464554331, 'max_depth': 11, 'alpha': 0.5556, 'lambda': 20.904988923562946, 'max_bin': 348}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:23:25,077] Trial 354 finished with value: 0.6917435669337084 and parameters: {'n_estimators': 631, 'eta': 0.06080594733582857, 'max_depth': 11, 'alpha': 0.9563, 'lambda': 18.508103908603, 'max_bin': 362}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:23:45,711] Trial 355 finished with value: 0.6893602311353895 and parameters: {'n_estimators': 610, 'eta': 0.0665100796844983, 'max_depth': 11, 'alpha': 0.9326000000000001, 'lambda': 20.24699996374688, 'max_bin': 352}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:24:10,615] Trial 356 finished with value: 0.6916772483250233 and parameters: {'n_estimators': 649, 'eta': 0.05823023652038098, 'max_depth': 11, 'alpha': 0.9572, 'lambda': 22.65502342345988, 'max_bin': 338}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:24:33,074] Trial 357 finished with value: 0.689791095227055 and parameters: {'n_estimators': 660, 'eta': 0.06395173024155543, 'max_depth': 11, 'alpha': 0.2587, 'lambda': 18.593823601186703, 'max_bin': 357}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:24:56,696] Trial 358 finished with value: 0.6931366428927229 and parameters: {'n_estimators': 635, 'eta': 0.06129621255443985, 'max_depth': 11, 'alpha': 0.9596, 'lambda': 21.316927411139336, 'max_bin': 347}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:25:19,481] Trial 359 finished with value: 0.6903521796088666 and parameters: {'n_estimators': 616, 'eta': 0.05829482160601326, 'max_depth': 11, 'alpha': 0.9307000000000001, 'lambda': 20.13762563778897, 'max_bin': 352}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:25:30,616] Trial 360 finished with value: 0.6698461825539228 and parameters: {'n_estimators': 663, 'eta': 0.06299229114498743, 'max_depth': 5, 'alpha': 0.9396, 'lambda': 21.751462820330204, 'max_bin': 337}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:25:54,499] Trial 361 finished with value: 0.6894641425400583 and parameters: {'n_estimators': 640, 'eta': 0.06088613678940361, 'max_depth': 11, 'alpha': 0.9590000000000001, 'lambda': 20.990501004668857, 'max_bin': 347}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:26:16,195] Trial 362 finished with value: 0.6937245368453964 and parameters: {'n_estimators': 596, 'eta': 0.06569832712710053, 'max_depth': 11, 'alpha': 0.9316000000000001, 'lambda': 20.128043444969666, 'max_bin': 359}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:26:38,222] Trial 363 finished with value: 0.6916688434234421 and parameters: {'n_estimators': 557, 'eta': 0.05822611565044826, 'max_depth': 11, 'alpha': 0.9597, 'lambda': 18.61407811817281, 'max_bin': 339}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:26:57,211] Trial 364 finished with value: 0.6917508579835314 and parameters: {'n_estimators': 619, 'eta': 0.06808762934759116, 'max_depth': 11, 'alpha': 0.9441, 'lambda': 23.090070794569243, 'max_bin': 365}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:27:19,668] Trial 365 finished with value: 0.6944882931714422 and parameters: {'n_estimators': 657, 'eta': 0.060177333440661195, 'max_depth': 11, 'alpha': 0.9614, 'lambda': 20.07791938150227, 'max_bin': 348}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:27:40,768] Trial 366 finished with value: 0.6926014675398392 and parameters: {'n_estimators': 636, 'eta': 0.062476295782838184, 'max_depth': 11, 'alpha': 0.9292, 'lambda': 21.324126032352236, 'max_bin': 353}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:28:02,031] Trial 367 finished with value: 0.6938054490928987 and parameters: {'n_estimators': 584, 'eta': 0.06477002500402804, 'max_depth': 11, 'alpha': 0.9618000000000001, 'lambda': 18.92416725206383, 'max_bin': 342}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:28:23,285] Trial 368 finished with value: 0.6937333461042386 and parameters: {'n_estimators': 610, 'eta': 0.05983553079894058, 'max_depth': 11, 'alpha': 0.9363, 'lambda': 22.196571546938994, 'max_bin': 358}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:28:45,978] Trial 369 finished with value: 0.6903106025988249 and parameters: {'n_estimators': 663, 'eta': 0.0578598101997915, 'max_depth': 11, 'alpha': 0.9690000000000001, 'lambda': 20.758475486312257, 'max_bin': 346}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:29:08,626] Trial 370 finished with value: 0.6925558110379163 and parameters: {'n_estimators': 642, 'eta': 0.06213895315135574, 'max_depth': 11, 'alpha': 0.5089, 'lambda': 19.651170527236562, 'max_bin': 352}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:29:30,099] Trial 371 finished with value: 0.6894261249357014 and parameters: {'n_estimators': 626, 'eta': 0.06435300306900468, 'max_depth': 11, 'alpha': 0.9446, 'lambda': 18.5173117922205, 'max_bin': 339}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:29:52,650] Trial 372 finished with value: 0.690727830534636 and parameters: {'n_estimators': 606, 'eta': 0.060743076276503286, 'max_depth': 11, 'alpha': 0.9705, 'lambda': 20.090190753028782, 'max_bin': 370}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:30:18,057] Trial 373 finished with value: 0.6950755920473305 and parameters: {'n_estimators': 654, 'eta': 0.05791706705794107, 'max_depth': 11, 'alpha': 0.9245000000000001, 'lambda': 21.492122888885266, 'max_bin': 346}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:30:40,504] Trial 374 finished with value: 0.6951045998063665 and parameters: {'n_estimators': 663, 'eta': 0.06692788216345626, 'max_depth': 11, 'alpha': 0.9473, 'lambda': 19.24975498307123, 'max_bin': 357}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:31:03,993] Trial 375 finished with value: 0.6932545280837923 and parameters: {'n_estimators': 633, 'eta': 0.06268808412221527, 'max_depth': 11, 'alpha': 0.9740000000000001, 'lambda': 20.726861217658946, 'max_bin': 351}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:31:28,066] Trial 376 finished with value: 0.6936035937626871 and parameters: {'n_estimators': 646, 'eta': 0.05993341325078892, 'max_depth': 11, 'alpha': 0.9603, 'lambda': 22.640386747679017, 'max_bin': 345}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:31:50,881] Trial 377 finished with value: 0.6915577780775594 and parameters: {'n_estimators': 617, 'eta': 0.05628385837392229, 'max_depth': 11, 'alpha': 0.9245000000000001, 'lambda': 19.93604549029647, 'max_bin': 337}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:32:10,962] Trial 378 finished with value: 0.6892039309063589 and parameters: {'n_estimators': 540, 'eta': 0.06547479480639029, 'max_depth': 11, 'alpha': 0.9457000000000001, 'lambda': 19.164158732555535, 'max_bin': 364}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:32:35,711] Trial 379 finished with value: 0.6913584526450878 and parameters: {'n_estimators': 671, 'eta': 0.06193975590123225, 'max_depth': 11, 'alpha': 0.9697, 'lambda': 21.67848122684117, 'max_bin': 354}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:32:58,117] Trial 380 finished with value: 0.6934716912326822 and parameters: {'n_estimators': 600, 'eta': 0.05896404232148463, 'max_depth': 11, 'alpha': 0.9534, 'lambda': 20.87581329545185, 'max_bin': 342}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:33:16,098] Trial 381 finished with value: 0.6901656566831871 and parameters: {'n_estimators': 464, 'eta': 0.06355320117474528, 'max_depth': 11, 'alpha': 0.925, 'lambda': 18.35094960215382, 'max_bin': 349}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:33:39,996] Trial 382 finished with value: 0.6908242037460196 and parameters: {'n_estimators': 627, 'eta': 0.05698322442128824, 'max_depth': 12, 'alpha': 0.9767, 'lambda': 20.2694869834338, 'max_bin': 335}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:34:03,234] Trial 383 finished with value: 0.6936122679142057 and parameters: {'n_estimators': 649, 'eta': 0.06046944281219919, 'max_depth': 11, 'alpha': 0.9436, 'lambda': 19.4468568874046, 'max_bin': 360}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:34:24,301] Trial 384 finished with value: 0.6909440091085598 and parameters: {'n_estimators': 668, 'eta': 0.06909509889739583, 'max_depth': 11, 'alpha': 0.9772000000000001, 'lambda': 22.235026955569303, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:34:46,887] Trial 385 finished with value: 0.6907266382281667 and parameters: {'n_estimators': 642, 'eta': 0.05540880277855849, 'max_depth': 11, 'alpha': 0.7147, 'lambda': 21.109349967723446, 'max_bin': 354}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:35:07,872] Trial 386 finished with value: 0.6927896301759499 and parameters: {'n_estimators': 580, 'eta': 0.05855569904570605, 'max_depth': 11, 'alpha': 0.159, 'lambda': 18.50350488677027, 'max_bin': 348}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:35:30,415] Trial 387 finished with value: 0.69237228822673 and parameters: {'n_estimators': 623, 'eta': 0.06367901853909141, 'max_depth': 11, 'alpha': 0.9261, 'lambda': 20.32356763318242, 'max_bin': 340}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:35:51,158] Trial 388 finished with value: 0.6915011057401043 and parameters: {'n_estimators': 603, 'eta': 0.06633614927937137, 'max_depth': 11, 'alpha': 0.9610000000000001, 'lambda': 19.49244162643288, 'max_bin': 357}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:36:15,779] Trial 389 finished with value: 0.693825553570447 and parameters: {'n_estimators': 666, 'eta': 0.06140574614766375, 'max_depth': 11, 'alpha': 0.9806, 'lambda': 23.117104095367417, 'max_bin': 349}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:36:39,402] Trial 390 finished with value: 0.6937657983340323 and parameters: {'n_estimators': 649, 'eta': 0.059381069833038064, 'max_depth': 11, 'alpha': 0.9430000000000001, 'lambda': 21.49542809008194, 'max_bin': 336}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:37:04,493] Trial 391 finished with value: 0.6921190647288232 and parameters: {'n_estimators': 632, 'eta': 0.05717262338941783, 'max_depth': 11, 'alpha': 0.9209, 'lambda': 20.61035129785596, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:37:27,862] Trial 392 finished with value: 0.6905379372822544 and parameters: {'n_estimators': 677, 'eta': 0.06217663039215048, 'max_depth': 11, 'alpha': 0.6077, 'lambda': 19.0452417867061, 'max_bin': 363}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:37:50,582] Trial 393 finished with value: 0.6943751015761807 and parameters: {'n_estimators': 610, 'eta': 0.06429087814842258, 'max_depth': 11, 'alpha': 0.9573, 'lambda': 20.06002184372388, 'max_bin': 353}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:38:07,367] Trial 394 finished with value: 0.6868725619850651 and parameters: {'n_estimators': 695, 'eta': 0.060391921891537546, 'max_depth': 7, 'alpha': 0.6427, 'lambda': 18.35340397963216, 'max_bin': 340}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:38:31,896] Trial 395 finished with value: 0.6926022038045871 and parameters: {'n_estimators': 655, 'eta': 0.058131030315959904, 'max_depth': 11, 'alpha': 0.9760000000000001, 'lambda': 21.78669558605972, 'max_bin': 346}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:38:55,595] Trial 396 finished with value: 0.6938755578807553 and parameters: {'n_estimators': 628, 'eta': 0.05574215788183556, 'max_depth': 11, 'alpha': 0.2308, 'lambda': 19.745055420431306, 'max_bin': 351}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:39:17,048] Trial 397 finished with value: 0.6900796427650974 and parameters: {'n_estimators': 669, 'eta': 0.06753132661421667, 'max_depth': 12, 'alpha': 0.9455, 'lambda': 20.961016771860294, 'max_bin': 358}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:39:37,251] Trial 398 finished with value: 0.6920926957202568 and parameters: {'n_estimators': 566, 'eta': 0.062294082627608764, 'max_depth': 11, 'alpha': 0.9793000000000001, 'lambda': 19.091317726266514, 'max_bin': 335}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:39:57,463] Trial 399 finished with value: 0.6939255298249608 and parameters: {'n_estimators': 591, 'eta': 0.06511999785729063, 'max_depth': 11, 'alpha': 0.9256000000000001, 'lambda': 20.488755355057904, 'max_bin': 437}. Best is trial 301 with value: 0.7107383063531328.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7107\n",
      "\tBest params:\n",
      "\t\tn_estimators: 686\n",
      "\t\teta: 0.058168925372234524\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.9999\n",
      "\t\tlambda: 19.394176683760232\n",
      "\t\tmax_bin: 341\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681005    0.720551    0.717203    0.721099   \n",
      "1                    TP  406.000000  405.000000  405.000000  405.000000   \n",
      "2                    TN  353.000000  341.000000  346.000000  353.000000   \n",
      "3                    FP   77.000000   83.000000   82.000000   82.000000   \n",
      "4                    FN   63.000000   70.000000   66.000000   59.000000   \n",
      "5              Accuracy    0.844271    0.829811    0.835373    0.843159   \n",
      "6             Precision    0.840580    0.829918    0.831622    0.831622   \n",
      "7           Sensitivity    0.865672    0.852632    0.859873    0.872845   \n",
      "8           Specificity    0.820900    0.804200    0.808400    0.811500   \n",
      "9              F1 score    0.852941    0.841121    0.845511    0.851735   \n",
      "10  F1 score (weighted)    0.844128    0.829635    0.835180    0.842926   \n",
      "11     F1 score (macro)    0.843728    0.828944    0.834661    0.842633   \n",
      "12    Balanced Accuracy    0.843301    0.828438    0.834142    0.842170   \n",
      "13                  MCC    0.687868    0.658238    0.669854    0.686376   \n",
      "14                  NPV    0.848600    0.829700    0.839800    0.856800   \n",
      "15              ROC_AUC    0.843301    0.828438    0.834142    0.842170   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.702278    0.708199    0.649452    0.682835  \n",
      "1   406.000000  393.000000  395.000000  415.000000  \n",
      "2   348.000000  353.000000  345.000000  338.000000  \n",
      "3    81.000000   78.000000   82.000000   80.000000  \n",
      "4    64.000000   75.000000   77.000000   66.000000  \n",
      "5     0.838710    0.829811    0.823137    0.837597  \n",
      "6     0.833676    0.834395    0.828092    0.838384  \n",
      "7     0.863830    0.839744    0.836864    0.862786  \n",
      "8     0.811200    0.819000    0.808000    0.808600  \n",
      "9     0.848485    0.837061    0.832455    0.850410  \n",
      "10    0.838512    0.829786    0.823082    0.837379  \n",
      "11    0.838036    0.829473    0.822588    0.836397  \n",
      "12    0.837509    0.829385    0.822413    0.835699  \n",
      "13    0.676675    0.658965    0.645227    0.673205  \n",
      "14    0.844700    0.824800    0.817500    0.836600  \n",
      "15    0.837509    0.829385    0.822413    0.835699  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "y_pred_xgb_7_cat = np.where((y_pred_xgb_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 11:40:18,968] Trial 400 finished with value: 0.6814138533776274 and parameters: {'n_estimators': 636, 'eta': 0.06042229628935706, 'max_depth': 11, 'alpha': 0.9566, 'lambda': 22.448689801260283, 'max_bin': 343}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:40:37,486] Trial 401 finished with value: 0.6861922638930316 and parameters: {'n_estimators': 650, 'eta': 0.058465762190615445, 'max_depth': 11, 'alpha': 0.9423, 'lambda': 18.040520192468392, 'max_bin': 367}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:40:56,221] Trial 402 finished with value: 0.6866069469963044 and parameters: {'n_estimators': 697, 'eta': 0.06285192957620914, 'max_depth': 11, 'alpha': 0.9794, 'lambda': 19.64514061013442, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:41:14,328] Trial 403 finished with value: 0.6830196267527466 and parameters: {'n_estimators': 500, 'eta': 0.055523010155438465, 'max_depth': 11, 'alpha': 0.9573, 'lambda': 21.07548532553588, 'max_bin': 339}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:41:32,567] Trial 404 finished with value: 0.6842441021881521 and parameters: {'n_estimators': 619, 'eta': 0.060464213239413145, 'max_depth': 11, 'alpha': 0.9354, 'lambda': 18.922501424249322, 'max_bin': 354}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:41:52,631] Trial 405 finished with value: 0.6868296027548926 and parameters: {'n_estimators': 675, 'eta': 0.06517880567303758, 'max_depth': 11, 'alpha': 0.9666, 'lambda': 20.155966631785567, 'max_bin': 346}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:42:13,900] Trial 406 finished with value: 0.6858053196971314 and parameters: {'n_estimators': 656, 'eta': 0.057600249245102465, 'max_depth': 11, 'alpha': 0.9212, 'lambda': 21.878656946782673, 'max_bin': 361}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:42:32,769] Trial 407 finished with value: 0.6884707629903819 and parameters: {'n_estimators': 604, 'eta': 0.061898537357129545, 'max_depth': 11, 'alpha': 0.9780000000000001, 'lambda': 20.884549842758588, 'max_bin': 342}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:42:50,787] Trial 408 finished with value: 0.6845616034830935 and parameters: {'n_estimators': 636, 'eta': 0.06957245337273847, 'max_depth': 11, 'alpha': 0.9513, 'lambda': 18.285602855535977, 'max_bin': 348}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:43:04,697] Trial 409 finished with value: 0.6809529116786948 and parameters: {'n_estimators': 689, 'eta': 0.06378214186570041, 'max_depth': 6, 'alpha': 0.922, 'lambda': 19.72214742538956, 'max_bin': 356}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:43:25,216] Trial 410 finished with value: 0.6825149533369789 and parameters: {'n_estimators': 660, 'eta': 0.05931826218772854, 'max_depth': 11, 'alpha': 0.3154, 'lambda': 21.404983728580618, 'max_bin': 334}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:43:44,516] Trial 411 finished with value: 0.6852124116061169 and parameters: {'n_estimators': 618, 'eta': 0.06660811059847815, 'max_depth': 11, 'alpha': 0.9806, 'lambda': 20.356160017353556, 'max_bin': 351}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:44:06,131] Trial 412 finished with value: 0.6860086994367766 and parameters: {'n_estimators': 639, 'eta': 0.05533648621000543, 'max_depth': 11, 'alpha': 0.36910000000000004, 'lambda': 22.776725332639966, 'max_bin': 339}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:44:26,944] Trial 413 finished with value: 0.6817110179121569 and parameters: {'n_estimators': 678, 'eta': 0.06107654064798824, 'max_depth': 11, 'alpha': 0.9618000000000001, 'lambda': 23.571252353728074, 'max_bin': 345}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:44:48,689] Trial 414 finished with value: 0.6850074340288816 and parameters: {'n_estimators': 651, 'eta': 0.05748728931108841, 'max_depth': 11, 'alpha': 0.9462, 'lambda': 19.031481098440732, 'max_bin': 370}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:45:08,537] Trial 415 finished with value: 0.6837646584950752 and parameters: {'n_estimators': 589, 'eta': 0.06328808210727471, 'max_depth': 12, 'alpha': 0.9786, 'lambda': 22.06218862178132, 'max_bin': 360}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:45:33,419] Trial 416 finished with value: 0.6862399816315565 and parameters: {'n_estimators': 696, 'eta': 0.059580336565984215, 'max_depth': 11, 'alpha': 0.9353, 'lambda': 39.45845206841139, 'max_bin': 354}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:45:51,906] Trial 417 finished with value: 0.6874114509078684 and parameters: {'n_estimators': 619, 'eta': 0.06169787459885963, 'max_depth': 11, 'alpha': 0.9600000000000001, 'lambda': 19.71710227404421, 'max_bin': 347}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:46:13,891] Trial 418 finished with value: 0.6861861616224594 and parameters: {'n_estimators': 668, 'eta': 0.06770524617218175, 'max_depth': 11, 'alpha': 0.9255, 'lambda': 36.40313080839594, 'max_bin': 342}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:46:32,386] Trial 419 finished with value: 0.679869306780408 and parameters: {'n_estimators': 640, 'eta': 0.0646486897917093, 'max_depth': 11, 'alpha': 0.9845, 'lambda': 18.010311309036876, 'max_bin': 336}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:46:52,619] Trial 420 finished with value: 0.6844028936698962 and parameters: {'n_estimators': 604, 'eta': 0.05689504066118127, 'max_depth': 11, 'alpha': 0.9165000000000001, 'lambda': 20.85634658566027, 'max_bin': 351}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:47:13,064] Trial 421 finished with value: 0.6850889943744045 and parameters: {'n_estimators': 624, 'eta': 0.0592758084296715, 'max_depth': 11, 'alpha': 0.9619000000000001, 'lambda': 18.962500547911876, 'max_bin': 364}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:47:35,064] Trial 422 finished with value: 0.6866392019310485 and parameters: {'n_estimators': 655, 'eta': 0.054697055219834874, 'max_depth': 11, 'alpha': 0.9410000000000001, 'lambda': 20.360833526632437, 'max_bin': 355}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:47:55,159] Trial 423 finished with value: 0.6877084982500565 and parameters: {'n_estimators': 682, 'eta': 0.06242998262143071, 'max_depth': 11, 'alpha': 0.9844, 'lambda': 21.304741304688076, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:48:14,637] Trial 424 finished with value: 0.6867143628493186 and parameters: {'n_estimators': 641, 'eta': 0.06037470153350997, 'max_depth': 11, 'alpha': 0.9614, 'lambda': 19.571656580417514, 'max_bin': 349}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:48:35,593] Trial 425 finished with value: 0.6823209417531209 and parameters: {'n_estimators': 660, 'eta': 0.058211036554190146, 'max_depth': 11, 'alpha': 0.48260000000000003, 'lambda': 24.413679448263224, 'max_bin': 340}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:48:54,159] Trial 426 finished with value: 0.6858458849656168 and parameters: {'n_estimators': 704, 'eta': 0.07053254044085583, 'max_depth': 11, 'alpha': 0.9409000000000001, 'lambda': 20.28970248948659, 'max_bin': 358}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:49:11,950] Trial 427 finished with value: 0.6831293238450621 and parameters: {'n_estimators': 542, 'eta': 0.0642805478760732, 'max_depth': 11, 'alpha': 0.9809, 'lambda': 18.636029920199945, 'max_bin': 374}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:49:19,985] Trial 428 finished with value: 0.6710810414972244 and parameters: {'n_estimators': 189, 'eta': 0.06640176947840117, 'max_depth': 11, 'alpha': 0.9244, 'lambda': 22.085474657437356, 'max_bin': 334}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:49:37,969] Trial 429 finished with value: 0.684229478521239 and parameters: {'n_estimators': 605, 'eta': 0.06136284135892235, 'max_depth': 11, 'alpha': 0.9601000000000001, 'lambda': 21.304292292593342, 'max_bin': 346}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:49:58,378] Trial 430 finished with value: 0.6826798835826697 and parameters: {'n_estimators': 631, 'eta': 0.056751072183734506, 'max_depth': 11, 'alpha': 0.9422, 'lambda': 19.569215395627737, 'max_bin': 353}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:50:13,469] Trial 431 finished with value: 0.6818065923718665 and parameters: {'n_estimators': 364, 'eta': 0.058977851722057675, 'max_depth': 11, 'alpha': 0.9118, 'lambda': 18.226765683942446, 'max_bin': 340}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:50:27,454] Trial 432 finished with value: 0.6779629718466674 and parameters: {'n_estimators': 571, 'eta': 0.09992161417571482, 'max_depth': 11, 'alpha': 0.9658, 'lambda': 20.436990283914234, 'max_bin': 349}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:50:47,276] Trial 433 finished with value: 0.6870196727899278 and parameters: {'n_estimators': 674, 'eta': 0.0631181814194431, 'max_depth': 11, 'alpha': 0.9825, 'lambda': 22.964662869156, 'max_bin': 360}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:51:02,138] Trial 434 finished with value: 0.6739871419263728 and parameters: {'n_estimators': 587, 'eta': 0.09474968366832157, 'max_depth': 12, 'alpha': 0.09970000000000001, 'lambda': 19.095034952217215, 'max_bin': 345}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:51:22,333] Trial 435 finished with value: 0.6840114272817981 and parameters: {'n_estimators': 647, 'eta': 0.054874734414521925, 'max_depth': 11, 'alpha': 0.9450000000000001, 'lambda': 19.96614936148751, 'max_bin': 332}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:51:42,988] Trial 436 finished with value: 0.6815340187394742 and parameters: {'n_estimators': 619, 'eta': 0.06087941544156732, 'max_depth': 11, 'alpha': 0.9621000000000001, 'lambda': 21.517669205935057, 'max_bin': 356}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:52:03,361] Trial 437 finished with value: 0.6852243364837969 and parameters: {'n_estimators': 686, 'eta': 0.05774505220348005, 'max_depth': 11, 'alpha': 0.9211, 'lambda': 20.86415283232203, 'max_bin': 338}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:52:22,056] Trial 438 finished with value: 0.6863676732687461 and parameters: {'n_estimators': 661, 'eta': 0.06506518123215953, 'max_depth': 11, 'alpha': 0.9992000000000001, 'lambda': 18.107023268714787, 'max_bin': 351}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:52:42,946] Trial 439 finished with value: 0.6889755973831779 and parameters: {'n_estimators': 632, 'eta': 0.05994032385576685, 'max_depth': 11, 'alpha': 0.9473, 'lambda': 19.352630371739835, 'max_bin': 481}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:53:01,764] Trial 440 finished with value: 0.6852583502900226 and parameters: {'n_estimators': 606, 'eta': 0.06845010408569355, 'max_depth': 11, 'alpha': 0.9807, 'lambda': 22.256867240059638, 'max_bin': 343}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:53:21,594] Trial 441 finished with value: 0.6887983103537633 and parameters: {'n_estimators': 668, 'eta': 0.06197697756273544, 'max_depth': 11, 'alpha': 0.9323, 'lambda': 20.442431643579535, 'max_bin': 347}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:53:43,500] Trial 442 finished with value: 0.6872201159541398 and parameters: {'n_estimators': 699, 'eta': 0.06372606199787124, 'max_depth': 11, 'alpha': 0.9646, 'lambda': 18.92230634719813, 'max_bin': 365}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:54:05,146] Trial 443 finished with value: 0.6842143172682461 and parameters: {'n_estimators': 645, 'eta': 0.056733964083464844, 'max_depth': 11, 'alpha': 0.9827, 'lambda': 23.71110543085711, 'max_bin': 357}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:54:24,712] Trial 444 finished with value: 0.6842878996932233 and parameters: {'n_estimators': 624, 'eta': 0.059291983678935935, 'max_depth': 11, 'alpha': 0.6666000000000001, 'lambda': 19.68213258961918, 'max_bin': 338}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:54:44,265] Trial 445 finished with value: 0.6861965277489299 and parameters: {'n_estimators': 683, 'eta': 0.06228835516224263, 'max_depth': 11, 'alpha': 0.9157000000000001, 'lambda': 21.113371807865413, 'max_bin': 351}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:55:04,229] Trial 446 finished with value: 0.6846075851785132 and parameters: {'n_estimators': 653, 'eta': 0.06585416106058492, 'max_depth': 11, 'alpha': 0.9487000000000001, 'lambda': 17.87254353628104, 'max_bin': 343}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:55:13,383] Trial 447 finished with value: 0.6613523738063775 and parameters: {'n_estimators': 432, 'eta': 0.05616488442823556, 'max_depth': 6, 'alpha': 0.9633, 'lambda': 20.189651259625602, 'max_bin': 353}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:55:36,266] Trial 448 finished with value: 0.6234191706679538 and parameters: {'n_estimators': 598, 'eta': 0.009576562955954347, 'max_depth': 11, 'alpha': 0.9369000000000001, 'lambda': 21.81316165744949, 'max_bin': 361}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:55:54,619] Trial 449 finished with value: 0.6845060257955736 and parameters: {'n_estimators': 637, 'eta': 0.060155275885129506, 'max_depth': 11, 'alpha': 0.9793000000000001, 'lambda': 18.801974351032072, 'max_bin': 347}. Best is trial 301 with value: 0.7107383063531328.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7107\n",
      "\tBest params:\n",
      "\t\tn_estimators: 686\n",
      "\t\teta: 0.058168925372234524\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.9999\n",
      "\t\tlambda: 19.394176683760232\n",
      "\t\tmax_bin: 341\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681005    0.720551    0.717203    0.721099   \n",
      "1                    TP  406.000000  405.000000  405.000000  405.000000   \n",
      "2                    TN  353.000000  341.000000  346.000000  353.000000   \n",
      "3                    FP   77.000000   83.000000   82.000000   82.000000   \n",
      "4                    FN   63.000000   70.000000   66.000000   59.000000   \n",
      "5              Accuracy    0.844271    0.829811    0.835373    0.843159   \n",
      "6             Precision    0.840580    0.829918    0.831622    0.831622   \n",
      "7           Sensitivity    0.865672    0.852632    0.859873    0.872845   \n",
      "8           Specificity    0.820900    0.804200    0.808400    0.811500   \n",
      "9              F1 score    0.852941    0.841121    0.845511    0.851735   \n",
      "10  F1 score (weighted)    0.844128    0.829635    0.835180    0.842926   \n",
      "11     F1 score (macro)    0.843728    0.828944    0.834661    0.842633   \n",
      "12    Balanced Accuracy    0.843301    0.828438    0.834142    0.842170   \n",
      "13                  MCC    0.687868    0.658238    0.669854    0.686376   \n",
      "14                  NPV    0.848600    0.829700    0.839800    0.856800   \n",
      "15              ROC_AUC    0.843301    0.828438    0.834142    0.842170   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.702278    0.708199    0.649452    0.682835    0.697318  \n",
      "1   406.000000  393.000000  395.000000  415.000000  415.000000  \n",
      "2   348.000000  353.000000  345.000000  338.000000  349.000000  \n",
      "3    81.000000   78.000000   82.000000   80.000000   67.000000  \n",
      "4    64.000000   75.000000   77.000000   66.000000   68.000000  \n",
      "5     0.838710    0.829811    0.823137    0.837597    0.849833  \n",
      "6     0.833676    0.834395    0.828092    0.838384    0.860996  \n",
      "7     0.863830    0.839744    0.836864    0.862786    0.859213  \n",
      "8     0.811200    0.819000    0.808000    0.808600    0.838900  \n",
      "9     0.848485    0.837061    0.832455    0.850410    0.860104  \n",
      "10    0.838512    0.829786    0.823082    0.837379    0.849845  \n",
      "11    0.838036    0.829473    0.822588    0.836397    0.849019  \n",
      "12    0.837509    0.829385    0.822413    0.835699    0.849078  \n",
      "13    0.676675    0.658965    0.645227    0.673205    0.698041  \n",
      "14    0.844700    0.824800    0.817500    0.836600    0.836900  \n",
      "15    0.837509    0.829385    0.822413    0.835699    0.849078  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "y_pred_xgb_8_cat = np.where((y_pred_xgb_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 11:56:21,266] Trial 450 finished with value: 0.6783934065450097 and parameters: {'n_estimators': 669, 'eta': 0.058609579700703046, 'max_depth': 11, 'alpha': 0.9999, 'lambda': 20.6705490217405, 'max_bin': 332}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:56:38,911] Trial 451 finished with value: 0.6748816821328416 and parameters: {'n_estimators': 616, 'eta': 0.08421782686194412, 'max_depth': 12, 'alpha': 0.9523, 'lambda': 19.760263499805827, 'max_bin': 341}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:56:55,617] Trial 452 finished with value: 0.6636595341656804 and parameters: {'n_estimators': 705, 'eta': 0.06372251763926923, 'max_depth': 11, 'alpha': 0.9216000000000001, 'lambda': 1.937838182027619, 'max_bin': 348}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:57:18,559] Trial 453 finished with value: 0.6764698965257651 and parameters: {'n_estimators': 647, 'eta': 0.054674957346990884, 'max_depth': 11, 'alpha': 0.9660000000000001, 'lambda': 22.522961950144275, 'max_bin': 355}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:57:40,875] Trial 454 finished with value: 0.6744156014154529 and parameters: {'n_estimators': 666, 'eta': 0.06137799497280832, 'max_depth': 11, 'alpha': 0.9389000000000001, 'lambda': 21.127422545564144, 'max_bin': 337}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:58:02,489] Trial 455 finished with value: 0.6697789184377957 and parameters: {'n_estimators': 626, 'eta': 0.06738221809232255, 'max_depth': 11, 'alpha': 0.029300000000000003, 'lambda': 18.82166642085564, 'max_bin': 343}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:58:22,085] Trial 456 finished with value: 0.6762355637048497 and parameters: {'n_estimators': 584, 'eta': 0.08073294449303313, 'max_depth': 11, 'alpha': 0.9122, 'lambda': 26.013308101706485, 'max_bin': 351}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:58:43,410] Trial 457 finished with value: 0.677114904848508 and parameters: {'n_estimators': 687, 'eta': 0.07205375018555113, 'max_depth': 11, 'alpha': 0.9808, 'lambda': 19.86113311066228, 'max_bin': 367}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:59:07,591] Trial 458 finished with value: 0.6776889576874107 and parameters: {'n_estimators': 642, 'eta': 0.05771507778171169, 'max_depth': 11, 'alpha': 0.5795, 'lambda': 17.805192075840328, 'max_bin': 357}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:59:25,853] Trial 459 finished with value: 0.6637648364732003 and parameters: {'n_estimators': 520, 'eta': 0.06547499479459756, 'max_depth': 11, 'alpha': 0.9607, 'lambda': 2.835221050954825, 'max_bin': 336}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 11:59:48,956] Trial 460 finished with value: 0.6752585618744008 and parameters: {'n_estimators': 608, 'eta': 0.06319566793323514, 'max_depth': 11, 'alpha': 0.9343, 'lambda': 21.629363733092692, 'max_bin': 415}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:00:12,048] Trial 461 finished with value: 0.677314857816474 and parameters: {'n_estimators': 655, 'eta': 0.06037181329037821, 'max_depth': 11, 'alpha': 0.9809, 'lambda': 20.448889704262257, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:00:38,802] Trial 462 finished with value: 0.6770823069220409 and parameters: {'n_estimators': 681, 'eta': 0.05817140391745928, 'max_depth': 11, 'alpha': 0.9504, 'lambda': 19.344568313713214, 'max_bin': 349}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:01:01,689] Trial 463 finished with value: 0.6777644124148339 and parameters: {'n_estimators': 628, 'eta': 0.062037976966552624, 'max_depth': 11, 'alpha': 0.9155000000000001, 'lambda': 20.763525777907496, 'max_bin': 362}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:01:21,204] Trial 464 finished with value: 0.6757675057750927 and parameters: {'n_estimators': 483, 'eta': 0.05521300427343284, 'max_depth': 11, 'alpha': 0.9660000000000001, 'lambda': 18.75409469871624, 'max_bin': 340}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:01:46,465] Trial 465 finished with value: 0.6748425195327927 and parameters: {'n_estimators': 658, 'eta': 0.05937842889962662, 'max_depth': 11, 'alpha': 0.9385, 'lambda': 19.901857131557264, 'max_bin': 353}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:02:05,251] Trial 466 finished with value: 0.6756829308283607 and parameters: {'n_estimators': 555, 'eta': 0.07606989913324133, 'max_depth': 11, 'alpha': 0.9845, 'lambda': 20.897461062192722, 'max_bin': 330}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:02:28,699] Trial 467 finished with value: 0.6768921845308864 and parameters: {'n_estimators': 610, 'eta': 0.06439392809808538, 'max_depth': 12, 'alpha': 0.9993000000000001, 'lambda': 22.28155990214302, 'max_bin': 347}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:02:50,940] Trial 468 finished with value: 0.6732315487131395 and parameters: {'n_estimators': 707, 'eta': 0.06108435110816533, 'max_depth': 11, 'alpha': 0.9568000000000001, 'lambda': 19.454360318868932, 'max_bin': 381}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:03:14,479] Trial 469 finished with value: 0.6760535779596355 and parameters: {'n_estimators': 636, 'eta': 0.05586732423554808, 'max_depth': 11, 'alpha': 0.934, 'lambda': 17.93053446822197, 'max_bin': 359}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:03:35,772] Trial 470 finished with value: 0.6794493536546984 and parameters: {'n_estimators': 669, 'eta': 0.0664474697682462, 'max_depth': 11, 'alpha': 0.908, 'lambda': 21.5881900068713, 'max_bin': 343}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:03:56,708] Trial 471 finished with value: 0.6752191489768988 and parameters: {'n_estimators': 589, 'eta': 0.06284456078769103, 'max_depth': 11, 'alpha': 0.9689000000000001, 'lambda': 20.1619743747435, 'max_bin': 334}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:04:18,133] Trial 472 finished with value: 0.6746586762264408 and parameters: {'n_estimators': 689, 'eta': 0.07000749271337703, 'max_depth': 11, 'alpha': 0.9495, 'lambda': 18.59100033669557, 'max_bin': 472}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:04:41,078] Trial 473 finished with value: 0.674421320517326 and parameters: {'n_estimators': 645, 'eta': 0.05719617663992565, 'max_depth': 11, 'alpha': 0.9679000000000001, 'lambda': 19.479996830847316, 'max_bin': 352}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:05:04,522] Trial 474 finished with value: 0.6748817965427825 and parameters: {'n_estimators': 624, 'eta': 0.059619828680676515, 'max_depth': 11, 'alpha': 0.9812000000000001, 'lambda': 20.686803835397768, 'max_bin': 347}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:05:21,633] Trial 475 finished with value: 0.6744491085265556 and parameters: {'n_estimators': 672, 'eta': 0.06143535506393143, 'max_depth': 8, 'alpha': 0.9292, 'lambda': 23.34893541932507, 'max_bin': 356}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:05:37,880] Trial 476 finished with value: 0.6730987870149706 and parameters: {'n_estimators': 654, 'eta': 0.08882683593633084, 'max_depth': 11, 'alpha': 0.9508000000000001, 'lambda': 21.6646398463659, 'max_bin': 339}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:06:00,590] Trial 477 finished with value: 0.6755780848652064 and parameters: {'n_estimators': 607, 'eta': 0.05819309735722155, 'max_depth': 11, 'alpha': 0.9087000000000001, 'lambda': 20.104149687315285, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:06:21,499] Trial 478 finished with value: 0.6749590689077571 and parameters: {'n_estimators': 570, 'eta': 0.06406757723886485, 'max_depth': 11, 'alpha': 0.9797, 'lambda': 18.98675465441488, 'max_bin': 362}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:06:45,951] Trial 479 finished with value: 0.6755659001331121 and parameters: {'n_estimators': 634, 'eta': 0.05453857631712665, 'max_depth': 11, 'alpha': 0.9286000000000001, 'lambda': 22.646401767415384, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:07:09,359] Trial 480 finished with value: 0.6743124842619057 and parameters: {'n_estimators': 697, 'eta': 0.06850977459210117, 'max_depth': 11, 'alpha': 0.9538000000000001, 'lambda': 21.11733778870113, 'max_bin': 340}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:07:31,004] Trial 481 finished with value: 0.6773492134258204 and parameters: {'n_estimators': 619, 'eta': 0.060404398043976876, 'max_depth': 11, 'alpha': 0.9838, 'lambda': 18.219110625362006, 'max_bin': 356}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:07:51,314] Trial 482 finished with value: 0.6782630022649725 and parameters: {'n_estimators': 664, 'eta': 0.06296801390341887, 'max_depth': 11, 'alpha': 0.731, 'lambda': 20.350785153715087, 'max_bin': 347}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:08:12,395] Trial 483 finished with value: 0.6772568572812278 and parameters: {'n_estimators': 641, 'eta': 0.0655216328619299, 'max_depth': 11, 'alpha': 0.9410000000000001, 'lambda': 19.30378805808265, 'max_bin': 336}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:08:35,513] Trial 484 finished with value: 0.6780739369887657 and parameters: {'n_estimators': 679, 'eta': 0.057126754900295895, 'max_depth': 11, 'alpha': 0.996, 'lambda': 21.562693351655263, 'max_bin': 353}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:08:47,795] Trial 485 finished with value: 0.6683772459117923 and parameters: {'n_estimators': 288, 'eta': 0.06007864021150512, 'max_depth': 11, 'alpha': 0.9652000000000001, 'lambda': 19.988692843457, 'max_bin': 365}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:09:06,810] Trial 486 finished with value: 0.6650689038095839 and parameters: {'n_estimators': 596, 'eta': 0.06202679673418972, 'max_depth': 12, 'alpha': 0.41400000000000003, 'lambda': 1.0694011105166155, 'max_bin': 344}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:09:28,458] Trial 487 finished with value: 0.6749847218382355 and parameters: {'n_estimators': 647, 'eta': 0.05897202862510891, 'max_depth': 11, 'alpha': 0.9026000000000001, 'lambda': 20.755042751361877, 'max_bin': 369}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:09:51,654] Trial 488 finished with value: 0.6781558826993234 and parameters: {'n_estimators': 711, 'eta': 0.056581414058077956, 'max_depth': 11, 'alpha': 0.9253, 'lambda': 17.590193933321718, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:10:15,943] Trial 489 finished with value: 0.6750755044105515 and parameters: {'n_estimators': 621, 'eta': 0.05457876411727461, 'max_depth': 11, 'alpha': 0.998, 'lambda': 26.707873853622846, 'max_bin': 332}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:10:39,137] Trial 490 finished with value: 0.6711711722494712 and parameters: {'n_estimators': 661, 'eta': 0.0647761283370081, 'max_depth': 11, 'alpha': 0.9500000000000001, 'lambda': 18.674123817879092, 'max_bin': 359}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:11:02,718] Trial 491 finished with value: 0.6773819927538114 and parameters: {'n_estimators': 691, 'eta': 0.061497061972344495, 'max_depth': 11, 'alpha': 0.9676, 'lambda': 22.275244926062676, 'max_bin': 340}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:11:23,760] Trial 492 finished with value: 0.6751717225406426 and parameters: {'n_estimators': 631, 'eta': 0.058711492814662466, 'max_depth': 11, 'alpha': 0.9393, 'lambda': 19.443176899338102, 'max_bin': 347}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:11:45,809] Trial 493 finished with value: 0.6778097886269082 and parameters: {'n_estimators': 675, 'eta': 0.06335249290897489, 'max_depth': 11, 'alpha': 0.9678, 'lambda': 21.20629737696069, 'max_bin': 354}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:12:10,430] Trial 494 finished with value: 0.6797264997303701 and parameters: {'n_estimators': 649, 'eta': 0.0601107066981742, 'max_depth': 11, 'alpha': 0.9812000000000001, 'lambda': 20.002862303213224, 'max_bin': 374}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:12:32,525] Trial 495 finished with value: 0.6724770007036256 and parameters: {'n_estimators': 596, 'eta': 0.0741656890916571, 'max_depth': 11, 'alpha': 0.918, 'lambda': 18.372438338328593, 'max_bin': 343}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:12:54,104] Trial 496 finished with value: 0.6730816925874933 and parameters: {'n_estimators': 612, 'eta': 0.06573260167691829, 'max_depth': 11, 'alpha': 0.9490000000000001, 'lambda': 20.590258195317976, 'max_bin': 335}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:13:18,048] Trial 497 finished with value: 0.6726901530475781 and parameters: {'n_estimators': 659, 'eta': 0.06738709116263221, 'max_depth': 11, 'alpha': 0.9603, 'lambda': 27.584634272404408, 'max_bin': 350}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:13:39,730] Trial 498 finished with value: 0.6766319750974077 and parameters: {'n_estimators': 581, 'eta': 0.057124374669189404, 'max_depth': 11, 'alpha': 1.0, 'lambda': 23.60115941849809, 'max_bin': 328}. Best is trial 301 with value: 0.7107383063531328.\n",
      "[I 2023-12-11 12:14:01,205] Trial 499 finished with value: 0.6736402331891274 and parameters: {'n_estimators': 632, 'eta': 0.06187188674459855, 'max_depth': 11, 'alpha': 0.9279000000000001, 'lambda': 21.868002426219032, 'max_bin': 358}. Best is trial 301 with value: 0.7107383063531328.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7107\n",
      "\tBest params:\n",
      "\t\tn_estimators: 686\n",
      "\t\teta: 0.058168925372234524\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.9999\n",
      "\t\tlambda: 19.394176683760232\n",
      "\t\tmax_bin: 341\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681005    0.720551    0.717203    0.721099   \n",
      "1                    TP  406.000000  405.000000  405.000000  405.000000   \n",
      "2                    TN  353.000000  341.000000  346.000000  353.000000   \n",
      "3                    FP   77.000000   83.000000   82.000000   82.000000   \n",
      "4                    FN   63.000000   70.000000   66.000000   59.000000   \n",
      "5              Accuracy    0.844271    0.829811    0.835373    0.843159   \n",
      "6             Precision    0.840580    0.829918    0.831622    0.831622   \n",
      "7           Sensitivity    0.865672    0.852632    0.859873    0.872845   \n",
      "8           Specificity    0.820900    0.804200    0.808400    0.811500   \n",
      "9              F1 score    0.852941    0.841121    0.845511    0.851735   \n",
      "10  F1 score (weighted)    0.844128    0.829635    0.835180    0.842926   \n",
      "11     F1 score (macro)    0.843728    0.828944    0.834661    0.842633   \n",
      "12    Balanced Accuracy    0.843301    0.828438    0.834142    0.842170   \n",
      "13                  MCC    0.687868    0.658238    0.669854    0.686376   \n",
      "14                  NPV    0.848600    0.829700    0.839800    0.856800   \n",
      "15              ROC_AUC    0.843301    0.828438    0.834142    0.842170   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.702278    0.708199    0.649452    0.682835    0.697318    0.708034  \n",
      "1   406.000000  393.000000  395.000000  415.000000  415.000000  407.000000  \n",
      "2   348.000000  353.000000  345.000000  338.000000  349.000000  341.000000  \n",
      "3    81.000000   78.000000   82.000000   80.000000   67.000000   74.000000  \n",
      "4    64.000000   75.000000   77.000000   66.000000   68.000000   77.000000  \n",
      "5     0.838710    0.829811    0.823137    0.837597    0.849833    0.832036  \n",
      "6     0.833676    0.834395    0.828092    0.838384    0.860996    0.846154  \n",
      "7     0.863830    0.839744    0.836864    0.862786    0.859213    0.840909  \n",
      "8     0.811200    0.819000    0.808000    0.808600    0.838900    0.821700  \n",
      "9     0.848485    0.837061    0.832455    0.850410    0.860104    0.843523  \n",
      "10    0.838512    0.829786    0.823082    0.837379    0.849845    0.832077  \n",
      "11    0.838036    0.829473    0.822588    0.836397    0.849019    0.831125  \n",
      "12    0.837509    0.829385    0.822413    0.835699    0.849078    0.831298  \n",
      "13    0.676675    0.658965    0.645227    0.673205    0.698041    0.662269  \n",
      "14    0.844700    0.824800    0.817500    0.836600    0.836900    0.815800  \n",
      "15    0.837509    0.829385    0.822413    0.835699    0.849078    0.831298  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_9_cat = np.where((y_pred_xgb_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnwAAAHJCAYAAAAfAuQNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB3eklEQVR4nO3dd3yN5/8/8Nc52TuRIRtBYo/EDBV7E4oYNRKjalethiJUqVaNqu2DVIyENjahJYoqsTchEhKZyE5k3b8//HJ/HTmJ5ORE5OT1fDw86tz3dV/3+32dW/POdS+JIAgCiIiIiEhlScs7ACIiIiIqWyz4iIiIiFQcCz4iIiIiFceCj4iIiEjFseAjIiIiUnEs+IiIiIhUHAs+IiIiIhXHgo+IiIhIxbHgIyIiIlJxLPiIiIiIVBwLPpIhkUggkUiKbFO9enVIJBKEh4d/nKDok9O+ffsPHicfi6enJyQSCXbs2FHeoZS5T2nciahiYcFHREREpOJY8BERERGpOBZ8VGqvX7+Grq4uatasCUEQ5Lbp3bs3JBIJrl69CgAIDw+HRCKBp6cnHjx4gH79+qFKlSrQ09ND27ZtcfLkyUL3t2fPHnTo0AEmJibQ1tZG3bp1sWTJErx586ZAW4lEgvbt2+PFixfw8vKClZUV1NTUxNN/+acDw8LCsHLlStSpUwfa2tqwtbXF9OnTkZycXKDPM2fO4Msvv0S9evVgaGgIHR0d1K9fHwsXLkRGRkaB9j4+PpBIJAgODsbvv/+O5s2bQ09PD9WrVxfb7NixAwMGDICDgwN0dHRgaGiINm3a4Pfff5c7Bvmn9rKzs7F48WLUrFkT2tracHJywpYtW8R269atQ4MGDaCjowNbW1v4+PggLy9Pbp+XLl3CwIEDYWlpCU1NTdjZ2WH8+PF48eKF2Cb/ezt79qw4vvl/2rdvL9NfZGQkJk+eDAcHB2hpacHU1BR9+/ZFSEiIQmNUUsocI0WP18zMTCxbtgwNGzaErq4uDA0N8dlnn2Hv3r0F2r6/j4EDB8Lc3BxSqRQ7duwo1riX5tjcv38/WrRoAV1dXVSpUgWDBw9GZGSk3LxevXqFefPmoUGDBtDV1YWRkREaN26Mb7/9FmlpaQXaent7o27dutDR0YGRkRE6deokd8zevHmDVatWoWnTpjAxMYGuri7s7OzQp08fnDp1Sm4sRFQ86uUdAFV8JiYmGDJkCLZv346//voLXbp0kVn//PlzHD9+HC4uLnBxcZFZ9/TpU7Ru3RoNGjTA+PHjER0dDX9/f/To0QO7d+/G4MGDZdqPGTMG27Ztg52dHQYMGAAjIyP8999/mD9/Pv7++2+cPHkSGhoaMtu8fPkSrVu3hoGBAQYOHAhBEGBhYSHTZvr06fjnn3/g4eEBd3d3BAUFYfXq1Th37hzOnz8PbW1tse3y5cvx4MEDuLq6olevXsjIyMCFCxewePFinDlzBqdPn4a6esF/WitWrMBff/2FPn36oGPHjkhMTBTXTZgwAfXq1UO7du1gZWWFhIQEHD16FKNGjcKDBw+wdOlSuWM/ZMgQXLp0CT179oSGhgb279+PL7/8Epqamrhy5Qp2796N3r17o3Pnzjh8+DAWLVoEHR0dzJkzR6af7du3Y9y4cdDW1kbfvn1ha2uL0NBQbN26FYcPH8Z///0He3t7GBsbY+HChdixYwciIiKwcOFCsY93i7Nr166ha9euePXqFbp164bPP/8cCQkJOHDgANq2bYvAwED07NmzRGOkKGWNEVCy4zUrKwtdu3bFuXPnUK9ePUyaNAnp6enYt28fhg4diuvXr2P58uUF9vH48WO0atUKTk5OGD58OFJTU9GwYcNijbuix+b69etx6NAh9O3bF25ubrh06RICAgJw48YN3Lp1C1paWjJj0KFDB0RERMDFxQUTJkxAXl4eHj58iFWrVuGrr76Cnp4eACAiIgLt27dHeHg42rVrhx49eiA1NRVHjhxB9+7dsXHjRnz55Zdi3yNHjkRAQAAaNGiAkSNHQkdHBy9evMD58+cRFBRU4P8tRFQCAtE7AAgAhIULFxb6x8jISAAgPH36VNzuypUrAgBhwIABBfqcP3++AEDYvHmzuOzp06fivmbOnCnTPiQkRFBXVxeMjY2FpKQkcfn27dsFAMLAgQOFjIwMmW0WLlwoABBWrVolN58RI0YI2dnZBWIbNWqUAEAwNTUVwsPDxeW5ubnC559/LgAQFi9eLLPNkydPhLy8vAJ9eXt7CwCEPXv2yI1NV1dXuHbtWoHtBEEQHj9+XGBZZmam0L59e0FdXV14/vy5zDo3NzcBgNCsWTPh9evXMrFpaGgIRkZGQvXq1YXIyEhxXWJiomBmZiaYmZnJjMXDhw8FDQ0NoXbt2sKLFy9k9vP3338LUqlUcHd3l7t/ebKzs4WaNWsK2trawrlz52TWRUVFCdbW1kLVqlVlvsPijFFh8r/D7du3y41RGWOkyPH6ww8/CACE3r17y/QVExMj2NnZCQBkxufdfXh7e8vNtahxz89NkWPTwMBAuHXrlsy6oUOHCgCEvXv3yix3dXUVAAhLly4tsJ/4+HiZ79XNzU2QSCRCQECATLvXr18LjRs3FrS1tYXo6GhBEN6OvUQiEVxcXIScnJwCfSckJBSaNxF9GAs+kpH/A6c4f94t+ARBEJo3by5oaGgIMTEx4rKcnBzB2tpaMDAwEFJTU8Xl+T/cjIyMhOTk5AJx5P8Q37Fjh7isSZMmgoaGhswP73f3Y2pqKjRr1qxAPpqamkJsbKzcfPP3835RJwhvf3hKpVKhevXqcrd9X0JCggBA8PLyklme/0N12rRpxernXfv37xcACL6+vjLL83/w//333wW26dChgwBA+N///ldgnZeXlwBAprj9+uuvBQDC0aNH5cbQr18/QSqVyhQzRRUeBw4cEAAIs2bNkrt+9erVAgDhyJEj4rLSjNGHCj5ljJEix2vNmjUFiUQiPHz4sED7zZs3FzhW8vdRtWpVITMzU26uHyr4CvOhY/O7774rsM3p06cFAMKMGTPEZfm/2DVp0kTIzc0tcp83btwQAAiDBg2Suz7/OPntt98EQRCE5ORkAYDg6uoqt2glotLhKV2SSyjkWjzg7SmkiIiIAssnTpwILy8vbNu2Dd7e3gCAw4cP48WLF5gwYYJ4muddzs7OMDAwKLC8ffv28PX1xfXr1zFq1Cikp6fj5s2bMDMzw+rVq+XGpaWlhQcPHsiN9/1TuO9zc3MrsMzBwQF2dnYIDw9HYmIijI2NAQBpaWlYs2YNAgMD8ejRI6SkpMiMV1RUlNx9tGzZstD9P3v2DMuXL8fff/+NZ8+eFbjeqrA+3z9FDgDW1tYfXBcZGYlq1aoBAC5evAgACA4OxuXLlwtsExcXh7y8PISGhsrt8335/YWHh8PHx6fA+tDQUADAgwcP0KtXL5l1RY2RopQxRvmKe7ympKTgyZMnsLW1haOjY4H2nTt3BvD21Pf7GjduLHMKtSQUPTabNWtWYJmdnR2At9fo5vvvv/8AAN26dYNUWvQl4PnHQWJiotzjID4+HgDEf7MGBgbo06cPDh8+jKZNm2LAgAFo27YtWrZsCV1d3SL3RUQfxoKPlGbw4MGYMWMGtm7dim+//RYSiQSbNm0CAHz11Vdyt6latarc5ZaWlgCApKQkAG9/6AiCgPj4eCxatKhEceX3VZSi4oiIiEBSUhKMjY2RnZ2Njh074vLly2jQoAEGDx4Mc3Nz8brBRYsWyb15pKg4wsLC0KJFC7x+/RqfffYZunbtCiMjI6ipqSE8PBy+vr6F9mlkZFRgWf41WkWty87OFpe9fPkSAPDzzz/L3Ue+1NTUIte/39++fftK3F9xvquSUsYY5Svu8Zr/38LysbKykmknr6+SKs2xWdQ45Obmisvyr6m0sbH5YDz5x8GpU6eKvOHi3ePA398fy5cvx+7du7FgwQIAgLa2Njw8PLBixQqYm5t/cL9EJB8LPlIaHR0deHp6YuXKlTh16hQcHR1x8uRJtGrVCo0aNZK7TWxsrNzlMTExAP7vB1H+f5s2bSp3VqQoxXlQbWxsLJycnD4Yx8GDB3H58mWMGjWqwIN+o6OjiyxGC4tj5cqVePnyJbZv3w5PT0+ZdXv27IGvr+8H4y+N/NySkpJgaGiotP4OHjyIvn37lmjbT/2hwiU9XvOXvy86Olqm3bsUHYPSHJvFlT/LXdhM4bvyc1uzZg2mTp1arP51dHTg4+MDHx8fPH/+HP/88w927NiB33//HeHh4eJdykRUcnwsCynVhAkTxJm9LVu2IC8vD+PHjy+0/bVr15CSklJgeXBwMIC3BR4A6Ovro379+rh79y5evXql9Ljl/SAJCwvD8+fPUb16dfEH3ePHjwEAAwYMKFYfxVEWfZZEq1atAADnzp0r9jZqamoAZGd/StNfRVHc49XAwAA1a9ZEVFSUeAr7XWfOnAHw9hRxSRQ17h/jOMr/bk+dOlXkZR/vtlX0OLCzs8MXX3yBoKAg1K5dG//880+Z/NsnqixY8JFS1apVC126dMGhQ4ewefNmGBsbF3i0yruSkpKwePFimWVXrlzBrl27YGRkhP79+4vLv/nmG2RlZWH06NFyH9fx+vXrEs/+5VuzZo3MdYl5eXmYNWsW8vLy4OXlJS7PfwRG/g/sfGFhYXIf41EchfUZFBSErVu3KtRnSUyePBkaGhqYPn06Hj16VGB9VlZWgR/apqamAN4+cud97u7uqFmzJtatW4djx47J3efFixeRnp6uhOg/rpIcr6NHj4YgCJg1a5ZMgZaQkIDvv/9ebFMSRY17WRyb73NxcYGrqyuuXbuGFStWFFj/8uVLZGZmAnh7XeBnn32GP//8E9u2bZPb3+3btxEXFwfg7TV9ly5dKtAmLS0NKSkpUFNTk/tIGSIqHv7rIaWbMGECTp48iYSEBEydOhU6OjqFtm3Xrh22bt2KS5cuoU2bNuJzzfLy8rBp0yaZU4yjR4/G1atXsX79etSsWRPdunWDvb09Xr16hadPn+Kff/6Bl5cXNm7cWOKY27ZtiyZNmmDw4MEwMjJCUFAQbt68CRcXF8yePVts16dPH9SqVQurVq3CnTt30LRpUzx79gxHjhxBr1698OzZsxLve+LEidi+fTs8PDwwYMAA2NjY4M6dOzhx4gQ8PDzg7+9f4j5Lok6dOti2bRtGjx6N+vXro3v37nB0dER2djaePXuGc+fOwdzcXOaGmE6dOmHfvn34/PPP0aNHD+jo6KBatWoYMWIENDQ08Oeff6Jbt27o1asXXF1d0aRJE+jq6uL58+cICQlBWFgYoqOjK9zF+CU5XmfOnInjx4/j4MGDaNy4MXr27Ck+hy8uLg6zZ89G27ZtS7T/osa9LI5Nefz8/NC+fXvMnj0bAQEBcHNzgyAICA0NxcmTJ/HgwQOx+Ny9ezc6duyIMWPG4Ndff0XLli1hbGyMyMhI3Lp1C3fu3MHFixdhYWGBqKgotGrVCnXr1oWzszPs7OyQnJyMI0eOICYmBpMnT1bKJQdElVY53iFMnyD8/0euFKVatWpyH8uSLycnRzAzMxMACHfv3pXbJv8RFKNGjRLu378v9O3bVzA2NhZ0dHQEV1dX4cSJE4Xu//Dhw0KvXr0Ec3NzQUNDQ6hatarQvHlzYd68ecL9+/cL5OPm5lZoX/mP03jy5ImwYsUKwcnJSdDS0hKsra2FadOmyTyKJN+zZ8+EYcOGCdbW1oK2trZQr149Yfny5UJ2drbc/eU/+uLMmTOFxnHhwgWhQ4cOgrGxsaCvry+0adNGCAwMFM6cOSM+F/FdRT2eIz8ned9PUbHcunVLGDVqlGBvby9oamoKJiYmQv369YUvv/yywKNNcnJyBG9vb6FGjRqCurq63LxjY2OFOXPmCPXr1xd0dHQEPT09oVatWsKAAQOEnTt3yjybrjhjVJgPPZalqG2KO0aKHq8ZGRnCDz/8INSvX1/Q1tYWv9vdu3cXaPvuPgrzoXFX5rFZVDwJCQnC7NmzBUdHR0FLS0swMjISGjduLMydO1dIS0uTaZucnCz88MMPgrOzs6Cnpydoa2sL1atXF3r27Cls2rRJfFzT69evhUWLFgkdOnQQrK2tBU1NTcHS0lJwc3MTdu/ezUe1EJWSRBA+cCEGUQk9efIEtWvXRtu2bfHPP//IbRMeHo4aNWrIvcD8Y/L09ISvry+ePn1aqtd4kWr7VI5XIiJF8Ro+Urqff/4ZgiBg8uTJ5R0KERERgdfwkZJERERg586dCA0Nxc6dO9G0aVMMHDiwvMMiIiIisOAjJXn69Cnmz58PPT09dOvWDRs2bPjgk/iJiIjo4+A1fEREREQqjlMwRERERCqOBR8RERGRimPBR0RERKTiWPARERERqTjepUui169fIycnp7zDKBfm5uaIj48v7zDKVWUfA+ZfufMHOAbMv+Llr66uDhMTk+K1LeNYqALJyclBdnZ2eYfx0UkkEgBv86+sN61X9jFg/pU7f4BjwPxVP3+e0iUiIiJScSz4iIiIiFQcCz4iIiIiFceCj4iIiEjFseAjIiIiUnEs+IiIiIhUHAs+IiIiIhXHgo+IiIhIxbHgIyIiIlJxLPiIiIiIVBwLPiIiIiIVx4KPiIiISMWx4CMiIiJScSz4iIiIiFScenkHQJ+OaQee4kFManmHUU7ul3cAn4DKPgbMnyr7GKh+/kfG1CnvEMoNZ/iIiIiIVBwLPiIiIiIVx4KPiIiISMWx4CMiIiJScSz4iIiIiFQcCz4iIiIiFceCj4iIiEjFseAjIiIiUnEs+IiIiIhUHAs+IiIiIhXHgo+IiIhIxbHgIyIiIlJxLPiIiIiIVBwLPiIiIiIVx4KPiIiISMWx4CMiIiJScSz4iIiIqNLZsWMHWrVqBQcHB3Tr1g3nzp0rtO3ly5fh7u6O+vXro2bNmmjXrh02b95caPuDBw/CxsYGo0ePLovQFaJe3gEQERERfUwHDx6Ej48Pli5diubNm8PPzw89evTAmTNnYGNjU6C9rq4uvLy8ULduXejq6uLy5cuYM2cOdHV1MXz4cJm2kZGRWLx4MVq2bPmx0ikWzvBVEHfv3oWHhwfS0tLKOxQiIqIKbcuWLRgyZAiGDRuG2rVrY/HixbCzs8Pvv/8ut32DBg3Qr18/ODk5wc7ODgMGDED79u1x6dIlmXa5ubmYPHkyZs6cCXt7+4+RSrGx4CMiIqJKIysrC7du3YKbm5vM8q5du+LKlSvF6uPOnTu4cuUKWrduLbN81apVMDU1xdChQ5UWr7LwlO4nRBAEHDp0CKdOncLr169hbW2NAQMGwMHBAYsWLQIAeHl5AQDc3NwwadIk3LhxA3/88QeeP38OqVQKR0dHeHp6wtLSsjxTISIi+iS9evUKubm5MDMzk1letWpVxMXFFbmti4sLXr16hZycHHzzzTcYNmyYuC4kJAR79uzBqVOnyiTu0mLB9wnZu3cvLl++jLFjx8LKygr379/H2rVrMW/ePMyYMQO//PILVq9eDV1dXWhqagIAMjMz0bt3b9jb2+PNmzfw9/fHihUr8NNPP0EqlT+Bm52djezsbPGzRCKBjo7OR8mRiIiovEgkEkgkEgCAVCoV/y6RSCAIgsx6eQ4cOIC0tDRcu3YNS5cuRY0aNdC/f3+kpqZiypQpWLFiBUxNTcU+3/1veWPB94nIzMzEkSNHsHDhQjg6OgJ4+9vGgwcPcOrUKXTu3BkAYGRkBD09PXG7Vq1ayfQzYcIEjB07FpGRkYVePxAYGIj9+/eLn2vUqIHly5crOyUiIqJPipWVFUxNTaGmpoacnBxYWVmJ6+Li4mBjYyOzTN72ANCxY0dkZmZizZo1mDhxIm7cuIHnz59j1KhRYtu8vDwAgJ2dHR4+fIiaNWuWUVbFw4LvExEZGYns7Gx8//33MstzcnJQo0aNQreLiYmBv78/QkNDkZKSIh5gCQkJhRZ8/fv3R+/evcXPn8pvH0RERGUpOjoaANCoUSMcPHhQnDSRSCQ4deoUOnXqJLb5kOTkZKSnpyM6OhpGRkY4ffq0zPrly5cjLS0Nixcvhrq6erH7LQl1dXWYm5sXr63S904KEQQBAODt7Y0qVarIrFNXV0dsbKzc7ZYvXw4zMzOMHz8eJiYmEAQBM2bMQE5OTqH70tDQgIaGhvKCJyIiqgDyf9aOGzcO06ZNQ6NGjeDi4oJdu3bh2bNnGDlyJARBwLJlyxAdHY1ff/0VwNtn9llbW6NWrVoA3l6vt3HjRnh5eUEQBGhpacHJyUlmX4aGhgAgLs/fd3lhwfeJsLW1hYaGBhISElCvXr0C61++fAng/6aIASAlJQVRUVH48ssvUbduXQDAgwcPPk7AREREFZS7uztev36NVatWIS4uDk5OTjh27BhsbW0hCAJiY2Px4sULsX1eXh5+/PFHPHv2DOrq6qhWrRq8vb0xYsSIcsyiZFjwfSJ0dHTQp08f+Pr6Ii8vD3Xq1EFGRgYePnwIbW1tNGrUCBKJBFevXoWzszM0NTWhp6cHAwMD/PXXXzAxMUFCQgJ27dpV3qkQERF98jw9PeHp6Qng7SldKysr8bTr6tWrZdqOHj26xG/NeL+P8saC7xMyePBgGBoa4sCBA4iNjYWenp54B1CVKlUwaNAg7N69Gxs2bEC7du0wadIkTJs2Ddu3b8eMGTNgbW0NLy8v+Pj4lHcqRERE9AmRCOV9Upk+GcO2XMaDmNTyDoOIiKhMHBlTR+7yd2f4KlJZpKGhUeybNvimDSIiIiIVx4KPiIiISMWx4CMiIiJScSz4iIiIiFQcCz4iIiIiFceCj4iIiEjFseAjIiIiUnEs+IiIiIhUHAs+IiIiIhXHgo+IiIhIxbHgIyIiIlJxLPiIiIiIVBwLPiIiIiIVx4KPiIiISMWx4CMiIiJScSz4iIiIiFScenkHQJ+ONf1qIDs7u7zD+OgkEgmsrKwQHR0NQRDKO5xyUdnHgPlX7vwBjkFlz78y4AwfERERkYpjwUdERESk4ljwEREREak4FnxEREREKo4FHxEREZGKY8FHREREpOJY8BERERGpOBZ8RERERCqOBR8RERGRimPBR0RERKTiWPARERERqTi+S5dE0w48xYOY1PIOo5zcL+8APgGVfQyYP32cMTgyps5H2Q/RuzjDR0RERKTiWPARERERqTgWfEREREQqjgUfERERkYpjwUdERESk4ljwEREREak4FnxEREREKo4FHxEREZGKY8FHREREpOJY8BERERGpOBZ8RERERCqOBR8RERGRimPBR0RERKTiWPARERERqTgWfEREREQqjgUfERERkYpjwUdERFROduzYgVatWsHBwQHdu3fHpUuXCm177NgxDBkyBA0bNoSTkxP69OmD4OBgmTYDBw6EjY1NgT8jRowo40zoU8eC7yOJi4uDh4cHwsPDi71NcHAwPD09yywmIiIqPwcPHoSPjw+mTp2KoKAgtGjRAsOHD0dUVJTc9v/99x/atWuHnTt34vjx43B1dYWnpyfu3LkjttmyZQuuX78u/jl9+jTU1NTQu3fvj5UWfaLUyzsAIiKiymjLli0YMmQIhg0bBgBYvHgxzp49i99//x3e3t4F2i9evFjms7e3N06ePIlTp06hQYMGAAATExOZNgcPHoSOjg769OlTRllQRcEZPiIioo8sKysLt27dgpubm8xyNzc3XLlypVh95OXlITU1FcbGxoW22bt3L9zd3aGrq1uacEkFcIZPiW7cuIE//vgDz58/h1QqhaOjIzw9PWFpaVmg7d27d7Fo0SJ8++232LNnD168eIFq1arhq6++gr29fYF+fX19kZCQgDp16mDixInib3GPHz/Gnj17EB4ejpycHFSvXh2jRo2Cg4PDR8mZiIhK7tWrV8jNzYWZmZnMcjMzM8TFxRWrj02bNiE9Pb3Q2bvr16/jwYMHWLFiRanjpYqPM3xKlJmZid69e2PZsmVYsGABJBIJVqxYgby8vEK32blzJ0aMGIFly5bB0NAQy5cvR05Ojrj+zZs3OHz4MCZPnoxFixYhISEBO3fulNmnm5sbFi1ahB9++AFWVlZYtmwZMjIyCt1ndnY20tPTxT9FtSUiIuWSSCSQSCQAAKlUKn7OX/bu58L+HDhwAL/88gs2btwIc3NzuW327t2LOnXqwNnZuVh9FnffqvqnIuZfEpzhU6JWrVrJfJ4wYQLGjh2LyMhIaGtry91m0KBBaNSoEQBg8uTJ+Oqrr3D58mW4uroCAHJzczFu3DhxlrB79+7Yv3+/uH3+dRv5vvzyS3h5eeHevXtwcXGRu8/AwECZPmrUqIHly5eXMFsiIlKElZUVTE1NoaamhpycHFhZWYnrMjIyYGNjI7Psff7+/pg5cyb27duHXr16yW2Tnp6OQ4cOYfHixUX29T55Z6QqE1XOnwWfEsXExMDf3x+hoaFISUkRZ/YSEhJga2srdxtHR0fx7/r6+rC2tpa5Q0tLS0vmADQxMUFycrL4OSkpCf7+/rh79y4SExORl5eHrKwsJCQkFBpn//79Ze7YKulvCUREpLjo6GgAQKNGjXDw4EGZyYLjx4+jW7duYpv3BQYGYsaMGVi3bh2cnZ0Lbefv7483b96gc+fOhbZ5l0QigaWlJWJiYiAIggJZVWwVNX91dXWYm5sXr20Zx1KpLF++HGZmZhg/fjxMTEwgCAJmzJghc4q2ON4twNTU1Aqsf/dgXL9+PZKTkzFq1CiYm5tDQ0MD8+bNK3KfGhoa0NDQKFFMRESkHPn/Dx83bhymTZuGRo0awcXFBX5+foiKisKIESMgCAKWLVuG6Oho/PrrrwCAAwcOYNq0aVi0aBGcnZ0RGxsLANDW1oahoaHMPvbs2YNu3bqJP4tKEltFKniUTZXzZ8GnJCkpKYiKisKXX36JunXrAgAePHjwwe0ePXokXrSbmpqK6OhoWFtbF3u/9+/fx9ixY+Hs7Azg7WxiSkqKAhkQEdHH5O7ujtevX2PVqlWIi4uDk5MTdu7cKZ4Rio2NxYsXL8T2fn5+yMnJwbx58zBv3jxx+aBBg7B69Wrx85MnT3D58mXs2bPno+VCnz4WfEqip6cHAwMD/PXXXzAxMUFCQgJ27dr1we3++OMPGBgYwMjICHv37oWBgQFatGhR7P1aWlrin3/+gYODAzIyMuDn5wdNTc3SpEJERB+Jp6dnoQ/Yf7eIAyBz7XVRatasWejDm6ny4l26SiKVSjFt2jSEhYVhxowZ8PX1LdarbIYNG4YdO3bg22+/xevXrzF79myoqxe/Dp8wYQLS0tIwZ84c/Pbbb+jRoweMjIxKkwoRERGpGImgqierP3H5z+Hbvn079PT0yjscAMCwLZfxICa1vMMgIlJpR8bUKe8QCpBIJLCyskJ0dLTKXsNWlIqav4aGRrFv2uAMHxEREZGKY8FHREREpOJ400Y5qV+/PgICAso7DCIiIqoEOMNHREREpOJY8BERERGpOBZ8RERERCqOBR8RERGRimPBR0RERKTiWPARERERqTgWfEREREQqjgUfERERkYpjwUdERESk4ljwEREREak4hQq+rKws/PXXX4iMjFR2PERERESkZAoVfJqamti+fTuSk5OVHQ8RERERKZnCp3QtLCyQmJioxFCIiIiIqCyoK7phz549ceDAATRp0gS6urrKjInKyZp+NZCdnV3eYXx0EokEVlZWiI6OhiAI5R1OuajsY8D8K3f+AMeAVJ/CBd/z58+RkpKCSZMmoUGDBjAxMZFZL5FI4OXlVeoAiYiIiKh0FC74goKCxL9fvnxZbhsWfERERETlT+GCz9/fX5lxEBEREVEZ4XP4iIiIiFScwjN8+W7cuIF79+4hOTkZAwcOhJmZGR4/fgwLCwsYGhoqI0YiIiIiKgWFC743b97gp59+wp07d8RlXbt2hZmZGQ4fPgxTU1OMHDlSKUESERERkeIUPqW7Z88ehIWFYcaMGfD19ZVZ17hxY9y+fbvUwRERERFR6Sk8w/fff/9h8ODBaNGiBfLy8mTWmZmZISEhodTBEREREVHpKTzDl5ycDFtbW7nrJBIJsrKyFA6KiIiIiJRH4YKvSpUqePbsmdx1ERERsLCwUDgoIiIiIlIehQu+Fi1aIDAwEE+fPhWXSSQSxMfH4+jRo2jdurVSAiQiIiKi0pEICr40MCMjAwsXLsTz589hZ2eHiIgI2NvbIzY2FtbW1li8eDE0NTWVHS+VoWFbLuNBTGp5h0FE9Ek6MqZOeYdQZir7u4Qrav4aGhowNzcvVluFb9rQ0dHBkiVLcOzYMVy7dg2WlpbQ0tJCv3790KtXLxZ7RERERJ+IUj14WVNTE/369UO/fv2UFA4RERERKZvC1/BNnjwZ4eHhctc9e/YMkydPVrRrIiIiIlIihQu++Ph45OTkyF2XnZ2N+Ph4hYMiIiIiIuVRuOArSmxsLHR0dMqiayIiIiIqoRJdwxccHIyzZ8+Kn7du3VqgsMvKykJERATq1aunnAiJiIiIqFRKVPBlZWUhOTlZ/JyWlobs7GyZNhoaGnB1dYWHh4dyIiQiIiKiUilRwde1a1d07doVADBp0iTMmDED1atXL4u4iIiIiEhJFH4sy7p165QZBxERERGVkVI9hy87OxvBwcG4e/cuUlJSMHbsWFhZWSEkJAT29vaoWrWqsuIkIiIiIgUpXPAlJydj0aJFiIyMhLGxMRITE5GRkQEACAkJwc2bNzF27FilBUpEREREilH4sSx+fn5IT0/HsmXLsH79epl19evXx71790odHBERERGVnsIF37Vr1+Dh4QEHBwdIJBKZdaampnj58mWpgyMiIiKi0lO44MvIyIC5ubncdTk5OcjLy1M4KCIiIiJSHoULPgsLCzx69EjuusePH8Pa2lrhoIiIiIhIeRQu+Nq2bYuDBw8iJCQEgiAAACQSCR4/fozjx4/js88+U1qQRERERKQ4hQs+d3d3ODk5YcWKFRg3bhwA4IcffsC8efNQq1Yt9OzZU2lBEhERfUp27NiBVq1awcHBAd27d8elS5cKbXvs2DEMGTIEDRs2hJOTE/r06YPg4OAC7ZKSkjB37lw0bdoUDg4OcHNzw99//12GWVBlovBjWdTV1eHt7Y1///0X165dQ1JSEgwMDODi4gJXV1dIpQrXkuVq0qRJ6NmzJ3r16lXeoRAR0Sfo4MGD8PHxwdKlS9G8eXPs3LkTw4cPR3BwMGxsbAq0/++//9CuXTt8++23MDQ0hL+/Pzw9PXHkyBE0aNAAwNtXlw4dOhSmpqbYvHkzrKys8OLFC+jp6X3s9EhFSYT887GVTHBwMHbs2IEdO3bILE9OToaWlha0tLTKdP+fYmE5bMtlPIhJLe8wiIg+SUfG1AEA9O7dGw0aNMCPP/4ornNzc0P37t3h7e1drL46dOiAvn37Yvr06QCA33//HRs3bsTZs2ehoaGh/OA/QCKRwMrKCtHR0aiMZUFFzV9DQ6PQG2jfVzGn4cqQoaFhmRd7ypSTk1PeIRARVRpZWVm4desW3NzcZJa7ubnhypUrxeojLy8PqampMDY2FpedOnUKLi4umDdvHho3boyOHTvi119/RW5urjLDp0pM4VO6eXl5OH78OM6fP4/4+HhkZ2cXaOPr6/vBfnx8fGBvbw9NTU38/fffUFdXR5cuXeDh4fHBbdPT07Fz506EhIQgOzsbDg4OGDVqFKpXrw4ACA8Ph6+vL548eQKJRAJLS0t8+eWXyMzMFB8Wnb+fgQMHwsPDo8DMm4eHB8aNG4erV6/izp07MDc3x4QJE2BoaIiNGzfiyZMnsLe3x5QpU2BpaQkAiImJwe+//47Q0FBkZmbC1tYWQ4cORaNGjcSc4+Pj4evrK45RQEAAgLdT/wEBAYiJiYGJiQm6d++OPn36iDlPmjQJHTt2RExMDC5fvozmzZvjq6++gq+vLy5duoS0tDQYGxujc+fO6N+//wfHkIiIiu/Vq1fIzc2FmZmZzHIzMzPExcUVq49NmzYhPT1d5v/tERERuHDhAvr374+dO3fi6dOnmDt3LnJzc8VZQKLSULjg27VrF44cOYLq1aujUaNGUFdX/LW8Z8+eRe/evbF06VI8evQI69evR506dcQCSR5BELBs2TLo6+vD29sburq6OHXqFL7//nusWbMG+vr6WLt2LapXr46xY8dCKpUiPDwcampqcHJygqenJ/z9/bFmzRoAgLa2dqH7+uOPPzBy5EiMHDkSu3btwpo1a1C1alX069cPZmZm2LBhA7Zt24a5c+cCADIzM9G0aVMMGTIEGhoaOHv2LJYvX441a9bAzMwMM2fOxKxZs9CpUyd07txZ3E9YWBhWrVqFQYMGwdXVFY8ePcLWrVthYGCA9u3bi+0OHTqEAQMGYMCAAQDeXhB85coVTJ8+HWZmZnj58iUSEhIKzSc7O1umQJdIJNDR0Sn6SyIiquQkEon4ogGpVFrgpQPvri9MYGAgfvnlF2zfvl3mVJwgCDA1NcXPP/8MNTU1NG7cGLGxsdiwYQO++eYb5Sfznvy4PxS/qqoM+StcpZ0/fx7u7u4YNmxYqYOoVq0aBg0aBACwsrLCiRMncPv27SILvrt37+LZs2fYunWreL3DyJEjERISgv/++w+dO3dGQkIC+vTpI15Ea2VlJW6vq6sLiUQiM6VemPbt28PV1RXA27uTv/vuOwwYMABNmjQBAPTs2VPm9XLVq1cXZxkBYMiQIbh8+TKuXLmC7t27Q19fH1KpFDo6OjL7P3LkCBo2bIiBAwcCAKytrREZGYlDhw7JFHwNGjRA3759xc8JCQmwsrJCnTp1IJFIPng+PzAwEPv37xc/16hRA8uXL//gOBARVWZWVlYwNTWFmpoacnJyZH6mZGRkwMbGRmbZ+/z9/TFz5kzs27evwPXbtra20NDQgK2trbisZcuWWLRoEUxNTaGpqan8hOTIP1NVWaly/goXfFlZWUUWZCVhb28v89nExARJSUlFbhMWFobMzEyMHj26QFwxMTEAgF69emHTpk04d+4cGjZsiFatWin0ZVarVk38e36B9m7MRkZGyM7ORnp6OnR1dZGZmYn9+/fj6tWreP36NXJzc5GVlVXkrBsAREVFoVmzZjLLnJyccPToUeTl5Yl3PtesWVOmTfv27bFkyRJ8/fXXaNy4MVxcXNC4ceNC99O/f3/07t1b/KzKv9EQESlLdHQ0AKBRo0Y4ePAgWrVqJa47fvw4unXrJrZ5X2BgIGbMmIF169bB2dm5QLvGjRsjMDAQUVFR4v/rr1y5gqpVq36UV5XmX/YUExNToW5aUJaKmr+6unqxb9pQuOBr1KgRQkNDxVvKS0Pe6eAPDXheXh5MTEzg4+NTYJ2uri6At9fftW3bFteuXcONGzcQEBCAr7/+Gi1atChRfGpqakXGnF8w5cfs5+eHmzdvYsSIEbC0tISmpiZ++eWXD95gIQhCgeJL3ji8f1OJg4MDfvvtN9y4cQO3bt3CqlWr0LBhQ8yYMUPufjQ0NMrlLjAiooos///H48aNw7Rp09CoUSO4uLjAz88PUVFRGDFihHi5UXR0NH799VcAwIEDBzBt2jQsWrQIzs7OiI2NBfD2UiJDQ0MAwIgRI7Bt2zbMnz8fXl5eePr0KX799VeMHj36oxYggiBUqIJH2VQ5f4ULPi8vL/z444/Q0tKCs7Mz9PX1C7SRt0xZHBwckJiYCKlUCgsLi0LbWVtbw9raGr1798bq1atx5swZtGjRAurq6mX2vt/79+/Dzc1NLCwzMzMRHx8v00be/m1tbfHgwQOZZY8ePYK1tfUHn2uoq6sLV1dXuLq6olWrVli6dClSU1PL9DsgIqqM3N3d8fr1a6xatQpxcXFwcnLCzp07xdOxsbGxePHihdjez88POTk5mDdvHubNmycuHzRoEFavXg0AsLGxwe7du+Hj44MuXbrA0tISY8aMwaRJkz5qbqS6FC74dHV1YW1tLXOn6fv8/f0VDuxDGjZsCEdHR/z888/44osvYG1tjdevX+P69eto3rw57OzssHPnTrRq1QoWFhZ4+fIlnjx5gpYtWwIAzM3NkZmZidu3b6NatWpKffaepaUlLl++LJ6e9ff3L/Abg7m5Oe7fv482bdpAXV0dhoaG6N27N7y9vbF//37xpo0TJ05g7NixRe7vyJEjMDExQfXq1SGRSPDff//B2NhYnOkkIiLl8vT0hKenp9x1+UVcvnevmS5Ks2bNcOTIkVJGRiSfwgXf5s2bcfHiRTRv3hw2NjaluktXERKJBN7e3tizZw82bNiA5ORkGBsbo27dujAyMoJUKkVKSgp+++038S0gLVu2FB/D4uTkhC5dumD16tVISUkRH8uiDKNGjcKGDRvw3XffwcDAAO7u7sjIyJBp4+HhgS1btmDKlCnIzs5GQEAAHBwcMH36dAQEBOCPP/6AiYkJPDw8ZG7YkEdbWxsHDx5EdHQ0pFIpatWqBW9v7wr7thMiIiJSLoXftDFq1CgMGDBA5m5Rqtj4pg0iosLlv2lDFVXUN00oS0XN/6O8aUNdXR01atRQdHMiIiIi+kgUPg/bokUL3Lx5Ew0bNlRmPKJz585h8+bNcteZm5tj5cqVZbJfIiIiIlWjcMHXpk0bbNq0CTk5OYXepevg4KBwYM2aNUPt2rXlrpP3mBQiIiIikk/hgu/7778H8PZhk8ePH5fbpjR36ero6PB1X0RERERKoHDBN2HCBGXGQURERERlROGC70OPCiEiIiKiTwMf1EZERESk4kr1tOTU1FScP38ekZGRyMrKklknkUh42peIiIjoE6BwwZeQkABvb2+8efMGb968gaGhIVJTU5GXlwc9PT2+1ouIiIjoE6HwKd1du3bB1tYWW7ZsAQB4e3tj586d8PLygoaGBr799lulBUlEREREilO44Hv06BG6du0KDQ0NcZm6ujq6d++Ojh07ws/PTykBEhEREVHpKFzwJSUlwcTEBFKpFFKpFOnp6eK6evXq4cGDB0oJkIiIiIhKR+GCz8jICKmpqQDevuosLCxMXBcfH8+3YRARERF9IhS+aaN27dp4+vQpmjVrhhYtWmD//v3Izs6Guro6Dh06hPr16yszTiIiIiJSkMIFX9++fREXFwcAGDhwIKKiohAQEAAAqFu3Lry8vJQTIRERERGVisIFn4ODAxwcHAAA2tramDNnDtLT0yGRSPgOXCIiIqJPiEIFX1ZWFqZMmYJx48ahWbNm4nI+e69iW9OvBrKzs8s7jI9OIpHAysoK0dHREAShvMMpF5V9DJh/5c4f4BiQ6lPopg1NTU1kZWVBW1tb2fEQERERkZIpfJduw4YNcevWLWXGQkRERERlQOFr+Pr3749ffvkFmpqaaNGiBUxMTCCRSGTa6OvrlzpAIiIiIiodhQu+/Fen7du3D/v27ZPbxt/fX9HuiYiIiEhJFC74BgwYUGBGj4iIiIg+PQoXfB4eHsqMg4iIiIjKiMI3bRARERFRxaDwDB8A5OXl4fr164iKikJWVlaB9QMHDixN90RERESkBAoXfCkpKViwYAFevHhRaBsWfERERETlT+FTunv27IGmpibWrVsHAPjhhx+wZs0a9O7dG9bW1tiwYYPSgiQiIiIixSlc8N25cwe9evVClSpV3nYklcLS0hIjRoxAw4YN8fvvvystSCIiIiJSnMKndF++fAkLCwtIpVJIJBJkZmaK61xcXPDrr78qJUD6eKYdeIoHManlHUY5uV/eAXwCPq0xODKmTnmHQESkMhSe4TM0NER6ejoAwMTEBM+fPxfXpaamIjc3t/TREREREVGpKTzDV6NGDTx//hzOzs5o2rQp9u/fDx0dHairq2PPnj2oXbu2MuMkIiIiIgUpXPB1794dsbGxAIAhQ4YgNDRUvIGjatWq8PLyUk6ERERERFQqChd8jRo1Ev9uaGiIn376STyta2NjAzU1tdJHR0RERESlVqoHL79LIpHA3t5eWd0RERERkZKUquBLT09HUFAQ7t69i5SUFBgYGKB+/fro2rUr9PT0lBUjEREREZWCwgVfXFwcFi1ahISEBJiZmcHY2BjR0dG4ffs2Tp06hYULF6Jq1arKjJWIiIiIFKBwwbd9+3ZkZWXh+++/h6Ojo7j84cOHWLFiBXbs2IE5c+YoJUgiIiIiUlyp3rQxdOhQmWIPAJycnDBkyBDcuXOn1MERERERUekpXPBpaGjA1NRU7jozMzNoaGgoHBQRERERKY/CBV+zZs1w8eJFuesuXrwIZ2dnhYMiIiIiIuVR+Bq+tm3bYuPGjVi5ciXatm0LY2NjJCYm4ty5cwgLC8NXX32FsLAwsb2Dg4NSAiYiIiKiklG44Pvhhx8AAC9fvsSlS5cKrF+yZInMZ39/f0V3RURERESloHDBN2HCBGXGQURERERlRKGCLy8vD46OjjAyMuIDlomIiIg+cQrdtCEIAr755hs8evRI2fEQERERkZIpVPCpqanB2NgYgiAoOx4iogJ27NiBVq1awcHBAd27d5d73XC+2NhYTJo0CZ999hlsbW2xYMGCAm2OHTuGHj16oG7duqhVqxY6d+6MnTt3lmUKRETlSuHHsri6uuLs2bPKjOWT4ePjgx07dnyS+5g0aRKOHj2q/ICIPlEHDx6Ej48Ppk6diqCgILRo0QLDhw9HVFSU3PZZWVkwNTXF1KlTUa9ePbltjI2NMXXqVBw6dAh//fUXhgwZAi8vLwQHB5dhJkRE5UfhmzaqV6+OixcvYtGiRWjZsiWMjY0hkUhk2rRs2bLUARJR5bZlyxYMGTIEw4YNAwAsXrwYZ8+exe+//w5vb+8C7e3s7LB48WIAhT8dwNXVVebz2LFjERgYiMuXL8PNzU3JGRARlT+FC75169YBAF69eoV79+7JbcNHsRBRaWRlZeHWrVuYNGmSzHI3NzdcuXJFKfsQBAHnz5/Hw4cPMXv2bKX0SUT0qVG44Fu4cKEy4/hk/fPPPzh27BhevHgBLS0tNGjQAJ6enjAyMgIA3L17F4sWLcLcuXOxe/duREVFwdHREV9//TXCwsLw+++/49WrV2jatCkmTJgALS0tse/c3Fz873//w7lz5yCVStG1a1cMHjxYnClNSkrChg0bcPv2bRgbG2PIkCEF4jty5AjOnDmDuLg46Ovrw8XFBcOHD4e2tvbHGSCiMvTq1Svk5ubCzMxMZrmZmRni4uJK1XdycjJcXFyQlZUFNTU1rF+/Hm5ubrw2mYhUksIFX2HXxqianJwcDB48GNbW1khKSoKvry/Wr19f4FTSvn37MHr0aGhpaWHVqlVYtWoVNDQ0MHXqVGRmZmLFihU4fvw4+vXrJ25z9uxZdOzYEUuXLsWTJ0+wefNmmJmZoXPnzgCA9evXIyEhAQsXLoS6ujq2b9+OpKQkmf1KJBJ4eXnBwsICcXFx2Lp1K/z8/DB27NhCc8rOzkZ2drZMHzo6OkoYLSLlkUgk4i8/Uqm0wCUj764vbj/vMjAwwKlTp5CWloYLFy7gm2++gbGxMVq3bq2cBCqQ/PEpzniqqso+Bsxf9fNXuODLl56ejkePHiElJQVNmzaFvr6+MuL6ZHTs2FH8e9WqVeHl5YW5c+ciMzNTZhZtyJAhqFOnjrjN7t27sXbtWlStWhXA2+sZ7969K1PwmZqaYtSoUZBIJLC2tsazZ89w9OhRdO7cGS9evMD169fxww8/oHbt2gCAr776CtOnT5eJr1evXuLfLSwsMHjwYGzdurXIgi8wMBD79+8XP9eoUQPLly9XYHSIyo6VlRVMTU2hpqaGnJwcWFlZiesyMjJgY2Mjs0weTU1N6OnpFdrOxsYGANClSxdERkZi06ZN+Pzzz5WXRAVjaWlZ3iGUu8o+BsxfdfMvVcG3f/9+HDx4EFlZWQCAZcuWQV9fH4sXL0ajRo1kipuK6unTp9i3bx/Cw8ORmpoqnu5JSEiAra2t2K5atWri342MjKClpSUWe8DbuwKfPHki03ft2rVlfptwdHTEkSNHkJeXh6ioKKipqaFmzZriehsbmwIPur5z5w4CAwMRGRmJjIwM5ObmIjs7u0BB+q7+/fujd+/e4mdV/o2GKq7o6GgAQKNGjXDw4EG0atVKXHf8+HF069ZNbFOYrKwspKWlfbCdRCKBIAhISUn5YFtVJJFIYGlpiZiYmEp7SruyjwHzr5j5q6urw9zcvHhtFd1JUFAQ9u/fj65du6Jp06b48ccfxXXOzs64fPlyhS/4MjMzsWTJEjRu3BhTpkyBoaEhEhIS8MMPPyAnJ0emrZqamvh3iUQi8zlfXl5esfddnAMuPj4ey5YtQ5cuXTB48GDo6+vjwYMH2LhxI3JzcwvdTkNDAxoaGsWOhag85P8bGDduHKZNm4ZGjRrBxcUFfn5+iIqKwogRIyAIApYtW4bo6Gj8+uuv4rZ37twBAKSlpeHly5e4ffs2NDU14ejoCABYu3YtGjdujGrVqiE7OxunT5/G77//jmXLllWo/9krmyAIlTp/gGPA/FU3f4ULvhMnTqB3794YPnx4gULGyspKJX5LfvHiBVJSUjBs2DDxovH3Z+lKIzQ0tMBnS0tLSKVS2NraIjc3F2FhYahVq5YYT1pamtj+yZMnyMvLw8iRIyGVvn2k4sWLF5UWH9GnwN3dHa9fv8aqVasQFxcHJycn7Ny5U5xhj42NxYsXL2S26datm/j3W7duITAwELa2tuIDm9PT0+Ht7Y2YmBhoa2ujZs2a8PPzQ7t27VT2f/ZEVLkpXPDFxcWhcePGctfp6OggPT1d4aA+FWZmZlBXV8eJEyfQpUsXPH/+HH/88YfS+n/58iV8fX3RpUsXhIWF4fjx4xg5ciQAwNraGk2aNMGmTZvw5ZdfQk1NDTt27ICmpqa4vaWlJXJzc3HixAm4uLjg4cOHOHXqlNLiI/pUeHp6wtPTU+661atXF1hW2EOZ882ZMwdz5swRP0skEpX5RZWISB6F37Shq6tb4I7RfHFxcTA0NFQ4qE+FoaEhJk6ciIsXL+Kbb77BgQMHMGLECKX1365dO2RlZcHb2xv/+9//0KNHD/EOXQCYOHEiTE1N4ePjgxUrVqBz587i42CAtw+/HjlyJA4ePIgZM2bg3Llz4sNpiYiIiPJJBAXPX6xZswaRkZH4/vvvoampiaFDh+LHH3+Evb09FixYADs7O3z11VfKjpfK0LAtl/EgJrW8wyACABwZU+ej7evdGb7KeEq3sucPcAyYf8XMX0NDo+xv2hg8eDC8vb3xzTffoEWLFgDeXtcXHh6OhISEAo8PISIiIqLyofApXUtLS3z//fewsbFBUFAQgLdvpTAwMMCiRYsKPBmfiIiIiMpHqZ7DZ2tri3nz5iE7OxspKSnQ19eXuamAiIiIiMqfwjN871JXV4eOjg6f7UZERET0CSrVDF9oaCgCAgJw79495OTkQF1dHfXq1cOgQYPEB5wSERERUflSeIbvzp07WLhwIcLCwtCmTRu4u7ujTZs2CAsLg4+PD27fvq3MOImIiIhIQQrP8O3atQs1atTA/PnzZd7ZmpGRgcWLF2P37t1YtmyZUoIkIiIiIsUpPMP37Nkz9O3bV6bYA96+ZcPd3R3Pnj0rdXBEREREVHoKF3xGRkaQSCTyO5VKVeJNG0RERESqQOGCr3Pnzjh69ChycnJklufk5ODo0aMyrwgjIiIiovKj8DV86urqiI+Px5QpU9CiRQsYGxsjMTERly9fhlQqhYaGBo4cOSK27927t1ICJiIiIqKSKdVNG/lOnDhR5HqABR8RERFReVG44Pvtt9+UGQcRERERlRGFCz5zc3NlxkFEREREZUThmzZ+/PFH3LhxQ4mhEBEREVFZUHiGLyoqCsuWLYOlpSW6deuG9u3bQ1dXV5mxEREREZESSARBEBTd+Nq1awgKCsKNGzegpaWFtm3bonv37rC3t1dmjPSRxMfHIzs7u7zD+OgkEgmsrKwQHR2NUvxzqNAq+xgw/8qdP8AxYP4VM38NDY1iX2Kn8AwfADg7O8PZ2RkxMTEICgpCcHAw/v77b9StWxfdu3dHixYtIJUqfNaYiIiIiJSgVAVfPktLS4waNQoDBgzAypUrcffuXdy/fx9VqlRB37590b1790LfykFEREREZUspBd/Lly9x6tQp/P3330hOTkaTJk3g6uqKkJAQ7NixAy9evMCYMWOUsSsiIiIiKqFSFXx37tzBiRMncPXqVWhqasLNzQ09evSAlZUVAMDNzQ3Hjh3Dvn37WPARERERlROFC77p06fjxYsXsLCwwPDhw9GhQwe5d+nWqlUL6enppQqSiIiIiBSncMFXpUoVfPHFF3BxcSny+jwHBwe+lYOIiIioHClc8M2fP794O1BX51s5iIiIiMpRiQq+yZMnF7utRCLB2rVrSxwQERERESlXiQo+W1vbAsuuX7+OOnXqQEdHR2lBEREREZHylKjg+/bbb2U+5+bmYtiwYRg1ahQcHByUGhgRERERKUepXoPBhykTERERffr43jMiIiIiFceCj4iIiEjFseAjIiIiUnElumkjLCxM5nNeXh4A4MWLF3Lb80YOIiIiovJXooLP29tb7vLCnrfn7+9f8oiIiIiISKlKVPBNmDChrOIgIiIiojJSooKvffv2ZRQGEREREZUV3rRBREREpOJY8BERERGpOBZ8RERERCqOBR8RERGRimPBR0RERKTiWPARERERqTgWfEREREQqjgUfERERkYpjwUdERESk4ljwEREREak4FnyksMTEREyZMgV16tRBnTp1MGXKFCQlJRW5jSAI+OWXX+Ds7IyaNWti4MCBePjwoUwbPz8/DBw4EE5OTrCxsflgn0RERFQ0FnyfkODgYHh6ehbZJiAgALNmzfo4AcmRmJiItLQ0AMDkyZNx7949+Pn5wc/PD/fu3cPUqVOL3H79+vXYvHkzlixZgqNHj8Lc3BxDhw5Famqq2CYjIwPt27fHlClTyjQXIiKiykK9vAOgkunbty969OjxUfeZk5OD4OBg7Nu3D6dOncLhw4ehqamJM2fO4PDhw3B2dgYA/PTTT+jbty8eP36MWrVqFehHEARs3boVU6dORc+ePQEAq1evRpMmTRAYGIgRI0YAAMaNGwcA+Pfffz9ShkRERKqNM3wVjLa2NgwMDD7Kvu7fv4/FixejWbNmmDZtGkxMTBAQEID69evj6tWrMDQ0FIs9AHBxcYGhoSGuXr0qt79nz54hLi4Obm5u4jItLS20atUKV65cKfN8iIiIKqtKPcPn4+MDe3t7SKVSnD17Furq6hg8eDDatm2Lbdu24b///oORkRFGjx6Npk2bIi8vD5s2bcKdO3eQmJgIMzMzdOvWTZytysrKwrfffgsnJyeMHz8eABAXF4dZs2ZhxIgR6Ny5c7Hiunz5Mnbt2oWEhATUqVMHEyZMgJmZGYC3p3RDQkLw888/AwDWrVuHtLQ01KlTB0eOHEFOTg5cXV3h6ekJdfWSf72vXr1CYGAgAgIC8OjRI3To0AFLly5F586doampKbaLi4uDqalpge1NTU0RFxcnt+/85fm55DM3N0dkZGSJYyUiIqLiqdQFHwCcPXsWffv2xdKlS/Hvv/9iy5YtCAkJQfPmzdG/f38cPXoUv/32G9avXw81NTWYmppi+vTpMDQ0xMOHD7F582YYGxvD1dUVmpqamDp1KubOnYumTZuiWbNmWLt2LerXr1/sYu/NmzcIDAzEpEmToK6ujq1bt2LNmjX4/vvvC93m7t27MDExwcKFCxETE4PVq1ejevXqhe4zOzsb2dnZ4meJRAIdHR1IJBJs374dK1euRMuWLXHhwgXY2NjI7UMikYh/ClsnbzkASKVSmfWCIMjdJv9zYf0py7v7qawq+xgw/8qdP8AxYP6qn3+lL/iqVauGAQMGAAD69++PAwcOwMDAQCyWBg4ciJMnTyIiIgKOjo7w8PAQt7WwsMDDhw9x8eJFuLq6AgCqV6+OIUOGiDOBsbGxJbrJIjc3F6NHj0bt2rUBAJMmTcL06dMLvS4OAPT19TFmzBhIpVLY2NigadOmuHPnTqEFX2BgIPbv3y9+rlGjBpYvXw4zMzPMmDEDVapUga+vLzp06IABAwZgxIgR6NChA6TS/7sCoHbt2nj58iWsrKxk+n716hVq165dYDkANGjQAMDbAu/d9ampqbC3ty+wTf4MoqWlJYyNjQsbMqWxtLQs83186ir7GDD/yp0/wDFg/qqbf6Uv+Ozt7cW/S6VSGBgYyCwzMjICACQnJwMATp48idOnTyM+Ph5ZWVnIyclB9erVZfrs3bs3QkJCcOLECcydOxeGhobFjkdNTQ01a9YUP9vY2EBPTw+RkZGFFny2trYyxZiJiQmePXtW6D769++P3r17i5/zf6NJSEiARCLB6NGjMXr0aISEhGDfvn34/PPPoaenh88//1x8XEqtWrWQlJSEY8eOoWnTpgCAa9euISkpCbVq1UJ0dHSB/Wpra8PCwgJ//PGH+I8qKysLwcHBmDdvXoFtXr58CQCIiYlBRkZGkeNWGhKJBJaWloiJiYEgCGW2n09ZZR8D5l+58wc4Bsy/Yuavrq4Oc3Pz4rUt41g+ee9f5yaRSKCmpibzGQDy8vLw77//wtfXFyNHjoSjoyN0dHRw6NAhhIaGyvSRnJyMFy9eQCqVIjo6Gk2aNCl1nEVNM78bb37bog5YDQ0NaGhoFFguCILMds2aNUOzZs2waNEiBAUFYd++fejcuTOCgoJQt25ddOjQATNnzsTy5csBAHPmzEHnzp1Rs2ZNsZ927drB29tbvLN47NixWLt2LWrUqIEaNWpg7dq10NHRQb9+/cRt4uLiEBcXh6dPnwJ4e/OInp4ebGxsYGJiUpzhUsj7+VdGlX0MmH/lzh/gGDB/1c2/0hd8JfHgwQM4OTmhW7du4rLY2NgC7TZs2AB7e3t06tQJGzZsQMOGDWFra1usfeTm5iIsLEyczXvx4gXS0tIKvZbuY9DW1oa7uzvc3d0RExMDPT09AMDatWuxYMECDBs2DADQtWtXLFmyRGbbJ0+eiLOjADBx4kRkZmZi7ty5SEpKQtOmTbF7927o6+uLbXbu3ImVK1eKnz///HMAwMqVKzF48OAyy5OIiEhVseArAUtLS5w9exY3btyAhYUF/vnnHzx+/BgWFhZimxMnTuDRo0f4+eefYWZmhuvXr+PXX3/F0qVLi3XXrJqaGrZt2wYvLy/x77Vr1y70dO7H9u71DSYmJli7dm2R7aOiomQ+SyQSzJgxAzNmzCh0mw+tJyIiopLhc/hKoEuXLmjZsiVWr16NefPmITU1VWa2LyoqCn5+fhgzZoz46JExY8YgLS0Ne/fuLdY+tLS04O7ujl9//RXfffcdNDU18fXXX5dFOkRERFRJSARVPVlNJRYfHy/zuJbKQiKRwMrKCtHR0Sp77caHVPYxYP6VO3+AY8D8K2b+Ghoaxb5pgzN8RERERCqO1/B9REuXLsX9+/flruvfv794cwIRERGRMrHg+4i++uorZGVlyV337l2qRERERMrEgu8jqlKlSnmHQERERJUQr+EjIiIiUnEs+IiIiIhUHAs+IiIiIhXHgo+IiIhIxbHgIyIiIlJxLPiIiIiIVBwLPiIiIiIVx4KPiIiISMWx4CMiIiJScSz4iIiIiFQcCz4iIiIiFceCj4iIiEjFseAjIiIiUnEs+IiIiIhUHAs+IiIiIhXHgo+IiIhIxbHgIyIiIlJxLPiIiIiIVBwLPiIiIiIVx4KPiIiISMWx4CMiIiJScSz4iIiIiFQcCz4iIiIiFceCj4iIiEjFseAjIiIiUnEs+IiIiIhUHAs+IiIiIhXHgo+IiIhIxbHgIyIiIlJxLPiIiIiIVBwLPiIiIiIVx4KPiIiISMWx4CMiIiJScSz4iIiIiFQcCz4iIiIiFceCj4iIiEjFseAjIiIiUnEs+IiIiIhUHAs+IiIiIhXHgo+IiIhIxbHgIyIiIlJxLPhIYYmJiZgyZQrq1KmDOnXqYMqUKUhKSipyG0EQ8Msvv8DZ2Rk1a9bEwIED8fDhQ5k2fn5+GDhwIJycnGBjY/PBPomIiKhoLPjKQHBwMDw9PT/KvtatW4effvrpo+wLeFvkpaWlAQAmT56Me/fuwc/PD35+frh37x6mTp1a5Pbr16/H5s2bsWTJEhw9ehTm5uYYOnQoUlNTxTYZGRlo3749pkyZUqa5EBERVRYs+CqIuLg4eHh4IDw8/KPvOycnB3/99RfGjx8PZ2dnhIeHIzQ0FGfOnMHPP/+MZs2aoVmzZvjpp5/w119/4fHjx3L7EQQBW7duxdSpU9GzZ0/UqVMHq1evRkZGBgIDA8V248aNw+TJk+Hs7PyxUiQiIlJpLPioUPfv38fixYvRrFkzTJs2DSYmJggICED9+vVx9epVGBoayhRlLi4uMDQ0xNWrV+X29+zZM8TFxcHNzU1cpqWlhVatWuHKlStlng8REVFlpV7eAZSUj48P7O3tIZVKcfbsWairq2Pw4MFo27Yttm3bhv/++w9GRkYYPXo0mjZtiry8PGzatAl37txBYmIizMzM0K1bN/Ts2RMAkJWVhW+//RZOTk4YP348gLezabNmzcKIESPQuXPnD8YUHBwMf39/pKSkoHHjxqhTp06BNleuXMG+ffsQGRkJExMTuLm54fPPP4eamhoAwMPDA2PHjsWVK1dw9+5dGBsbY/jw4WjdujWAt6dPAWD27NkAgHr16sHHx0fs/9ChQzhy5AhycnLg6uoKT09PqKuX/Ot99eoVAgMDERAQgEePHqFDhw5YunQpOnfuDE1NTbFdXFwcTE1NC2xvamqKuLg4uX3nLzczM5NZbm5ujsjIyBLHSkRERMVT4Qo+ADh79iz69u2LpUuX4t9//8WWLVsQEhKC5s2bo3///jh69Ch+++03rF+/HmpqajA1NcX06dNhaGiIhw8fYvPmzTA2Noarqys0NTUxdepUzJ07F02bNkWzZs2wdu1a1K9fv1jFXmhoKDZs2IChQ4eiRYsWuHHjBvbt2yfT5saNG1i7di28vLxQt25dxMbGYtOmTQCAQYMGie38/f0xbNgweHp64p9//sGaNWtgZ2cHW1tbLF26FHPnzsX8+fNhZ2cnU8zdvXsXJiYmWLhwIWJiYrB69WpUr1690Pizs7ORnZ0tfpZIJNDR0YFEIsH27duxcuVKtGzZEhcuXICNjY3cPiQSifinsHXylgOAVCqVWS8Igtxt8j8X1p+yvLufyqqyjwHzr9z5AxwD5q/6+VfIgq9atWoYMGAAAKB///44cOAADAwMxAJn4MCBOHnyJCIiIuDo6AgPDw9xWwsLCzx8+BAXL16Eq6srAKB69eoYMmSIOBMYGxuLWbNmFSuWY8eOoXHjxujXrx8AwNraGo8ePcKNGzfENoGBgejXrx/at28PAKhatSoGDx6MXbt2yRR8rVq1QqdOnQAAQ4YMwe3bt3HixAmMHTsWhoaGAAADAwMYGxvLxKCvr48xY8ZAKpXCxsYGTZs2xZ07dwot+AIDA7F//37xc40aNbB8+XKYmZlhxowZqFKlCnx9fdGhQwcMGDAAI0aMQIcOHSCV/t8VALVr18bLly9hZWUl0/erV69Qu3btAssBoEGDBgDeFnjvrk9NTYW9vX2BbfJnEC0tLQvkXBYsLS3LfB+fuso+Bsy/cucPcAyYv+rmXyELPnt7e/HvUqkUBgYGMsuMjIwAAMnJyQCAkydP4vTp04iPj0dWVhZycnJQvXp1mT579+6NkJAQnDhxAnPnzhULrA+JiopCixYtZJY5OjrKFHxhYWF4/Pgx/vzzT3FZXl4esrOz8ebNG2hpaYnbvat27dqIiIj4YAy2trYyxZiJiQmePXtWaPv+/fujd+/e4uf832gSEhIgkUgwevRojB49GiEhIdi3bx8+//xz6Onp4fPPPxcfl1KrVi0kJSXh2LFjaNq0KQDg2rVrSEpKQq1atRAdHV1gv9ra2rCwsMAff/wh/qPKyspCcHAw5s2bV2Cbly9fAgBiYmKQkZHxwXFQlEQigaWlJWJiYiAIQpnt51NW2ceA+Vfu/AGOAfOvmPmrq6vD3Ny8eG3LOJYy8f61aRKJRLwWLv8z8Lao+vfff+Hr64uRI0fC0dEROjo6OHToEEJDQ2X6SE5OxosXLyCVShEdHY0mTZoUK5biHBh5eXnw8PBAy5YtC6zT0NAo1n6K8m7uwNv8i4pLQ0ND7n4FQZDZLv/u20WLFiEoKAj79u1D586dERQUhLp166JDhw6YOXMmli9fDgCYM2cOOnfujJo1a4r9tGvXDt7e3ujRowcAYOzYsVi7di1q1KiBGjVqYO3atdDR0UG/fv3EbeLi4hAXF4enT58CeHvziJ6eHmxsbGBiYlKKkSra+/lXRpV9DJh/5c4f4Bgwf9XNv0IWfCXx4MEDODk5oVu3buKy2NjYAu02bNgAe3t7dOrUCRs2bEDDhg1ha2v7wf5tbW0LFI+PHj2S+ezg4IAXL158cKo4NDRU5g7W0NBQ1KhRA8D/Fbl5eXkfjEnZtLW14e7uDnd3d8TExEBPTw8AsHbtWixYsADDhg0DAHTt2hVLliyR2fbJkyfiTCsATJw4EZmZmZg7dy6SkpLQtGlT7N69G/r6+mKbnTt3YuXKleLnzz//HACwcuVKDB48uMzyJCIiUlUqX/BZWlri7NmzuHHjBiwsLPDPP//g8ePHsLCwENucOHECjx49ws8//wwzMzNcv34dv/76K5YuXfrBO1179OiB+fPn4+DBg2jevDlu3bqFmzdvyrQZMGAAli9fDlNTU7Ru3RoSiQTPnj3Ds2fPMGTIELHdxYsX4eDggDp16uD8+fN4/PgxJkyYAODtaWpNTU3cuHEDVapUgaamJnR1dZU4UsXzbtFqYmKCtWvXFtk+KipK5rNEIsGMGTMwY8aMQrf50HoiIiIqGZV/Dl+XLl3QsmVLrF69GvPmzUNqaqrMbF9UVBT8/PwwZswY8XEhY8aMQVpaGvbu3fvB/h0dHTF+/HicOHECs2fPxs2bN8UZqXxNmjTBnDlzcPv2bXh7e2PevHk4cuRIgceTeHh44N9//8WsWbNw9uxZTJ06VZxlVFNTg5eXF06dOoXx48d/1LdrEBERUcUmEVT1ZHUF4+HhgZkzZxa4AeRjio+Pl3lcS2UhkUhgZWWF6Oholb1240Mq+xgw/8qdP8AxYP4VM38NDY1i37Sh8jN8RERERJWdyl/DV1pLly7F/fv35a7r379/gdO3qurNmzd48+ZNeYdRZjIyMpCVlVXeYZSrT2kMJBIJ9PX1VfohqEREHxMLvg/46quvCv0h+O6dpaUVEBCgtL6ULS0tDRKJBAYGBir7A1hDQ6NSns5+16c0BllZWUhNTYWBgUF5h0JEpBJY8H1AlSpVyjuEcpeTkyM+zJroY9DU1ERmZmZ5h0FEpDJ4DR99kKrO6hEREVUWLPiIiIiIVBwLPqr0WrZsiU2bNn2wzZYtW8o0Dn9/f9StW7dM96EMFSVOIiL6Pyz4SGVFRUVhxowZcHZ2RvXq1dGiRQssWLAAr169KnFfx44dw/Dhw5UWm7wCsm/fvjh37pzS9vG+o0ePws7OrsDbT/K5urpi/vz5ZbZ/IiIqP7xpgxTW+38PPtq+joypU6L2ERER6Nu3LxwcHLBu3TrY29vj4cOHWLJkCU6fPo3Dhw/DxMSk2P2ZmpqWNOQS09HRgY6OTpn137VrV5iYmCAgIADTp0+XWRcSEoLHjx9j/fr1ZbZ/IiIqP5zhI5U0b948aGhoYPfu3WjdujVsbGzQsWNH7N27FzExMVi+fLlM+9TUVEyaNAm1a9eGs7Mztm3bJrP+/Rm55ORkzJ49G40aNYKTkxMGDRqEu3fvymxz8uRJ9OjRAw4ODmjQoAHGjh0LABg4cCAiIyPh4+MDGxsb2NjYAJA9Vfr48WPY2Njg8ePHMn1u2rQJLVu2FJ8E/+jRI4wYMQK1a9dG48aNMWXKlEJnMDU0NDBgwADs27evwJPk9+7di8aNG6N+/frYtGkTOnXqhFq1aqFZs2bw9vZGWlpaoWP99ddfY/To0TLLFixYgIEDB4qfBUHA+vXr0bp1a9SsWROdO3fGkSNHCu2TiIiUiwUfqZzXr18jODgYo0aNKjBjZmFhgc8//xyHDx+WKXrWrVuHunXr4sSJE5g8eTJ8fHzwzz//yO1fEASMHDkScXFx2LlzJ44fP46GDRti8ODBeP36NQDgr7/+wtixY9GpUycEBQXB398fjRo1AgBs2bIFVlZWmDlzJq5fv47r168X2EetWrXQqFEj/PnnnzLLDxw4gH79+kEikSA2NhYDBgxAvXr1cPz4cezatQsJCQkYP358oWMzdOhQRERE4OLFi+Ky9PR0HD58GMOGDQMASKVSLF68GKdPn8bq1atx4cIFLFmypKgh/6Dly5fD398fy5Ytw+nTpzFu3DhMnTpVJg4iIio7PKVLKufp06cQBAG1a9eWu75WrVpITEzEy5cvYWZmBgBo0aIFJk+eDACoWbMmQkJCsGXLFrRr167A9hcuXMCDBw9w8+ZNaGlpAXg7oxUUFISjR49i+PDh+PXXX+Hu7o6ZM2eK29WvXx8AYGJiAjU1Nejr68PCwqLQPPr3748dO3Zg9uzZAIAnT57g1q1bWLNmDQDg999/R8OGDeHt7S1u88svv6B58+Z48uQJatasWaBPR0dHNG3aFP7+/nB1dQUAHD58GLm5ueJbY8aNGye2t7e3x6xZs+Dt7Y1ly5YVGmtR0tPTsWXLFvj7+6NZs2YAgGrVqiEkJAR+fn5o3bq1Qv0SEVHxseCjSid/Zu/d5wvmFyL5XFxcsHXrVrnb3759G2lpaWjQoIHM8szMTERERAAA7t69iy+++KJUcbq7u2PJkiW4evUqXFxcEBgYiPr168PR0REAcOvWLfz7779yC9uIiAi5BR/wdpZv4cKF+OGHH6Cvr4+9e/eiZ8+eMDIyQnZ2Ni5cuIC1a9ciNDQUKSkpyM3NRWZmJtLT06Grq1viPB49eoTMzEwMHTpUZnl2dnaBMSQiorLBgo9UTvXq1SGRSPDo0SN07969wPonT57A2Nj4g29RKeyB03l5ebCwsMD+/fsLrMt/I4m2trYCkcuqWrUqXF1dceDAAbi4uODAgQMydwoLgoAuXbpg7ty5crctjLu7O3x8fHDo0CG0bt0aly9fFmciIyMjMXLkSAwfPhyzZs2CsbExQkJCMGPGjEJfuyaVSgtcE5iTkyP+PS8vD8DbGUlLS0uZdpqamh8YBSIiUgYWfKRyqlSpgnbt2sHX1xfjxo2TuY4vLi4Of/75JwYOHChT0F29elWmj2vXrqFWrVpy+2/YsCHi4+Ohrq4OOzs7uW3q1q2L8+fPY/DgwXLXa2hoIDc394O59O/fH0uXLoW7uzsiIiLg7u4urmvQoAGOHTsGOzs7qKsX/5+yvr4+evfuDX9/f0RERKBatWri6d2bN28iJycHCxcuhFT69hLfw4cPF9mfqakpHj58KLPs7t270NDQAPD2NLKWlhaioqJ4+paIqJzwpg1SSUuWLEFWVha++OIL/Pfff4iKisKZM2cwdOhQWFpaYs6cOTLtL1++jPXr1+PJkyfYsWMHjhw5gjFjxsjt+7PPPoOLiwtGjx6N4OBgPH/+HCEhIVi+fDlu3rwJAPjmm29w4MABrFixAqGhobh//77MI0/s7Oxw6dIlREdHF/lcwJ49eyI1NRXe3t5wdXWFlZWVuM7T0xOJiYmYOHEirl+/joiICJw9exbffPPNB4vJoUOH4sqVK9i5cycGDx4sFr/VqlVDTk4Otm3bhoiICOzfvx87d+4ssq82bdrg5s2b2LdvH8LCwrBixQqZAlBfXx/jx4+Hj48PAgICEB4ejjt37mDHjh0ICAgosm8iIlIOFnykkhwcHHD8+HFUq1YNEyZMQJs2bTB79my4urri0KFDBZ7BN2HCBNy6dQvdunXD6tWrsWDBArRv315u3xKJBDt37kSrVq0wY8YMfPbZZ5g4cSIiIyPFm0BcXV2xadMmnDx5El27doWHh4fM3bgzZ87E8+fP0aZNGzRs2LDQPAwMDNC5c2fcu3dPvKkin6WlJQ4cOIC8vDx88cUX6NixIxYsWAADAwNxdq4wLVq0QM2aNZGSkoJBgwaJyxs0aICFCxdi/fr16NixIwIDA2VuCpGnffv2+Prrr/HDDz+gV69eSE1NlXkkCwDMnj0b06dPx2+//Yb27dtj2LBhOHXqFOzt7Yvsm4iIlEMivH/xDVVa8fHxcq/TSk5OhqGhYTlE9PFoaGgUeo0aADRt2hSzZs0SH12iij40Bh/bxzzuJBIJrKysEB0dXeB6xMqgsucPcAyYf8XMX0NDA+bm5sVqy2v4iIqQkZGBkJAQxMfHi3fHEhERVTQ8pUtUBD8/P0yYMAFjx44t8OgWIiKiioIzfERFGDdunMyDiImIiCoizvARERERqTgWfEREREQqjgUfERERkYpjwUfFkv96LKKPoSI9FoGIqCJgwUcfpKuri5SUFBZ99NGkp6dDS0urvMMgIlIZvEuXPkhdXR16enpITU0t71DKjKamJrKysso7jHL1qYyBIAhQV1dnwUdEpEQs+KhY1NXVVfZtGxX1CevKxDEgIlJtPKVLREREpOJY8BERERGpOBZ8RERERCqOBR8RERGRiuNNGyRSV6/ch0Nlzx/gGDD/yp0/wDFg/hUr/5LEKxF4S16ll52dDQ0NjfIOg4iIiMoIT+kSsrOzsWbNGmRkZJR3KOUiIyMDc+bMqbT5AxwD5l+58wc4Bsxf9fNnwUcAgAsXLlTa568JgoCnT59W2vwBjgHzr9z5AxwD5q/6+bPgIyIiIlJxLPiIiIiIVBwLPoKGhgYGDhxYaW/cqOz5AxwD5l+58wc4Bsxf9fPnXbpEREREKo4zfEREREQqjgUfERERkYpjwUdERESk4ljwEREREam4ivXSOCqWoKAgHDp0CImJibC1tYWnpyfq1q1baPt79+7B19cXkZGRMDExQd++fdG1a1eZNv/99x/8/f0RGxuLqlWrYujQoWjRokVZp6IwZY9BcHAw1q9fX2A7Pz8/aGpqlkkOpVGS/F+/fo3ff/8dYWFhiImJQY8ePeDp6VmgXUU6BpSdf0X7/oGSjcGlS5dw8uRJhIeHIycnB7a2thg0aBCaNGki005Vj4Hi5K/qx8CDBw+wa9cuREVF4c2bNzA3N0fnzp3Ru3dvmXaqegwUJ/+KeAzIEEilXLhwQRgyZIjw119/Cc+fPxe2b98uDB8+XIiPj5fbPjY2Vhg+fLiwfft24fnz58Jff/0lDBkyRLh48aLY5uHDh8LgwYOFP//8U4iMjBT+/PNPYciQIcKjR48+VlolUhZjcObMGWHkyJHC69evZf58ihTJf9u2bUJwcLAwa9YsYfv27QXaVKRjoCzyr0jfvyCUfAy2b98uHDhwQAgNDRVevHgh7Nq1SxgyZIgQFhYmtlHlY6A4+av6MRAWFiacO3dOePbsmRAbGyucPXtWGD58uHDq1CmxjSofA8XJv6IdA+/jKV0Vc+TIEXTs2BGdOnUSf6MxMzPDyZMn5bY/efIkzMzM4OnpCVtbW3Tq1AkdOnTA4cOHxTZHjx5Fo0aN0L9/f9jY2KB///5o0KABjh49+rHSKpGyGAMAkEgkMDY2lvnzKSpp/hYWFvDy8oKbmxt0dXXltqlIx0BZ5A9UnO8fKPkYeHp6wt3dHbVq1YKVlRWGDRsGKysrXL16VWyjysdAcfIHVPsYqFGjBtq2bQs7OztYWFigXbt2aNy4Me7fvy+2UeVjoDj5AxXrGHgfCz4VkpOTg7CwMDRu3FhmeaNGjfDw4UO524SGhqJRo0Yyy5o0aYKwsDDk5OQAAB49elSgTePGjfHo0SMlRq8cZTUGAJCZmYmJEyfiq6++wo8//oinT58qP4FSUiT/4qgox0BZ5Q9UjO8fUM4Y5OXlISMjA/r6+uKyynQMyMsfqFzHwNOnT/Hw4UPUq1dPXFaZjgF5+QMV5xiQh9fwqZDk5GTk5eXByMhIZrmRkRESExPlbpOYmCi3fW5uLlJSUmBiYoLExMQCv8UYGxsX2md5KqsxsLa2xsSJE2Fvb4+MjAwcO3YM8+fPx88//wwrK6uySqfEFMm/OCrKMVBW+VeU7x9QzhgcOXIEb968QevWrcVllekYkJd/ZTkGvvrqKyQnJyM3NxeDBg1Cp06dxHWV4RgoKv+KdAzIw4JPBUkkkmItK2yd8P9fvlLUNoIgFLm+vCl7DBwdHeHo6Ciud3Jywpw5c3D8+HGMHj1aGSErVUnzV8SnfAwoO/+K9v0Dio/B+fPnsW/fPsyaNavAD8z3qeIxUFj+leUYWLx4MTIzM/Ho0SPs3r0blpaWaNu2baHtVe0YKCr/ingMvIsFnwoxNDSEVCot8BtMUlJSof/jlvfbWXJyMtTU1MTTGfLaFNVneSqrMXifVCpFzZo1ERMTo4ywlUaR/IujohwDZZX/+z7V7x8o3Rj8+++/2LhxI7755psCp+4qwzFQVP7vU9VjwMLCAgBgb2+PpKQk7Nu3Tyx4KsMxUFT+7/uUjwF5eA2fClFXV4eDgwNu3bols/zWrVtwcnKSu03t2rULtL958yYcHBygrv729wFHR0fcvn27QJ/v/qbzqSirMXifIAiIiIj45C7YVST/4qgox0BZ5f++T/X7BxQfg/Pnz2PdunWYOnUqnJ2dC6xX9WPgQ/m/TxWPgfcJgiBzHbOqHwPvez9/ees/1WNAHhZ8KqZ37974+++/cfr0aURGRmLHjh1ISEhAly5dAAC7d+/Gb7/9Jrbv2rUrEhISxGfQnT59GqdPn0afPn3ENj179sTNmzdx4MABREVF4cCBA7h9+zZ69er10fMrjrIYg3379uHGjRuIjY1FeHg4NmzYgPDw8ALPK/wUlDR/AAgPD0d4eDgyMzORnJyM8PBwREZGiusr0jFQFvlXpO8fKPkY5Bc7I0eOhKOjIxITE5GYmIj09HSxjSofA8XJX9WPgRMnTuDKlSuIjo5GdHQ0zpw5g8OHD+Ozzz4T26jyMVCc/CvaMfA+ntJVMa6urkhJScEff/yB169fw87ODt7e3jA3Nwfw9iGzCQkJYnsLCwt4e3vD19cXQUFBMDExgZeXF1q1aiW2cXJywtdff429e/fC398flpaW+Prrr1G7du2Pnl9xlMUYpKWlYfPmzUhMTISuri5q1KiBRYsWoVatWh89vw8paf4AMHv2bPHvYWFhOH/+PMzNzbFu3ToAFesYKIv8K9L3D5R8DP766y/k5ubif//7H/73v/+Jy93c3DBp0iQAqn0MFCd/VT8GBEHAnj17EBcXB6lUCktLS3zxxRfo3Lmz2EaVj4Hi5F/RjoH3SYT8q9OJiIiISCXxlC4RERGRimPBR0RERKTiWPARERERqTgWfEREREQqjgUfERERkYpjwUdERESk4ljwEREREak4FnxEBAAIDg6Gh4cHnjx5Inf9jz/+KD6Elj5tQUFBCA4O/qj79PHxwYwZMz7qPpXpzZs3CAgIwN27d8s7FKIywYKPiEjFnDx58qMXfBXdmzdvsH//fhZ8pLJY8BGRSsjJyUFubu5H29+bN28+2r4+BYIgICsrq7zDUDpVzYvofXyXLhEpZPHixXj16hVWrVoFiUQiLhcEAVOnToW1tTW8vb0RFxeHyZMn44svvkBubi5OnTqF5ORk2NnZ4YsvvkDDhg1l+o2OjkZAQABu376N9PR0VK1aFd26dUP37t3FNnfv3sWiRYswefJkhIeH48KFC0hMTMTKlSsRGhqK9evX47vvvsP58+cREhKCnJwc1K9fH15eXqhatarYz61bt3DixAmEhYUhJSUFVapUQcOGDTFkyBAYGhqK7QICArB//378+OOPCAwMxJ07d6ChoYHNmzfjyZMnOHz4MEJDQ5GYmAhjY2PUrl0bX3zxhfjeTuDtKfP169djwYIFOH/+PC5fvozc3Fw0b94cY8eORWZmJrZt24Zbt25BU1MTbdu2xbBhw6Cu/n//m87JycHBgwdx7tw5xMXFQUdHBy4uLhg+fLgY76RJkxAfHw8A8PDwAACZ9wKnp6dj//79uHTpEl69egVDQ0O0bt0aQ4YMgba2trgvDw8PdOvWDXZ2djh+/DhiYmLg5eVVohfF5/fh4OCAAwcOICEhAXZ2dhg9ejRq166Nw4cPIygoCMnJyahVqxbGjx8PS0tLcXsfHx+kpKRg7Nix8PPzQ3h4OPT19dGhQwd4eHhAKv2/OYvU1FTs3bsXISEhSE5OhqmpKdq0aYOBAwdCQ0Pjg3lt3boVALB//37s378fwP+9SzcmJgZ//vknHjx4gFevXkFPTw81atTAsGHDYG9vX+C4nDp1Kp4/f47g4GBkZmaiVq1aGDNmDKytrWXG58aNGzh06BCePHmC3NxcmJubo127dujfv7/Y5smTJ9i/fz8ePHiArKws2NjYoF+/fnB1dS3290AEsOAjovfk5eXJnSl7/7XbPXv2xE8//YTbt2+jUaNG4vLr168jNjYWXl5eMu1PnDgBc3NzeHp6QhAEHDx4EEuXLsWiRYvg6OgIAIiMjMR3330HMzMzjBw5EsbGxrhx4wa2b9+OlJQUDBo0SKbP3bt3w9HREePGjYNUKoWRkZG4bsOGDWjUqBGmTZuGhIQE+Pv7w8fHBytWrICenh4AICYmBo6OjujYsSN0dXURHx+PI0eOYMGCBVixYoVMsQUAv/zyC1xdXdGlSxdxhi8+Ph7W1tZwdXWFvr4+EhMTcfLkSXh7e2PlypUyhSMAbNy4ES1atMDXX3+Np0+fYs+ePcjNzcWLFy/QsmVLdO7cGbdv38bBgwdRpUoV9O7dW/xefvrpJ9y/fx/u7u5wdHREQkICAgIC4OPjgx9//BGampqYOXMmVq5cCV1dXYwZMwYAxILnzZs38PHxwcuXL9G/f39Uq1YNz58/R0BAAJ49e4b58+fLFO8hISF48OABBgwYAGNjY5nxLa5r164hPDwcX3zxBQBg165d+PHHH+Hm5obY2FiMGTMG6enp8PX1xS+//IKffvpJJobExESsXr0a/fr1g4eHB65du4Y///wTaWlpYn5ZWVlYtGgRYmJi4OHhgWrVquH+/fs4cOAAwsPD4e3tLRPT+3np6+tj7ty5WLp0KTp27IiOHTsCgPjdvXr1Cvr6+hg2bBgMDQ2RmpqKs2fPYu7cufjpp58KFHJ79uyBk5MTxo8fj4yMDOzatQvLly/HqlWrxCL19OnT2LRpE+rVq4dx48bByMgI0dHRePbsmdjPnTt3sHTpUtSuXRvjxo2Drq4u/v33X6xevRpZWVlo3759ib8PqrxY8BGRjHnz5hW67t0ZK2dnZ1StWhUnTpyQKfiCgoJQtWpVNG3aVGbbvLw8fPfdd9DU1AQANG7cGJMmTYK/vz/mz58PAPD19YWOjg4WL14MXV1dAECjRo2Qk5ODAwcOoEePHtDX1xf7rFq1Kr755hu5sdasWRMTJkwQP9vZ2WH+/PkICgrC559/DgAys1WCIMDJyQn169fHxIkTcePGDTRr1kymTzc3N3HWLF+rVq3QqlUrmTydnZ0xbtw4nD9/Hj179pRp7+zsjJEjR4q5PXr0CBcuXMDIkSPF4q5Ro0a4efMmzp07Jy67ePEibty4gRkzZqBly5Zif9WqVYO3tzeCg4PRtWtX1KhRA5qamtDR0REL6XzHjx9HREQEli5dipo1awIAGjZsiCpVqmDlypW4ceOGzPeWmZmJFStWyIx5SWVnZ2PevHni7KFEIsHPP/+Mu3fvYvny5WJxl5ycjB07duD58+cys2YpKSmYPXu2+F00btwYWVlZOHnyJNzd3WFmZoazZ88iIiIC06dPR+vWrcUx1NbWxq5du3Dr1i2ZY1ReXsnJyQCAKlWqFBi3evXqoV69euLn/O94xowZOHXqFEaNGiXT3tbWFlOnThU/S6VSrFq1Co8fP4ajoyMyMzPh6+sLJycnLFiwQByD92e7//e//8HOzg4LFiyAmpoaAKBJkyZITk7Gnj170K5dO5lZTqKisOAjIhmTJ0+GjY1NgeW+vr54+fKl+FkqlaJbt27w8/NDQkICzMzMEBMTgxs3bmDEiBEyszQA0LJlS7HYAyCejrxw4QLy8vKQk5ODO3fuoEuXLtDS0pKZZWzatClOnDiB0NBQmYLk3cLnfW3btpX57OTkBHNzc9y9e1cs+JKSkuDv74/r16/j1atXMrOYkZGRBQo+efvLzMwUT5HGx8cjLy9PXBcVFVWgvYuLi8xnGxsbhISEwNnZucDyW7duiZ+vXr0KPT09uLi4yIxN9erVYWxsjLt3737wdOvVq1dhb2+P6tWry/TRpEkTSCQS3L17V2Z8GzRoUKpiDwDq168vc6o4/9jK3+f7y+Pj42UKPh0dnQLfQ9u2bfH333/j3r17aNeuHe7cuQMtLS2ZwhsA2rdvj127dhWYhS5pXrm5ueKp9JiYGJmxk/cdvx9vtWrVAAAJCQlwdHTEw4cPkZGRga5duxb4d5IvJiYGUVFRGDFihBhDPmdnZ1y7dg0vXryAra1tsfOgyo0FHxHJsLGxEWd/3qWrqytT8AFAx44dERAQgJMnT2LYsGEICgqCpqYmOnToUGB7Y2NjuctycnKQmZmJzMxM5Obm4sSJEzhx4oTc2FJSUmQ+m5iYFJpHYfvL7yMvLw9LlizB69evMWDAANjb20NLSwuCIGDevHlyL+SXt781a9bgzp07GDBgAGrWrAkdHR1IJBIsW7ZMbh/vFxr5p43lLX93+6SkJKSlpWHYsGFy831/bORJSkpCTEwMhg4dWqw+5I1hSZUkX+DtjOC75J1Gzo8rNTVV/K+xsXGB4snIyAhqamqlzsvX1xdBQUFwd3dHvXr1oK+vD4lEgo0bN8r9jg0MDOTmlt82fzbR1NS00H0mJiYCAHbu3ImdO3fKbVOc75woHws+IlKYrq4u3NzccPr0afTt2xfBwcFo06aNeI3cu/J/gL2/TF1dHdra2lBTU4NUKkW7du3QrVs3ufuzsLCQ+VzY7EhR+8u/KeD58+eIiIjAxIkTZa6FiomJKbTP96Wnp+PatWsYOHAg+vXrJy7Pzs4WixFlMTAwgIGBAebOnSt3vY6OTrH60NTUlDnV/f76dxU1vh9LUlJSgWX5321+0aivr4/Q0FAIgiATc1JSEnJzcwtcR1nSvM6dOwc3N7cCxXZKSorcY/1D8uN5/xcoeW369etX6Ez2+9cOEhWFBR8RlUqPHj1w8uRJ/PLLL0hLS5O5m/Zdly5dwvDhw8XTuhkZGbh69Srq1q0LqVQKLS0t1K9fH0+fPkW1atUK3DBRUufPn5c5xffw4UPEx8eLF+Tn/9B/9w5OADh16lSJ9iMIQoE+/v77b5lTu8rg4uKCf//9F3l5eahdu3aRbd+fHXy3j8DAQBgYGBQonj9VGRkZuHLlisxp0vPnz0MikYjX1TVs2BAXL15ESEgIWrRoIbY7e/YsgLencD8k/zuUN24SiaTA8Xjt2jW8evVK5q7i4nJycoKuri5OnTqFNm3ayC1Ara2tYWVlhYiIiEJndYlKggUfEZWKtbU1mjRpguvXr6NOnTqoXr263HZSqRRLlixB7969kZeXh4MHDyIjI0PmzlsvLy/Mnz8fCxYsQNeuXWFubo6MjAzExMTg6tWrWLhwYbHjevLkCTZu3IhWrVrh5cuX2Lt3L6pUqSLOHlpbW6Nq1arYvXs3BEGAvr4+rl69KnPd3Ifo6uqibt26OHToEAwMDGBubo579+7hzJkzCs38FKVNmzY4f/48li1bhp49e6JWrVpQU1PDy5cvcffuXTRv3lwsduzt7fHvv//i33//hYWFBTQ1NWFvb4+ePXvi0qVLWLhwIXr16gV7e3sIgoCEhATcvHkTffr0+WAx+bEZGBhgy5YtSEhIgJWVFa5fv46///4bXbt2hZmZGQCgXbt2CAoKwrp16xAXFwd7e3s8ePAAgYGBaNq0qcz1e4XR0dGBubk5rly5goYNG0JfX18sjJ2dnXH27FnY2NigWrVqCAsLw6FDh4o8JVsUbW1tjBw5Ehs3bsT333+PTp06wcjICDExMYiIiBDvPh43bhyWLVuGH374AW5ubqhSpQpSU1MRFRWFp0+fFnrDEpE8LPiIqNRat26N69evFzq7BwDdu3dHdnY2tm/fjqSkJNjZ2eHbb79FnTp1xDa2trZYvnw5/vjjD+zduxdJSUnQ09ODlZVVgbt+P2TChAn4559/sGbNGmRnZ4vP4cs/Daiuro45c+Zgx44d2LJlC6RSKRo2bIj58+dj4sSJxd7PtGnTsH37dvj5+SEvLw9OTk747rvv8OOPP5Yo3g+RSqWYPXs2jh07hn/++QeBgYFQU1ODqakp6tatK3Ojg4eHBxITE7Fp0yZkZGSIz+HT1tbGokWLcODAAfz111+Ii4uDpqYmzMzM0LBhQ5m7sD8VxsbGGDNmDHbu3Ilnz55BX18f/fv3l7lbWlNTEwsXLsSePXtw+PBhJCcno0qVKujTp0+BR/kU5auvvoKfnx9++uknZGdni8/h8/Lygrq6Og4cOIDMzEzUqFEDM2fOxN69exXOq2PHjjAxMcHBgwexceNGAG/vgndzcxPbNGjQAEuXLsWff/4JX19fpKamwsDAALa2tuLdyETFJRHef7gWEVEJrVixAqGhoVi3bl2BU1/5D14ePnw4+vbtW+ax5D/geNmyZXJvPqGKI//By7/88kt5h0JU4XGGj4gUkp2djadPn+Lx48cICQnByJEjS33dHRERlQ3+35mIFPL69Wt899130NHRQefOndGjR4/yDomIiArBU7pEREREKo7vZCEiIiJScSz4iIiIiFQcCz4iIiIiFceCj4iIiEjFseAjIiIiUnEs+IiIiIhUHAs+IiIiIhXHgo+IiIhIxbHgIyIiIlJx/w8Xukjj/k+eZgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.711967</td>\n",
       "      <td>0.020486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>201.800000</td>\n",
       "      <td>9.554522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>174.900000</td>\n",
       "      <td>6.951419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>38.200000</td>\n",
       "      <td>4.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>34.300000</td>\n",
       "      <td>5.396501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.838596</td>\n",
       "      <td>0.017719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.840614</td>\n",
       "      <td>0.019213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.854553</td>\n",
       "      <td>0.023977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.820780</td>\n",
       "      <td>0.018592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.847420</td>\n",
       "      <td>0.019163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.838508</td>\n",
       "      <td>0.017699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.837874</td>\n",
       "      <td>0.017614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.837670</td>\n",
       "      <td>0.017611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.676115</td>\n",
       "      <td>0.035435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.836270</td>\n",
       "      <td>0.022928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.837670</td>\n",
       "      <td>0.017611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.711967     0.020486\n",
       "1                    TP       201.800000     9.554522\n",
       "2                    TN       174.900000     6.951419\n",
       "3                    FP        38.200000     4.237400\n",
       "4                    FN        34.300000     5.396501\n",
       "5              Accuracy         0.838596     0.017719\n",
       "6             Precision         0.840614     0.019213\n",
       "7           Sensitivity         0.854553     0.023977\n",
       "8           Specificity         0.820780     0.018592\n",
       "9              F1 score         0.847420     0.019163\n",
       "10  F1 score (weighted)         0.838508     0.017699\n",
       "11     F1 score (macro)         0.837874     0.017614\n",
       "12    Balanced Accuracy         0.837670     0.017611\n",
       "13                  MCC         0.676115     0.035435\n",
       "14                  NPV         0.836270     0.022928\n",
       "15              ROC_AUC         0.837670     0.017611"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.681005</td>\n",
       "      <td>0.720551</td>\n",
       "      <td>0.717203</td>\n",
       "      <td>0.721099</td>\n",
       "      <td>0.702278</td>\n",
       "      <td>0.708199</td>\n",
       "      <td>0.649452</td>\n",
       "      <td>0.682835</td>\n",
       "      <td>0.697318</td>\n",
       "      <td>0.708034</td>\n",
       "      <td>0.698798</td>\n",
       "      <td>0.022370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>405.200000</td>\n",
       "      <td>7.067924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>346.700000</td>\n",
       "      <td>5.478240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>4.948625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>6.168919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.844271</td>\n",
       "      <td>0.829811</td>\n",
       "      <td>0.835373</td>\n",
       "      <td>0.843159</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.829811</td>\n",
       "      <td>0.823137</td>\n",
       "      <td>0.837597</td>\n",
       "      <td>0.849833</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.008012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.829918</td>\n",
       "      <td>0.831622</td>\n",
       "      <td>0.831622</td>\n",
       "      <td>0.833676</td>\n",
       "      <td>0.834395</td>\n",
       "      <td>0.828092</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.860996</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.837544</td>\n",
       "      <td>0.009869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.859873</td>\n",
       "      <td>0.872845</td>\n",
       "      <td>0.863830</td>\n",
       "      <td>0.839744</td>\n",
       "      <td>0.836864</td>\n",
       "      <td>0.862786</td>\n",
       "      <td>0.859213</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.855437</td>\n",
       "      <td>0.012366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.820900</td>\n",
       "      <td>0.804200</td>\n",
       "      <td>0.808400</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.811200</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.808600</td>\n",
       "      <td>0.838900</td>\n",
       "      <td>0.821700</td>\n",
       "      <td>0.815240</td>\n",
       "      <td>0.010237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.841121</td>\n",
       "      <td>0.845511</td>\n",
       "      <td>0.851735</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.837061</td>\n",
       "      <td>0.832455</td>\n",
       "      <td>0.850410</td>\n",
       "      <td>0.860104</td>\n",
       "      <td>0.843523</td>\n",
       "      <td>0.846335</td>\n",
       "      <td>0.008152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.844128</td>\n",
       "      <td>0.829635</td>\n",
       "      <td>0.835180</td>\n",
       "      <td>0.842926</td>\n",
       "      <td>0.838512</td>\n",
       "      <td>0.829786</td>\n",
       "      <td>0.823082</td>\n",
       "      <td>0.837379</td>\n",
       "      <td>0.849845</td>\n",
       "      <td>0.832077</td>\n",
       "      <td>0.836255</td>\n",
       "      <td>0.007996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.843728</td>\n",
       "      <td>0.828944</td>\n",
       "      <td>0.834661</td>\n",
       "      <td>0.842633</td>\n",
       "      <td>0.838036</td>\n",
       "      <td>0.829473</td>\n",
       "      <td>0.822588</td>\n",
       "      <td>0.836397</td>\n",
       "      <td>0.849019</td>\n",
       "      <td>0.831125</td>\n",
       "      <td>0.835660</td>\n",
       "      <td>0.007987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.843301</td>\n",
       "      <td>0.828438</td>\n",
       "      <td>0.834142</td>\n",
       "      <td>0.842170</td>\n",
       "      <td>0.837509</td>\n",
       "      <td>0.829385</td>\n",
       "      <td>0.822413</td>\n",
       "      <td>0.835699</td>\n",
       "      <td>0.849078</td>\n",
       "      <td>0.831298</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>0.007969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.687868</td>\n",
       "      <td>0.658238</td>\n",
       "      <td>0.669854</td>\n",
       "      <td>0.686376</td>\n",
       "      <td>0.676675</td>\n",
       "      <td>0.658965</td>\n",
       "      <td>0.645227</td>\n",
       "      <td>0.673205</td>\n",
       "      <td>0.698041</td>\n",
       "      <td>0.662269</td>\n",
       "      <td>0.671672</td>\n",
       "      <td>0.016105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.848600</td>\n",
       "      <td>0.829700</td>\n",
       "      <td>0.839800</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>0.844700</td>\n",
       "      <td>0.824800</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>0.836900</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.835120</td>\n",
       "      <td>0.013293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.843301</td>\n",
       "      <td>0.828438</td>\n",
       "      <td>0.834142</td>\n",
       "      <td>0.842170</td>\n",
       "      <td>0.837509</td>\n",
       "      <td>0.829385</td>\n",
       "      <td>0.822413</td>\n",
       "      <td>0.835699</td>\n",
       "      <td>0.849078</td>\n",
       "      <td>0.831298</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>0.007969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.681005    0.720551    0.717203    0.721099   \n",
       "1                    TP  406.000000  405.000000  405.000000  405.000000   \n",
       "2                    TN  353.000000  341.000000  346.000000  353.000000   \n",
       "3                    FP   77.000000   83.000000   82.000000   82.000000   \n",
       "4                    FN   63.000000   70.000000   66.000000   59.000000   \n",
       "5              Accuracy    0.844271    0.829811    0.835373    0.843159   \n",
       "6             Precision    0.840580    0.829918    0.831622    0.831622   \n",
       "7           Sensitivity    0.865672    0.852632    0.859873    0.872845   \n",
       "8           Specificity    0.820900    0.804200    0.808400    0.811500   \n",
       "9              F1 score    0.852941    0.841121    0.845511    0.851735   \n",
       "10  F1 score (weighted)    0.844128    0.829635    0.835180    0.842926   \n",
       "11     F1 score (macro)    0.843728    0.828944    0.834661    0.842633   \n",
       "12    Balanced Accuracy    0.843301    0.828438    0.834142    0.842170   \n",
       "13                  MCC    0.687868    0.658238    0.669854    0.686376   \n",
       "14                  NPV    0.848600    0.829700    0.839800    0.856800   \n",
       "15              ROC_AUC    0.843301    0.828438    0.834142    0.842170   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.702278    0.708199    0.649452    0.682835    0.697318    0.708034   \n",
       "1   406.000000  393.000000  395.000000  415.000000  415.000000  407.000000   \n",
       "2   348.000000  353.000000  345.000000  338.000000  349.000000  341.000000   \n",
       "3    81.000000   78.000000   82.000000   80.000000   67.000000   74.000000   \n",
       "4    64.000000   75.000000   77.000000   66.000000   68.000000   77.000000   \n",
       "5     0.838710    0.829811    0.823137    0.837597    0.849833    0.832036   \n",
       "6     0.833676    0.834395    0.828092    0.838384    0.860996    0.846154   \n",
       "7     0.863830    0.839744    0.836864    0.862786    0.859213    0.840909   \n",
       "8     0.811200    0.819000    0.808000    0.808600    0.838900    0.821700   \n",
       "9     0.848485    0.837061    0.832455    0.850410    0.860104    0.843523   \n",
       "10    0.838512    0.829786    0.823082    0.837379    0.849845    0.832077   \n",
       "11    0.838036    0.829473    0.822588    0.836397    0.849019    0.831125   \n",
       "12    0.837509    0.829385    0.822413    0.835699    0.849078    0.831298   \n",
       "13    0.676675    0.658965    0.645227    0.673205    0.698041    0.662269   \n",
       "14    0.844700    0.824800    0.817500    0.836600    0.836900    0.815800   \n",
       "15    0.837509    0.829385    0.822413    0.835699    0.849078    0.831298   \n",
       "\n",
       "           ave       std  \n",
       "0     0.698798  0.022370  \n",
       "1   405.200000  7.067924  \n",
       "2   346.700000  5.478240  \n",
       "3    78.600000  4.948625  \n",
       "4    68.500000  6.168919  \n",
       "5     0.836374  0.008012  \n",
       "6     0.837544  0.009869  \n",
       "7     0.855437  0.012366  \n",
       "8     0.815240  0.010237  \n",
       "9     0.846335  0.008152  \n",
       "10    0.836255  0.007996  \n",
       "11    0.835660  0.007987  \n",
       "12    0.835343  0.007969  \n",
       "13    0.671672  0.016105  \n",
       "14    0.835120  0.013293  \n",
       "15    0.835343  0.007969  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_xgb0</th>\n",
       "      <th>y_pred_xgb1</th>\n",
       "      <th>y_pred_xgb2</th>\n",
       "      <th>y_pred_xgb3</th>\n",
       "      <th>y_pred_xgb4</th>\n",
       "      <th>y_pred_xgb_ave</th>\n",
       "      <th>y_pred_xgb_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4176702</td>\n",
       "      <td>0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>5.940942</td>\n",
       "      <td>5.841841</td>\n",
       "      <td>5.980500</td>\n",
       "      <td>5.923776</td>\n",
       "      <td>5.865616</td>\n",
       "      <td>6.008779</td>\n",
       "      <td>0.224458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL272401</td>\n",
       "      <td>1</td>\n",
       "      <td>7.26</td>\n",
       "      <td>7.211039</td>\n",
       "      <td>7.226815</td>\n",
       "      <td>6.982445</td>\n",
       "      <td>6.860160</td>\n",
       "      <td>7.205894</td>\n",
       "      <td>7.124392</td>\n",
       "      <td>0.148884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL118</td>\n",
       "      <td>2</td>\n",
       "      <td>5.93</td>\n",
       "      <td>5.523479</td>\n",
       "      <td>5.472725</td>\n",
       "      <td>5.347790</td>\n",
       "      <td>5.231540</td>\n",
       "      <td>5.331552</td>\n",
       "      <td>5.472847</td>\n",
       "      <td>0.225605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3655939</td>\n",
       "      <td>3</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.948651</td>\n",
       "      <td>7.021563</td>\n",
       "      <td>6.802073</td>\n",
       "      <td>6.848300</td>\n",
       "      <td>6.799564</td>\n",
       "      <td>6.783359</td>\n",
       "      <td>0.238838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3621537</td>\n",
       "      <td>4</td>\n",
       "      <td>5.88</td>\n",
       "      <td>6.336583</td>\n",
       "      <td>6.313459</td>\n",
       "      <td>6.421102</td>\n",
       "      <td>6.643893</td>\n",
       "      <td>6.193159</td>\n",
       "      <td>6.298033</td>\n",
       "      <td>0.231747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL2047606</td>\n",
       "      <td>4487</td>\n",
       "      <td>5.44</td>\n",
       "      <td>5.577601</td>\n",
       "      <td>5.604442</td>\n",
       "      <td>5.500355</td>\n",
       "      <td>5.612972</td>\n",
       "      <td>5.602675</td>\n",
       "      <td>5.556341</td>\n",
       "      <td>0.064277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL217781</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.48</td>\n",
       "      <td>6.779581</td>\n",
       "      <td>6.800874</td>\n",
       "      <td>6.690016</td>\n",
       "      <td>6.884105</td>\n",
       "      <td>6.757020</td>\n",
       "      <td>6.898599</td>\n",
       "      <td>0.266303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL2105763</td>\n",
       "      <td>4489</td>\n",
       "      <td>9.92</td>\n",
       "      <td>7.096272</td>\n",
       "      <td>7.166739</td>\n",
       "      <td>6.965294</td>\n",
       "      <td>7.008142</td>\n",
       "      <td>6.948980</td>\n",
       "      <td>7.517571</td>\n",
       "      <td>1.077038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL3415969</td>\n",
       "      <td>4490</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.624371</td>\n",
       "      <td>5.619924</td>\n",
       "      <td>6.255660</td>\n",
       "      <td>5.671523</td>\n",
       "      <td>5.635003</td>\n",
       "      <td>5.759413</td>\n",
       "      <td>0.226291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL467066</td>\n",
       "      <td>4491</td>\n",
       "      <td>7.55</td>\n",
       "      <td>7.296854</td>\n",
       "      <td>7.397299</td>\n",
       "      <td>7.373176</td>\n",
       "      <td>7.256786</td>\n",
       "      <td>7.401993</td>\n",
       "      <td>7.379351</td>\n",
       "      <td>0.092935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_xgb0  y_pred_xgb1  \\\n",
       "0         CHEMBL4176702            0     6.50     5.940942     5.841841   \n",
       "1          CHEMBL272401            1     7.26     7.211039     7.226815   \n",
       "2             CHEMBL118            2     5.93     5.523479     5.472725   \n",
       "3         CHEMBL3655939            3     6.28     6.948651     7.021563   \n",
       "4         CHEMBL3621537            4     5.88     6.336583     6.313459   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "4487      CHEMBL2047606         4487     5.44     5.577601     5.604442   \n",
       "4488       CHEMBL217781         4488     7.48     6.779581     6.800874   \n",
       "4489      CHEMBL2105763         4489     9.92     7.096272     7.166739   \n",
       "4490      CHEMBL3415969         4490     5.75     5.624371     5.619924   \n",
       "4491       CHEMBL467066         4491     7.55     7.296854     7.397299   \n",
       "\n",
       "      y_pred_xgb2  y_pred_xgb3  y_pred_xgb4  y_pred_xgb_ave  y_pred_xgb_std  \n",
       "0        5.980500     5.923776     5.865616        6.008779        0.224458  \n",
       "1        6.982445     6.860160     7.205894        7.124392        0.148884  \n",
       "2        5.347790     5.231540     5.331552        5.472847        0.225605  \n",
       "3        6.802073     6.848300     6.799564        6.783359        0.238838  \n",
       "4        6.421102     6.643893     6.193159        6.298033        0.231747  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "4487     5.500355     5.612972     5.602675        5.556341        0.064277  \n",
       "4488     6.690016     6.884105     6.757020        6.898599        0.266303  \n",
       "4489     6.965294     7.008142     6.948980        7.517571        1.077038  \n",
       "4490     6.255660     5.671523     5.635003        5.759413        0.226291  \n",
       "4491     7.373176     7.256786     7.401993        7.379351        0.092935  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where((y_pred_optimized_xgb >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id, xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "xgb_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a1124bd7-9f1e-4bf2-9f96-468dd7034eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.714164</td>\n",
       "      <td>0.034275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.837089</td>\n",
       "      <td>0.018395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.839028</td>\n",
       "      <td>0.023966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.853983</td>\n",
       "      <td>0.026018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.818570</td>\n",
       "      <td>0.026507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.846134</td>\n",
       "      <td>0.018999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.836981</td>\n",
       "      <td>0.018416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.836293</td>\n",
       "      <td>0.018383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.836275</td>\n",
       "      <td>0.018465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.673418</td>\n",
       "      <td>0.036808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.835264</td>\n",
       "      <td>0.026593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.836275</td>\n",
       "      <td>0.018465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.714164     0.034275\n",
       "1              Accuracy         0.837089     0.018395\n",
       "2             Precision         0.839028     0.023966\n",
       "3           Sensitivity         0.853983     0.026018\n",
       "4           Specificity         0.818570     0.026507\n",
       "5              F1 score         0.846134     0.018999\n",
       "6   F1 score (weighted)         0.836981     0.018416\n",
       "7      F1 score (macro)         0.836293     0.018383\n",
       "8     Balanced Accuracy         0.836275     0.018465\n",
       "9                   MCC         0.673418     0.036808\n",
       "10                  NPV         0.835264     0.026593\n",
       "11              ROC_AUC         0.836275     0.018465"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAG8CAYAAADaV3/fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACHEUlEQVR4nO3de3gU1f0/8PfsJTdCCHGBJARIaIACikL5aRUUsNW2SKUooihVvBeQagUJFxGpRAgIahGoVb9eSlUUuVi1VrTg/VGrYlUQiBARSEiW3Ai57u78/pjsZmd2Znc2O2F3J+/X8/DA7s7OnpNdMp8953M+RxBFUQQRERGRiVmi3QAiIiKijsaAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpseAJ4S7774bgiDgiiuugNvtjnZziIiIqB06VcAzffp0CIIAQRBgs9nQt29fzJgxA1VVVarHFxYW4oknnsDjjz+Ojz/+GLfffnvAMbt27cLEiRORlZWFLl264JxzzsE//vGPju4KmpqaMHv2bDgcDnTp0gWXX345jhw5EvQ5LpcL9957L/Ly8pCcnIz+/fvjz3/+Mzwej++YLVu24Fe/+hUcDgcEQcDu3bsDzvO3v/0NY8eORVpaGgRBQHV1tcG9IyIiMlanCngA4Ne//jVKS0tRUlKCJ598Ev/85z8xc+bMgOP+9re/YfXq1dixYwduu+02vPfee9ixYwcKCgpkx3300UcYNmwYXnnlFfzvf//DTTfdhOuvvx7//Oc/O7Qfd911F7Zu3YoXX3wRH3zwAerq6jBhwoSgo1BFRUX461//isceewx79+7FypUrsWrVKqxdu9Z3zKlTpzBq1CisWLFC8zz19fX49a9/jYULFxraJyIiog4jdiI33HCDOHHiRNl9d999t5iRkSG77+WXXxYzMzPFL7/8Unb/Dz/8IObn54tFRUVBX2f8+PHijTfeaESTVVVXV4t2u1188cUXffcdPXpUtFgs4ptvvqn5vMsuu0y86aabZPddccUV4rRp0wKOPXTokAgg4Gfgb+fOnSIAsaqqKuw+EBERnU6dboTH38GDB/Hmm2/CbrfL7p88eTJKS0txzjnnyO7v27cvDhw4gHnz5gU9b01NDTIyMoIeM3ToUKSmpmr+GTp0qOZzP//8c7S0tODSSy/13ZednY0zzzwTH330kebzRo8ejXfeeQf79+8HAHz11Vf44IMPMH78+KBtJSIiine2aDfgdHvttdeQmpoKt9uNxsZGAMCaNWsMO//mzZvx2Wef4fHHHw963BtvvIGWlhbNx5VBmL+ysjIkJCSge/fusvt79eqFsrIyzecVFBSgpqYGP/3pT2G1WuF2u1FYWIipU6cGbSsREVG8i3rAs2fPHrz66qs4dOgQqqqqMHfuXJx77rkApCTbF198EV9++SXKy8uRkpKCs846C9dee23IERQt48aNw4YNG1BfX48nn3wS+/fvx+zZsw3py65duzB9+nQ88cQTQUdoAKBfv36GvKY/URQhCILm45s2bcLGjRvx/PPPY+jQodi9ezfuuusuZGdn44YbbjC8PURERLEi6lNaTU1NyM3NxU033RTwWHNzMw4dOoQrr7wSRUVFmDNnDkpLS7Fy5cp2v16XLl2Qn5+PYcOG4S9/+QuampqwdOnSSLoAAHj33Xfx29/+FmvWrMH1118f8vhIprQyMzPR3NwcsLqsvLwcvXr10nzePffcg/nz5+Oaa67BWWedhd///vf405/+hOXLl+vvKBERURyK+gjP8OHDMXz4cNXHUlJSsHjxYtl9N954IxYuXAin0wmHwxHx6y9ZsgS/+c1vMGPGDGRnZ7frHLt27cKECRNQVFSE2267TddzIpnS+tnPfga73Y4dO3ZgypQpAIDS0lJ88803QYPB+vp6WCzyGNdqtcqWpRMREZlR1AOecNXX10MQBKSkpGge09LSEhBMaAUQY8eOxdChQ/Hggw/iscceC7s9u3btwmWXXYY777wTV155pS+HJiEhIei0WyRTWt26dcPNN9+MOXPm4IwzzkBGRgbmzp2Ls846C7/85S99x/3iF7/ApEmTcMcddwAAfvvb36KwsBB9+/bF0KFD8eWXX2LNmjWy0bXKykocPnwYx44dAwDs27cPgDSqlJmZCUDKISorK0NxcTEA4Ouvv0bXrl3Rt2/fdk81EhERdahoLxPzd9VVV4mffPKJ5uNNTU1iQUGB+OijjwY9z6ZNm8SrrrrK9+eRRx4RRVF9WbooiuI//vEPMSEhQTx8+HDYbb7hhhtEAAF/xowZE/a5wtHQ0CDecccdYkZGhpicnCxOmDAhoP39+vUTlyxZ4rtdW1sr3nnnnWLfvn3FpKQksX///uKiRYvEpqYm3zFPP/20an/8z7NkyRLVY55++ukO7TMREVF7CaIoilGIs1RNmTJFlrTsz+VyYc2aNThx4gSWLFkS1giPIAhITk5GVVUVXC5Xh7Q9WgRBgMPhgNPpRAy9lYZg3+KTmfsGmLt/7Ft8MnPfbDZbwIrkdp/LkLN0MJfLhYcffhgVFRW47777ggY7gDR9pTaF5XK5gubNxCPvqqyWlhbTfdDZt/hk5r4B5u4f+xafzNw3I0V9lVYo3mCnrKwMixcvRteuXaPdJCIiIoozUR/haWxslBXLKy8vR0lJCVJTU9G9e3esWbMGhw4dQkFBATwej2+jytTUVNhsUW8+ERERxYGoRwzff/+9rA7Oc889BwAYM2YMrrrqKvz3v/8FgIDtHJYsWRKyuB8REREREAMBz9ChQ/HSSy9pPh7sMSIiIiI9Yj6Hh4iIiChSDHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpseAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpseAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpseAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpseAh4iIiEyPAQ8RERGZni3aDdizZw9effVVHDp0CFVVVZg7dy7OPfdc3+OffPIJ3n77bRw8eBAnT57EypUrkZubG70GExERUdyJ+ghPU1MTcnNzcdNNN2k+PmjQIFx77bWnuWVERERkFlEf4Rk+fDiGDx+u+fhFF10EACgvL9d9zpaWFrS0tPhuC4KA5ORkCIIAQRDa39gY5O2P2foFsG/xysx9A8zdP/YtPnWGvhkh6gFPR9i6dSs2b97su52Xl4eioiI4HI4otqpjZWZmRrsJHYZ9i09m7htg7v6xb/HJzH0zgikDnkmTJmHChAm+294I0el0ykZ+zEAQBGRmZqKsrAyiKEa7OYZi3+KTmfsGmLt/7Ft8MnPf7Ha7YYMVpgx47HY77HZ7wP2iKJruw+DFvsUn9i1+mbl/7Ft8MmPfjOxP1JOWiYiIiDoaAx4iIiIyvahPaTU2NqKsrMx3u7y8HCUlJUhNTYXD4UBdXR2cTicqKysBAMeOHQMApKenIz09PRpNJiIiojgT9YDn+++/x9KlS323n3vuOQDAmDFjMGvWLPz3v//F+vXrfY8/8sgjAIDJkydjypQpp7WtREREFJ+iHvAMHToUL730kubjY8eOxdixY09fg4iIiMh0mMNDREREpseAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpseAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpseAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6tnCf8O233+KLL77Avn37UFlZiebmZnTt2hU5OTk488wzcf755yMtLa0j2kpERETULroDnl27dmH79u04duwYkpKS0K9fP/Tv3x8JCQmoq6vD4cOH8emnn+K5557D+eefj6uvvho9evToyLYTERER6aIr4CkoKEB5eTkuvPBCzJo1C/3794fFEjgbVldXh08//RTvvvsu/vSnP+GOO+7Az3/+c8MbTURERBQOXQHPiBEj8Nvf/hYpKSlBj0tNTcXFF1+Miy++GHv27EFdXZ0hjSQiIiKKhK6A5+qrrw77xEOGDAn7OUREREQdgau0iIiIyPR0jfDs2bMnrJNydIeIiIhiia6AZ+nSpWGddNOmTe1qDBEREVFH0L0sPSUlBeeffz7OOussCILQkW0iIiLqcGJtFTwbVgDVlUB6BiwzFkBIS492s6iD6Ap4Zs6ciV27duGdd97BV199hXHjxmHs2LFwOBwRN2DPnj149dVXcejQIVRVVWHu3Lk499xzfY+LooiXX34Z77zzDurq6jBgwADcfPPN6NOnT8SvTUREnZdnwwqgeK90w3kcng3LYS0oim6jqMPoCnjGjBmDMWPG4Pjx4/jPf/6Dd955B5s3b8bQoUPxi1/8Aueeey5strCLNgMAmpqakJubi3HjxmH16tUBj2/fvh2vv/46Zs6ciaysLGzZsgXLli3DI488guTk5Ha9JhEREaorg98mUwkrSunVqxemTp2Kq6++Grt378Z//vMfPPbYY0hKSsLkyZMxfvz4sBswfPhwDB8+XPUxURTxxhtvYNKkSTjvvPMAALNmzcKtt96KDz74AJdcconq81paWtDS0uK7LQgCkpOTIQiC6abjvP0xW78A9i1emblvgLn71+n6lp4BOI/Lbsdj3zvD+2aEdg3LWCwWjBgxAgMHDsRrr72Gbdu2Yc+ePe0KeIIpLy9HdXU1zj77bN99drsdQ4YMwb59+zQDnq1bt2Lz5s2+23l5eSgqKjJkCi5WZWZmRrsJHYZ9i09m7htg7v51lr65lz4KZ+E9cFc6Yc1wwLFoFazpGVFsXWTM/L4ZoV0Bz+7du7Fz507897//RUJCAi6++GJceumlRrcN1dXVAIBu3brJ7u/WrRucTqfm8yZNmoQJEyb4bnsjRKfTKRv5MQNBEJCZmYmysjKIohjt5hiKfYtPZu4bYO7+dcq+3b0MAgAPgPKGJqChNFpNbDczv292u92wwQrdAU95eTn+85//4N1330VlZSWGDBmC22+/HT//+c+RkJBgSGO0KIe0Qr2hdrsddrs94H5RFE33YfBi3+IT+xa/zNw/9i0+V3CZ8X0zsj+66/Ds3bsXGRkZGDNmDMaNG4devXoZ1ggt6enpAKSRnu7du/vur62tDRj1ISIiMgpXcJmP7krLycnJ6Nu3L3744Qc888wzmscKgoB58+YZ0riePXsiPT0d//vf/5CXlwcAcLlc2LNnD6677jpDXoOIiCgAV3CZjq6Axzt/9uOPP4Y8NtyM6sbGRpSVlflul5eXo6SkBKmpqXA4HBg/fjy2bt2KrKwsZGZmYuvWrUhMTMTo0aPDeh0iIiLdVFZwUXzTFfCsW7euwxrw/fffy7aueO655wBItX9mzZqFiRMnorm5GU8++SROnTqF/Px8LFq0iDV4iIiow1hmLIBnw3JZDg/Ft/ZVCzTQ0KFD8dJLL2k+LggCpkyZgilTppzGVhERUWcmpKUzZ8dkIg54jh07hsOHDyMtLQ2DBw82ZeEjIiIiim+6A54333wTH374IWw2Gy688EJcfPHF2LhxI1577TXfsrH8/HwsXrwYSUlJHdZgIiIionDpCnjeffddPP300+jRoweSkpLw+OOPo6KiAq+//jp+8YtfoF+/fjh06BB27tyJ1157DZMnT+7odhMRERHppivgeeutt3D++efjzjvvhCAI2LZtGzZt2oTLL78cU6dO9R2XkpKCjz/+mAEPERERxRSLnoOOHTuGiy66yJefM27cOHg8Hpx11lmy44YNGxZ0ywciIiKiaNAV8NTX1yMtLc13u2vXrgCkER1/KSkpaGxsNLB5RERERJGL+rJ0IiLq3JT7VgnTZkLcuD6u9rGi2Kc74Pn2229x4sQJAG2beX377beoqKjwHVNaGn+7zBIRUXQp960SV8wDGht8t7mPFRlBd8Dz/PPPB9y3ceNGQxtDRESdkHKfquam4I8TtYOugGfJkiUd3Q4iIuqslPtWJSS2jfB4H48C5VQbp9bim66AZ8iQIR3dDiIi6qSU+1YJ02ZB3LiuQ/ex0hPMKKfaOLUW35i0TEREUaW6b1UHBxa6ghnlVBqn1uKaroDH4/Hg3XffRa9evXyjPaIoYuXKlbLjUlJSMGvWLFgsula7ExFRHIvrKR89wYxyqi1KU2tkDF2RyRdffIG//e1vSE1N9d0niiK++OILHDx4EIcPH8bhw4fxySef4KOPPuqwxhIRUezwjZI4jwPFe6VpqXihDF5UghnLjAVA/mDA0QvIH9whU2t0+uga4dm1axfOO+889O3bN+CxgoIC9O/fHwDw3HPP4aOPPsLo0aONbSUREcWeOJnyEWur4N6wAsfqauFOTYNlxoKAvCG1YEZ1qo3ilq6A5/vvv8d1110X8rjBgwfj448/jrhRREQUByKc8jldU2LekSg3AOCoL1+HwUznomtKq6amBg6HQ3afIAj4zW9+g/T0dN99Xbt2RW1traENJCKi2BTplE/AlNicG+AunAOxttrYhsbJSBR1LF0Bj91uD9gjSxAETJ8+HRkZbRF9Y2MjbDYu/CKijiPWVsFdVAD3glvhLiow/uJIYRAje3pA4CECJQeMzwXSka9D5qcr4OnVqxf2798f8rj9+/ejV69eETeKiEhLXCfKmkzE74VW4GHwCIx3JMqa2ZvJx52YroDnnHPOwY4dO1BTU6N5THV1NXbs2IERI0YY1jgiogCcnogdEb4XwrSZgKByGTJ4BEZIS4dt/kpkP7Udtvkr42fpPBlKV8Bz2WWXQRRFLF68GJ9++imam5t9jzU3N+OTTz7B4sWLAQDjx4/vmJYSEQGcnlARtWm+CN8LceN6QPT43SMAuQM4AkMdQlfCTbdu3TBv3jysWrUKq1evhsViQVpaGgCgtrYWHo/Hd4z3fiKijqBnOXG0RKsQX7hbIBjVzojfC+WIkKMnrItWh90OIj10ZxgPHDgQjz76KN5++218/fXXcDqdAIC+ffti2LBh+MUvfoGUlJQOaygRERDbtVGitvdSmFNLRrUz4vdCuay9uhLuogJfABaNADKuq0dTUGEtqUpJScHll1+Oyy+/vKPaQ0QUv5SBRkkx3Atu7fgLZ7j1cGIkD8o3QlRSDLhapD+tyc+WGfPhWfSHtl3TT1MAyQ1DzSvsTa/uuOMOlJSUqD52+PBh3HHHHZG2iYgoPikDDVfLaVlNFnY9nBjJg/KNEClfv7pSCjy8wY7f/R0uRoJBMl7YRXMqKirgcrlUH2tpaUFFRUXEjSIiikeynJbqSing8erAC2e4U0tquTdq2y+ctqkctREqrc08w6CcnnIvfbR9bSFTMLRK4PHjx5GcnGzkKYmonZiLcPr5Bx7uooK2qREgpi6cagGSt73K7RcipedzqBaAeTYslwceScntr+QMAM7jcBbeA9y9LOhzYjkpniKje/PQd99913f7ySefDAhsmpub8cMPP2DIkCHGtpCI2oW5CNEVdxfODprK0fM5VAvA1H5+/oGSroBe0Qd3pRNCiPbGclI8RUZXwNPc3CzbI+vUqVNoaWmRHWO323HBBRdgypQpxraQiNqHuQhRFXcXzhArptqtnZ/DUD8/XQG9ok/WDAc8oM5KV8Bz6aWX4tJLLwUAzJo1C3PmzEFubm5HtouIIsVchE4l0ilM34jKD8VAi9+KqbUPhF0bR9aWOsWG0kZ9DnUEUspRIseiVShvaDLm9SnuhJ3Ds27duo5oR1ANDQ3YtGkTPv30U9TU1CAvLw/Tp09Hfn7+aW8LUbyIuykVAtD+wCXSKUzv9guuGVfKHwhjab2v7d5l5l5JyUBqmrGfQx0Bvf8okSAIsKZnAA2lzG/rpNqdtFxTU4OKigrZNhNeRufx/PWvf8WPP/6IO+64AxkZGXjvvffwwAMP4OGHH5bt1k5EbeJuSoUARBC4qIx4hHNh967SQovyd7ooBRbO4/CsfQCW2fdqnlPWdn+pabAuf0L1NdsbeEQS0DO/rXMKO+CpqqrCY489hm+++UbzmE2bNkXUKH/evbrmzZvnC6SmTJmCzz77DG+99RauueaagOe0tLTIcowEQUBycjIEQYAghEpZiy/e/pitXwD7Fq/M3DfgNPRPJXDRei2xpgpu70VfZepI7cJum79S9VxurWDF35GS4OfUys9Jz1DtgzuM9ikJ3brDEuJY/5+P0LosXRCEsH7G8cDM/+eM7FPYAc9TTz2FQ4cO4brrrkO/fv1gt9sNa4wat9sNj8cT8DoJCQn47rvvVJ+zdetWbN682Xc7Ly8PRUVFcDgcHdrWaMrMzIx2EzoM+xafzNw3oOP6dzzDgWa/qZqEDAd6ZWWpH7vmXrj9ghQhOQWWbt1hzXDAsWgVjs+5sXWZucRaV4ssjXMdramSJ/QKFgAiIIp+9wmw1tVqnvN4z0xZ22FPQMKAwXAsWiVNJykcC3IuI/j/fMTWZemZq54KaGdCz0zNn3E8Mfv/uUiFHfDs3bsXv//97zFu3LiOaE+A5ORkDBw4EK+88gp69+6N9PR0fPDBByguLtZ8cydNmoQJEyb4bnsjRKfTGbC6LN4JgoDMzEyUlZVB9P/FZALsW3wyc9+Aju+fS5Em0NzcjNLSUvVjy8tkt8UuXSEs+ys8AMobmuBOku9v6E5K0TyX52RN4J3ZfYCjh9tu98yC+0S5/JypaSgtLZVGU+rrAZsNcLkBmxXo3Q/uW+ZKicINga/rTk0DcDTgXEZR/nzclU6UlZXBc8tcYP2DbQUJb5lr6Ouebmb+P2e32w0brGhXDs8ZZ5xhyIvrdccdd2DDhg34wx/+AIvFgry8PIwaNQqHDh1SPd5ut6uOPImiaLoPgxf7Fp/Yt/jVYf1TTk3V1cpeJ9QKqFBt0nw8JVW+lYPoAZzy4AbO40BTY9vt1mKAoihKU0clB9oec7mAkgNwr39QMz9GteKzkT9TtWXpogh07RbQJjN8Vs34f87I/oQd8Jx//vn44osvMGzYMMMaEUpmZiaWLl2KxsZGNDQ0oHv37nj44YfRs2fP09YGIopf0V6VE9brK1cf1dXKVkkFJAYnJUvBSn0dUOmU185RCZ40paUDlYqtgZQJzP7BDgCkprX1Qyt/J0jdnY5OrOeydPKnK+A5ePCg79/nn38+Hn/8cXg8HowcORKpqakBx/fv39+4FvpJSkpCUlIS6urq8NVXX2HatGkd8jpEZC6ne1WOMsDxjnboeX3ZRbquVhp1aWzwPS8ggGhd7o3KCum4yoq28+usxSTWVgGlPwY+kJAYuIGnP//zKV8rxGvqIdZWwbN2GXCkRLojJxeW2Yt1B6tay9Kpc9IV8CxYELjc79///jf+/e9/qx5v5CotANi9ezcAIDs7G2VlZfj73/+O7OxsjB071tDXISKTOs1Vp5UBFmyKKXbF62uNALkX3CoPOLwBVKiNNltvC9NmQlwxD2hukpKPD+6He8aVQFYOYLVJAZU3IFOO3tjsEOavgrhxHXBwH+DxS2m2WID+g3xLwcXaKukcNrv0OjYb0KUrkOGIqO6OVNPHb5qs5AA8i26HpfBx1s2hsOkKeGbMmNHR7Qiqvr4eL7zwAk6cOIHU1FScd955mDp1Kmw2Q/c+JSKzMqjqtO4dxUMFVIrX1xyBUmm3ro02W88vblyvyMtxAx438KNf/qNaQAYAWTlSsFNdCSjTKBISZSNUAYFJ3gBjRtDUfo6NDaybQ+2iK2KI9kjKBRdcgAsuuCCqbSCi+GVU1WlvYBJyR3FloJKTK416aL2+xgiN1gaaejbaVD1vOMpLA0d9vFwt8Bz9QQqoKp3Sn2D90UFtlEtzmkz5ekQ6cIiEiEzPsORYnVNjoXb6DqAxAhWs3boSobUCBqWcXClPxn87iICKy35cLmmqTCu/px0jaGqjXJYZC+BZ+4B89AiQErSJwhR2wLN+/XrNxywWC1JSUpCfn49zzz2XU05EZC46p8bCDbD0jkAFLEn3BhzO4/Asul22X5WQlg78bhqwerG0xNxfQiKQ3deXwyNMmwlx+T3ygMeeoBjhESCb22rWWO1ks7dvBE2t+nFaOqyLVsNdcLN8BVlK4GIZolDCjki+/fZb1NfXo76+HhaLBV27dsXJkyfh8XiQkiIVuXr99deRnZ2NJUuWID093eg2ExFFRVsScDOQkABh2ixjzqszQNLcqwoIWM1lLSgC1j4QGOwAUgJyXa0UILlcEJfdLQ92AOm2d8l7hkO+0gxQCYha5ea3L6FYI5gUa6sCR3QyHL7HuAko6RV2wDNnzhw89NBDuPXWW/Hzn/8cFosFHo8HH3/8Mf7xj3/g7rvvhtvtxkMPPYQXXngh6gnPRERGkSUBNzZAfOZRuBW5OR16wdWbG+OthhwkB8e7IagmtxtwNwA5ubAWFEGsrZaNQqGpUZ78bLUBeQNkK7fCCUa0Rrk8a5fJp84Sk9oe4yagFIawA57nnnsOv/3tb2VJxBaLBaNGjUJNTQ2effZZPPDAA5g4cSL++c9/GtpYIqKoUgYc/nkvOi+4EY1KKEdBkpKlURpl4FJ1Au6iAvVzWG2A26Xv9YC2Je6KUSj3glvlx3XrDgDwLL8n7NpDauf38dbg8b2wW7vYYQeXG6D4Zgn3Cd9//z1ycnJUH+vTpw9KSkoAALm5uTh58mREjSMiiimhknF1XHB9oxLO40DxXmlUQ0GsrYK7qADuBbfCXVQAsbYagDQKgvzBgKMXkD8YlsLHYV3+BKC2o3TxXmnaKaABKlNcwWj1WXl/fZ2sXwGBSkcEI8o2RFDkkMwv7IAnOTkZ3377repj33zzDZKTkwFIG955/01EZAbegMOa2VsKPHJy5QfoueDqGJXQCorEk9VSIFFZARwpgaf0iDSSo7XdkFpwo5bToyV/sGYCsjL4CkgkdilGkdobjCh/xn63AwLACIockvmFPaU1evRobN++HaIo4vzzz0e3bt1QU1ODjz76CP/85z8xfvx4ANJ2FL179za8wUREHS3Ylga2+SuRlZWF0tJSeGqqwq/vE2KvLCEtXTMoElcUyHKI8NAiaEc7UB/50Ss3ePHAgCmuogLFXlxi25RbBLWPLLMXa/6MO3ovLjKXsAOea6+9FlVVVdi2bRu2bdsme2zUqFGYOnUqAGDgwIE455xzjGgjkelxtUlsUd3SQCUHpT0XXF9ybmUFUFWpvrpKa/l7wFJwlWAnd4B8ywhlDZtgBIsUJCUkQJh+p+bnUnZ/apr03NpqBCxdT02TptwiwKCGjBJ2wGOz2XDnnXfiyiuvxJ49e1BXV4fU1FQMGTJElttzOndTJ4p3XG0SY9TyTQzLQWkNCGprAqeXglRY9hwt0Zd/Y7P5ggzfyqri7yALRASL/LVtdml/rOYm6bDGBogP3AXRbpfX+mn9XAbsFaZFxzQWg306XdpdGTAnJ0czeZmIwsTVJiGd1gujWoXiSHf99rb9ZI32cnFFhWXv8zzL7wFOVKg/R8lv2wXveQIK91kEQGjdP6t1us4z5wb5edyuwNVcJcVSAnWwz6fNLvVD5zQWg306XVgKmSgWGLS5pZmdzgujb0sD/xyeSHf91ioY6JWYBLhcUj6Pd5roh+/DSzIGgKoTEGur5cFghkMe8LjdQOuOYLDZWo8Nkgvk5WqRRoyCbVkRboV9ZfBUWSHlA3HEhwym65N59dVXo7CwEPn5+bj66quDHisIAl588UVDGkfUWRi1uaWpncZRMO+WBgDkoyzpGbDOXAhkZYV3wlBtzR8cULem3URPQDAo+3xVV8qrKpcUB9bUCaa6EpYFq9rO5w3O6mrbtrvw5iQtuh2WwsfD20es/lTbKFWMj/hwOi6+6Ap4Jk+ejIwM6RvnlVdeCSGSzH8iCsDETB2iNAqmHFlyL7wNx9Iz4E5NC3qBC9j3SovNFli1WA+rFejTX/q3MjFZEWD5f77chXPkx3urLuuVnqH5eXUvuFVeFbmxIWTAogz2UemUnyOGp3c5HRdfdAU8V111le/fU6ZM6bDGEBFpidooWKUid6axAe6yowCOtu7oPV91CXvANFZSsvSnpgoQ/aaPXK7wgx1Ampay2aSfy6Lb5UFCalrk00JWG9AnT1p9VV/n21Mr6M9dbaorRMAScnl7LE/vMvcurhiaw7Nnzx68/PLLWLJkiZGnJSLSHFXo8GmF+lPaj1VXai5hD7j4paZJIz2ijlwZvVp3FMcd97btii60rrby39ah4GYgN18KVoKNNvlzu6QVX0VP6W6OavAVZsASV9O7zL2LK4YGPLW1tdizZ4+RpyQiCqrDpxVSUuUXcH/pGerf6g/uAxIS5fcFW53VXnW10qqpx5a1JTeLHuDYYflxrpa2is3BEo6VwhyxENLSYSl8PKKAJZ6md+MqOCOu0iKi2NGu0Zog0wqGjP4oVzglJkn1cERRmo5S27zT45GCJP96N2rBjiAAZ/SUdjfXM/KTli6N3vhVW/ZsWK5SkFCDf8JxSbE8eTkxKbCN7RixUAYs3n3BghYvjNOE33gKzqgde2kREXWUYBtram2oGWwDST0bdWqd33P0h9Z8EqeUe5PRQ1pNld0HaGmWggXvtFHuAKn+jFKo9R1ZfaQigd10BhZNjW2roryqKwNHk7T4JRxbip6S74XVU7HyLDEp6IiF5vuhoPUehPPekLno/ewYjQEPEcWOIKM1WhfIoBtI6kgq9f7y9RTcIju/uGKedLuyQhpR8f5SVv5yrq2WVlqlZ0iBkezkIfpbXgqxtkpKZNbD5VIP8G68K/RzBQvwu+vb+rv8HgCAZcEqaZTi1En58V26Bh1x0R2waL0HTPjttKIV7HJKi4hiR7AkUI0LZNBpBR1JpZpFAZXTRK15MAFBTX2dYiWXIAVAVquunB3PhhX6iwsKgvq2E4tuD/1c0QM89md4cnLVc56Uydn1dcHPpzdg0XoPmPDbeUUp2NUV8MydO1fXyRoaNBL7iIhCEGurpBEMm9+WB/6jNcpcmaoTcBcVBM390JVUqvXL1p6gHrA0NUJIToGY0gXo7gisGwNRnhsTTM+s8H7ZZ+XIC/65XNJIjVZStVJzk/bFJilFfp7mpuA/X50Bi9Z7wITfTixKwa6ugCc1NVVXscGuXbuiZ8+eETeKiDqfgOXdvi0PNLhdvuFwy4z5qgmwupJKlb98bXYgNx9oagJ+PBh4vChCbKgHeveT9qlS1o0Jh/M40Le/vlVTSclSbRw9m3Z6j29shGxeLSFR+2LToBjh8Xh8P1+1n6HegEXrPWDCb+cVrWBXV8Bz//33d3AziMjsQq7KCTXMrVU/xlsLp51L09V++Qpp6aG3W2jdSNP3fOWqJ6sVyMmVcnxOnZQec7vl52huUq9dA0iJ0IBUlNDtlhKllRWVNTtlhaXwcYgnayGuuEca2UlIhDB/FQBRyk9qbvKNYrkX3Ko9/aYxAsWAhdorWp8dJi0T0WkRMlExyGor1dv+90eQE+BbtbRgpdTO5fdIozbK1VDK3B1XCzyLbvcl/+KupVJisFdTY1vhvj55gcFOK8+G5VIg4l3pZbMDuQNgmb1YygVyuwCIms9X1X+gVBOnd19Y126C9fFtsK7dBEvvvhA3rpeCK4+nbUuLYKNFzK0hk9AV8DidznadvLKSWfdE1CpEUBJ0tZX/4xk9ZMvELTMWBF6UqyvDXu7qWbtMFpDB7ZK1R5i/KnDpeesmmSjeCzz2QGDycckBafSkpFj7hYv3Qnzm0baVXrn5sMxeLI1+BQvcbDZpiktJsECYNkv7eXqDQZtN9X0gile6prTuvPNO/PKXv8RvfvMbZGZmBj3W5XLhs88+w5YtW3Deeedh8uTJhjSUiE4/Q4vDhUhUDDXMHezxgGklb2XhtQ9IByj2uVLtg/cYr9IjsG54RXaXOydXe1pJLXHY5dKXn3OkpG06zH9KLlhV5Jw8oOxI6wiQH9EDceM6QFH8T9dGpknJsOrYGJUoHukKeO699148++yzePPNN5Gfn4+hQ4ciLy8P3bp1g91uR11dHY4fP479+/fjq6++QmNjI8aPH48JEyZ0dPuJqAMZuW1DexMVfRfrygpp6bRsE0tRFpAhLV2eQOwfSAC+fa7UkpxjSusojGXGAiloO1IiVWK22YDkLkBjPXDkkBRQqTm4T7bCSnUjU5crsNJyVg7QUB+yeWaokkydjyCK+ney+/LLL7Fjxw58/fXXaG5uDni8Z8+euPDCC3HJJZege/fuhjbUCBUVFWhp0blcNE4IgoCsrCyUlpYijLcyLrBv0edecKt8hMHRS6oMHEQkfVO7kHo2LFevk5M/WPrb/7GERHn9HKstcATE0UsKjvyflz9YCgD8R29yB8C6aLW8TdWV+pec6yIA+T9ty6Xxb3feAOB306R9srwJxj2zgNIj+tuQP1haSabyPvq2mPAGi8r+tz5XjbuoQP7za13ZFsuBT7z8n2sPM/fNbrejR48ehpwrrMKDw4cPx/Dhw+FyuVBSUoKqqio0Nzeja9euyMnJQUYGk9uITOU018tQG1HSzDlRu19ZLFAZ7ADqSc6VTml0yK8GkDD9j9KFXbn6So+MHtLUUdA9rgQI96+FpXdfuP98V2C7i/e27YAOBAZFenj7qfI+KqcIA1alta5CUw1glD8/v81JlUESR4MoVrSr0rLNZkN+fr7RbVHldrvx8ssv4/3330d1dTW6d++OsWPH4oorroDFwkVmRB3ptNfLUEts1spj8QZfoXJksvu27R4uCMDvrge2PSd/nrJass0mrWZSG1nSI8MhBVDBlpHn5sPSu6/079LD6sforcCspXWlma73UVnY0dWiPYWp9Z6oBKEdvps9kU4xv7XE9u3bsWPHDsyaNQs5OTk4ePAg1q9fj5SUFIwfPz7azSMytdNeL0NlJMJ3sa50SoGJLIcH8mmZgABFkAcyogg89mdYCh+XP09ZLbm6EvCEsQxc6eA+wGINfkxzc9su4lq5OP67rSvZ7JCqOms81/807X0fK9VX6GrWHlIbAeSeWRQjYj7g2b9/P0aOHIkRI0YAkPKEPvjgA3z//feaz2lpaZHl6giCgOTkZAiCoKtidDzx9sds/QLYt3gVSd+sMxfCvf5BXyBinblQqiczf6Xmc7yPiTVV0rSM/zRSn1zg6A/yJzQ3wdKtu+ycrmV3ywOjrmlSrkx7eTzSn2CO/RD88aRk4JrbgGfXSkGPIEg7tTc1+X427vUPBh+FqqsFaqvh9gvuvD9T1WOV6utU30eh9ecn1lYHvl/K49Wm06Lwuef/ufhkZJ/CSlqOhm3btmHHjh1YtGgRsrOzUVJSgsLCQtxwww0YPXq06nNeeuklbN682Xc7Ly8PRUUcQiWKJe6qE3A+OA/uSiesGQ44Fq2CNUSOkOw5aekQIcJTWwNrhgOiqwUt+/f4jhWSU5D15DaU3vI7aSsIv/tzNr8nO5e7ulI+wmO1SSM8Uf71KCSlQGxsa3vCwKHo9fCzvtvu6ko4C+9B857/QW1rdvvAIRBsdjTv+artHEPORq9VTwUce/yem2XHAYDF0RO2nllhvUdK3jZGcg4iI8R8wCOKIl544QVs374dFosFHo8H11xzDSZNmqT5HK0RHqfTacpVWpmZmSgrKzNddj77Fp/09s21Yl7ASilbkJEc1ef4s9nl0yuOXrCteBKeoz9I1ZBbt1ewLFgFITUNbrXtHGKdzQ7bX7cE3O2ad5P6fl55A4CTtQErtGwrngw4VKythnvhbfKfSVKy/LaO9yhW8f9cfLLb7XA4HIacK+antD766CO8//77+OMf/4g+ffqgpKQEzzzzjC95WY3dbofdbg+4XxRF030YvNi3+GRU34xcCWPUuUL2TSW3I+TPImj+h+K5qWlSgFRdKa26mjYT4sb1Ul2bkzXae0fFFAHKfqn+jDIc6gHPyVrVKSXVc3TtBuuDf4P1yYfQXF6mmdt0uv8vGr3Ki79P4ouR/TEk4GlubkZFRQWysrIMXzm1ceNGTJw4EaNGjQIA9O3bFxUVFdi2bZtmwEPU2Ri5EkbrXOFceNxVJ9qCDa1jdS55110l2OWSRiRak5rhcsn6IS6/J06CnFZJyUCPTEV9Hqtvqbjs55KaJl+N5uWf9K1jpZ2Qlo5eq57y1XMJ2Ak+ClNRwT7bXPJO4Qg74PnXv/6FU6dO+baMOHjwIAoLC1FXV4eePXtiyZIlhg0/AUBTU1NAEGWxWEwXxRJFxMiVMMqRgtaVOuEEVc4H54U8Vu+FOKBKMASp4rDHHZgY7B2NsFgCfwbxFOwIAiyFjwOAfCf1pkZphMpmk6+Qch6X9r1a/Zzqzu+nuzq2oYJ8trnkncIRdsDzn//8BxdffLHv9j/+8Q+kpqbiyiuvxBtvvIEtW7bgtttuM6yBP/vZz7BlyxY4HA7k5OSgpKQEr732GsaNG2fYaxDFPSMLBNafUtyuk/4OI6hyK5cztx7brm/kAa8jBi8E2NgQf7k5SqIobQlxtARoVARqyu0yvKordQU34bwHp70sgZpgn20ueacwhB3wOJ1O9O7dGwDQ0NCAPXv24K677sJ5552H1NRUbNq0ydAG3nTTTdi0aROefPJJ1NTUICMjA5dccgk3JSXyY+g38ZRUecCQkir9HUZQZc1wwF12NODYdn0jD7aBpl6CEPUVV+GRluKKKwqgtvpKVV0tPEd/kAomKoIZWZDjn7/kPA7P2gd8W2i4N6zAsbramNo8NOhn+zRXAqf4FnbA09LSAqtVKqi1f/9+iKKIs846CwDQo0cPVFdXG9rA5ORkTJ8+HdOnTzf0vERmYug3cWUCbIY0RR1OUOVYtArHltwZeGyQb+RaIw/CtJkQV8yLbNQmroIdSPV2APVpOK0d2xsb5D8nv4AycFrQT+su8d5jpHKLR2NmeijYZzsmptwoboQd8DgcDuzduxdDhw7FZ599htzcXKSkpAAAamtrff8motinFmRoXUSEtHTZLuPSruPqowDW9AzY5q8MzLUL8o08YPRHLVels0hIbP1H4CotuF1A7gD1qS3l3l3egFLPVE87gtFoCxXoK9vtXvroaWwdxZqwl1RdeOGFeOWVV1BQUIC3334bF154oe+x77//HllZWYY2kIg6ji/IcB73bf7ovYhYlz8Ba0GR7MKmdnw4LDMWSDuTO3pJSbatwZRYWyUFNv6OlEiv1dmCHQCorZb+Tu8e+NiPh6RAUG36xhcotfIeE2yqJydX/Ri1YLSd73u0KNvtLLwn2k2iKAp7hOeKK66A1WrFvn37cO655+I3v/mN77Eff/wR5513nqENJKIOFCLpU/kNOWBvpTCTRP2/kUvnXt623NyMgY3VKq0kC3dKzZso3i0DqDoR+LjapqpJyRDmr4K4cV3A6Jxs1K51Q1HU1aoeY/XL4ZG9nvL1dYrq6JCine5KJ8y3+QLpFXbAIwgCfve736k+VlBQEGl7iOh0Ul40neVwF86BZfZiaZWQYpoJScmBzw9B64IXNK/EZgeycuQ1aOKRWrAjCNKfYHtteRPFtWjU1xHS0gGVKR49OV5CWjps81ciKyvLV4fH//Xamxwc1aXjinZbMxyIcP95imPtrhJYX1+P3bt34/3330ddXZ2RbSKi08QyY4EiiBGBkgNtUxbKb/KuFil/RDEl5Xt2bRVcK+bh2M0T4VoxD2JtdeB0yKLbIdZWBx8lyMkFykuN6GLsEcXQG4tmOKRpvtaEYhmbzbdDutbUo9G0piJ1ieLScWW7HYtWnbbXptjTrkrLmzdvxvbt29Hc3AwAWL58OVJTU/HnP/8Zw4YN0xwBIqLYIqSlS1McyhVQ3ouS8pu9ywXYbLAuf0L1fGorfQIucI0N0v0By81bCwp6c0riqVCglnCmspKSpffCf/RGbZrP5ZKC0tbl5LKX0xhNi3RaKaJVgFFcOu7fbkEQpE1LG0waSFNIYY/w/Pvf/8bmzZsxbtw4zJ8/X/bYiBEj8MUXXxjWOCI6DdQuQNWVcBcVQJg2S5peUjymSe3bvMb5fd++bd7vXa0FBd0u4EicT2XplZTcNmpS+Lh8tCbUSIjK6I9WcnE0k44jGh0iMlDYIzxvvvkmJkyYgGnTpsGjGJb1zv0SUfywzFggLQE/UtI6VdIaeBTvlRJgc/PluTatwZCu/bFO1gDJKQhYXu23rN1TcLP8HPGet6NXUjIshY9rj7SEUXDRN4JzcJ/8Aa1l6adxWikmqjUToR0jPOXl5Tj77LNVH0tOTkZ9fX3EjSKi00dIS4dl9r1SYGNRrGGRjcS0jvS0BkNqowQBOUFNja0BjGJqJ8g5Oo3UtKDTSsK0mYAQ5Fe0d+oPfiM4ytwgrWXprEhMnVDYIzwpKSmoqalRfay8vBxpaWkRN4qI9NObnxHsOM0VU+kZvm/o7gW3ykccSoql+2TnEn0JtbqUFJtjNVaA1nykM3oB5UfVc3lS01p3I3dKy9C9u7z/bhrw2DL1ytI2uxSsKKsKK0dsLBag/yD1ZekhKhLr2umeKA6FHfCceeaZ2L59O0aOHImEhAQAUjKY2+3Gjh07NEd/iKhj6Fn2K9ZWwbPoD6rbDgAIecEEoJLA3CLd9tuPybNhRXj1dFwtJgx2AN+0YNeusC7bLt1TW902dQgApT/KE7MbG6QtPVYvBkSNVVy5+erTQ8r3pv8g2XHhTCvp2ek+lsVqVWiKvrADnquvvhoLFizA3XffjXPPPReAlNdTUlICp9OJP/3pT4Y3koiCUMnPCCgY6HJpr8QCAi+YnsCRGtkogbMcsmmqkgNtoxXUxu9nLKSlty4pbw0ItQJDrWAnKVlzZMa331hzE5CQKCWb66T8rFi9VZ69Du7TztmKQVGt+0MxLeyAJzMzEw888ACeffZZ/Pvf/wYAvPfeexg6dChmz54Nh8NheCOJKIjUNHmwkpoWWDBQudIKkOVxWGYsgGfR7X5Bkehb+gybLeDbsnvGlYEX7OK9gGYdW5U9ocwgowdw9S2AVi5SsmJvQT3JwoIlMOix2YHMHIgna1SLDYob17e9d40NUrK5ykVebfQjIEBQttnj8eVbxUXgEMUEbYpt7arDk5OTg0WLFqGlpQUnT55Eamqqb3qLiKKs9EegS9fgxyhGCzTr8fjvyu08LgVFqWnSlgmqIxQaQY3NGl5uT7xISwf+72Htx48flQcZJxX5j4lJ0nsly+G5Hnjsz9JojQgp+HG1ACUHNHdD13uRVxv9CNhOpLFRSjxvbpInQcdL4BDFuj8U29oV8HjZ7XZkZPDDRBRVdbXy202N0tYF/nJyVUdqZPQsg25saLvgJiYBbre+nB2rzZwBjzIPR6m5OfgWGll9AooHAgDWbgKAwERxrWlJlVE+VVp1kvyfK3qk10lKlr9enAQO4SRoU+cSdsCzefPmkMdMnjy5XY0honZQC1RSUqUgR2fiplhbJQUkVptU+E+PpiZpROLUydBVkVua9Z0z5imm5vRUgw42MqLMl1EKFYSGG4SojH74AoSD++QjOiqfoXjAuj+kJeyA5+WXXw55DAMeotPHMmMBPAtvk198w/yl79mwQj59pYsorSrS9QJm2bIxzDwke0LwoKXSGTQhWDMYUa6iU47yKW8rz6cIhK0FRVLSuf9IVIaDgQOZStgBz6ZNmwLuq6urw6effoo33ngjYLsJIupYQlo6kNVHd8CilrgadBTCapVWbWmtHiJtPTJb9x9rTRr3eACP2+8AMSAhWD2xeLk8GElIlFWr1spbUTuXVhDjfR1rXS3cqWlxM6JDpFe7d0v3l5qaiosvvhijR4/G008/bcQpiSgcOr/hA4Bn7TL5vkprH9CYGmktnufxMNjxZ7UhYcjZ0t5Qsp3mVRw7LAWirhbpj9biDr+AU23fK9l+VN7cGq3H/farCmcPLSEtHbb5K5H91HbY5q+MiyXoROGIKGlZKT8/H1u3bjXylESkh/IbvreKr1oOj3LTySMlsBQ9JV0MDx3wy+EJs2pyZ9E3D46FK3Hs/rv0T+l5uVqA3AGt+5b5JXv7B5wqicX+eSnuBbfKk4kVjyufG/S2DizkR2ZhyAiPV0lJCZKSkow8JVGnINZWwV04B+4ZV0p/CudADJXQ6kf5DR9AWLtj+y6YWmV0SGKzwzr7vrZqxOHmJrlcgM0GS9FTASMyYm1VW5DqL9Q+WMESlw3YQyuaO60TGSnsEZ5333034L6WlhYcPnwYO3fuxIUXXmhIw4g6k4Ck4SBF/9Qov+G7F9wqP6D1IirWVkkJr/6y+rT924gRHZsJlqCrFf8DgJxcuNc/CPfB/e0/t8aITEDSsM0O5OYH5NKEs+zakCXaLORHJhF2wLN+/XrV++12Oy688EL8/ve/j7hRRLHO8GF+tYuI/7SHzhL5snb5q6uF5+gPbdsPyJ8Fz9ESiCsKQjRSAPrkAuWl8hVhggXS6iUBSO8O1J+K/4BHLdjx5uto1dTxZ7UBdru0tLu+Tl89G5WRHbX3O5xl14Ys0WYhPzKJsAOexx57LOA+u92O9PR0I9pDBuLce8cxfL8ePUX/dHyz1ixy19ggr9Lrr/SIFOyoPSYjqm/0eUYPqf3Fe4GqEyHbGLdS04Img8v86QFYBw2V/g+uXdaWN5WTqz3KEqOBBQv5kVmEHfD06NGjI9pBHYCb6AUXUUBo8DC/ZcYC+U7aObnS3/7TXHougMHaESygCRj1CUN6BnCivP3PjxfOciAxUd+xj/0ZWLspcKrSZvN9xpSfP2HaLGkPrBgLLFjIj8zC0FVaFGM6ydx7sMAl2GMRBYQGfxsX0tIDthgQa6vD/2atZ6RIKScXKDuiY4RHjSDtkG62kR2LVVEvBwBExVSeIOU/HTsc+HxvABnk/6Dy8yduXBdzgUUsjBLHQhvIHHQFPLNmzYKg3JtHgyAIWLt2bUSNIoPE6BC50YIFLkGDmggCwtMxzK/2zTrUL3/LjAXwzJ2uXTfHZge6pgHVVQBEIDEJwvQ7pXMvn6tvqwR5i8Jfmh0P7HYgq3/wYo5ia99zBwQel9A6EhTs/6Dy81ZZoV1KIEpiYZQ4FtpA5qAr4BkyZIjugIdiR6eZew8WuAR7LIKAMFrD/MF++fuCoWBFAr2jOd5jGht8IwvuPnn6EnLjidC6Ii3cwonJXaTVZqE0N0GY/keID86VTwsmpcBdVBB8mkr5+as/JY2WAbFzYY+FUeJYaAOZgu4RHoo/nWbuPVjgEuSxuAwIlb/sS4qlJejpGdLKqFDbSxw7HJivU+mUlquXFBvb1ljRnirR9XVAsY4LqyhC3Lg+8GdafQKoPhF0mkr5+UOlUz6t6PfeRm20JxZGiWOhDWQKgiiKYe6GF78qKirQ0tIS+sA4IggCsrKyUFpaCrO9lXr7ppbr0pbDo/1YNLX3fQuo1eLPZpdX7yVjqe0kb7UC3R3aeVOOXrAuf0LX6YO+t/mDg3558ZUVaG4CEhIhzF8FS+++ul7Xn/JzGQv/f4xqA39Xxie73W7YYql2Jy3X19fj2LFjaG5uDnhsyJAhETVKadasWaioCMwTuPTSS3HLLbcY+loUf4KNZIUa5YpGQqRYWwX3hhU4VlsFd22NVKslw6H62sr24Xe/Bx57QLqweUSEvXs3BbLZpemrUEnbymAHkKbMgiWKhzNNOm1mW50k5XsbYhpHVlagsQHiinuAtYEbPYcrFkaJY6ENZA5hBzxutxtPPPEE3n33XXg0yqqr7ageieXLl8te6/Dhw1i2bBnOP/98Q1+HOh+tnJhwA6Fwjve+pm8NUGMDUFkBT8HNvsq6vpVka5e1TVM5jwMPLdTujNUq/c1RHinQCJXrYbUBeQOkbR1O1rQFGwmJ0pYRepbqZ/WRT02lpkn319WGPU0qblyvHXSFCpyUbfW7zVVORJKwA57XX38dn3/+OWbMmIF169bh5ptvhtVqxTvvvIP6+nrceOONhjcyLS1Ndnvbtm3o1auX5khSS0uLbOpKEAQkJydDEATTJV97+2O2fgGnqW9aOTF1tW0Xn9ZAyDZ/peZp3CqBk+bxWhdiV4tvryLfc5UbfWoS2rHCysT0LLHvmwfb/JUQa6rg8S/K2NgA9M0DfvwhdP6P1QpLt+6wBPls6Kb8XNjsUqCTngHrzIXB/x/YE+Tvvz3Bd3w4n03+PolPnaFvRgg74HnvvfcwadIkjB49GuvWrUN+fj769++PX/ziFygsLMS3336Ls88+27AGKrlcLrz//vu47LLLNH8QW7duxebNm3238/LyUFRUBIfD0WHtirbMzMxoN6HDhOqbu+oEnA/Og7vSCWuGA45Fq2DVOZVQlnEGWvynI1wtqtMT1rpaZGVlaZ7nWF0t3DqPP94zE81BauX4P/dH3f/ZObUloyPgsTbUIysrC8fX3Au34nhrczN6bXwTzsJ74HaWw3OyBkJqGjzVlUCL3zT+4YOwrLk3rM+cFuXnImHgEPRa9ZSu55b27guX3/5ett59fZ+hcD6bXp3590k8M3PfjBB2wHP8+HHk5ub6gg3/kZRLLrkETz/9NK699lrjWqjw6aef4tSpUxg7dqzmMZMmTcKECRN8t71tdTqdpkxazszMRFlZmemS1fT2zbVinu8brLvsKI4tuTPoaIzsuTo/D+7UNJSWlgZ9HDiq63jxlrkQ1j8IS2013BXHA0YRZM/N6QccCrHyitrFXenEsX174S4vC3wsNQ3lDU3A3csgAGidLJRGgvwTiz1uNO/5KqzPnBbxlrnA+gd9U0/uW+YG/cz5cylGh1zVVb7nhvPZ5O+T+GTmvtntdsMGK8IOeJKSkuByuSAIAlJTU1FRUYFBgwYBABISElBXV2dIw7Ts3LkT55xzDjIytL9N2e122O32gPtFUTTdh8HLjH0Ta6pwfPUitJSXBc89UBa+q3Tq/1mcDLI3UlKylJPR+trBzqm2xF3z+K7dYJ2/Ej2TEnBs+gTA5R/wSJWLXSvmwTJjASx3LIbnkSXqe1hRZJqb4F7/YGDScVIyLDMWwH3kUODKJ+/7fHC/vBJzdWXE//9EReAriqJU3FCP+lOK23W+9oT12fR7bbP9PvFi3+KLkf0JO+DJzs5Gebm0b87AgQPx+uuvY/DgwbDZbNi+fTuys7MNa5xSRUUF/ve//2Hu3Lkd9hoUO9wblsOtp8Kqyi973VQudv5Bjt7kzvasJHE+OE8lwbi1em9lRVt/KwJHIMgg1ZWwLFgl38csMweA+sony9pNsM1fCcuae9G856u28xhQGyaiisIpqfJpvJRU3z+5yolIEnbAc8EFF+DYsWMAgClTpmDJkiWYOXOmdDKbDXPmzDG2hX527tyJbt26YcSIER32GhRD9FZYDfLLPhS1b79GrmAJtkLG7a2qq6V4L9wzrtRedZWU3M79rzoRqxVwK/fE8nOyRlod53LBlwdVckD6TARZ+eRYtArHltxpbNHKSCoKZzjkI50Z5s1XJGqvsAOeX/3qV75/5+XlYc2aNfjss88gCAKGDRvWYSM8Ho8Hu3btwpgxY2D1Lr8lc9NbYTWCX/aR1ukRa6ukpeN+u5wL0/8oLTGurlRd7eV9PWuGA+6yo36vJiAg+ThYsJOU0jkDHsGiv3pysGAH0F7ZVumUlqf7/3y9+2MBsKZnSCu8jJw+iKCicFxWDSc6zeKm0vJXX32FwsJCPPLII+0OqlhpOc6crIH1yYfQHCKHpyOrwQZUv1VUvFWtjpuYpH0h9Vtq3Ovu+3F8zf3Slg7h1M4JNWphdmoVj42WlAxh/iqpgJ+yenFtNaxPrQ75uVQTLICOharGZv59wr7FJyMrLYcd8MyfPx/jxo3DqFGjkJqqf+ogFjDgiS+x0Df3glvl37oVWwUEPB6GhCFnw3P3Mrjm3di2aSQQ+oJus7VOwVCHyegBa5H6kvBQQXAwkTz3dIiF/3MdhX2LT0YGPJawn2Cx4P/+7/9w++2345FHHsFXX31luh8wkY9yWiHUbQDS1JSfpGTA0Usa3fHTvOcruO6YAtSdlB9vswG5AwKOByCNHlnbvSNM5yOE+BWXmKR+f7Bp0UhybbjzN1HUhP2b88EHH8SxY8fwn//8B++//z4+/vhjZGRkYMyYMRg7diwLH1HMMKKkfsC2AS6XbAdry4wF8Cy8TT6F1SdXupAqXld1+kstB6f1XJaip6TVQ/47oGf1AWqrWVU5lIwerbldzsCyBYAUTLZu4yElLfuP/ArBc2Ai2b07Bnb+5lYT1FlFlMPj8Xiwe/du7Nq1C59//jlcLhd++tOfYunSpUa20TCc0opdar+ELd26R9S39kwfyNqRnAIcP9a6OkcAEhLkK3daz6c398Jz9Adpvya9icbeER7/i7Gjl3SR1NpVmwBBAH7y07Zg1f9n5Rfo+FbLKVfC2eywbnhF+/w6c8vUxEKeTrD/F2b5faKGfYtPMbFbOiBNb40YMQIjRozAd999h0cffRTfffedIQ2jzkWtBknE+xO1Y/pA1g4ZMXCZcuv5lCu9xNoq6aKiuKhpbg6ZmCQlISsTl9USmZ3H250zZDqCoF6YTxR9e5LpKjuQkysfRcvJDf6yaenoteqpdl1cYqImDqfVqJOKKOBpaGjAhx9+iF27duHAgQNISEjAqFGjjGobdSYq1ZIj1p7pg3B++WucT7OAnNq5BQswewmw7TmO2oTLapVynSqdUrHJ5iZpl3OvkmLpsBABhmX24s61pDsGptWIoqFdAc8333yDnTt34tNPP0VzczPy8/Nxyy23YNSoUUhJSTG6jRTDDMsHiKRasoZ21SZJTQs+gqKoxAwE/gwCgrWSYrjn3QhUnQg8n+gBVi+URisoPC53W7DjLTbpP4LmapHycxRTWEr+oy7Se9n+Kad4yI9hzR7qrMIOeGbNmgWn04lu3brh0ksvxbhx45CTk9MRbaM4EFE5fH8RVEvWYvj0QUKi1C7FhUz5M0BSsvx5rhb1YMcrnD2TyI/YNjLo++woije6WnzTW3o+C5F+ng37/9CBYmJajSgKwg54cnNzceONN2LEiBGwWMJe1U5mY1Q+gLJacn0dXPNvwfGemdIu0l27hTxFJN+ufc89/L36Ad5tHJT7XAGBfU5IApqa9FcDpuCsNiBvAIRpsyBuXNf6HmhMedps6rlPxXvhvvVyAALQu6+0yk3tMxLp55n5MUQxK+yI5Z577sHIkSMZ7JBEV12a0CwzFgD5g6VVSN7gwnkczXu+kna01sH37dp5XPpWv+h2iLXV+p67dpn0XI8iSLHZpXYpR5z8L2TKPtdWMdiJlGABLBap4vHiR2AtKIKld18pyMwIsmLD7ZKCHk0icPSHts9Iwc1wFxW0fU4i/Twb9P+BiIwXN1tLGIHL0o2ntswWECPKYwhV3Vj38wApYPGuuqmr1d4Pa850BOxjlT9Yu4aOXy4Pfvd7YPW9DHI6imIpeSTVrTXpLDHgHQm01tXCnZqm8nj0l51HItq/TzoS+xaforq1RDxjwHN6RFo+v73PVy3sp6V1ryRL777qz1PUYpFdyPw3BAXC28ySQtNabu4NMpU/fyPoDapjfGuISMXi7xOjsG/xKapbSxCFFGEeg//0VsKQs2GduVD/85QJw1oaGyAun6vdvqwcuIsK4F5wq3SRg7S82br8CSCli/xYBjsG01ix1jrNicYG6X3O6CH9HdZWGwLQu1/gth1Bpp68dZXcC271LXX3YY4OUdzgpjxkvAjrfHhXkQiCgF6t31o8NZUhp8mEtHRYCh+HZ9Ht+kYAvNszKNsrWIDSH9s26FSutlEuodeSfkbrBdFc37g6XLfugKNn8F3kU9N8IzK+0bdKZ2sys9/P22IB+g/SNfWkRbsYJZijQxRHGPCQ4Tqizofe5b5CWro07aFrykMaSRCmzZS2fGhqbF0i7gFcilGb1m/yYm2V9kVYqTrIUnTS5ugZmE+jnMbyCzT8l1kHTDn1H6T5OdE9FaUcxbHZYXX09OXwEFF8YMBD7aa9DLwDRjTCmSZTjti0Jr2ioV5aoePVJw8AtLd88JeaBqA18PKO/Hh5V5VR+7QuO0dlhTR6dqIc7tlXSyvjMhywLFgFAK2jOK3HVDrhLioIGLnpkKJ6ys9Tbj6yH/27KfMliMxMV8Aza9YsCGFUgn3sscfa3SCKH1qjLgH3t1a7FabNlIKL9qxgUVZAbg1A1ATscA60/Tt3gGy1FgD13bS1qAVaogh0SQNO1eo/D7Wx2WAtKJJGZyqdbcFja90j7+fKWlAEd+GctmMqK+CZdxNgt7cFRzMWGJ5ErAyi9OaUEVFs0RXwDBkyRBbwfPPNN6iursagQYPQrVs31NTUYN++fejevTuGDh3aYY2lGKM16qK8v7XarWyn8HCr0LrdwW/70ZzicB6XVtUoV+Poyck5UiLValF+2wekqTBvPlDuAOCH4s5VOTkxSXqPg7wnQXXpKv2tNWrnf/+REvljbpf0xxsArX1AqsNj4LJw5fRXOF/+iCh26B7h8Xrvvfewb98+/OUvf4HD4fDdX1FRgWXLlmHIkCHGt5Jik/LiX10pBRha+1Fp7DauS+mPwW8r+KbbDu4L/ZrJKaGnpFwt8Cy8DTijZ/Dj/Hfd7iyy+kTWb29AohZMeu/X60hJW45VB2ztINZWwb1hBY5p1OEhotgVdg7Ptm3bcNVVV8mCHQDo0aMHJk+ejC1btmDs2LFGtY9imG+o37uapnUkB7kDpGXlylU2CYkBiadGbbboOVoCcUWBFFQlJAI9soAfDwYeeLIGYm21/DUa6jXOqtiXqakROHY47LaZnkFBnu/z5L8haFo64HJJS8LTM4CsHODHQ/pPavCyce90rTSWdTQm98oiInVhBzzHjx/X3BG9S5cuKC8vj7hRFB+8Q/0BlW/ramFd/gQ8R3+QprG8Qcgd9wHbnpMFN54Ny1XzfSwzFkDo1r3tnDm58gurt3pyK3FFgTz3Qy3YAYCmRngKbpISmVvzPpCkMcKjtS8TGatOyn0K3LV8hTxodh5vC6a9QVFyihSwet9Ll0v+OfEbHTIkuOZeWURxK+yAp0ePHvjPf/6DESNGBDz2zjvvGFYRkeKIRt0d2eqnxgZg23OB34Y18n08G5bDMn+l727L7MWaq2/E2qrwVkm5XNIf72agWsUKe2ZxROd0UJmy0qx90xpMawlWX8eQncwjrDFFRNETdsDzu9/9Dhs2bMCCBQswatQopKeno7q6Gh9++CEOHjyIP/zhDx3RTophmkuB9Xwb1srbqHTCtWKeLFfCMmO+7xu6Z8Ny3zd0z4YVkXUgJVXaNFSZY8RgR12f/tojaHrZ7NJ7n54BYdpMKffL//OjNXISIsAIWl/HgNEZ72fdfy8tIooPYQc83vycF198EX//+99996enp+P222/HuHHjDGscxSa1qQHVi4yOb8MBeUBe9XVAZYUsVwKA+jf0gAuXINXY0XtRznAAOndV71QEC9AzGzh+RH7/8aOtOVoHAmsS6ZWbr7mSzrNhuWYtpYgCjDBHZ7SmwGzzV5p23yIiM2tX4cGxY8dizJgxOHbsGE6ePImuXbsiOzubyzU7Cb1TA3qKwHm/kSunImT1WAD1b+Pe+5QXsvyftlVP1prqEixA9zN8tVs8BTfr7X7nkJQM69pN0vsy53r5Y81N0mjb2mWte0uFedG32eWfBZWRF8uCVYbvOh5uUUJDpsCIKGa0u9KyIAjo3bu3kW2heKFzaiCc8v3KY6UidH4FAU9UAFbFXrd1tVLCdGqaNM3iXare1Ahx+T1ttXHUnNHDby+mKmnPJWqTlAJ3UQGEaTNVH/Ys+oM8mPRWsz58UD41qLbzeU6uPLhVljFIzwhv6wcVWqMzYZ2TCcpEptKugOfo0aN4+eWXsWfPHpw8eRKFhYXo378/Xn75ZQwePBhnnnmm0e2kWKIcUUlNC8jBaM+3cdlFKjVNKmjnDVr897ey2aUVVI0NbTtoJyW3TYnpWbbsN53h2bAiMH+ns6s+AVSfkEbJlJTlBQCpAnFBEdwzrpDfb7FK04vegoHe1XX+xSC9K6+isPdaUExQJjKVsAOekpIS3HfffUhOTsaQIUPw8ccf+x5rbGzEjh07GPCYnHJqAC5XyIuLniXByosUbHb1BngvPP4X3WCjOYAUINnkWxD48Ju7toBAUAC6pAber7XVhwBYF62W3eVecKv8mBArr9rFwARlQ/flIqKoCTvg+cc//oF+/frh3nvvhc1mkwU8+fn5+OSTTwxtIHWs9tQmCaiXUnCL/ACVi4uub9xqS9TVpKYBZUfUH1OTmCRVA27dQ0uYNjP4lAq1CUjPEYFTddrHW23yRGaryq+Y0zFyYsBrRDqtBhhU+4eIDBF24sK+fftw+eWXIzExMSBJuVu3bqiurjaqbXQa+AIR53Ff/Zuwn68MTNQuLnqXqKux2aQpq4we0tQHIB/dSUqWpk7UJCZJf5cc8PVRXH6PrM+dcjuIkATpZyd6Ah9Sm/5rLR7o2xfLS3kb0sgJ8gcDjl5A/uAOGTk5Ha+hR6T/v4jIOGGP8IiiCJtN/WmnTp2C3a4xDUGxKdKhf+XxyhU4XuEsUT+4T6qL4zv2DNmUR8CUSGqadrvd7sCALNT0FwEQpU059fK+nxkOebJ5hiPgUCNGTkLpqKTnsDHxmShmhD3C069fP3z66aeqj+3evRv9+/ePuFFKlZWV+Mtf/oKbbroJ06ZNwz333IODByMsfEYSZeAR7tC/8vjcfNULg/Ibt7fYnHvBrXAXFfj2t7IWFAH9BwV/DWW+SGpawFYTPu2tE0P6f3ZJyb4g1/s+WzN7R3VkJVKGjcxE+v+LiAwT9gjP+PHj8eijjyIxMREXXXQRAMDpdOKbb77Bzp07cffddxvawLq6OixevBhDhw7FwoULkZaWFnQ/LwpPpImZep+vuuxcYw8t/3Mm9MyE+5a5odsxezE8ax+QVmjJRiZYGE6TzQYpq9gaetTLW7eotlo+Ymazw1L4uC/INU1hPoNGZpj4TBQ7wg54LrjgApSVleHll1/Gv/71LwDA6tWrYbVaMWXKFIwcOdLQBm7fvh1nnHEGZs5sqwfSs2dPQ1+jM4t06F/v85VTBKh0yg/w20PLWlAknbO2GnhqNdwPzpVPK3jzRbzqagGIUiJzONMwnVnuAN/qKXfBzaEDnta6RbJAFdAc0Yt7BiVWn47pOyLSp111eK644gqMGTMGX331Faqrq5GWloazzz67QzYO/e9//4uzzz4ba9aswZ49e5CRkYFLL70Uv/zlLzWf09LSgpaWtm+hgiAgOTkZgiCYrhq0tz+x3i/32mVtycHO423JxErVlb6+uDcsh1uxsss2f2Xgxai6MrAQHoXk+8ycOhn64PQMCIIA68yFcK9/0Be4WmcuDPjsxctnMphg/TRD/7Swb/GpM/TNkHOJYY4579mzB/3790dSUuAFq7GxEQcPHsSQIUMMa+B1110HALjssstw/vnno7i4GM888wxuu+02jBkzRvU5L730EjZv3uy7nZeXh6IifsuKph9/dwHQ0tx2h80uVeH1vw9AwsChgM0Gd6UT7hMVssetmb2R/dR2uKsr4Sy8B80H9gY8n/Tx/iwB4MeJ52uXAAAgJKcg68ltsDL/hIjiWNgBz9VXX43CwkLk5+cHPHbw4EEsWLAAmzZtMqyBU6dOxU9+8hMsW7bMd9///d//4fvvv0dhYaHqc7RGeJxOp+x+MxAEAZmZmSgrK4tKvoRYUwW3X46CdeZC1SkO1x+uCMj9QG6+fHokKRnIzNFeJp4/WBrh8Z5z/i2sn6MkWAABUuqSRZBWqalJSpaSvdMzgEMHgk8FntED6O7QfG8DmhDlz2RHM3P/2Lf4ZOa+2e12OByBqz3bo917aalxuVywGLwnUffu3ZGTkyO7LycnJ2iBQ7vdrro8XhRF030YvKLVN/eG5bLkY/f6B+VFCb15O1arPODJyVVN6PQsv0f+AvYEoFt33+OyPiqntkiqm+P9EfUZICUmK3eiFwT5thyhhoxPVAAnKmTvra6mmPj/G2Du/rFv8cmMfTOyP7oCnvr6etTX1/tuV1dXw+mUJ502Nzfj3XffRXp6umGNA4BBgwbh2LFjsvuOHTvWIflCJAmrBkmQ1Syy6spA2+hDQiKE6XeqJ3QqgpiEAYPhuXuZ9B+5tkq2Z5cwbRbEFfcwd0dL65YN7gW3ygND5S8QUWwb8fEu+a+rlX7O/oESa8gQURzTFfC8/vrrspyYVatWaR47adKkyFvl57LLLsPixYuxZcsWXHDBBSguLsY777yD2267zdDX6azUghutbSDUjg26mkV5gfSOPjQ2QNy4DvALdnznrnRKF9+kZKChHs0H9gB/uKKtzo5f4rP4zKPSFNiRkqA5KJ1WXS3E2mp9W2c0NwUEtwErspjDQ0RxTFfAc/bZZyMpKQmiKOIf//gHfv3rXwfMqdntdvTt29fQhGVA2p9r7ty5eP755/HKK6+gZ8+euOGGG3DhhRca+jqdlVpwozVqo3Zs0DojwaacFK/h8V/FBUijDv5LpUsOQBoe8hMQ6HiTVwgA0NigXjAvITFwewiPR1YWAGANGSIyF10Bz8CBAzFw4EAAQFNTE37xi18gI+P0fdv72c9+hp/97Gen7fU6FbXgRmvURuXYYHVGhGkzIa6YJ11cRcj3ZaqrlaZavBfSHw/Jn6xaFyZEMGOzdeKRHo1gT20aKjUNSEtXHxkrKZa9L6whQ0RmEXaG8VVXXXVagx0KjzfPxX/LhqBUSt9rbrwYZpl8ceN6Kb/G45GCHcECWCzS396EWW/Zfq3VRJoEIEuezI6eWdJUmBkJFiD9jNaNUtX+22oEg3W1gVtxZDhgXbQa1g2vtG3G6uVqCWs7hbA/b0REURL2Kq1nn30WNTU1+OMf/xjw2F/+8hd0794dv//97w1pHIVPK//GS5mHI0ybJeXTKBKU1b7ZB5viUM3v0crhUaqsgPbojXcaS/m4CByXJ7Oj9MfAhFwzyB0g/V1XK+UyuVz6d3j3JnTnDpBGdADA5fLtXSZ7T9uRpKw6zelXOoCIKFaEHfD897//xZVXXqn62Nlnn40tW7Yw4ImmEHsAKS9Q4sZ1uqctgk1fqeYC6V02rtxmQiZIAKPMQzFjsAPIp56cx6XgJSlZ/+q0ulrpvfCeo+SAtO+YzdYWoC5YJb1n4SYpczdwIooTYU9pVVZWau5l1aNHD5w4cSLiRlEEQk07ddQFSuW8sqkxs041nQ7KPBu1aapg0jMC358jJQG7gWtOZYY6d7DbREQxIuwRnqSkpIAaPF5Op1O14B+dPrIpitQ0wOWSJwcbtCliAJXz+o8IeY7+AHHpnfLEZWof73umZ/QsKbntM+F/vEtRWTlEAroWruQiongR9gjPgAED8Nprr8Gl+IXpcrnw+uuvY9CgQYY1jsLnvWhZlz/RWmX3QNBv8cK0mYYknQrTZrYl1CYmAU2NsnOKG9dHHuwkJgFWQ4uDxyhB6qdyg1Wb3TfyIkybiYBl+v6sVunYwsd9uTryUTbF9F+Eu4Fblz8Ba0GROXdOJyJTCPvqceWVV2LJkiWYM2cOLr74YmRkZODEiRPYuXMnnE4nbr311o5oJ7WHjmXksuJyKknOerj3/Q946N62O5oa25aZO49L+SJ1te3pQRurDeiTJ88xMS1R2tuqT54UtJ4oB6qrpPuOlEA8WSsFkMHym/r0l72PQlq6NOLnn/djs0uBDkdmiKgTCDvgGTBgAObNm4ennnoKzz//vO/+Xr16Yd68eaqbilKU6Jm+akdOj3JFFr7/LvgTjhySEm0j2fcqu4+0J1Rncvh7oP8goP5U2+hYY4O0nUaoHB61AFP5ecjNb3ednbC2HyEiigHtmh8455xzsHbtWpSWlqK2thZpaWnIysoyum0UIV35FRpBkeyC5r+/UnKKlPDqXRGlJ4hxudvaotzIUkuutPGlta4W7tZcpE5XVLC1+nGAxobAHBzlqi2V4NbIfJtQ5Q+IiGJNRAkRWVlZDHRiTDjfvMXaKunCaWtNNG/dwRwIvKBFxGr1TaUFbGSpxpu7Ul0Ja89M4Ja5cC+7O7I2mI03+LPZgdx81XpKSnqSknV/frgcnYjijK6AZ8+ePejfvz+SkpKwZ8+ekMcbvZ8W6RfON2/PhhXyAnY2G4S0dCkQCmf6yGIFPEEqJdtsvpViSE4JcTJBylVpbVez8zhwdxzWdUpKluoEeRSJ2lZba1XpIPk3gkUlwVsALALgEeXPTc9oe38NGGHR/fnpqNV+REQdRFfAs3TpUhQWFiI/Px9Lly4NefymTZsibhi1k/Kb9sF9cBcVqH9T1zg27Okjb7Bjs7dOtSgu5k2N0h/ncWnjSn+tK7raiIHTNfHGZoel8PHAQn5JyUBKamtl6SC6nwHU18mnqPJ/Ko2QKXcwb90R3bD8GZ0jN2ZYjs48JKLORVfAs2TJEuTk5Pj+TTFM+c3buwv2otulXBz/X+ypaarH+qa4vGw2ICevLYenvBRoaQ4cbdATJCmrI3fpKq1GUtvaIF65WuBZ+wCE6Xe2TTOdrJECGD3VkTMcsCxarRpQWGYskN5L73lad0Q3LH9G58hNe2r2xBrmIRF1LroCHv8pKk5XxS5ZTo4ycPBebNvziz13gPp+XFoJyN6ASU/wkuGAZcb8tm/aZlFyQLZth3uG+nYsAWz21uBGfcpLdXm5gT83M4zc6MY8JKJOpTNUces0AnJytHinrrSKDObkyvdZar3ohQx0vHLzpf2xgk3dWCxA/0EQps2EZ9EfAuvD5OYDP3wvjSTFOqtNyjtSas8FNDcfQlp68PpIHZg/Y4aRG92Yh0TUqegKeNavX6/7hIIgYMaMGe1uEEUg4AIrABmOwHwQ79SVcn8rqw2w24GaSqChXso38SObAtBis0OYNlPaRiKY/oOknJTCOYHTPK2JuO7FM4GyI8HPEwusFkAtZzs9oy1IDLaxaWKSNLWX4WgbUQky+tCRozCdKa+lU41mEZG+gOfbb7+V3a6vr0d9fT0sFgu6du2KkydPwuPxICUlBV26dOmQhpIOAbuTi/J8kIP75KuGUlKl0RxvAUFX6+oov/wQVFa0bkkxX9/Krdz80NtIJCZBmDZL+veRksDHncfh/vNd8RHsAECzYhTKagX69G+7oAYLEh29pG1AlIKMPnTkKExnymvpVKNZRKQv4Fm3bp3v38XFxVi9ejVuvvlmXHDBBbBYLPB4PPjoo4+wceNG3HXXXR3VVgrBMmMBPAU3y6eb/LaTCFjhU18Hy6LVAETpQnf4e/UTlxTDs3aZyjSWANisfsvSBSloCrUfV1MjxBX3tBUUVPPjweDniGV9+gM2GzzL7wkcqVHmVznL4S6cA8vsxbKRlKiNPjCvhYhMKuwcnr///e/47W9/i9GjR/vus1gsGD16NKqrq/Hss8/igQceMLSRFJy76gRcK+ZJFyebTX5B9RsZ0FrhAyD4KISrRX0kxreE3C9o0ZND1PraulYsxaOSYmjW2cnJlUaufH0XgZIDUqCam++bQora6APzWojIpMLeLf3gwYPo06eP6mN9+/ZFSUlJpG2iMDkfnCcFLM7j0oU0Kdm3G7r/yICQlg6kKKYcneWBU1UWC4LuxG005TL4uKcS7Hh3Op+9WH0fLFeLb0f7aLLMWADkD1b9/BARxbOwR3iSk5Px9ddf46yzzgp47Ouvv0ZycrLKs6gjuSud8jtS09TzQgBpI0p/NVWB+Tb9B7Xl83hl5QClRzqmTk5OrjSF5Q5SrTmWWa1SdeRgPxv/isgBuVZ+ojyFxLwWIjKrsAOeiy66CK+++ircbjdGjx6N9PR0VFdX4/3338cbb7yBCRMmdEQ7KQhrhgPusqNtd9TVSls5+G/66f23svCfAPmAhM0mBTs/HpIfd+xHaQWXkQGPd/l5/an4DXYAX86ObFpQEOQrs/xGdSwzFsCz9gHg6A9ASwuUW0Vo6UwrqIiIjBZ2wDN16lTU1NTgtddew2uvvSZ77MILL8TUqVMNaxzp41i0CseW3CldCOtqZUUGfbRGFBISA2vgqOXhuF3SH9V9ntoprZv097HDxpwvEhZL4L5XeggWCNPvhNA1TZZkjKbGwKDR+5S0dNjuXYOsrCwc27cX7vUP6kpO7kwrqIiIjBZ2wGO1WjFr1ixMmjQJ33zzDerq6pCamoqhQ4eid+/eHdFGCsGangHb/JUQRVEa2dGTDNxa+A+/+z3w2APSyE9CIpCUEuL5QerJqFGOdPg7VScVKIwFysBPL9EDceM6WAqKZMGHe8Gt8uPqalWfHtYUUjtXUHFkiIgogkrL2dnZyM7ONrItZIRg+SH+vIX/igrkdXcCKOe8lLeDEaT8nPJSxQahaM15iYFNQq02IG8AhGmzpH2vnMfDz6NRO74jVju185wcGSIiascqLQBoaWnBjh078Mgjj2DZsmUoLS0FAHz22Wc4flzHxZY6jGyVTe4A6Y9F8Tb79mtC4MU6JVW2Ske4f638fMpzqfKu8BKlaZ2mpsBDRE9s5O34pudagziNkRgfiyWwQrVK4NERq53afU7W1iEiCn+Ep7a2FkuXLsWRI0d8CcsNDdLIwGeffYavvvoKt9xyi+ENJX3UpkjchXPkeTk5uRDS0qXNRpUX+AxH4Lf/giJpWmTRH9T3jAqgHAHSGhEKc3qsI7RusyGumKdvSqv/oJBFAfVOIcnqJ+mYamr3CirW1iEiCj/g2bhxI+rr67F8+XL069cP1157re+xoUOHYvv27YY2kDqOZ8MK+UU+KVl28ZZduL3J0ErZfaX8l8Pfty/pN1YoV6+pSUwCKp3wbFgu7Re2cT1QXSmtuAKkn5H/Fh2A5hSSWFOF0tl/kPYsC3KcEbhnFBFROwKeL774Atdddx369+8Pj+ICd8YZZ+DEiROGNY4MohzF8d5WTm2kpslGGHRtFprSRX3biqRkaSm2sxwxMZITij0hMM9IqalR+lNZIR8RUq6GUxZSVJlCcm9Y3hbsBDnOCKytQ0TUjhyehoYG9OjRQ/Uxl8sVEARRDFBOYXjr9CgDodbjxNoqaRosVLAD+C7SAfklhY/DsmAlkJhoRA86Xs8sqf0ZPaRgrVt3KbFai54RIS+1KSStRGciIuoQYY/w9OzZE/v378eZZ54Z8FhxcbHhK7deeuklbN68WXZft27d8MQTGpWEOzn3vv8Bq++TknEFCzCnUD6l4V+nB5CmabzJwy4XxNpqaWRHrRZPUrI0XeNffLDqBNyzr5aSnTMcsCxY5csP8iz6Q+hRk1jRUA/rfY/6brqLCqQq1FqCLWPPyZUKEQabQlLm1SimE4mIyFhhBzyjR4/G9u3b0adPH4wYMQIAIAgCiouL8a9//QuTJk0yvJF9+vTB4sWLfbctulYKdVLeYAeQ/l69CHjo6bbHlUvB3e62AKbkQFtgFECAMH8lxGf+Im0k6nIBENsKEjY2AJUV0uakKalA1QnjChRGwmYDcvKkQK+2WntkRjm6ojW91Fod2reMvbpSXtFaZ50b68yFsD75EJrLy1gbh4joNAg74Jk4cSL27duHhx56CF26SBtRFhYW4uTJkzjnnHMwfvx4wxtpsViQnp5u+HlNSRlkiB54Hrlfs+pvAG+lYGUtH5tVCnZC7YYea7ug5w7w5a+4Z1ypfoz/Mn0v5c+gNdCRBSYR5MUIaenoteoplJaWQtQqzEhERIYJO+Cx2WxYsGABPvroI3zxxReoqalB165d8bOf/QwXXHBBh4y+lJWV4fbbb4fNZsOAAQMwdepU9OrVS/P4lpYWtLS0TbsIgoDk5GQIggBBOI27gJ8G3v74+qW29cOPJcpnAY6e0kXd7QIO+QUxdbWwzF4Mz/2z5RWSrTZpZCce2OxS39IzYJ25MPR7npsPS7fusrusMxfKtnywzlxo6AhMwPtmImbuG2Du/rFv8akz9M2Qc4lhfL1sbm7GAw88gKuuugrDhg0zrBHBfPnll2hqakJ2djaqq6uxZcsWHD16FGvWrEHXrl1Vn6PM+8nLy0NRUedYpdL4zReoKLgt+EH2BPTZ9hEAwF1didJbfgfRb8VQwpCz4ao4Dk9Fme8+S49MeKorgZbmDmm3kRIGDoXjvjVwPjgP7konrBkOOBatgnPpn9C8/1u/IwXAbkdC3gA4ljwMK5OGiYhMK6yABwBuuOEGzJs3D0OHDu2oNgXV2NiI2bNnY+LEiZo7s2uN8DidTtn9ZiAIAjIzM1FWVuabGnGtmCdfYZWYJE8ezhsA26I1vpuu+bfIp28cvaQREv9zWG2Ax629L1Z7JSSGt+JJj9wBgbuX5w+WRm3+8md5DpLf47b5K41tRxBq75tZmLlvgLn7x77FJzP3zW63w+FwGHKusKe0Bg4ciOLi4qgFPElJSejbt69vOws1drsddrs94H5RFE33YfDy9k2srZIu5t5aMDm5EKbf2ZZg25ogK/s5qFTi9a3sKimWkpp1VVgOlyAVLgyVFxQute0hqiuBrt2kQMilEvRWV0bls9EZPpNmZeb+sW/xyYx9M7I/YQc8v//977Fq1Sqkp6fjvPPOQ1JSkmGN0aOlpQVHjx7F4MGDT+vrxouAJeU2Gyy9+2om2KoFSN7EXGtBkVSvR89mpO2Rmx9676r28E5N+bf7ZI1UW+jw98GfQ0REphR2wHPvvffC5XJh/fr1WL9+PRITEwOSip599lnDGvjcc89h5MiRcDgcqKmpwSuvvIKGhgaMGTPGsNeId7I9maoUla4rnUGfqxYgyZJzde2+Hs4O6n7PmXwj8NgDYT4vyPlaE7GFaTOlFWX+7WpqVB9J8lt9RURE5hV2wHPeeeed1kzwyspKPProo6itrUVaWhoGDBiAwsJCzWrPnZHzwXnaVZGrTkCsrQYgqm9oGWInbd/UVqUTqKxQfw2taaKgRCnYae8SdmVeUm4+rItWA2gtGhhqmsxi8W0Eyvo3RETmF3bAM2vWrI5oh6a77rrrtL5ePHIHG8URPVLAArQFRa0bVVpmzFfdXkKsrYJn7bK2Zeg5ubAsWg1P4Rz1oCcrBzjyQ/iFBtubrCxYgJvmADu2qFcz1rMnVf9B3F+KiKgT0R3wNDc349NPP4XT6URaWhpGjhyJtLS0jmwb6WTNcMBddlT7ALUAoLpSc7d0KVnZb4Sk5IBUQTkzJzDgSUySVnC1p6pyqFkwQVBfFSZ6gKfXwLp2k/rz1IoG5uRK//arhkxERJ2HriqBlZWVmDNnDtauXYsXXngBjz/+OO68807s37+/o9tHOjgWrWrbuDN3gBSE+Gstwhdwn9Zu6WoBkjcw6pMHKTcGQGIShAUPAbVB9pwKJliQlJQM/OSn2o8HGR0K2Mi06ClYZt8rTb0REVGnpOsK8OKLL6KyshJXXnklBgwYgNLSUmzduhVPPvkkVq48fbVLSJ01PQO2+St9y/ekDUCXB0z3KO/zbFgesBwdgLQ3lFqicuvoiH8isLhxHVB/Sn9jbTZpxCVU7k5Kqny/qRMV8gApQXsXdu8KM3/uooKAKT1OaRERdR66Ap6vv/4akyZNwuTJkwEAw4cPR2ZmJoqKilBdXc19rmKMkJYOy4z5viRlKV9nQcAFXraLemoa4HJJy9BP1qifWG1UqLpS2ixUb/Kxyw14dEx/1VbDvf5B9Fr6KMobmuA+8gPEFfdIIzsJiRDmr9L3ev7tDHabiIhMTdeUVnV1NYYMGSK7z3u7pkbj4khR5dmwQhrRcB4Hive2JS778Y6EWJc/IY28lByQjvdf/eTVmt+D5BT5/ckpQEY4VTBF9YCn+xnSNJS3HpCrBSjei2PTJ8C1Yh6Ermmwrt0E6+PbYF27SaotFA61KT0iIuo0dAU8Ho8HCQkJsvu8t91ut/GtosjpHNEQa6uk6Z6D+7TPlZgES+HjUn5PuaLC9ZGS1lo/EZYq6NYagHgUn6eWZs2ALRwBeT1MWiYi6lR0Z3EeO3ZMthO6p/Vb+rFjxwKO7d+/vwFNo4iobBehxjcSFExWn7ZaNcrNQ0VRuz5PKIJFipMSEqWtK4LVzolwCkotr4eIiDoP3QHPunXrVO9fu3ZtwH2bNmksF6YOI9ZUwe0tEFhfBySlSCudUlKBDIdsREOsrWorQqgMJCwWwGKVFxL0r9WTkBg8X8e7BNztAn481HZ/YhLQpavUtpRU6e/GBin/ubEBKD0SvIOcgiIiogjoCnhmzJjR0e2gCLk3LJeP1HiDkpzcgJGNoKM6/QdJf/s/7hdsCPNXQbx/NjSL6LhapP27Zi8OWBXmX9HYveDW4IFTn/5ARSnQ3AwkJECYdnoLXhIRkbnoCnjGjh3bwc2giGlN+WgUHZSx2X21erSWsHtZeveFOzc/5PRTsCkksbYqsMJzTq6UON36mnC52gKixgZp+TunpIiIqJ1Yic0stDb5VJsKUh6bmy8LTkSVQoKyaTDlsnVlReQQ00+qFZ5nLw4cAfLHZeRERBQBBjwmYZ25EO71D7bl8Kjk7njJ6u+obLMgm/JqLdIHQHsazD/Y8S5fD0ajwrMsqFLZ44uIiKi9GPCYRDirkEIeG0mRPu/2FMEoR5jqaqURnbragJEfa3oG3KlpXEZOREQRYcDTCchGTlQSiANoLWn3vy8pWarOfLJGXqgwNfSGsrIRJm+Qo5bAnJqG7Ke2o7S01LdtBhERUXsw4OkE1KaovCM8asGQ1pSX2qord+Gc4AnMqvyCF5dL+zBOYxERkUEY8HQGQaaoPGuXtQUszuPwrH0A1kWrVae85EHScvU6PsrcGxVBl8V7R47SM2CduTDkuYiIiPRgwNMZBKu67F8cEAAOH5S2mggy/RU0YNEzKhNiWbz39QQhwu0qiIiIWjHg6QSCrspS7oXmcWtOf/noqOMTVIhl8UREREZjwNMJBF2VZbMGz6NRW6EVYcASalk8ERGR0RjwdHY5ecGTjlWmqCINWLiRJxERnW4MeDo5y+zF8Cy6PaD+jTdxWC2YYcBCRETxhgFPJyekpcNS+HjQjT6JiIjiHQMe4ogNERGZHgMekgm7KjMREVEcsES7ARRbfDV2nMeB4r1tG4cSERHFMQY8JBfJxqFEREQxigEPySmXoXM/KyIiMgEGPCRjmbEAyB8MOHoB+YNZFJCIiEyBScskwxVbRERkRhzhISIiItOLu4Bn69atmDJlCp555ploN4WIiIjiRFwFPMXFxXj77bfRr1+/aDeFiIiI4kjcBDyNjY1Yu3Ytbr/9dnTp0iXazSEiIqI4EjdJy08++SSGDx+OYcOGYcuWLUGPbWlpQUtLi++2IAhITk6GIAgQBKGjm3paeftjtn4B7Fu8MnPfAHP3j32LT52hb0aIi4Dnww8/xKFDh7B8ub6qv1u3bsXmzZt9t/Py8lBUVASHw9FRTYy6zMzMaDehw7Bv8cnMfQPM3T/2LT6ZuW9GiPmAx+l04plnnsGiRYuQkJCg6zmTJk3ChAkTfLe9EaLT6ZSN/JiBIAjIzMxEWVkZRFGMdnMMxb7FJzP3DTB3/9i3+GTmvtntdsMGK2I+4Dl48CBqamowf/58330ejwd79+7Fm2++ieeffx4WizwVyW63w263B5xLFEXTfRi82Lf4xL7FLzP3j32LT2bsm5H9ifmA56yzzsJDDz0ku2/Dhg3Izs7GxIkTA4IdIiIiIqWYD3iSk5PRt29f2X2JiYno2rVrwP1EREREajg8QkRERKYX8yM8au6///5oN4GIiIjiCEd4iIiIyPQY8BAREZHpMeAhIiIi02PAQ0RERKbHgIeIiIhMjwEPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAhIiIi02PAQ0RERKbHgIeIiIhMjwEPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAhIiIi02PAQ0RERKbHgIeIiIhMjwEPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAhIiIi02PAQ0RERKbHgIeIiIhMjwEPERERmZ4t2g0I5a233sJbb72FiooKAEBOTg4mT56M4cOHR7llREREFC9iPuDJyMjAtddei8zMTADAu+++i5UrV2LlypXo06dPlFtHRERE8SDmA56RI0fKbk+dOhVvvfUWDhw4wICHiIiIdIn5gMefx+PBxx9/jKamJgwcOFDzuJaWFrS0tPhuC4KA5ORk2Gxx1V1dBEEAANjtdoiiGOXWGIt9i09m7htg7v6xb/HJzH0z8rotiHHw0zl8+DAWLVqElpYWJCUl4Y9//CNGjBihefxLL72EzZs3+26PGjUKd9555+loKhERERmspaUFdrs9onPExSqt7OxsrFq1CoWFhbj00kuxbt06HDlyRPP4SZMm4ZlnnvH9mTZtGh599FE0NDScxlafHg0NDSgoKGDf4gz7Fr/M3D/2LT6ZvW+PPvqobNamveIi4LHZbMjMzMRPfvITXHvttcjNzcUbb7yhebzdbkdKSorvT3JyMj788EPTDfUBgCiKOHToEPsWZ9i3+GXm/rFv8cnsffvwww8NOVdcBDxKoigaEu0RERFR5xDzAc/zzz+PvXv3ory8HIcPH8YLL7yAb7/9FhdeeGG0m0ZERERxIuaXLdXU1OCxxx5DVVUVUlJS0K9fPyxatAjDhg3TfQ673Y7JkydHnPAUi9i3+MS+xS8z9499i0/smz5xsUqLiIiIKBIxP6VFREREFCkGPERERGR6DHiIiIjI9BjwEBERkenF/CqtSLz11lt46623UFFRAQDIycnB5MmTMXz48Ci3zFhbt27FCy+8gPHjx2P69OnRbk7ElFuDAEC3bt3wxBNPRKlFxqqsrMTGjRuxe/duNDc3IysrCzNmzED//v2j3bSIzJo1y/d/zd+ll16KW265JQotMo7b7cbLL7+M999/H9XV1ejevTvGjh2LK664AhZL/H9vbGhowKZNm/Dpp5+ipqYGeXl5mD59OvLz86PdtLDs2bMHr776Kg4dOoSqqirMnTsX5557ru9xURTx8ssv45133kFdXR0GDBiAm2++OW42og7Vv08++QRvv/02Dh48iJMnT2LlypXIzc2NXoPDEKxvLpcLL774Ir788kuUl5cjJSUFZ511Fq699lpkZGTofg1TBzwZGRm49tprkZmZCQB49913sXLlSqxcuTJuPuChFBcX4+2330a/fv2i3RRD9enTB4sXL/bdNsNFBQDq6uqwePFiDB06FAsXLkRaWhqOHz+OlJSUaDctYsuXL4fH4/HdPnz4MJYtW4bzzz8/iq0yxvbt27Fjxw7MmjULOTk5OHjwINavX4+UlBSMHz8+2s2L2F//+lf8+OOPuOOOO5CRkYH33nsPDzzwAB5++OGwLijR1tTUhNzcXIwbNw6rV68OeHz79u14/fXXMXPmTGRlZWHLli1YtmwZHnnkESQnJ0ehxeEJ1b+mpiYMGjQIP//5z/H4449HoYXtF6xvzc3NOHToEK688krk5uairq4Ozz77LFauXIkVK1bofg1TBzwjR46U3Z46dSreeustHDhwwBQBT2NjI9auXYvbb78dW7ZsiXZzDGWxWJCenh7tZhhu+/btOOOMMzBz5kzffT179oxii4yTlpYmu71t2zb06tULQ4YMiVKLjLN//36MHDnSt2lxz5498cEHH+D777+Pcssi19zcjE8++QTz5s3zvVdTpkzBZ599hrfeegvXXHNNlFuo3/DhwzVH8EVRxBtvvIFJkybhvPPOAyCNSt5666344IMPcMkll5zOprZLsP4BwEUXXQQAKC8vP11NMkywvqWkpMi+AAPAjTfeiIULF8LpdMLhcOh6DXN8bdbB4/Hgww8/RFNTEwYOHBjt5hjiySefxPDhw8MqwhgvysrKcPvtt2PWrFl45JFHcPz48Wg3yRD//e9/0b9/f6xZswa33HIL5s2bh7fffjvazTKcy+XC+++/j3HjxkEQhGg3J2I//elP8c033+DYsWMAgJKSEuzbt88U0+NutxsejyegsFtCQgK+++67KLXKeOXl5aiursbZZ5/tu89ut2PIkCHYt29fFFtG7VFfXw9BEMIaHTf1CA8gDasvWrQILS0tSEpKwty5c5GTkxPtZkXsww8/xKFDh7B8+fJoN8VwAwYMwKxZs5CdnY3q6mps2bIF9957L9asWYOuXbtGu3kRKS8vx44dO3DZZZdh0qRJKC4uxtNPPw273Y4xY8ZEu3mG+fTTT3Hq1CmMHTs22k0xxMSJE1FfX48//elPsFgs8Hg8uOaaazB69OhoNy1iycnJGDhwIF555RX07t0b6enp+OCDD1BcXOxLBzCD6upqAFI+oL9u3brB6XRGoUXUXs3NzXj++ecxatQoBjz+srOzsWrVKpw6dQqffPIJ1q1bh6VLl8Z10ON0OvHMM89g0aJFSEhIiHZzDOf/rblv374YOHAgZs+ejXfffRcTJkyIYssi5/F48JOf/ATXXnstACAvLw8//vgj3nrrLVMFPDt37sQ555wTV/kfwXz00Ud4//338cc//hF9+vRBSUkJnnnmGV/ycry74447sGHDBvzhD3+AxWJBXl4eRo0ahUOHDkW7aYZTjjhys4H44nK58Mgjj0AUxbAXQ5g+4LHZbL5vKT/5yU/w/fff44033sBtt90W5Za138GDB1FTU4P58+f77vN4PNi7dy/efPNNPP/886ZJ8gWApKQk9O3bF6WlpdFuSsS6d+8eEGzn5OTgk08+iVKLjFdRUYH//e9/mDt3brSbYpiNGzdi4sSJGDVqFAApEK+oqMC2bdtMEfBkZmZi6dKlaGxsRENDA7p3746HH37YNPllAHw5gd5Vdl61tbUBoz4Um1wuFx5++GFUVFTgvvvuC3uxh+kDHiVRFNHS0hLtZkTkrLPOwkMPPSS7b8OGDcjOzsbEiRNNFewAQEtLC44ePYrBgwdHuykRGzRokC8PxOvYsWPo0aNHlFpkvJ07d6Jbt26+BF8zaGpqCvh/ZbFYTDc6kJSUhKSkJNTV1eGrr77CtGnTot0kw/Ts2RPp6en43//+h7y8PADSBXTPnj247rrrotw6CsUb7JSVlWHJkiXtSm8wdcDz/PPPY/jw4TjjjDPQ2NiIDz/8EN9++y0WLVoU7aZFJDk5GX379pXdl5iYiK5duwbcH4+ee+45jBw5Eg6HAzU1NXjllVfQ0NBgiimfyy67DIsXL8aWLVtwwQUXoLi4GO+8805cjzj683g82LVrF8aMGQOr1Rrt5hjmZz/7GbZs2QKHw4GcnByUlJTgtddew7hx46LdNEPs3r0bgJQCUFZWhr///e/Izs6Ou9GrxsZGlJWV+W6Xl5ejpKQEqampcDgcGD9+PLZu3YqsrCxkZmZi69atSExMjJtcrFD9q6urg9PpRGVlJQD4vlylp6fH/KrXYH3r3r071qxZg0OHDqGgoAAej8eXk5WamgqbTV8oY+rd0jds2IBvvvkGVVVVSElJQb9+/TBx4kRTrmq6//77kZuba4rCg4888gj27t2L2tpapKWlYcCAAbjmmmviOu/K3+eff47nn38eZWVl6NmzJy677DL88pe/jHazDPHVV1+hsLAQjzzyCLKzs6PdHMMoC/NlZGRg1KhRmDx5su5ftrHso48+wgsvvIATJ04gNTUV5513HqZOnRp39aG+/fZbLF26NOD+MWPGYNasWb7Cg2+//TZOnTqF/Px83HzzzXHzRTFU/3bt2oX169cHPD558mRMmTLldDSx3YL17aqrrsIdd9yh+rwlS5Zg6NChul7D1AEPEREREdCJ6vAQERFR58WAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTi/8SoURkCL2VWMOpbBoP1q1bhz179mDdunXRbgoRdSAGPEQEAFi2bJns9iuvvIJvv/0W9913n+x+s2zxQUSdCwMeIgIADBw4UHY7LS0NgiAE3K/U1NSExMTEjmwaEVHEGPAQkW73338/Tp48iZtvvhnPP/88SkpKMHLkSNx1112YMmWK6iaFs2bNwpAhQzBr1izffdXV1XjppZfwxRdf+DbjHDt2LK644oqgu6yvXLkSJSUleOyxx2CxyFMQFy5cCLfbjaKiIgDAm2++iY8//hhHjx5FU1MTevbsiYsuugiXXXZZ0A0/y8vLcccdd2DmzJkBu4Wr9bG0tBQvvfQSvv76a9TX16NXr1741a9+hV//+te+YzweD7Zu3Yr33nsPTqcTdrsdDocDF198McaPH6/9AyciwzDgIaKwVFVVYe3atZg4cSKmTp0KQRDCen51dTUWLFgAi8WCyZMno1evXti/fz+2bNmCiooKzJw5U/O5F198MVauXIlvvvkGw4YN891/9OhRFBcX48Ybb/Tdd/z4cYwaNQo9e/aEzWbDDz/8gC1btuDo0aNBXyMcR44cwb333guHw4Hrr78e6enp2L17N55++mmcPHkSV111FQDg1Vdfxcsvv4wrrrgCQ4YMgcvlwrFjx3Dq1ClD2kFEoTHgIaKw1NXV4e6778aZZ57Zrue/9NJLOHXqFNasWQOHwwEAOOuss5CQkIC///3vuPzyyzXzhIYPH45u3bph165dsoBn586dsNlsGD16tO++G264wfdvj8eDwYMHo2vXrli/fj2uv/56pKamtqv9/p599lkkJyfjz3/+M1JSUgAAw4YNg8vlwrZt2/Cb3/wGqamp+O6779C3b1/ZyNA555wT8esTkX5clk5EYenSpUu7gx0A+OKLLzB06FB0794dbrfb92f48OEAgD179mg+12q14sILL8Qnn3yC+vp6AFIw8/7772PkyJHo2rWr79hDhw6hqKgIN910E6655hpMnToVjz32GDweD0pLS9vdfq/m5mZ88803+H//7/8hMTExoC8tLS04cOAAACA/Px8//PADnnzySezevdvXdiI6fTjCQ0Rh6d69e0TPr6mpweeff46pU6eqPl5bWxv0+RdffDFee+01fPjhh7jkkkuwe/duVFVVYdy4cb5jnE4n7rvvPmRnZ2P69Ono2bMn7HY7iouL8dRTT6G5uTmiPgDSSJfb7cabb76JN998U/WYkydPAgAmTZqEpKQkvP/++9ixYwcsFgsGDx6M6667Dj/5yU8ibgsRhcaAh4jCopWzY7fb4XK5Au73XvS9unbtin79+uGaa65RPU+ogConJwf5+fnYtWsXLrnkEuzatQvdu3fH2Wef7Tvm008/RVNTE+bOnYsePXr47i8pKQl6bgBISEgAALS0tATtR5cuXWCxWHDRRRfhV7/6leq5evbsCUAamZowYQImTJiAU6dO4euvv8YLL7yAwsJCbNiwgavciE4DBjxEZIgePXrghx9+kN33zTffoLGxUXbfiBEj8OWXX6JXr17tzqMZO3YsnnzySXz33Xf4/PPPcdlll8lWbXmDMrvd7rtPFEW88847Ic/drVs32O32gL589tlnstuJiYkYOnQoDh06hH79+gVd+eWvS5cu+PnPf47Kyko888wzqKioYG0jotOAAQ8RGeKiiy7Cpk2bsGnTJgwZMgRHjhzBm2++6Uvm9br66qvx9ddfY/HixfjNb36D7OxsNDc3o6KiAl9++SVuvfVWnHHGGUFfa/To0Xjuuefw6KOPoqWlJWD5+LBhw2Cz2fDoo4/i8ssvR0tLC9566y1dq6IEQcCFF16InTt3IjMzE/369UNxcTE++OCDgGNvvPFGLF68GPfddx8uvfRS9OjRAw0NDSgrK8Pnn3+OJUuWAABWrFiBvn37on///khLS4PT6cTrr7+OHj16IDMzM2SbiChyDHiIyBCXX3456uvrsWvXLvzzn/9Efn4+/vSnP2HVqlWy47p3747ly5fjlVdewauvvooTJ04gOTkZPXv2xDnnnIMuXbqEfK2UlBSce+65+OCDDzBo0CBkZ2fLHu/duzfmzJmDF198EQ899BC6du2K0aNHY8KECXjwwQdDnv/6668HAGzfvh2NjY0488wzMX/+fFktIUCaXisqKsIrr7yCF198ETU1NejSpQuysrJ8SdgAcOaZZ+KTTz7BO++8g4aGBqSnp2PYsGG48sordY8MEVFkBFEUxWg3goiIiKgjcVk6ERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAhIiIi02PAQ0RERKbHgIeIiIhMjwEPERERmR4DHiIiIjI9BjxERERkev8fhD4SaU5UVv8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6616 with a standard deviation of 0.0183\n",
      "XGBoost optimized model r2_score 0.7139 with a standard deviation of 0.0276\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb.joblib']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"OUTPUT/xgb_reg.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.648423     0.031385\n",
      "1                    TP       193.000000     5.142416\n",
      "2                    TN       169.500000     5.986095\n",
      "3                    FP        43.600000     4.765618\n",
      "4                    FN        43.100000     6.026792\n",
      "5              Accuracy         0.806989     0.008805\n",
      "6             Precision         0.815939     0.016984\n",
      "7           Sensitivity         0.817819     0.021253\n",
      "8           Specificity         0.795600     0.019413\n",
      "9              F1 score         0.816546     0.008341\n",
      "10  F1 score (weighted)         0.806987     0.008750\n",
      "11     F1 score (macro)         0.806359     0.008990\n",
      "12    Balanced Accuracy         0.806711     0.008763\n",
      "13                  MCC         0.613460     0.017771\n",
      "14                  NPV         0.797560     0.024753\n",
      "15              ROC_AUC         0.806711     0.008763\n",
      "CPU times: user 5 s, sys: 10.2 s, total: 15.2 s\n",
      "Wall time: 502 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=16)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=16)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:18:51,866] A new study created in memory with name: KNNregressor\n",
      "[I 2023-12-11 12:18:52,279] Trial 0 finished with value: 0.5933661418330989 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 0 with value: 0.5933661418330989.\n",
      "[I 2023-12-11 12:18:53,011] Trial 1 finished with value: 0.6199120180174899 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 1 with value: 0.6199120180174899.\n",
      "[I 2023-12-11 12:18:53,526] Trial 2 finished with value: 0.48578274506187913 and parameters: {'n_neighbors': 30, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 53}. Best is trial 1 with value: 0.6199120180174899.\n",
      "[I 2023-12-11 12:18:54,188] Trial 3 finished with value: 0.5905927293611303 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 1 with value: 0.6199120180174899.\n",
      "[I 2023-12-11 12:18:54,752] Trial 4 finished with value: 0.607592613809135 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 74}. Best is trial 1 with value: 0.6199120180174899.\n",
      "[I 2023-12-11 12:18:55,409] Trial 5 finished with value: 0.6288914024289171 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 5 with value: 0.6288914024289171.\n",
      "[I 2023-12-11 12:18:56,049] Trial 6 finished with value: 0.5355056971507524 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 58}. Best is trial 5 with value: 0.6288914024289171.\n",
      "[I 2023-12-11 12:18:56,641] Trial 7 finished with value: 0.49137838417715035 and parameters: {'n_neighbors': 29, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 5 with value: 0.6288914024289171.\n",
      "[I 2023-12-11 12:18:57,279] Trial 8 finished with value: 0.622811328190111 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 5 with value: 0.6288914024289171.\n",
      "[I 2023-12-11 12:18:57,889] Trial 9 finished with value: 0.625318416514077 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 52}. Best is trial 5 with value: 0.6288914024289171.\n",
      "[I 2023-12-11 12:18:58,476] Trial 10 finished with value: 0.5643404074727734 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 5 with value: 0.6288914024289171.\n",
      "[I 2023-12-11 12:18:59,216] Trial 11 finished with value: 0.6447024435043115 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 11 with value: 0.6447024435043115.\n",
      "[I 2023-12-11 12:18:59,928] Trial 12 finished with value: 0.6447024435043115 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 11 with value: 0.6447024435043115.\n",
      "[I 2023-12-11 12:19:00,636] Trial 13 finished with value: 0.64495563749271 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:01,335] Trial 14 finished with value: 0.6447024435043115 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:02,014] Trial 15 finished with value: 0.5866091246376328 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:02,699] Trial 16 finished with value: 0.64495563749271 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:03,372] Trial 17 finished with value: 0.6114666958415338 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:04,081] Trial 18 finished with value: 0.5868403097368844 and parameters: {'n_neighbors': 14, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:04,828] Trial 19 finished with value: 0.5952063908004118 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:05,521] Trial 20 finished with value: 0.64495563749271 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:06,206] Trial 21 finished with value: 0.6430999297662173 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:06,865] Trial 22 finished with value: 0.64495563749271 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:07,489] Trial 23 finished with value: 0.6360014817974702 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:08,138] Trial 24 finished with value: 0.64495563749271 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:08,833] Trial 25 finished with value: 0.6360014817974702 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:09,506] Trial 26 finished with value: 0.5933661418330989 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:10,169] Trial 27 finished with value: 0.64495563749271 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:10,853] Trial 28 finished with value: 0.600019343121278 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:11,336] Trial 29 finished with value: 0.5933661418330989 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 61}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:12,079] Trial 30 finished with value: 0.6206319714454169 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:12,783] Trial 31 finished with value: 0.64495563749271 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:13,420] Trial 32 finished with value: 0.639842523893426 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 13 with value: 0.64495563749271.\n",
      "[I 2023-12-11 12:19:14,093] Trial 33 finished with value: 0.6473117695438881 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:14,745] Trial 34 finished with value: 0.6311861044941716 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:15,439] Trial 35 finished with value: 0.639842523893426 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:15,905] Trial 36 finished with value: 0.6174020496614674 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 66}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:16,660] Trial 37 finished with value: 0.639842523893426 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:17,329] Trial 38 finished with value: 0.6311861044941716 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:17,809] Trial 39 finished with value: 0.6334576818478992 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 55}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:18,210] Trial 40 finished with value: 0.625318416514077 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:18,985] Trial 41 finished with value: 0.5745460171245002 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:19,668] Trial 42 finished with value: 0.64495563749271 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:20,334] Trial 43 finished with value: 0.6473117695438881 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:21,021] Trial 44 finished with value: 0.6447024435043115 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:21,664] Trial 45 finished with value: 0.6473117695438881 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:22,200] Trial 46 finished with value: 0.6414529995919096 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:22,975] Trial 47 finished with value: 0.6447024435043115 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:23,599] Trial 48 finished with value: 0.639842523893426 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:24,182] Trial 49 finished with value: 0.6101997416076383 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 48}. Best is trial 33 with value: 0.6473117695438881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6473\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 51\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.647596\n",
      "1                    TP  393.000000\n",
      "2                    TN  343.000000\n",
      "3                    FP   87.000000\n",
      "4                    FN   76.000000\n",
      "5              Accuracy    0.818687\n",
      "6             Precision    0.818750\n",
      "7           Sensitivity    0.837953\n",
      "8           Specificity    0.797700\n",
      "9              F1 score    0.828240\n",
      "10  F1 score (weighted)    0.818564\n",
      "11     F1 score (macro)    0.818125\n",
      "12    Balanced Accuracy    0.817814\n",
      "13                  MCC    0.636496\n",
      "14                  NPV    0.818600\n",
      "15              ROC_AUC    0.817814\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_0_cat = np.where((y_pred_knn_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:19:25,178] Trial 50 finished with value: 0.608945087063453 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:25,891] Trial 51 finished with value: 0.638629406799405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:26,612] Trial 52 finished with value: 0.6355869296585942 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:27,361] Trial 53 finished with value: 0.6355869296585942 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:28,108] Trial 54 finished with value: 0.5975245814882638 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:28,835] Trial 55 finished with value: 0.6390387464537675 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:29,541] Trial 56 finished with value: 0.638629406799405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:30,266] Trial 57 finished with value: 0.6287683627380101 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:30,921] Trial 58 finished with value: 0.6093083703071016 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 52}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:31,649] Trial 59 finished with value: 0.6355869296585942 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:32,453] Trial 60 finished with value: 0.6158017854696268 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:33,143] Trial 61 finished with value: 0.6371640887695831 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:33,850] Trial 62 finished with value: 0.638629406799405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:34,539] Trial 63 finished with value: 0.6390387464537675 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:35,250] Trial 64 finished with value: 0.6371640887695831 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:35,997] Trial 65 finished with value: 0.6355869296585942 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:36,727] Trial 66 finished with value: 0.624364865121989 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:37,448] Trial 67 finished with value: 0.6334380369400302 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:37,957] Trial 68 finished with value: 0.54581262260634 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:38,751] Trial 69 finished with value: 0.638629406799405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:39,493] Trial 70 finished with value: 0.6264015180790146 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:40,222] Trial 71 finished with value: 0.6371640887695831 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:40,928] Trial 72 finished with value: 0.6371640887695831 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:41,664] Trial 73 finished with value: 0.6287683627380101 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:42,359] Trial 74 finished with value: 0.6355869296585942 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:43,114] Trial 75 finished with value: 0.6318848917531767 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:43,828] Trial 76 finished with value: 0.6334380369400302 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:44,596] Trial 77 finished with value: 0.6371640887695831 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:45,057] Trial 78 finished with value: 0.5188841302983687 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:45,832] Trial 79 finished with value: 0.6390387464537675 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:46,504] Trial 80 finished with value: 0.6334380369400302 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:47,206] Trial 81 finished with value: 0.638629406799405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:47,946] Trial 82 finished with value: 0.6355869296585942 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:48,671] Trial 83 finished with value: 0.6371640887695831 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:49,387] Trial 84 finished with value: 0.638629406799405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:50,122] Trial 85 finished with value: 0.6264015180790146 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:50,886] Trial 86 finished with value: 0.6371640887695831 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:51,618] Trial 87 finished with value: 0.6142212789748982 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:52,409] Trial 88 finished with value: 0.6355869296585942 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:53,176] Trial 89 finished with value: 0.638629406799405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:53,916] Trial 90 finished with value: 0.6334380369400302 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:54,654] Trial 91 finished with value: 0.6371640887695831 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:55,417] Trial 92 finished with value: 0.6355869296585942 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:56,181] Trial 93 finished with value: 0.6018516669122689 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:56,927] Trial 94 finished with value: 0.638629406799405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:57,351] Trial 95 finished with value: 0.6340979237567699 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:58,152] Trial 96 finished with value: 0.6355869296585942 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:58,855] Trial 97 finished with value: 0.6158017854696268 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:19:59,577] Trial 98 finished with value: 0.6255289964641666 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:20:00,273] Trial 99 finished with value: 0.6390387464537675 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 33 with value: 0.6473117695438881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6473\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 51\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.647596    0.676203\n",
      "1                    TP  393.000000  389.000000\n",
      "2                    TN  343.000000  343.000000\n",
      "3                    FP   87.000000   81.000000\n",
      "4                    FN   76.000000   86.000000\n",
      "5              Accuracy    0.818687    0.814238\n",
      "6             Precision    0.818750    0.827660\n",
      "7           Sensitivity    0.837953    0.818947\n",
      "8           Specificity    0.797700    0.809000\n",
      "9              F1 score    0.828240    0.823280\n",
      "10  F1 score (weighted)    0.818564    0.814291\n",
      "11     F1 score (macro)    0.818125    0.813750\n",
      "12    Balanced Accuracy    0.817814    0.813955\n",
      "13                  MCC    0.636496    0.627551\n",
      "14                  NPV    0.818600    0.799500\n",
      "15              ROC_AUC    0.817814    0.813955\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_1_cat = np.where((y_pred_knn_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:20:01,150] Trial 100 finished with value: 0.6453911861316908 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 33 with value: 0.6473117695438881.\n",
      "[I 2023-12-11 12:20:01,855] Trial 101 finished with value: 0.6544509804207109 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 101 with value: 0.6544509804207109.\n",
      "[I 2023-12-11 12:20:02,508] Trial 102 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:03,214] Trial 103 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:03,926] Trial 104 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:04,650] Trial 105 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:05,344] Trial 106 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:06,020] Trial 107 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:06,727] Trial 108 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:07,439] Trial 109 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:08,131] Trial 110 finished with value: 0.6527541282439235 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:08,793] Trial 111 finished with value: 0.6527541282439235 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:09,554] Trial 112 finished with value: 0.6527541282439235 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:10,279] Trial 113 finished with value: 0.6527541282439235 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:11,024] Trial 114 finished with value: 0.6527541282439235 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:11,775] Trial 115 finished with value: 0.6527541282439235 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:12,555] Trial 116 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:13,271] Trial 117 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:13,993] Trial 118 finished with value: 0.6500663162055687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:14,757] Trial 119 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:15,475] Trial 120 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:16,210] Trial 121 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:16,907] Trial 122 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:17,628] Trial 123 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:18,371] Trial 124 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:19,105] Trial 125 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:19,851] Trial 126 finished with value: 0.6544509804207109 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:20,558] Trial 127 finished with value: 0.5305040971370883 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:21,356] Trial 128 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:22,091] Trial 129 finished with value: 0.6305918160950903 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:22,783] Trial 130 finished with value: 0.6544509804207109 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:23,487] Trial 131 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:24,155] Trial 132 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:24,912] Trial 133 finished with value: 0.6544509804207109 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:25,595] Trial 134 finished with value: 0.6527541282439235 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:26,306] Trial 135 finished with value: 0.6544509804207109 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:27,019] Trial 136 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:27,730] Trial 137 finished with value: 0.6204154155868346 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:28,426] Trial 138 finished with value: 0.6505181958638804 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:29,188] Trial 139 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:29,778] Trial 140 finished with value: 0.6474981097362073 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:30,548] Trial 141 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:31,249] Trial 142 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:31,980] Trial 143 finished with value: 0.6544509804207109 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:32,715] Trial 144 finished with value: 0.6527541282439235 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:33,435] Trial 145 finished with value: 0.656711662482864 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:34,122] Trial 146 finished with value: 0.6544509804207109 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:34,788] Trial 147 finished with value: 0.6527541282439235 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:35,531] Trial 148 finished with value: 0.5745114288545524 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:36,321] Trial 149 finished with value: 0.6544509804207109 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 102 with value: 0.656711662482864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6567\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 42\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.647596    0.676203    0.646090\n",
      "1                    TP  393.000000  389.000000  399.000000\n",
      "2                    TN  343.000000  343.000000  341.000000\n",
      "3                    FP   87.000000   81.000000   87.000000\n",
      "4                    FN   76.000000   86.000000   72.000000\n",
      "5              Accuracy    0.818687    0.814238    0.823137\n",
      "6             Precision    0.818750    0.827660    0.820988\n",
      "7           Sensitivity    0.837953    0.818947    0.847134\n",
      "8           Specificity    0.797700    0.809000    0.796700\n",
      "9              F1 score    0.828240    0.823280    0.833856\n",
      "10  F1 score (weighted)    0.818564    0.814291    0.822946\n",
      "11     F1 score (macro)    0.818125    0.813750    0.822398\n",
      "12    Balanced Accuracy    0.817814    0.813955    0.821931\n",
      "13                  MCC    0.636496    0.627551    0.645257\n",
      "14                  NPV    0.818600    0.799500    0.825700\n",
      "15              ROC_AUC    0.817814    0.813955    0.821931\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_2_cat = np.where((y_pred_knn_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:20:37,263] Trial 150 finished with value: 0.6334760802152195 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:37,990] Trial 151 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:38,736] Trial 152 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:39,470] Trial 153 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:40,215] Trial 154 finished with value: 0.6365890927302544 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:40,943] Trial 155 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:41,459] Trial 156 finished with value: 0.618838140065413 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:42,271] Trial 157 finished with value: 0.6365890927302544 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:42,982] Trial 158 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:43,704] Trial 159 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:44,451] Trial 160 finished with value: 0.6365890927302544 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:45,167] Trial 161 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:45,896] Trial 162 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:46,604] Trial 163 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:47,328] Trial 164 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:48,038] Trial 165 finished with value: 0.6334760802152195 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:48,705] Trial 166 finished with value: 0.6365890927302544 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:49,394] Trial 167 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:50,123] Trial 168 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:50,867] Trial 169 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:51,589] Trial 170 finished with value: 0.580004245606833 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:52,335] Trial 171 finished with value: 0.6365890927302544 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:53,051] Trial 172 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:53,749] Trial 173 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:54,495] Trial 174 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:55,044] Trial 175 finished with value: 0.6323062964533147 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:55,841] Trial 176 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:56,608] Trial 177 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:57,337] Trial 178 finished with value: 0.6365890927302544 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:58,091] Trial 179 finished with value: 0.618838140065413 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:58,817] Trial 180 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:20:59,533] Trial 181 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:00,223] Trial 182 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:00,972] Trial 183 finished with value: 0.6365890927302544 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:01,735] Trial 184 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:02,484] Trial 185 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:03,210] Trial 186 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:03,863] Trial 187 finished with value: 0.6323062964533147 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:04,630] Trial 188 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:05,394] Trial 189 finished with value: 0.6334760802152195 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:06,142] Trial 190 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:06,900] Trial 191 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:07,630] Trial 192 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:08,385] Trial 193 finished with value: 0.6365890927302544 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:09,103] Trial 194 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:09,820] Trial 195 finished with value: 0.6365890927302544 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:10,568] Trial 196 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:11,250] Trial 197 finished with value: 0.6340704277283201 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:11,962] Trial 198 finished with value: 0.6354432456416644 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:12,664] Trial 199 finished with value: 0.6365890927302544 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6567\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 42\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.647596    0.676203    0.646090    0.662056\n",
      "1                    TP  393.000000  389.000000  399.000000  397.000000\n",
      "2                    TN  343.000000  343.000000  341.000000  348.000000\n",
      "3                    FP   87.000000   81.000000   87.000000   87.000000\n",
      "4                    FN   76.000000   86.000000   72.000000   67.000000\n",
      "5              Accuracy    0.818687    0.814238    0.823137    0.828699\n",
      "6             Precision    0.818750    0.827660    0.820988    0.820248\n",
      "7           Sensitivity    0.837953    0.818947    0.847134    0.855603\n",
      "8           Specificity    0.797700    0.809000    0.796700    0.800000\n",
      "9              F1 score    0.828240    0.823280    0.833856    0.837553\n",
      "10  F1 score (weighted)    0.818564    0.814291    0.822946    0.828490\n",
      "11     F1 score (macro)    0.818125    0.813750    0.822398    0.828188\n",
      "12    Balanced Accuracy    0.817814    0.813955    0.821931    0.827802\n",
      "13                  MCC    0.636496    0.627551    0.645257    0.657201\n",
      "14                  NPV    0.818600    0.799500    0.825700    0.838600\n",
      "15              ROC_AUC    0.817814    0.813955    0.821931    0.827802\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_3_cat = np.where((y_pred_knn_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:21:13,516] Trial 200 finished with value: 0.6475117773430408 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:14,186] Trial 201 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:14,851] Trial 202 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:15,499] Trial 203 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:16,172] Trial 204 finished with value: 0.6481759362181269 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:16,806] Trial 205 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:17,543] Trial 206 finished with value: 0.6414953178936571 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:18,310] Trial 207 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:19,030] Trial 208 finished with value: 0.6475117773430408 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:19,741] Trial 209 finished with value: 0.6481759362181269 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:20,441] Trial 210 finished with value: 0.6355169575112356 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:21,132] Trial 211 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:21,776] Trial 212 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:22,451] Trial 213 finished with value: 0.6475117773430408 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:23,144] Trial 214 finished with value: 0.6481759362181269 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:23,808] Trial 215 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:24,516] Trial 216 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:25,182] Trial 217 finished with value: 0.6475117773430408 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:25,918] Trial 218 finished with value: 0.6481759362181269 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:26,667] Trial 219 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:27,400] Trial 220 finished with value: 0.6481759362181269 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:28,111] Trial 221 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:28,855] Trial 222 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:29,550] Trial 223 finished with value: 0.6475117773430408 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:30,267] Trial 224 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:30,788] Trial 225 finished with value: 0.6414953178936571 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:31,551] Trial 226 finished with value: 0.5997397558676973 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:32,279] Trial 227 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:33,001] Trial 228 finished with value: 0.6481759362181269 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:33,703] Trial 229 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:34,406] Trial 230 finished with value: 0.6481759362181269 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:35,075] Trial 231 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:35,729] Trial 232 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:36,432] Trial 233 finished with value: 0.6475117773430408 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:37,144] Trial 234 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:37,799] Trial 235 finished with value: 0.6318114753320606 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:38,510] Trial 236 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:39,223] Trial 237 finished with value: 0.6475117773430408 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:39,932] Trial 238 finished with value: 0.6244593577886465 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:40,624] Trial 239 finished with value: 0.6481759362181269 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:41,333] Trial 240 finished with value: 0.6435848143289223 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:42,063] Trial 241 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:42,762] Trial 242 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:43,502] Trial 243 finished with value: 0.6475117773430408 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:44,299] Trial 244 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:44,997] Trial 245 finished with value: 0.6481759362181269 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:45,716] Trial 246 finished with value: 0.6481759362181269 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:46,430] Trial 247 finished with value: 0.6319588566838038 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:47,088] Trial 248 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:47,715] Trial 249 finished with value: 0.648111347260625 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6567\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 42\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.647596    0.676203    0.646090    0.662056   \n",
      "1                    TP  393.000000  389.000000  399.000000  397.000000   \n",
      "2                    TN  343.000000  343.000000  341.000000  348.000000   \n",
      "3                    FP   87.000000   81.000000   87.000000   87.000000   \n",
      "4                    FN   76.000000   86.000000   72.000000   67.000000   \n",
      "5              Accuracy    0.818687    0.814238    0.823137    0.828699   \n",
      "6             Precision    0.818750    0.827660    0.820988    0.820248   \n",
      "7           Sensitivity    0.837953    0.818947    0.847134    0.855603   \n",
      "8           Specificity    0.797700    0.809000    0.796700    0.800000   \n",
      "9              F1 score    0.828240    0.823280    0.833856    0.837553   \n",
      "10  F1 score (weighted)    0.818564    0.814291    0.822946    0.828490   \n",
      "11     F1 score (macro)    0.818125    0.813750    0.822398    0.828188   \n",
      "12    Balanced Accuracy    0.817814    0.813955    0.821931    0.827802   \n",
      "13                  MCC    0.636496    0.627551    0.645257    0.657201   \n",
      "14                  NPV    0.818600    0.799500    0.825700    0.838600   \n",
      "15              ROC_AUC    0.817814    0.813955    0.821931    0.827802   \n",
      "\n",
      "          Set4  \n",
      "0     0.650331  \n",
      "1   383.000000  \n",
      "2   351.000000  \n",
      "3    78.000000  \n",
      "4    87.000000  \n",
      "5     0.816463  \n",
      "6     0.830803  \n",
      "7     0.814894  \n",
      "8     0.818200  \n",
      "9     0.822771  \n",
      "10    0.816528  \n",
      "11    0.816230  \n",
      "12    0.816538  \n",
      "13    0.632624  \n",
      "14    0.801400  \n",
      "15    0.816538  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_4_cat = np.where((y_pred_knn_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:21:48,635] Trial 250 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:49,344] Trial 251 finished with value: 0.6499769426661457 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:50,036] Trial 252 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:50,768] Trial 253 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:51,357] Trial 254 finished with value: 0.6433728038030322 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:52,130] Trial 255 finished with value: 0.6499769426661457 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:52,819] Trial 256 finished with value: 0.6103395058862009 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:53,539] Trial 257 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:54,280] Trial 258 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:54,990] Trial 259 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:55,678] Trial 260 finished with value: 0.6499769426661457 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:56,396] Trial 261 finished with value: 0.5765821575833282 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:57,120] Trial 262 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:57,881] Trial 263 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:58,612] Trial 264 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:21:59,362] Trial 265 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:00,091] Trial 266 finished with value: 0.611907852470388 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:00,611] Trial 267 finished with value: 0.6448404885441523 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:01,355] Trial 268 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:02,070] Trial 269 finished with value: 0.6499769426661457 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:02,772] Trial 270 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:03,480] Trial 271 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:04,221] Trial 272 finished with value: 0.6499769426661457 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:04,971] Trial 273 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:05,711] Trial 274 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:06,505] Trial 275 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:07,187] Trial 276 finished with value: 0.6499769426661457 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:07,907] Trial 277 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:08,858] Trial 278 finished with value: 0.6349508823384932 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:09,627] Trial 279 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:10,367] Trial 280 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:11,106] Trial 281 finished with value: 0.6499769426661457 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:11,845] Trial 282 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:12,588] Trial 283 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:13,280] Trial 284 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:14,018] Trial 285 finished with value: 0.627444013704257 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:14,743] Trial 286 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:15,397] Trial 287 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:16,118] Trial 288 finished with value: 0.6499769426661457 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:16,779] Trial 289 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:17,279] Trial 290 finished with value: 0.6433728038030322 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:18,028] Trial 291 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:18,755] Trial 292 finished with value: 0.6499769426661457 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:19,469] Trial 293 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:20,198] Trial 294 finished with value: 0.6449278773432513 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:20,908] Trial 295 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:21,628] Trial 296 finished with value: 0.6499769426661457 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:22,331] Trial 297 finished with value: 0.6502159040228671 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:23,004] Trial 298 finished with value: 0.6466783462183859 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 102 with value: 0.656711662482864.\n",
      "[I 2023-12-11 12:22:23,659] Trial 299 finished with value: 0.6433728038030322 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 102 with value: 0.656711662482864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6567\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 42\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.647596    0.676203    0.646090    0.662056   \n",
      "1                    TP  393.000000  389.000000  399.000000  397.000000   \n",
      "2                    TN  343.000000  343.000000  341.000000  348.000000   \n",
      "3                    FP   87.000000   81.000000   87.000000   87.000000   \n",
      "4                    FN   76.000000   86.000000   72.000000   67.000000   \n",
      "5              Accuracy    0.818687    0.814238    0.823137    0.828699   \n",
      "6             Precision    0.818750    0.827660    0.820988    0.820248   \n",
      "7           Sensitivity    0.837953    0.818947    0.847134    0.855603   \n",
      "8           Specificity    0.797700    0.809000    0.796700    0.800000   \n",
      "9              F1 score    0.828240    0.823280    0.833856    0.837553   \n",
      "10  F1 score (weighted)    0.818564    0.814291    0.822946    0.828490   \n",
      "11     F1 score (macro)    0.818125    0.813750    0.822398    0.828188   \n",
      "12    Balanced Accuracy    0.817814    0.813955    0.821931    0.827802   \n",
      "13                  MCC    0.636496    0.627551    0.645257    0.657201   \n",
      "14                  NPV    0.818600    0.799500    0.825700    0.838600   \n",
      "15              ROC_AUC    0.817814    0.813955    0.821931    0.827802   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.650331    0.670738  \n",
      "1   383.000000  382.000000  \n",
      "2   351.000000  344.000000  \n",
      "3    78.000000   87.000000  \n",
      "4    87.000000   86.000000  \n",
      "5     0.816463    0.807564  \n",
      "6     0.830803    0.814499  \n",
      "7     0.814894    0.816239  \n",
      "8     0.818200    0.798100  \n",
      "9     0.822771    0.815368  \n",
      "10    0.816528    0.807555  \n",
      "11    0.816230    0.807220  \n",
      "12    0.816538    0.807192  \n",
      "13    0.632624    0.614441  \n",
      "14    0.801400    0.800000  \n",
      "15    0.816538    0.807192  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_5_cat = np.where((y_pred_knn_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:22:24,595] Trial 300 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:25,292] Trial 301 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:25,940] Trial 302 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:26,579] Trial 303 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:27,216] Trial 304 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:27,910] Trial 305 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:28,593] Trial 306 finished with value: 0.6416914123156833 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:29,287] Trial 307 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:30,016] Trial 308 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:30,722] Trial 309 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:31,439] Trial 310 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:32,170] Trial 311 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:32,924] Trial 312 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:33,603] Trial 313 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:34,318] Trial 314 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:35,034] Trial 315 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:35,747] Trial 316 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:36,426] Trial 317 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:37,175] Trial 318 finished with value: 0.6621831357765847 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:37,780] Trial 319 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:38,445] Trial 320 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:38,850] Trial 321 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:39,259] Trial 322 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:39,674] Trial 323 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:40,124] Trial 324 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:40,540] Trial 325 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:41,037] Trial 326 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:41,657] Trial 327 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:42,275] Trial 328 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:42,771] Trial 329 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:43,540] Trial 330 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 76}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:44,166] Trial 331 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:44,755] Trial 332 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:45,403] Trial 333 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:46,092] Trial 334 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:46,819] Trial 335 finished with value: 0.6416914123156833 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 75}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:47,407] Trial 336 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:47,969] Trial 337 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:48,829] Trial 338 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:49,656] Trial 339 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:50,321] Trial 340 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 77}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:51,100] Trial 341 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:51,692] Trial 342 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:52,463] Trial 343 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 64}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:53,144] Trial 344 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:53,699] Trial 345 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:54,113] Trial 346 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:54,528] Trial 347 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:54,944] Trial 348 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:55,361] Trial 349 finished with value: 0.6568009232098724 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6622\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 67\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.647596    0.676203    0.646090    0.662056   \n",
      "1                    TP  393.000000  389.000000  399.000000  397.000000   \n",
      "2                    TN  343.000000  343.000000  341.000000  348.000000   \n",
      "3                    FP   87.000000   81.000000   87.000000   87.000000   \n",
      "4                    FN   76.000000   86.000000   72.000000   67.000000   \n",
      "5              Accuracy    0.818687    0.814238    0.823137    0.828699   \n",
      "6             Precision    0.818750    0.827660    0.820988    0.820248   \n",
      "7           Sensitivity    0.837953    0.818947    0.847134    0.855603   \n",
      "8           Specificity    0.797700    0.809000    0.796700    0.800000   \n",
      "9              F1 score    0.828240    0.823280    0.833856    0.837553   \n",
      "10  F1 score (weighted)    0.818564    0.814291    0.822946    0.828490   \n",
      "11     F1 score (macro)    0.818125    0.813750    0.822398    0.828188   \n",
      "12    Balanced Accuracy    0.817814    0.813955    0.821931    0.827802   \n",
      "13                  MCC    0.636496    0.627551    0.645257    0.657201   \n",
      "14                  NPV    0.818600    0.799500    0.825700    0.838600   \n",
      "15              ROC_AUC    0.817814    0.813955    0.821931    0.827802   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.650331    0.670738    0.612010  \n",
      "1   383.000000  382.000000  374.000000  \n",
      "2   351.000000  344.000000  354.000000  \n",
      "3    78.000000   87.000000   73.000000  \n",
      "4    87.000000   86.000000   98.000000  \n",
      "5     0.816463    0.807564    0.809789  \n",
      "6     0.830803    0.814499    0.836689  \n",
      "7     0.814894    0.816239    0.792373  \n",
      "8     0.818200    0.798100    0.829000  \n",
      "9     0.822771    0.815368    0.813928  \n",
      "10    0.816528    0.807555    0.809906  \n",
      "11    0.816230    0.807220    0.809694  \n",
      "12    0.816538    0.807192    0.810706  \n",
      "13    0.632624    0.614441    0.620643  \n",
      "14    0.801400    0.800000    0.783200  \n",
      "15    0.816538    0.807192    0.810706  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_6_cat = np.where((y_pred_knn_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:22:56,115] Trial 350 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 75}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:56,767] Trial 351 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:57,532] Trial 352 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:58,602] Trial 353 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:59,259] Trial 354 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:22:59,957] Trial 355 finished with value: 0.6387853031962673 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:00,671] Trial 356 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:01,255] Trial 357 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 79}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:01,913] Trial 358 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:03,116] Trial 359 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 75}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:03,796] Trial 360 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:04,534] Trial 361 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:05,174] Trial 362 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:06,100] Trial 363 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:06,640] Trial 364 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 76}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:07,309] Trial 365 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:07,887] Trial 366 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:08,575] Trial 367 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:09,075] Trial 368 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:09,520] Trial 369 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:09,966] Trial 370 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:10,410] Trial 371 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:10,864] Trial 372 finished with value: 0.5703036511027493 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:11,361] Trial 373 finished with value: 0.5478194488532522 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:11,960] Trial 374 finished with value: 0.5532712519816676 and parameters: {'n_neighbors': 19, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:12,567] Trial 375 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:13,171] Trial 376 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 57}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:13,830] Trial 377 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:14,636] Trial 378 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:15,461] Trial 379 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:16,112] Trial 380 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:16,959] Trial 381 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:17,638] Trial 382 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:18,379] Trial 383 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:18,972] Trial 384 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:19,749] Trial 385 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:20,500] Trial 386 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:21,245] Trial 387 finished with value: 0.6398030490228042 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:21,911] Trial 388 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:22,713] Trial 389 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:23,488] Trial 390 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:24,252] Trial 391 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:24,688] Trial 392 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:25,443] Trial 393 finished with value: 0.6387853031962673 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:26,214] Trial 394 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:26,991] Trial 395 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:27,788] Trial 396 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:28,536] Trial 397 finished with value: 0.6517275717010903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:29,199] Trial 398 finished with value: 0.6546658784643471 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:29,738] Trial 399 finished with value: 0.6487187856881974 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 64}. Best is trial 300 with value: 0.6621831357765847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6622\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 67\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.647596    0.676203    0.646090    0.662056   \n",
      "1                    TP  393.000000  389.000000  399.000000  397.000000   \n",
      "2                    TN  343.000000  343.000000  341.000000  348.000000   \n",
      "3                    FP   87.000000   81.000000   87.000000   87.000000   \n",
      "4                    FN   76.000000   86.000000   72.000000   67.000000   \n",
      "5              Accuracy    0.818687    0.814238    0.823137    0.828699   \n",
      "6             Precision    0.818750    0.827660    0.820988    0.820248   \n",
      "7           Sensitivity    0.837953    0.818947    0.847134    0.855603   \n",
      "8           Specificity    0.797700    0.809000    0.796700    0.800000   \n",
      "9              F1 score    0.828240    0.823280    0.833856    0.837553   \n",
      "10  F1 score (weighted)    0.818564    0.814291    0.822946    0.828490   \n",
      "11     F1 score (macro)    0.818125    0.813750    0.822398    0.828188   \n",
      "12    Balanced Accuracy    0.817814    0.813955    0.821931    0.827802   \n",
      "13                  MCC    0.636496    0.627551    0.645257    0.657201   \n",
      "14                  NPV    0.818600    0.799500    0.825700    0.838600   \n",
      "15              ROC_AUC    0.817814    0.813955    0.821931    0.827802   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.650331    0.670738    0.612010    0.623967  \n",
      "1   383.000000  382.000000  374.000000  384.000000  \n",
      "2   351.000000  344.000000  354.000000  327.000000  \n",
      "3    78.000000   87.000000   73.000000   91.000000  \n",
      "4    87.000000   86.000000   98.000000   97.000000  \n",
      "5     0.816463    0.807564    0.809789    0.790879  \n",
      "6     0.830803    0.814499    0.836689    0.808421  \n",
      "7     0.814894    0.816239    0.792373    0.798337  \n",
      "8     0.818200    0.798100    0.829000    0.782300  \n",
      "9     0.822771    0.815368    0.813928    0.803347  \n",
      "10    0.816528    0.807555    0.809906    0.790968  \n",
      "11    0.816230    0.807220    0.809694    0.790035  \n",
      "12    0.816538    0.807192    0.810706    0.790317  \n",
      "13    0.632624    0.614441    0.620643    0.580140  \n",
      "14    0.801400    0.800000    0.783200    0.771200  \n",
      "15    0.816538    0.807192    0.810706    0.790317  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_7_cat = np.where((y_pred_knn_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:23:30,732] Trial 400 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:31,476] Trial 401 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:32,172] Trial 402 finished with value: 0.6410406895810925 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:32,719] Trial 403 finished with value: 0.6423202408990953 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 67}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:33,449] Trial 404 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:34,157] Trial 405 finished with value: 0.6241828112238113 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:34,888] Trial 406 finished with value: 0.6410406895810925 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:35,646] Trial 407 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:36,382] Trial 408 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:37,037] Trial 409 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:37,616] Trial 410 finished with value: 0.5909454586291677 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 61}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:38,385] Trial 411 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:39,082] Trial 412 finished with value: 0.6358078093801904 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:39,776] Trial 413 finished with value: 0.6410406895810925 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:40,497] Trial 414 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:41,216] Trial 415 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:41,965] Trial 416 finished with value: 0.6024407202605402 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:42,741] Trial 417 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:43,481] Trial 418 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:44,214] Trial 419 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:44,819] Trial 420 finished with value: 0.6423202408990953 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:45,711] Trial 421 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:46,495] Trial 422 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:47,112] Trial 423 finished with value: 0.6410406895810925 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 75}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:47,871] Trial 424 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:48,562] Trial 425 finished with value: 0.6386127221979566 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:49,236] Trial 426 finished with value: 0.6410406895810925 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:50,014] Trial 427 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:50,715] Trial 428 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:51,234] Trial 429 finished with value: 0.6423202408990953 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:51,997] Trial 430 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:52,573] Trial 431 finished with value: 0.6357085528221367 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 76}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:53,351] Trial 432 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:54,105] Trial 433 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:54,726] Trial 434 finished with value: 0.6410406895810925 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:55,486] Trial 435 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:56,174] Trial 436 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:56,819] Trial 437 finished with value: 0.6410406895810925 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:57,559] Trial 438 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:58,298] Trial 439 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:59,000] Trial 440 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:23:59,544] Trial 441 finished with value: 0.6222692852629497 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:00,204] Trial 442 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:00,939] Trial 443 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:01,573] Trial 444 finished with value: 0.6423202408990953 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:02,345] Trial 445 finished with value: 0.6432952463720666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:03,094] Trial 446 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:03,533] Trial 447 finished with value: 0.6410406895810925 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:03,997] Trial 448 finished with value: 0.6410406895810925 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 67}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:04,758] Trial 449 finished with value: 0.645597591117324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 300 with value: 0.6621831357765847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6622\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 67\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.647596    0.676203    0.646090    0.662056   \n",
      "1                    TP  393.000000  389.000000  399.000000  397.000000   \n",
      "2                    TN  343.000000  343.000000  341.000000  348.000000   \n",
      "3                    FP   87.000000   81.000000   87.000000   87.000000   \n",
      "4                    FN   76.000000   86.000000   72.000000   67.000000   \n",
      "5              Accuracy    0.818687    0.814238    0.823137    0.828699   \n",
      "6             Precision    0.818750    0.827660    0.820988    0.820248   \n",
      "7           Sensitivity    0.837953    0.818947    0.847134    0.855603   \n",
      "8           Specificity    0.797700    0.809000    0.796700    0.800000   \n",
      "9              F1 score    0.828240    0.823280    0.833856    0.837553   \n",
      "10  F1 score (weighted)    0.818564    0.814291    0.822946    0.828490   \n",
      "11     F1 score (macro)    0.818125    0.813750    0.822398    0.828188   \n",
      "12    Balanced Accuracy    0.817814    0.813955    0.821931    0.827802   \n",
      "13                  MCC    0.636496    0.627551    0.645257    0.657201   \n",
      "14                  NPV    0.818600    0.799500    0.825700    0.838600   \n",
      "15              ROC_AUC    0.817814    0.813955    0.821931    0.827802   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.650331    0.670738    0.612010    0.623967    0.640500  \n",
      "1   383.000000  382.000000  374.000000  384.000000  407.000000  \n",
      "2   351.000000  344.000000  354.000000  327.000000  338.000000  \n",
      "3    78.000000   87.000000   73.000000   91.000000   78.000000  \n",
      "4    87.000000   86.000000   98.000000   97.000000   76.000000  \n",
      "5     0.816463    0.807564    0.809789    0.790879    0.828699  \n",
      "6     0.830803    0.814499    0.836689    0.808421    0.839175  \n",
      "7     0.814894    0.816239    0.792373    0.798337    0.842650  \n",
      "8     0.818200    0.798100    0.829000    0.782300    0.812500  \n",
      "9     0.822771    0.815368    0.813928    0.803347    0.840909  \n",
      "10    0.816528    0.807555    0.809906    0.790968    0.828669  \n",
      "11    0.816230    0.807220    0.809694    0.790035    0.827683  \n",
      "12    0.816538    0.807192    0.810706    0.790317    0.827575  \n",
      "13    0.632624    0.614441    0.620643    0.580140    0.655375  \n",
      "14    0.801400    0.800000    0.783200    0.771200    0.816400  \n",
      "15    0.816538    0.807192    0.810706    0.790317    0.827575  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_8_cat = np.where((y_pred_knn_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:24:05,695] Trial 450 finished with value: 0.5268400779584783 and parameters: {'n_neighbors': 21, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:06,329] Trial 451 finished with value: 0.6268926834019318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:07,071] Trial 452 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:07,735] Trial 453 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:08,453] Trial 454 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:09,216] Trial 455 finished with value: 0.6268926834019318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:09,985] Trial 456 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:10,725] Trial 457 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:11,458] Trial 458 finished with value: 0.629925324201762 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 75}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:12,210] Trial 459 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:12,980] Trial 460 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:13,402] Trial 461 finished with value: 0.629925324201762 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 78}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:14,162] Trial 462 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:14,860] Trial 463 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:15,590] Trial 464 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:16,269] Trial 465 finished with value: 0.6268926834019318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:17,017] Trial 466 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:17,717] Trial 467 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:18,338] Trial 468 finished with value: 0.6268926834019318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:19,112] Trial 469 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:19,676] Trial 470 finished with value: 0.6268926834019318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:20,449] Trial 471 finished with value: 0.6163936747033674 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:21,071] Trial 472 finished with value: 0.6268926834019318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 77}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:21,796] Trial 473 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:22,473] Trial 474 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:23,207] Trial 475 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:23,661] Trial 476 finished with value: 0.6268926834019318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 63}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:24,407] Trial 477 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:25,096] Trial 478 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:25,512] Trial 479 finished with value: 0.629925324201762 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 75}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:26,271] Trial 480 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:26,936] Trial 481 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:27,603] Trial 482 finished with value: 0.6017553021736334 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:28,360] Trial 483 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:29,075] Trial 484 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:29,818] Trial 485 finished with value: 0.56453220529067 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:30,502] Trial 486 finished with value: 0.6268926834019318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:31,309] Trial 487 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:32,055] Trial 488 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:32,477] Trial 489 finished with value: 0.629925324201762 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:33,243] Trial 490 finished with value: 0.6149353046070235 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:33,961] Trial 491 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:34,554] Trial 492 finished with value: 0.6268926834019318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 68}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:35,083] Trial 493 finished with value: 0.6268926834019318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 72}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:35,880] Trial 494 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:36,648] Trial 495 finished with value: 0.6341086734062324 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:37,383] Trial 496 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:37,806] Trial 497 finished with value: 0.629925324201762 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:38,479] Trial 498 finished with value: 0.6295881952517958 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 300 with value: 0.6621831357765847.\n",
      "[I 2023-12-11 12:24:39,185] Trial 499 finished with value: 0.6290213007490842 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 300 with value: 0.6621831357765847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6622\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 67\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.647596    0.676203    0.646090    0.662056   \n",
      "1                    TP  393.000000  389.000000  399.000000  397.000000   \n",
      "2                    TN  343.000000  343.000000  341.000000  348.000000   \n",
      "3                    FP   87.000000   81.000000   87.000000   87.000000   \n",
      "4                    FN   76.000000   86.000000   72.000000   67.000000   \n",
      "5              Accuracy    0.818687    0.814238    0.823137    0.828699   \n",
      "6             Precision    0.818750    0.827660    0.820988    0.820248   \n",
      "7           Sensitivity    0.837953    0.818947    0.847134    0.855603   \n",
      "8           Specificity    0.797700    0.809000    0.796700    0.800000   \n",
      "9              F1 score    0.828240    0.823280    0.833856    0.837553   \n",
      "10  F1 score (weighted)    0.818564    0.814291    0.822946    0.828490   \n",
      "11     F1 score (macro)    0.818125    0.813750    0.822398    0.828188   \n",
      "12    Balanced Accuracy    0.817814    0.813955    0.821931    0.827802   \n",
      "13                  MCC    0.636496    0.627551    0.645257    0.657201   \n",
      "14                  NPV    0.818600    0.799500    0.825700    0.838600   \n",
      "15              ROC_AUC    0.817814    0.813955    0.821931    0.827802   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.650331    0.670738    0.612010    0.623967    0.640500    0.667519  \n",
      "1   383.000000  382.000000  374.000000  384.000000  407.000000  403.000000  \n",
      "2   351.000000  344.000000  354.000000  327.000000  338.000000  341.000000  \n",
      "3    78.000000   87.000000   73.000000   91.000000   78.000000   74.000000  \n",
      "4    87.000000   86.000000   98.000000   97.000000   76.000000   81.000000  \n",
      "5     0.816463    0.807564    0.809789    0.790879    0.828699    0.827586  \n",
      "6     0.830803    0.814499    0.836689    0.808421    0.839175    0.844864  \n",
      "7     0.814894    0.816239    0.792373    0.798337    0.842650    0.832645  \n",
      "8     0.818200    0.798100    0.829000    0.782300    0.812500    0.821700  \n",
      "9     0.822771    0.815368    0.813928    0.803347    0.840909    0.838710  \n",
      "10    0.816528    0.807555    0.809906    0.790968    0.828669    0.827679  \n",
      "11    0.816230    0.807220    0.809694    0.790035    0.827683    0.826762  \n",
      "12    0.816538    0.807192    0.810706    0.790317    0.827575    0.827166  \n",
      "13    0.632624    0.614441    0.620643    0.580140    0.655375    0.653626  \n",
      "14    0.801400    0.800000    0.783200    0.771200    0.816400    0.808100  \n",
      "15    0.816538    0.807192    0.810706    0.790317    0.827575    0.827166  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_9_cat = np.where((y_pred_knn_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.658134</td>\n",
       "      <td>0.032933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>195.600000</td>\n",
       "      <td>6.866990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>171.600000</td>\n",
       "      <td>6.535374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>3.865805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>3.894440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.817453</td>\n",
       "      <td>0.008723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.824971</td>\n",
       "      <td>0.015546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.828512</td>\n",
       "      <td>0.014777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.805320</td>\n",
       "      <td>0.016214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.826589</td>\n",
       "      <td>0.009589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.817439</td>\n",
       "      <td>0.008707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.816797</td>\n",
       "      <td>0.008712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.816914</td>\n",
       "      <td>0.008749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.633933</td>\n",
       "      <td>0.017538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.809060</td>\n",
       "      <td>0.017218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.816914</td>\n",
       "      <td>0.008749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.658134     0.032933\n",
       "1                    TP       195.600000     6.866990\n",
       "2                    TN       171.600000     6.535374\n",
       "3                    FP        41.500000     3.865805\n",
       "4                    FN        40.500000     3.894440\n",
       "5              Accuracy         0.817453     0.008723\n",
       "6             Precision         0.824971     0.015546\n",
       "7           Sensitivity         0.828512     0.014777\n",
       "8           Specificity         0.805320     0.016214\n",
       "9              F1 score         0.826589     0.009589\n",
       "10  F1 score (weighted)         0.817439     0.008707\n",
       "11     F1 score (macro)         0.816797     0.008712\n",
       "12    Balanced Accuracy         0.816914     0.008749\n",
       "13                  MCC         0.633933     0.017538\n",
       "14                  NPV         0.809060     0.017218\n",
       "15              ROC_AUC         0.816914     0.008749"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.647596</td>\n",
       "      <td>0.676203</td>\n",
       "      <td>0.646090</td>\n",
       "      <td>0.662056</td>\n",
       "      <td>0.650331</td>\n",
       "      <td>0.670738</td>\n",
       "      <td>0.612010</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0.640500</td>\n",
       "      <td>0.667519</td>\n",
       "      <td>0.649701</td>\n",
       "      <td>0.020549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>391.100000</td>\n",
       "      <td>10.492855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>7.453560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>82.300000</td>\n",
       "      <td>6.307843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>82.600000</td>\n",
       "      <td>10.178409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.818687</td>\n",
       "      <td>0.814238</td>\n",
       "      <td>0.823137</td>\n",
       "      <td>0.828699</td>\n",
       "      <td>0.816463</td>\n",
       "      <td>0.807564</td>\n",
       "      <td>0.809789</td>\n",
       "      <td>0.790879</td>\n",
       "      <td>0.828699</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.816574</td>\n",
       "      <td>0.011812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.827660</td>\n",
       "      <td>0.820988</td>\n",
       "      <td>0.820248</td>\n",
       "      <td>0.830803</td>\n",
       "      <td>0.814499</td>\n",
       "      <td>0.836689</td>\n",
       "      <td>0.808421</td>\n",
       "      <td>0.839175</td>\n",
       "      <td>0.844864</td>\n",
       "      <td>0.826210</td>\n",
       "      <td>0.011648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.837953</td>\n",
       "      <td>0.818947</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>0.855603</td>\n",
       "      <td>0.814894</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.792373</td>\n",
       "      <td>0.798337</td>\n",
       "      <td>0.842650</td>\n",
       "      <td>0.832645</td>\n",
       "      <td>0.825678</td>\n",
       "      <td>0.020951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.797700</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.796700</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.818200</td>\n",
       "      <td>0.798100</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.782300</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.821700</td>\n",
       "      <td>0.806520</td>\n",
       "      <td>0.014096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.828240</td>\n",
       "      <td>0.823280</td>\n",
       "      <td>0.833856</td>\n",
       "      <td>0.837553</td>\n",
       "      <td>0.822771</td>\n",
       "      <td>0.815368</td>\n",
       "      <td>0.813928</td>\n",
       "      <td>0.803347</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.825796</td>\n",
       "      <td>0.012353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.818564</td>\n",
       "      <td>0.814291</td>\n",
       "      <td>0.822946</td>\n",
       "      <td>0.828490</td>\n",
       "      <td>0.816528</td>\n",
       "      <td>0.807555</td>\n",
       "      <td>0.809906</td>\n",
       "      <td>0.790968</td>\n",
       "      <td>0.828669</td>\n",
       "      <td>0.827679</td>\n",
       "      <td>0.816560</td>\n",
       "      <td>0.011751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.818125</td>\n",
       "      <td>0.813750</td>\n",
       "      <td>0.822398</td>\n",
       "      <td>0.828188</td>\n",
       "      <td>0.816230</td>\n",
       "      <td>0.807220</td>\n",
       "      <td>0.809694</td>\n",
       "      <td>0.790035</td>\n",
       "      <td>0.827683</td>\n",
       "      <td>0.826762</td>\n",
       "      <td>0.816009</td>\n",
       "      <td>0.011750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.817814</td>\n",
       "      <td>0.813955</td>\n",
       "      <td>0.821931</td>\n",
       "      <td>0.827802</td>\n",
       "      <td>0.816538</td>\n",
       "      <td>0.807192</td>\n",
       "      <td>0.810706</td>\n",
       "      <td>0.790317</td>\n",
       "      <td>0.827575</td>\n",
       "      <td>0.827166</td>\n",
       "      <td>0.816099</td>\n",
       "      <td>0.011576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.636496</td>\n",
       "      <td>0.627551</td>\n",
       "      <td>0.645257</td>\n",
       "      <td>0.657201</td>\n",
       "      <td>0.632624</td>\n",
       "      <td>0.614441</td>\n",
       "      <td>0.620643</td>\n",
       "      <td>0.580140</td>\n",
       "      <td>0.655375</td>\n",
       "      <td>0.653626</td>\n",
       "      <td>0.632335</td>\n",
       "      <td>0.023549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.799500</td>\n",
       "      <td>0.825700</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.801400</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.783200</td>\n",
       "      <td>0.771200</td>\n",
       "      <td>0.816400</td>\n",
       "      <td>0.808100</td>\n",
       "      <td>0.806270</td>\n",
       "      <td>0.019865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.817814</td>\n",
       "      <td>0.813955</td>\n",
       "      <td>0.821931</td>\n",
       "      <td>0.827802</td>\n",
       "      <td>0.816538</td>\n",
       "      <td>0.807192</td>\n",
       "      <td>0.810706</td>\n",
       "      <td>0.790317</td>\n",
       "      <td>0.827575</td>\n",
       "      <td>0.827166</td>\n",
       "      <td>0.816099</td>\n",
       "      <td>0.011576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.647596    0.676203    0.646090    0.662056   \n",
       "1                    TP  393.000000  389.000000  399.000000  397.000000   \n",
       "2                    TN  343.000000  343.000000  341.000000  348.000000   \n",
       "3                    FP   87.000000   81.000000   87.000000   87.000000   \n",
       "4                    FN   76.000000   86.000000   72.000000   67.000000   \n",
       "5              Accuracy    0.818687    0.814238    0.823137    0.828699   \n",
       "6             Precision    0.818750    0.827660    0.820988    0.820248   \n",
       "7           Sensitivity    0.837953    0.818947    0.847134    0.855603   \n",
       "8           Specificity    0.797700    0.809000    0.796700    0.800000   \n",
       "9              F1 score    0.828240    0.823280    0.833856    0.837553   \n",
       "10  F1 score (weighted)    0.818564    0.814291    0.822946    0.828490   \n",
       "11     F1 score (macro)    0.818125    0.813750    0.822398    0.828188   \n",
       "12    Balanced Accuracy    0.817814    0.813955    0.821931    0.827802   \n",
       "13                  MCC    0.636496    0.627551    0.645257    0.657201   \n",
       "14                  NPV    0.818600    0.799500    0.825700    0.838600   \n",
       "15              ROC_AUC    0.817814    0.813955    0.821931    0.827802   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.650331    0.670738    0.612010    0.623967    0.640500    0.667519   \n",
       "1   383.000000  382.000000  374.000000  384.000000  407.000000  403.000000   \n",
       "2   351.000000  344.000000  354.000000  327.000000  338.000000  341.000000   \n",
       "3    78.000000   87.000000   73.000000   91.000000   78.000000   74.000000   \n",
       "4    87.000000   86.000000   98.000000   97.000000   76.000000   81.000000   \n",
       "5     0.816463    0.807564    0.809789    0.790879    0.828699    0.827586   \n",
       "6     0.830803    0.814499    0.836689    0.808421    0.839175    0.844864   \n",
       "7     0.814894    0.816239    0.792373    0.798337    0.842650    0.832645   \n",
       "8     0.818200    0.798100    0.829000    0.782300    0.812500    0.821700   \n",
       "9     0.822771    0.815368    0.813928    0.803347    0.840909    0.838710   \n",
       "10    0.816528    0.807555    0.809906    0.790968    0.828669    0.827679   \n",
       "11    0.816230    0.807220    0.809694    0.790035    0.827683    0.826762   \n",
       "12    0.816538    0.807192    0.810706    0.790317    0.827575    0.827166   \n",
       "13    0.632624    0.614441    0.620643    0.580140    0.655375    0.653626   \n",
       "14    0.801400    0.800000    0.783200    0.771200    0.816400    0.808100   \n",
       "15    0.816538    0.807192    0.810706    0.790317    0.827575    0.827166   \n",
       "\n",
       "           ave        std  \n",
       "0     0.649701   0.020549  \n",
       "1   391.100000  10.492855  \n",
       "2   343.000000   7.453560  \n",
       "3    82.300000   6.307843  \n",
       "4    82.600000  10.178409  \n",
       "5     0.816574   0.011812  \n",
       "6     0.826210   0.011648  \n",
       "7     0.825678   0.020951  \n",
       "8     0.806520   0.014096  \n",
       "9     0.825796   0.012353  \n",
       "10    0.816560   0.011751  \n",
       "11    0.816009   0.011750  \n",
       "12    0.816099   0.011576  \n",
       "13    0.632335   0.023549  \n",
       "14    0.806270   0.019865  \n",
       "15    0.816099   0.011576  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_knn0</th>\n",
       "      <th>y_pred_knn1</th>\n",
       "      <th>y_pred_knn2</th>\n",
       "      <th>y_pred_knn3</th>\n",
       "      <th>y_pred_knn4</th>\n",
       "      <th>y_pred_knn_ave</th>\n",
       "      <th>y_pred_knn_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4176702</td>\n",
       "      <td>0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.222504</td>\n",
       "      <td>6.222504</td>\n",
       "      <td>6.222504</td>\n",
       "      <td>6.158350</td>\n",
       "      <td>6.334348</td>\n",
       "      <td>6.276702</td>\n",
       "      <td>0.112550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL272401</td>\n",
       "      <td>1</td>\n",
       "      <td>7.26</td>\n",
       "      <td>7.461916</td>\n",
       "      <td>7.461916</td>\n",
       "      <td>7.461916</td>\n",
       "      <td>7.172578</td>\n",
       "      <td>7.461916</td>\n",
       "      <td>7.380040</td>\n",
       "      <td>0.118508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL118</td>\n",
       "      <td>2</td>\n",
       "      <td>5.93</td>\n",
       "      <td>6.041041</td>\n",
       "      <td>6.132011</td>\n",
       "      <td>6.076076</td>\n",
       "      <td>6.033050</td>\n",
       "      <td>6.033050</td>\n",
       "      <td>6.040871</td>\n",
       "      <td>0.060499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3655939</td>\n",
       "      <td>3</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.713693</td>\n",
       "      <td>6.713693</td>\n",
       "      <td>6.714544</td>\n",
       "      <td>6.713693</td>\n",
       "      <td>6.960928</td>\n",
       "      <td>6.682758</td>\n",
       "      <td>0.201442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3621537</td>\n",
       "      <td>4</td>\n",
       "      <td>5.88</td>\n",
       "      <td>7.679700</td>\n",
       "      <td>7.168095</td>\n",
       "      <td>6.627393</td>\n",
       "      <td>6.969395</td>\n",
       "      <td>6.627393</td>\n",
       "      <td>6.825329</td>\n",
       "      <td>0.553805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL2047606</td>\n",
       "      <td>4487</td>\n",
       "      <td>5.44</td>\n",
       "      <td>5.720983</td>\n",
       "      <td>5.537764</td>\n",
       "      <td>5.537764</td>\n",
       "      <td>5.537764</td>\n",
       "      <td>5.537764</td>\n",
       "      <td>5.552007</td>\n",
       "      <td>0.083576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL217781</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.48</td>\n",
       "      <td>6.994105</td>\n",
       "      <td>6.994105</td>\n",
       "      <td>6.994105</td>\n",
       "      <td>6.994105</td>\n",
       "      <td>6.994105</td>\n",
       "      <td>7.075088</td>\n",
       "      <td>0.181082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL2105763</td>\n",
       "      <td>4489</td>\n",
       "      <td>9.92</td>\n",
       "      <td>6.835557</td>\n",
       "      <td>6.646484</td>\n",
       "      <td>6.646484</td>\n",
       "      <td>6.849567</td>\n",
       "      <td>6.835557</td>\n",
       "      <td>7.288942</td>\n",
       "      <td>1.179840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL3415969</td>\n",
       "      <td>4490</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.560000</td>\n",
       "      <td>5.560000</td>\n",
       "      <td>5.944204</td>\n",
       "      <td>5.560000</td>\n",
       "      <td>5.560000</td>\n",
       "      <td>5.655701</td>\n",
       "      <td>0.146493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL467066</td>\n",
       "      <td>4491</td>\n",
       "      <td>7.55</td>\n",
       "      <td>7.480265</td>\n",
       "      <td>7.340253</td>\n",
       "      <td>7.480265</td>\n",
       "      <td>7.357975</td>\n",
       "      <td>7.577692</td>\n",
       "      <td>7.464408</td>\n",
       "      <td>0.088886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_knn0  y_pred_knn1  \\\n",
       "0         CHEMBL4176702            0     6.50     6.222504     6.222504   \n",
       "1          CHEMBL272401            1     7.26     7.461916     7.461916   \n",
       "2             CHEMBL118            2     5.93     6.041041     6.132011   \n",
       "3         CHEMBL3655939            3     6.28     6.713693     6.713693   \n",
       "4         CHEMBL3621537            4     5.88     7.679700     7.168095   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "4487      CHEMBL2047606         4487     5.44     5.720983     5.537764   \n",
       "4488       CHEMBL217781         4488     7.48     6.994105     6.994105   \n",
       "4489      CHEMBL2105763         4489     9.92     6.835557     6.646484   \n",
       "4490      CHEMBL3415969         4490     5.75     5.560000     5.560000   \n",
       "4491       CHEMBL467066         4491     7.55     7.480265     7.340253   \n",
       "\n",
       "      y_pred_knn2  y_pred_knn3  y_pred_knn4  y_pred_knn_ave  y_pred_knn_std  \n",
       "0        6.222504     6.158350     6.334348        6.276702        0.112550  \n",
       "1        7.461916     7.172578     7.461916        7.380040        0.118508  \n",
       "2        6.076076     6.033050     6.033050        6.040871        0.060499  \n",
       "3        6.714544     6.713693     6.960928        6.682758        0.201442  \n",
       "4        6.627393     6.969395     6.627393        6.825329        0.553805  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "4487     5.537764     5.537764     5.537764        5.552007        0.083576  \n",
       "4488     6.994105     6.994105     6.994105        7.075088        0.181082  \n",
       "4489     6.646484     6.849567     6.835557        7.288942        1.179840  \n",
       "4490     5.944204     5.560000     5.560000        5.655701        0.146493  \n",
       "4491     7.480265     7.357975     7.577692        7.464408        0.088886  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where((y_pred_optimized_knn >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id, knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "knn_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "efa46682-fb4e-45a7-be4c-9b5504c6dba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.662777</td>\n",
       "      <td>0.040381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.819323</td>\n",
       "      <td>0.016878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.828496</td>\n",
       "      <td>0.023371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.827605</td>\n",
       "      <td>0.021054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.810262</td>\n",
       "      <td>0.025108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.827822</td>\n",
       "      <td>0.017333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.819328</td>\n",
       "      <td>0.016896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.818651</td>\n",
       "      <td>0.016863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.818932</td>\n",
       "      <td>0.016994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.637805</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.809252</td>\n",
       "      <td>0.022764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.818932</td>\n",
       "      <td>0.016994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.662777     0.040381\n",
       "1              Accuracy         0.819323     0.016878\n",
       "2             Precision         0.828496     0.023371\n",
       "3           Sensitivity         0.827605     0.021054\n",
       "4           Specificity         0.810262     0.025108\n",
       "5              F1 score         0.827822     0.017333\n",
       "6   F1 score (weighted)         0.819328     0.016896\n",
       "7      F1 score (macro)         0.818651     0.016863\n",
       "8     Balanced Accuracy         0.818932     0.016994\n",
       "9                   MCC         0.637805     0.033800\n",
       "10                  NPV         0.809252     0.022764\n",
       "11              ROC_AUC         0.818932     0.016994"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c149767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_knn.to_csv('mat_met_knn_opt.csv')\n",
    "knn_5preds.to_csv('knn_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6533 with a standard deviation of 0.0380\n",
      "KNN optimized model r2_score 0.6641 with a standard deviation of 0.0342\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn.joblib']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"OUTPUT/knn_reg.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.686083     0.020023\n",
      "1                    TP       200.400000     7.604092\n",
      "2                    TN       174.000000     7.055337\n",
      "3                    FP        39.100000     4.653553\n",
      "4                    FN        35.700000     5.121849\n",
      "5              Accuracy         0.833481     0.018352\n",
      "6             Precision         0.836625     0.020452\n",
      "7           Sensitivity         0.848852     0.020438\n",
      "8           Specificity         0.816580     0.020096\n",
      "9              F1 score         0.842600     0.018162\n",
      "10  F1 score (weighted)         0.833430     0.018308\n",
      "11     F1 score (macro)         0.832790     0.018460\n",
      "12    Balanced Accuracy         0.832712     0.018275\n",
      "13                  MCC         0.665886     0.036958\n",
      "14                  NPV         0.829720     0.024651\n",
      "15              ROC_AUC         0.832712     0.018275\n",
      "CPU times: user 37.1 s, sys: 20 ms, total: 37.2 s\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:25:27,752] A new study created in memory with name: SVM_regressor_CV\n",
      "[I 2023-12-11 12:25:46,139] Trial 0 finished with value: 0.003862857892464744 and parameters: {'C': 0.03125, 'gamma': 0.25}. Best is trial 0 with value: 0.003862857892464744.\n",
      "[I 2023-12-11 12:26:08,572] Trial 1 finished with value: 0.030823057269553377 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 1 with value: 0.030823057269553377.\n",
      "[I 2023-12-11 12:26:26,825] Trial 2 finished with value: -0.0008844952818438778 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 1 with value: 0.030823057269553377.\n",
      "[I 2023-12-11 12:26:46,872] Trial 3 finished with value: 0.2825359236349435 and parameters: {'C': 64.0, 'gamma': 0.125}. Best is trial 3 with value: 0.2825359236349435.\n",
      "[I 2023-12-11 12:27:03,476] Trial 4 finished with value: 0.688072636008774 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:27:22,847] Trial 5 finished with value: 0.0895688755425632 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:27:42,166] Trial 6 finished with value: 0.11726954568339938 and parameters: {'C': 0.015625, 'gamma': 0.0078125}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:28:01,460] Trial 7 finished with value: 0.0012814168251316649 and parameters: {'C': 0.015625, 'gamma': 0.25}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:28:20,290] Trial 8 finished with value: -2.6274213894350585e-05 and parameters: {'C': 0.015625, 'gamma': 2.0}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:28:39,082] Trial 9 finished with value: 0.018254456237992666 and parameters: {'C': 0.125, 'gamma': 0.25}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:28:56,180] Trial 10 finished with value: 0.688072636008774 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:29:13,660] Trial 11 finished with value: 0.688072636008774 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:29:32,791] Trial 12 finished with value: 0.057338552476703844 and parameters: {'C': 2.0, 'gamma': 0.5}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:29:51,214] Trial 13 finished with value: 0.5278866920397635 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:30:09,355] Trial 14 finished with value: 0.6877483226831982 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:30:27,349] Trial 15 finished with value: 0.5194936565718994 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:30:46,248] Trial 16 finished with value: 0.601462295551386 and parameters: {'C': 2.0, 'gamma': 0.00390625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:31:06,332] Trial 17 finished with value: 0.007153971698491535 and parameters: {'C': 0.0625, 'gamma': 6.103515625e-05}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:31:26,347] Trial 18 finished with value: 0.6757587819406761 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:31:46,904] Trial 19 finished with value: 0.5602030298690526 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:32:07,553] Trial 20 finished with value: 0.24695674676916704 and parameters: {'C': 0.5, 'gamma': 0.00048828125}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:32:25,831] Trial 21 finished with value: 0.688072636008774 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:32:44,783] Trial 22 finished with value: 0.688072636008774 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:33:06,075] Trial 23 finished with value: 0.04525313935454365 and parameters: {'C': 2.0, 'gamma': 1.0}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:33:26,435] Trial 24 finished with value: 0.24840885087112086 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:33:46,416] Trial 25 finished with value: 0.5415389721638102 and parameters: {'C': 2.0, 'gamma': 0.001953125}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:34:09,384] Trial 26 finished with value: 0.03480541179840931 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:34:31,077] Trial 27 finished with value: 0.32174626293669395 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:34:49,972] Trial 28 finished with value: 0.5784970709653121 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 4 with value: 0.688072636008774.\n",
      "[I 2023-12-11 12:35:09,998] Trial 29 finished with value: 0.6888701615496149 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:35:28,547] Trial 30 finished with value: 0.6888701615496149 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:35:47,124] Trial 31 finished with value: 0.6888701615496149 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:36:09,613] Trial 32 finished with value: 0.029272084054835314 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:36:29,912] Trial 33 finished with value: 0.6888701615496149 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:36:50,458] Trial 34 finished with value: 0.6888701615496149 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:37:10,915] Trial 35 finished with value: 0.4128321476547625 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:37:31,844] Trial 36 finished with value: 0.07515646203245677 and parameters: {'C': 0.03125, 'gamma': 0.0625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:37:53,374] Trial 37 finished with value: 0.029752095286513968 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:38:13,240] Trial 38 finished with value: 0.6766715288802415 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:38:33,007] Trial 39 finished with value: -0.0003380562802863363 and parameters: {'C': 0.0078125, 'gamma': 6.103515625e-05}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:38:53,374] Trial 40 finished with value: 0.28253592363494356 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:39:12,841] Trial 41 finished with value: 0.6888701615496149 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:39:32,570] Trial 42 finished with value: 0.6888701615496149 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:39:54,078] Trial 43 finished with value: 0.032922269951550745 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:40:14,407] Trial 44 finished with value: 0.6879853066062009 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:40:35,986] Trial 45 finished with value: 0.05195651312281174 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:40:55,804] Trial 46 finished with value: 0.42705753841813526 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:41:15,725] Trial 47 finished with value: 0.05593387172462964 and parameters: {'C': 0.25, 'gamma': 0.0001220703125}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:41:44,405] Trial 48 finished with value: 0.629377716837364 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:42:07,958] Trial 49 finished with value: 0.6567295018857303 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 29 with value: 0.6888701615496149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6889\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.698260\n",
      "1                    TP  395.000000\n",
      "2                    TN  351.000000\n",
      "3                    FP   79.000000\n",
      "4                    FN   74.000000\n",
      "5              Accuracy    0.829811\n",
      "6             Precision    0.833333\n",
      "7           Sensitivity    0.842217\n",
      "8           Specificity    0.816300\n",
      "9              F1 score    0.837752\n",
      "10  F1 score (weighted)    0.829764\n",
      "11     F1 score (macro)    0.829402\n",
      "12    Balanced Accuracy    0.829248\n",
      "13                  MCC    0.658856\n",
      "14                  NPV    0.825900\n",
      "15              ROC_AUC    0.829248\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_0_cat = np.where((y_pred_svm_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:42:30,354] Trial 50 finished with value: 0.6721198448143058 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:42:49,851] Trial 51 finished with value: 0.6854215282076999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:43:09,252] Trial 52 finished with value: 0.6854215282076999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:43:28,787] Trial 53 finished with value: 0.6854215282076999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:43:48,101] Trial 54 finished with value: 0.6139975682619869 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:44:08,389] Trial 55 finished with value: 0.3337492345793757 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:44:30,463] Trial 56 finished with value: 0.0015305844496418142 and parameters: {'C': 0.015625, 'gamma': 0.25}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:44:52,945] Trial 57 finished with value: 0.0019045880811537131 and parameters: {'C': 0.03125, 'gamma': 1.0}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:45:15,005] Trial 58 finished with value: 0.5883210085746278 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:45:36,219] Trial 59 finished with value: 0.00287373306603842 and parameters: {'C': 0.0078125, 'gamma': 0.000244140625}. Best is trial 29 with value: 0.6888701615496149.\n",
      "[I 2023-12-11 12:45:55,891] Trial 60 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:46:15,195] Trial 61 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:46:34,469] Trial 62 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:46:59,549] Trial 63 finished with value: 0.03881466254025984 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:47:19,664] Trial 64 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:47:41,481] Trial 65 finished with value: 0.16620397702892797 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:48:02,333] Trial 66 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:48:24,184] Trial 67 finished with value: 0.29466963364629317 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:48:43,994] Trial 68 finished with value: 0.681776070689741 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:49:02,908] Trial 69 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:49:24,199] Trial 70 finished with value: 0.042987786918232705 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:49:43,569] Trial 71 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:50:02,497] Trial 72 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:50:22,751] Trial 73 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:50:41,568] Trial 74 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:51:03,195] Trial 75 finished with value: 0.06737919363890371 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:51:22,707] Trial 76 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:51:42,589] Trial 77 finished with value: 0.5654958894242607 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:52:02,973] Trial 78 finished with value: 0.2486050852798284 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:52:21,903] Trial 79 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:52:41,761] Trial 80 finished with value: 0.334947842018552 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:53:01,363] Trial 81 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:53:20,616] Trial 82 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:53:39,790] Trial 83 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:53:59,810] Trial 84 finished with value: 0.2218024610329305 and parameters: {'C': 0.0625, 'gamma': 0.00390625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:54:21,211] Trial 85 finished with value: 0.0393380442905568 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:54:40,677] Trial 86 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:55:00,767] Trial 87 finished with value: 0.4042336109204247 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:55:26,881] Trial 88 finished with value: 0.6375929643317695 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:55:46,998] Trial 89 finished with value: 0.4781237978215884 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:56:06,567] Trial 90 finished with value: 0.6847003397142697 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:56:25,073] Trial 91 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:56:42,391] Trial 92 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:57:01,770] Trial 93 finished with value: 0.5297299350895416 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:57:20,726] Trial 94 finished with value: 0.6927384155239443 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:57:42,858] Trial 95 finished with value: 0.05236692290083424 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:58:04,174] Trial 96 finished with value: 0.06165629755810599 and parameters: {'C': 0.5, 'gamma': 0.25}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:58:24,804] Trial 97 finished with value: 0.412030642647599 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:58:43,807] Trial 98 finished with value: 0.664580878192743 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:59:04,404] Trial 99 finished with value: 0.02869900409195526 and parameters: {'C': 0.015625, 'gamma': 0.0009765625}. Best is trial 60 with value: 0.6927384155239443.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6927\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.698260    0.719248\n",
      "1                    TP  395.000000  406.000000\n",
      "2                    TN  351.000000  340.000000\n",
      "3                    FP   79.000000   84.000000\n",
      "4                    FN   74.000000   69.000000\n",
      "5              Accuracy    0.829811    0.829811\n",
      "6             Precision    0.833333    0.828571\n",
      "7           Sensitivity    0.842217    0.854737\n",
      "8           Specificity    0.816300    0.801900\n",
      "9              F1 score    0.837752    0.841451\n",
      "10  F1 score (weighted)    0.829764    0.829601\n",
      "11     F1 score (macro)    0.829402    0.828889\n",
      "12    Balanced Accuracy    0.829248    0.828312\n",
      "13                  MCC    0.658856    0.658243\n",
      "14                  NPV    0.825900    0.831300\n",
      "15              ROC_AUC    0.829248    0.828312\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_1_cat = np.where((y_pred_svm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:59:32,256] Trial 100 finished with value: 0.03172124697209684 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 60 with value: 0.6927384155239443.\n",
      "[I 2023-12-11 12:59:52,953] Trial 101 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:00:14,198] Trial 102 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:00:34,629] Trial 103 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:00:55,434] Trial 104 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:01:16,087] Trial 105 finished with value: 0.16639326127881943 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:01:37,196] Trial 106 finished with value: 0.2157217721066394 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:01:58,436] Trial 107 finished with value: 0.0014297918090018014 and parameters: {'C': 0.0078125, 'gamma': 0.125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:02:19,395] Trial 108 finished with value: 0.6841947835076385 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:02:39,080] Trial 109 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:02:59,536] Trial 110 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:03:19,841] Trial 111 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:03:40,494] Trial 112 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:04:00,920] Trial 113 finished with value: 0.3183219842939554 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:04:20,053] Trial 114 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:04:39,646] Trial 115 finished with value: 0.053305599168287365 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:04:59,811] Trial 116 finished with value: 0.6892581847004134 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:05:18,994] Trial 117 finished with value: 0.006614007830328705 and parameters: {'C': 0.125, 'gamma': 2.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:05:38,328] Trial 118 finished with value: 0.5607516389847873 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:05:57,762] Trial 119 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:06:16,570] Trial 120 finished with value: 0.029390990371500548 and parameters: {'C': 0.25, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:06:36,369] Trial 121 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:06:55,909] Trial 122 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:07:15,090] Trial 123 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:07:34,163] Trial 124 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:07:53,322] Trial 125 finished with value: 0.5549631835980172 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:08:12,423] Trial 126 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:08:32,383] Trial 127 finished with value: 0.03209650003504394 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:08:52,119] Trial 128 finished with value: 0.6920315002562323 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:09:10,223] Trial 129 finished with value: 0.4875895320434429 and parameters: {'C': 0.5, 'gamma': 0.00390625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:09:29,647] Trial 130 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:09:49,095] Trial 131 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:10:08,073] Trial 132 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:10:26,367] Trial 133 finished with value: 0.6771816549174446 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:10:45,723] Trial 134 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:11:03,511] Trial 135 finished with value: 0.3295852387197982 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:11:21,847] Trial 136 finished with value: 0.548263029676345 and parameters: {'C': 2.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:11:41,216] Trial 137 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:12:00,289] Trial 138 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:12:19,273] Trial 139 finished with value: 8.332911802467757e-05 and parameters: {'C': 0.015625, 'gamma': 0.25}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:12:38,439] Trial 140 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:12:57,500] Trial 141 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:13:16,668] Trial 142 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:13:34,709] Trial 143 finished with value: 0.41190250972713904 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:13:54,094] Trial 144 finished with value: 0.00034997482560422677 and parameters: {'C': 0.03125, 'gamma': 1.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:14:13,417] Trial 145 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:14:31,530] Trial 146 finished with value: 0.5383261436170423 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:14:50,387] Trial 147 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:15:09,423] Trial 148 finished with value: 0.6976081712310384 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:15:28,034] Trial 149 finished with value: 0.07596166374222797 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.698260    0.719248    0.692609\n",
      "1                    TP  395.000000  406.000000  398.000000\n",
      "2                    TN  351.000000  340.000000  355.000000\n",
      "3                    FP   79.000000   84.000000   73.000000\n",
      "4                    FN   74.000000   69.000000   73.000000\n",
      "5              Accuracy    0.829811    0.829811    0.837597\n",
      "6             Precision    0.833333    0.828571    0.845011\n",
      "7           Sensitivity    0.842217    0.854737    0.845011\n",
      "8           Specificity    0.816300    0.801900    0.829400\n",
      "9              F1 score    0.837752    0.841451    0.845011\n",
      "10  F1 score (weighted)    0.829764    0.829601    0.837597\n",
      "11     F1 score (macro)    0.829402    0.828889    0.837225\n",
      "12    Balanced Accuracy    0.829248    0.828312    0.837225\n",
      "13                  MCC    0.658856    0.658243    0.674450\n",
      "14                  NPV    0.825900    0.831300    0.829400\n",
      "15              ROC_AUC    0.829248    0.828312    0.837225\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_2_cat = np.where((y_pred_svm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 13:15:49,454] Trial 150 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:16:07,468] Trial 151 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:16:30,034] Trial 152 finished with value: 0.03188011163395045 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:16:47,954] Trial 153 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:17:06,832] Trial 154 finished with value: 0.16577974854502225 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:17:25,704] Trial 155 finished with value: 0.31505158354922047 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:17:43,691] Trial 156 finished with value: 0.6768584263651641 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:18:02,474] Trial 157 finished with value: 0.2816261369368352 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:18:20,463] Trial 158 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:18:38,667] Trial 159 finished with value: 0.4195284839224108 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:18:56,904] Trial 160 finished with value: 0.6755287452507579 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:19:14,334] Trial 161 finished with value: 0.6684338203860917 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:19:32,436] Trial 162 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:19:50,217] Trial 163 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:20:09,972] Trial 164 finished with value: 0.035911875685063266 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:20:28,086] Trial 165 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:20:46,346] Trial 166 finished with value: 0.5199041989238025 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:21:05,545] Trial 167 finished with value: 0.5531374201548401 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:21:23,957] Trial 168 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:21:43,320] Trial 169 finished with value: 0.055726701026105455 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:22:00,744] Trial 170 finished with value: 0.40821263195710394 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:22:18,356] Trial 171 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:22:36,733] Trial 172 finished with value: 0.24617629405960523 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:22:54,452] Trial 173 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:23:11,894] Trial 174 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:23:29,666] Trial 175 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:23:49,146] Trial 176 finished with value: 0.034453804316414496 and parameters: {'C': 1.0, 'gamma': 4.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:24:05,919] Trial 177 finished with value: 0.6043879406735368 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:24:23,932] Trial 178 finished with value: 0.664849655046059 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:24:42,220] Trial 179 finished with value: 0.13387310372472028 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:25:00,119] Trial 180 finished with value: 0.47525169139770573 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:25:17,498] Trial 181 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:25:34,860] Trial 182 finished with value: 0.6211107872034083 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:25:53,020] Trial 183 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:26:10,424] Trial 184 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:26:28,267] Trial 185 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:26:45,620] Trial 186 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:27:05,191] Trial 187 finished with value: 0.09213581559901292 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:27:23,304] Trial 188 finished with value: 0.5732798946150182 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:27:42,018] Trial 189 finished with value: 0.07664758197974168 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:28:00,460] Trial 190 finished with value: 0.2151201947898481 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:28:18,036] Trial 191 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:28:35,527] Trial 192 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:28:53,392] Trial 193 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:29:11,400] Trial 194 finished with value: 0.4081618322527258 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:29:30,505] Trial 195 finished with value: 0.04382030759673246 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:29:48,037] Trial 196 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:30:05,536] Trial 197 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:30:23,160] Trial 198 finished with value: 0.6834558423301832 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:30:41,982] Trial 199 finished with value: 0.31505158354922047 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.698260    0.719248    0.692609    0.738147\n",
      "1                    TP  395.000000  406.000000  398.000000  402.000000\n",
      "2                    TN  351.000000  340.000000  355.000000  353.000000\n",
      "3                    FP   79.000000   84.000000   73.000000   82.000000\n",
      "4                    FN   74.000000   69.000000   73.000000   62.000000\n",
      "5              Accuracy    0.829811    0.829811    0.837597    0.839822\n",
      "6             Precision    0.833333    0.828571    0.845011    0.830579\n",
      "7           Sensitivity    0.842217    0.854737    0.845011    0.866379\n",
      "8           Specificity    0.816300    0.801900    0.829400    0.811500\n",
      "9              F1 score    0.837752    0.841451    0.845011    0.848101\n",
      "10  F1 score (weighted)    0.829764    0.829601    0.837597    0.839627\n",
      "11     F1 score (macro)    0.829402    0.828889    0.837225    0.839345\n",
      "12    Balanced Accuracy    0.829248    0.828312    0.837225    0.838937\n",
      "13                  MCC    0.658856    0.658243    0.674450    0.679525\n",
      "14                  NPV    0.825900    0.831300    0.829400    0.850600\n",
      "15              ROC_AUC    0.829248    0.828312    0.837225    0.838937\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_3_cat = np.where((y_pred_svm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 13:31:06,412] Trial 200 finished with value: 0.031088212487045873 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:31:23,899] Trial 201 finished with value: 0.5411635494817346 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:31:42,383] Trial 202 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:32:00,976] Trial 203 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:32:19,659] Trial 204 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:32:38,081] Trial 205 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:32:55,745] Trial 206 finished with value: 0.4310216802657788 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:33:14,599] Trial 207 finished with value: 0.6899083268770603 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:33:32,702] Trial 208 finished with value: 0.16695100229218074 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:33:51,414] Trial 209 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:34:08,573] Trial 210 finished with value: 0.5365269158302415 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:34:27,372] Trial 211 finished with value: 0.28382291711426155 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:34:45,943] Trial 212 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:35:04,239] Trial 213 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:35:23,288] Trial 214 finished with value: 0.6838722857037638 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:35:41,506] Trial 215 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:36:01,117] Trial 216 finished with value: 0.03480883975881679 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:36:19,340] Trial 217 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:36:38,139] Trial 218 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:36:57,041] Trial 219 finished with value: 0.05542367965077457 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:37:14,768] Trial 220 finished with value: 0.4678372970599189 and parameters: {'C': 0.5, 'gamma': 0.0625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:37:33,036] Trial 221 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:37:51,752] Trial 222 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:38:10,754] Trial 223 finished with value: 0.6923836143719199 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:38:29,278] Trial 224 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:38:47,469] Trial 225 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:39:06,196] Trial 226 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:39:23,578] Trial 227 finished with value: 0.6700706563698604 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:39:42,340] Trial 228 finished with value: 0.03349444193490507 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:40:00,456] Trial 229 finished with value: 0.24788052796563828 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:40:18,266] Trial 230 finished with value: 0.0019723380086281938 and parameters: {'C': 0.015625, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:40:36,434] Trial 231 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:40:54,203] Trial 232 finished with value: 0.6941226350428603 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:41:12,943] Trial 233 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:41:31,246] Trial 234 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:41:49,488] Trial 235 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:42:07,274] Trial 236 finished with value: 0.6397281858988901 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:42:25,294] Trial 237 finished with value: 0.6764791154457663 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:42:43,767] Trial 238 finished with value: 0.22021278910939984 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:43:01,872] Trial 239 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:43:20,288] Trial 240 finished with value: 0.07891550504353731 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:43:38,593] Trial 241 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:43:56,052] Trial 242 finished with value: 0.48408078339437105 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:44:14,187] Trial 243 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:44:32,465] Trial 244 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:44:50,434] Trial 245 finished with value: 0.5891805208302474 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:45:08,805] Trial 246 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:45:28,105] Trial 247 finished with value: 0.09180893420386088 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:45:46,461] Trial 248 finished with value: 0.6974213830551494 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:46:05,719] Trial 249 finished with value: 0.04411915165527234 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 101 with value: 0.6976081712310384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698260    0.719248    0.692609    0.738147   \n",
      "1                    TP  395.000000  406.000000  398.000000  402.000000   \n",
      "2                    TN  351.000000  340.000000  355.000000  353.000000   \n",
      "3                    FP   79.000000   84.000000   73.000000   82.000000   \n",
      "4                    FN   74.000000   69.000000   73.000000   62.000000   \n",
      "5              Accuracy    0.829811    0.829811    0.837597    0.839822   \n",
      "6             Precision    0.833333    0.828571    0.845011    0.830579   \n",
      "7           Sensitivity    0.842217    0.854737    0.845011    0.866379   \n",
      "8           Specificity    0.816300    0.801900    0.829400    0.811500   \n",
      "9              F1 score    0.837752    0.841451    0.845011    0.848101   \n",
      "10  F1 score (weighted)    0.829764    0.829601    0.837597    0.839627   \n",
      "11     F1 score (macro)    0.829402    0.828889    0.837225    0.839345   \n",
      "12    Balanced Accuracy    0.829248    0.828312    0.837225    0.838937   \n",
      "13                  MCC    0.658856    0.658243    0.674450    0.679525   \n",
      "14                  NPV    0.825900    0.831300    0.829400    0.850600   \n",
      "15              ROC_AUC    0.829248    0.828312    0.837225    0.838937   \n",
      "\n",
      "          Set4  \n",
      "0     0.701272  \n",
      "1   410.000000  \n",
      "2   346.000000  \n",
      "3    83.000000  \n",
      "4    60.000000  \n",
      "5     0.840934  \n",
      "6     0.831643  \n",
      "7     0.872340  \n",
      "8     0.806500  \n",
      "9     0.851506  \n",
      "10    0.840643  \n",
      "11    0.840124  \n",
      "12    0.839434  \n",
      "13    0.681359  \n",
      "14    0.852200  \n",
      "15    0.839434  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_4_cat = np.where((y_pred_svm_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 13:46:28,137] Trial 250 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:46:47,761] Trial 251 finished with value: 0.3152388373857279 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:47:06,798] Trial 252 finished with value: 0.4112508763901469 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:47:25,825] Trial 253 finished with value: 0.5322550390910257 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:47:43,578] Trial 254 finished with value: 0.42288448691791364 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:48:02,038] Trial 255 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:48:20,017] Trial 256 finished with value: 0.695974970249168 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:48:37,605] Trial 257 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:49:00,172] Trial 258 finished with value: 0.03268564302444956 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:49:18,149] Trial 259 finished with value: 0.6939051740996873 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:49:35,815] Trial 260 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:49:55,207] Trial 261 finished with value: 0.27786886814106226 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:50:14,252] Trial 262 finished with value: 0.013038227282853754 and parameters: {'C': 0.25, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:50:32,390] Trial 263 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:50:50,839] Trial 264 finished with value: 0.6817973792741432 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:51:09,198] Trial 265 finished with value: 0.6939023858556832 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:51:26,905] Trial 266 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:51:44,632] Trial 267 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:52:04,549] Trial 268 finished with value: 0.035502497843165934 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:52:21,078] Trial 269 finished with value: 0.6601186221917176 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:52:38,953] Trial 270 finished with value: 0.6103698374391442 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:52:57,168] Trial 271 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:53:16,063] Trial 272 finished with value: 0.5532545406871971 and parameters: {'C': 2.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:53:36,437] Trial 273 finished with value: 0.13169460342418754 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:53:54,352] Trial 274 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:54:14,069] Trial 275 finished with value: 0.05419710223989012 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:54:32,573] Trial 276 finished with value: 0.3290676356116006 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:54:51,691] Trial 277 finished with value: 0.2443009290393685 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:55:10,603] Trial 278 finished with value: 0.21367821173170265 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:55:30,958] Trial 279 finished with value: 0.03304192474293531 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:55:48,976] Trial 280 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:56:08,596] Trial 281 finished with value: 0.04306749843589468 and parameters: {'C': 0.0078125, 'gamma': 0.00390625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:56:26,833] Trial 282 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:56:44,263] Trial 283 finished with value: 0.677251335625713 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:57:02,283] Trial 284 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:57:20,418] Trial 285 finished with value: 0.4771485299250914 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:57:38,443] Trial 286 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:58:00,236] Trial 287 finished with value: 0.6307611756618939 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:58:18,206] Trial 288 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:58:36,644] Trial 289 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:58:54,655] Trial 290 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:59:12,630] Trial 291 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:59:31,395] Trial 292 finished with value: 0.053682774025982496 and parameters: {'C': 0.125, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 13:59:50,109] Trial 293 finished with value: 0.695974970249168 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 14:00:08,151] Trial 294 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 14:00:27,853] Trial 295 finished with value: 0.09202316976818563 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 14:00:46,204] Trial 296 finished with value: 0.5322550390910257 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 14:01:05,433] Trial 297 finished with value: 0.002554665772442033 and parameters: {'C': 0.0625, 'gamma': 1.0}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 14:01:23,893] Trial 298 finished with value: 0.5254430954804918 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n",
      "[I 2023-12-11 14:01:41,785] Trial 299 finished with value: 0.6947844419788056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6976081712310384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698260    0.719248    0.692609    0.738147   \n",
      "1                    TP  395.000000  406.000000  398.000000  402.000000   \n",
      "2                    TN  351.000000  340.000000  355.000000  353.000000   \n",
      "3                    FP   79.000000   84.000000   73.000000   82.000000   \n",
      "4                    FN   74.000000   69.000000   73.000000   62.000000   \n",
      "5              Accuracy    0.829811    0.829811    0.837597    0.839822   \n",
      "6             Precision    0.833333    0.828571    0.845011    0.830579   \n",
      "7           Sensitivity    0.842217    0.854737    0.845011    0.866379   \n",
      "8           Specificity    0.816300    0.801900    0.829400    0.811500   \n",
      "9              F1 score    0.837752    0.841451    0.845011    0.848101   \n",
      "10  F1 score (weighted)    0.829764    0.829601    0.837597    0.839627   \n",
      "11     F1 score (macro)    0.829402    0.828889    0.837225    0.839345   \n",
      "12    Balanced Accuracy    0.829248    0.828312    0.837225    0.838937   \n",
      "13                  MCC    0.658856    0.658243    0.674450    0.679525   \n",
      "14                  NPV    0.825900    0.831300    0.829400    0.850600   \n",
      "15              ROC_AUC    0.829248    0.828312    0.837225    0.838937   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.701272    0.724369  \n",
      "1   410.000000  392.000000  \n",
      "2   346.000000  350.000000  \n",
      "3    83.000000   81.000000  \n",
      "4    60.000000   76.000000  \n",
      "5     0.840934    0.825362  \n",
      "6     0.831643    0.828753  \n",
      "7     0.872340    0.837607  \n",
      "8     0.806500    0.812100  \n",
      "9     0.851506    0.833156  \n",
      "10    0.840643    0.825316  \n",
      "11    0.840124    0.824980  \n",
      "12    0.839434    0.824836  \n",
      "13    0.681359    0.650010  \n",
      "14    0.852200    0.821600  \n",
      "15    0.839434    0.824836  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_5_cat = np.where((y_pred_svm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 14:02:03,206] Trial 300 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:02:21,154] Trial 301 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:02:42,602] Trial 302 finished with value: 0.03258532180058018 and parameters: {'C': 2.0, 'gamma': 8.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:03:00,737] Trial 303 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:03:17,731] Trial 304 finished with value: 0.4801140025269125 and parameters: {'C': 64.0, 'gamma': 3.0517578125e-05}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:03:36,025] Trial 305 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:03:55,384] Trial 306 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:04:14,420] Trial 307 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:04:34,820] Trial 308 finished with value: 0.28713280552371695 and parameters: {'C': 64.0, 'gamma': 0.125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:04:55,076] Trial 309 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:05:14,119] Trial 310 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:05:33,320] Trial 311 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:05:52,678] Trial 312 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:06:12,524] Trial 313 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:06:32,164] Trial 314 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:06:51,819] Trial 315 finished with value: 0.034399395472720404 and parameters: {'C': 64.0, 'gamma': 2.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:07:13,896] Trial 316 finished with value: 0.6791639368252992 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:07:33,062] Trial 317 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:07:52,765] Trial 318 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:08:11,449] Trial 319 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:08:30,404] Trial 320 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:08:50,021] Trial 321 finished with value: 0.5711822986334274 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:09:11,044] Trial 322 finished with value: 0.05096106618408234 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:09:31,351] Trial 323 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:09:50,989] Trial 324 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:10:10,695] Trial 325 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:10:30,230] Trial 326 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:10:50,436] Trial 327 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:11:10,151] Trial 328 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:11:29,676] Trial 329 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:11:49,388] Trial 330 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:12:08,467] Trial 331 finished with value: 0.5493419530322308 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:12:26,921] Trial 332 finished with value: 0.5272164486333724 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:12:47,010] Trial 333 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:13:06,586] Trial 334 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:13:26,778] Trial 335 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:13:46,424] Trial 336 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:14:10,866] Trial 337 finished with value: 0.6553248782997563 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:14:30,330] Trial 338 finished with value: 0.6894517524852853 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:14:50,161] Trial 339 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:15:10,622] Trial 340 finished with value: 0.03173893845077715 and parameters: {'C': 64.0, 'gamma': 4.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:15:30,152] Trial 341 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:15:51,087] Trial 342 finished with value: 0.5622184317626234 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:16:10,425] Trial 343 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:16:29,656] Trial 344 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:16:55,039] Trial 345 finished with value: 0.6424795561088974 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:17:14,688] Trial 346 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:17:34,973] Trial 347 finished with value: 0.08990874270558388 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:17:54,641] Trial 348 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:18:14,389] Trial 349 finished with value: 0.7036935515633165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7037\n",
      "\tBest params:\n",
      "\t\tC: 64.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698260    0.719248    0.692609    0.738147   \n",
      "1                    TP  395.000000  406.000000  398.000000  402.000000   \n",
      "2                    TN  351.000000  340.000000  355.000000  353.000000   \n",
      "3                    FP   79.000000   84.000000   73.000000   82.000000   \n",
      "4                    FN   74.000000   69.000000   73.000000   62.000000   \n",
      "5              Accuracy    0.829811    0.829811    0.837597    0.839822   \n",
      "6             Precision    0.833333    0.828571    0.845011    0.830579   \n",
      "7           Sensitivity    0.842217    0.854737    0.845011    0.866379   \n",
      "8           Specificity    0.816300    0.801900    0.829400    0.811500   \n",
      "9              F1 score    0.837752    0.841451    0.845011    0.848101   \n",
      "10  F1 score (weighted)    0.829764    0.829601    0.837597    0.839627   \n",
      "11     F1 score (macro)    0.829402    0.828889    0.837225    0.839345   \n",
      "12    Balanced Accuracy    0.829248    0.828312    0.837225    0.838937   \n",
      "13                  MCC    0.658856    0.658243    0.674450    0.679525   \n",
      "14                  NPV    0.825900    0.831300    0.829400    0.850600   \n",
      "15              ROC_AUC    0.829248    0.828312    0.837225    0.838937   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.701272    0.724369    0.657130  \n",
      "1   410.000000  392.000000  398.000000  \n",
      "2   346.000000  350.000000  351.000000  \n",
      "3    83.000000   81.000000   76.000000  \n",
      "4    60.000000   76.000000   74.000000  \n",
      "5     0.840934    0.825362    0.833148  \n",
      "6     0.831643    0.828753    0.839662  \n",
      "7     0.872340    0.837607    0.843220  \n",
      "8     0.806500    0.812100    0.822000  \n",
      "9     0.851506    0.833156    0.841438  \n",
      "10    0.840643    0.825316    0.833128  \n",
      "11    0.840124    0.824980    0.832691  \n",
      "12    0.839434    0.824836    0.832617  \n",
      "13    0.681359    0.650010    0.665390  \n",
      "14    0.852200    0.821600    0.825900  \n",
      "15    0.839434    0.824836    0.832617  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_6_cat = np.where((y_pred_svm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 14:18:38,441] Trial 350 finished with value: 0.5496652026344526 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:18:57,860] Trial 351 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:19:17,112] Trial 352 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:19:38,408] Trial 353 finished with value: 0.04638994815452334 and parameters: {'C': 64.0, 'gamma': 1.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:19:58,255] Trial 354 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:20:22,197] Trial 355 finished with value: 0.03700855644114857 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:20:41,803] Trial 356 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:21:05,674] Trial 357 finished with value: 0.5986572994593341 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:21:25,376] Trial 358 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:21:44,416] Trial 359 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:22:02,983] Trial 360 finished with value: 0.4785224127843358 and parameters: {'C': 64.0, 'gamma': 3.0517578125e-05}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:22:22,161] Trial 361 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:22:41,407] Trial 362 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:23:01,980] Trial 363 finished with value: 0.2915665045327483 and parameters: {'C': 64.0, 'gamma': 0.125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:23:21,365] Trial 364 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:23:42,772] Trial 365 finished with value: 0.6754469782612109 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:24:02,249] Trial 366 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:24:23,239] Trial 367 finished with value: 0.040038801454421624 and parameters: {'C': 64.0, 'gamma': 2.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:24:42,666] Trial 368 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:25:01,804] Trial 369 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:25:21,341] Trial 370 finished with value: 0.5708507740942872 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:25:42,102] Trial 371 finished with value: 0.0577761046879916 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:26:02,247] Trial 372 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:26:20,772] Trial 373 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:26:38,492] Trial 374 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:26:56,390] Trial 375 finished with value: 0.5228072095165659 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:27:14,046] Trial 376 finished with value: 0.5443650859354733 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:27:32,236] Trial 377 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:27:49,863] Trial 378 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:28:09,299] Trial 379 finished with value: 0.037404913549125715 and parameters: {'C': 64.0, 'gamma': 4.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:28:27,312] Trial 380 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:28:44,713] Trial 381 finished with value: 0.6821759374536922 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:29:02,846] Trial 382 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:29:25,462] Trial 383 finished with value: 0.6554330226944137 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:29:43,806] Trial 384 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:30:01,787] Trial 385 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:30:26,025] Trial 386 finished with value: 0.6431711298312195 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:30:46,247] Trial 387 finished with value: 0.5622786174009078 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:31:03,971] Trial 388 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:31:21,765] Trial 389 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:31:41,591] Trial 390 finished with value: 0.0965317317116194 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:31:59,785] Trial 391 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:32:17,469] Trial 392 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:32:36,331] Trial 393 finished with value: 0.5496652026344526 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:32:56,046] Trial 394 finished with value: 0.04638994815452334 and parameters: {'C': 64.0, 'gamma': 1.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:33:14,142] Trial 395 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:33:32,387] Trial 396 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:33:55,207] Trial 397 finished with value: 0.5986572994593341 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:34:13,416] Trial 398 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:34:31,366] Trial 399 finished with value: 0.6972571219259099 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7037\n",
      "\tBest params:\n",
      "\t\tC: 64.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698260    0.719248    0.692609    0.738147   \n",
      "1                    TP  395.000000  406.000000  398.000000  402.000000   \n",
      "2                    TN  351.000000  340.000000  355.000000  353.000000   \n",
      "3                    FP   79.000000   84.000000   73.000000   82.000000   \n",
      "4                    FN   74.000000   69.000000   73.000000   62.000000   \n",
      "5              Accuracy    0.829811    0.829811    0.837597    0.839822   \n",
      "6             Precision    0.833333    0.828571    0.845011    0.830579   \n",
      "7           Sensitivity    0.842217    0.854737    0.845011    0.866379   \n",
      "8           Specificity    0.816300    0.801900    0.829400    0.811500   \n",
      "9              F1 score    0.837752    0.841451    0.845011    0.848101   \n",
      "10  F1 score (weighted)    0.829764    0.829601    0.837597    0.839627   \n",
      "11     F1 score (macro)    0.829402    0.828889    0.837225    0.839345   \n",
      "12    Balanced Accuracy    0.829248    0.828312    0.837225    0.838937   \n",
      "13                  MCC    0.658856    0.658243    0.674450    0.679525   \n",
      "14                  NPV    0.825900    0.831300    0.829400    0.850600   \n",
      "15              ROC_AUC    0.829248    0.828312    0.837225    0.838937   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.701272    0.724369    0.657130    0.682070  \n",
      "1   410.000000  392.000000  398.000000  402.000000  \n",
      "2   346.000000  350.000000  351.000000  334.000000  \n",
      "3    83.000000   81.000000   76.000000   84.000000  \n",
      "4    60.000000   76.000000   74.000000   79.000000  \n",
      "5     0.840934    0.825362    0.833148    0.818687  \n",
      "6     0.831643    0.828753    0.839662    0.827160  \n",
      "7     0.872340    0.837607    0.843220    0.835759  \n",
      "8     0.806500    0.812100    0.822000    0.799000  \n",
      "9     0.851506    0.833156    0.841438    0.831437  \n",
      "10    0.840643    0.825316    0.833128    0.818611  \n",
      "11    0.840124    0.824980    0.832691    0.817644  \n",
      "12    0.839434    0.824836    0.832617    0.817401  \n",
      "13    0.681359    0.650010    0.665390    0.635339  \n",
      "14    0.852200    0.821600    0.825900    0.808700  \n",
      "15    0.839434    0.824836    0.832617    0.817401  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_7_cat = np.where((y_pred_svm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 14:34:53,231] Trial 400 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:35:11,720] Trial 401 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:35:32,595] Trial 402 finished with value: 0.0311197090345506 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:35:51,047] Trial 403 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:36:09,781] Trial 404 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:36:27,632] Trial 405 finished with value: 0.28133558905514466 and parameters: {'C': 64.0, 'gamma': 0.125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:36:46,390] Trial 406 finished with value: 0.6843027634273434 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:37:06,165] Trial 407 finished with value: 0.6632008491147192 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:37:24,718] Trial 408 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:37:41,860] Trial 409 finished with value: 0.2444566041441168 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:38:00,395] Trial 410 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:38:18,779] Trial 411 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:38:36,685] Trial 412 finished with value: 0.03910007036445382 and parameters: {'C': 0.5, 'gamma': 0.5}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:38:55,481] Trial 413 finished with value: 0.034020020999900466 and parameters: {'C': 64.0, 'gamma': 2.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:39:12,704] Trial 414 finished with value: 0.6691099999954385 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:39:30,790] Trial 415 finished with value: 0.5586136135017761 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:39:49,615] Trial 416 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:40:08,273] Trial 417 finished with value: 0.0010485502323084428 and parameters: {'C': 0.015625, 'gamma': 6.103515625e-05}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:40:26,623] Trial 418 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:40:45,223] Trial 419 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:41:03,955] Trial 420 finished with value: 0.5306417387603536 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:41:23,265] Trial 421 finished with value: 0.03148418101742748 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:41:41,818] Trial 422 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:41:59,733] Trial 423 finished with value: 0.07651269017446177 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:42:21,846] Trial 424 finished with value: 0.6458177651079607 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:42:40,058] Trial 425 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:42:57,641] Trial 426 finished with value: 0.21713034796256236 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:43:16,166] Trial 427 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:43:35,338] Trial 428 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:43:54,125] Trial 429 finished with value: 0.05417794868600517 and parameters: {'C': 0.0625, 'gamma': 0.00048828125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:44:13,458] Trial 430 finished with value: 0.6848387356761881 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:44:36,702] Trial 431 finished with value: 0.6439482000729111 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:44:54,531] Trial 432 finished with value: 0.40175272027158754 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:45:13,149] Trial 433 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:45:31,772] Trial 434 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:45:50,014] Trial 435 finished with value: 0.5431122064245694 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:46:08,956] Trial 436 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:46:28,288] Trial 437 finished with value: 0.019183024877671383 and parameters: {'C': 0.25, 'gamma': 1.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:46:46,964] Trial 438 finished with value: 0.09190360466597383 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:47:06,065] Trial 439 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:47:28,178] Trial 440 finished with value: 0.5957083173862403 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:47:47,312] Trial 441 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:48:06,690] Trial 442 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:48:25,876] Trial 443 finished with value: 0.6843027634273434 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:48:46,865] Trial 444 finished with value: 0.0311197090345506 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:49:04,853] Trial 445 finished with value: 0.6944253675149843 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:49:22,219] Trial 446 finished with value: 0.4668824678998784 and parameters: {'C': 64.0, 'gamma': 3.0517578125e-05}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:49:40,913] Trial 447 finished with value: 0.6844742843222165 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:49:59,053] Trial 448 finished with value: 0.6897241494498212 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:50:17,845] Trial 449 finished with value: 0.17956606215445184 and parameters: {'C': 0.5, 'gamma': 0.125}. Best is trial 300 with value: 0.7036935515633165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7037\n",
      "\tBest params:\n",
      "\t\tC: 64.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698260    0.719248    0.692609    0.738147   \n",
      "1                    TP  395.000000  406.000000  398.000000  402.000000   \n",
      "2                    TN  351.000000  340.000000  355.000000  353.000000   \n",
      "3                    FP   79.000000   84.000000   73.000000   82.000000   \n",
      "4                    FN   74.000000   69.000000   73.000000   62.000000   \n",
      "5              Accuracy    0.829811    0.829811    0.837597    0.839822   \n",
      "6             Precision    0.833333    0.828571    0.845011    0.830579   \n",
      "7           Sensitivity    0.842217    0.854737    0.845011    0.866379   \n",
      "8           Specificity    0.816300    0.801900    0.829400    0.811500   \n",
      "9              F1 score    0.837752    0.841451    0.845011    0.848101   \n",
      "10  F1 score (weighted)    0.829764    0.829601    0.837597    0.839627   \n",
      "11     F1 score (macro)    0.829402    0.828889    0.837225    0.839345   \n",
      "12    Balanced Accuracy    0.829248    0.828312    0.837225    0.838937   \n",
      "13                  MCC    0.658856    0.658243    0.674450    0.679525   \n",
      "14                  NPV    0.825900    0.831300    0.829400    0.850600   \n",
      "15              ROC_AUC    0.829248    0.828312    0.837225    0.838937   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.701272    0.724369    0.657130    0.682070    0.706967  \n",
      "1   410.000000  392.000000  398.000000  402.000000  416.000000  \n",
      "2   346.000000  350.000000  351.000000  334.000000  352.000000  \n",
      "3    83.000000   81.000000   76.000000   84.000000   64.000000  \n",
      "4    60.000000   76.000000   74.000000   79.000000   67.000000  \n",
      "5     0.840934    0.825362    0.833148    0.818687    0.854283  \n",
      "6     0.831643    0.828753    0.839662    0.827160    0.866667  \n",
      "7     0.872340    0.837607    0.843220    0.835759    0.861284  \n",
      "8     0.806500    0.812100    0.822000    0.799000    0.846200  \n",
      "9     0.851506    0.833156    0.841438    0.831437    0.863967  \n",
      "10    0.840643    0.825316    0.833128    0.818611    0.854317  \n",
      "11    0.840124    0.824980    0.832691    0.817644    0.853540  \n",
      "12    0.839434    0.824836    0.832617    0.817401    0.853719  \n",
      "13    0.681359    0.650010    0.665390    0.635339    0.707100  \n",
      "14    0.852200    0.821600    0.825900    0.808700    0.840100  \n",
      "15    0.839434    0.824836    0.832617    0.817401    0.853719  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_8_cat = np.where((y_pred_svm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 14:50:39,444] Trial 450 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:50:59,212] Trial 451 finished with value: 0.6510024965618937 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:51:18,026] Trial 452 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:51:34,910] Trial 453 finished with value: 0.6497890081040494 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:51:53,122] Trial 454 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:52:13,427] Trial 455 finished with value: 0.03371436896582053 and parameters: {'C': 64.0, 'gamma': 2.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:52:32,923] Trial 456 finished with value: 0.03420954059259651 and parameters: {'C': 0.015625, 'gamma': 0.0625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:52:51,885] Trial 457 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:53:10,870] Trial 458 finished with value: -0.0016647885668962358 and parameters: {'C': 0.03125, 'gamma': 0.5}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:53:30,147] Trial 459 finished with value: 0.07262209721590704 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:53:48,778] Trial 460 finished with value: 0.47247030357734765 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:54:07,157] Trial 461 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:54:25,993] Trial 462 finished with value: 0.5344057881844733 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:54:44,692] Trial 463 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:55:02,974] Trial 464 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:55:21,293] Trial 465 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:55:40,675] Trial 466 finished with value: 0.029198854479474623 and parameters: {'C': 64.0, 'gamma': 4.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:55:59,379] Trial 467 finished with value: 0.31675565284098295 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:56:22,470] Trial 468 finished with value: 0.6220263758876294 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:56:40,560] Trial 469 finished with value: 0.6755387462203989 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:56:59,501] Trial 470 finished with value: 0.42187736111237467 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:57:20,486] Trial 471 finished with value: 0.5433301596066913 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:57:38,857] Trial 472 finished with value: 0.6636269767512866 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:57:57,168] Trial 473 finished with value: 0.5211467478126026 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:58:15,741] Trial 474 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:58:40,691] Trial 475 finished with value: 0.614802114401041 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:59:00,843] Trial 476 finished with value: 0.09449637972396799 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:59:19,559] Trial 477 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:59:37,915] Trial 478 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 14:59:56,784] Trial 479 finished with value: 0.534847765513545 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:00:14,453] Trial 480 finished with value: 0.6724824886003973 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:00:33,230] Trial 481 finished with value: 0.6755746285061915 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:00:56,239] Trial 482 finished with value: 0.5710248557129114 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:01:16,417] Trial 483 finished with value: 0.04362365340401626 and parameters: {'C': 64.0, 'gamma': 1.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:01:34,507] Trial 484 finished with value: 0.6742756628366358 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:01:56,612] Trial 485 finished with value: 0.028504351397291728 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:02:15,298] Trial 486 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:02:33,204] Trial 487 finished with value: 0.6034854631191711 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:02:50,015] Trial 488 finished with value: 0.6497890081040494 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:03:09,023] Trial 489 finished with value: 0.47225615001103305 and parameters: {'C': 64.0, 'gamma': 3.0517578125e-05}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:03:27,961] Trial 490 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:03:47,505] Trial 491 finished with value: 0.28105268987699017 and parameters: {'C': 64.0, 'gamma': 0.125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:04:06,407] Trial 492 finished with value: 0.12913887029422405 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:04:24,710] Trial 493 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:04:44,598] Trial 494 finished with value: 0.6510024965618937 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:05:03,580] Trial 495 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:05:22,384] Trial 496 finished with value: -0.005036424757188895 and parameters: {'C': 0.0078125, 'gamma': 2.0}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:05:40,646] Trial 497 finished with value: 0.6758065737694614 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:05:59,130] Trial 498 finished with value: 0.6755801063837289 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7036935515633165.\n",
      "[I 2023-12-11 15:06:18,338] Trial 499 finished with value: -0.0016647885668962358 and parameters: {'C': 0.03125, 'gamma': 0.5}. Best is trial 300 with value: 0.7036935515633165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7037\n",
      "\tBest params:\n",
      "\t\tC: 64.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698260    0.719248    0.692609    0.738147   \n",
      "1                    TP  395.000000  406.000000  398.000000  402.000000   \n",
      "2                    TN  351.000000  340.000000  355.000000  353.000000   \n",
      "3                    FP   79.000000   84.000000   73.000000   82.000000   \n",
      "4                    FN   74.000000   69.000000   73.000000   62.000000   \n",
      "5              Accuracy    0.829811    0.829811    0.837597    0.839822   \n",
      "6             Precision    0.833333    0.828571    0.845011    0.830579   \n",
      "7           Sensitivity    0.842217    0.854737    0.845011    0.866379   \n",
      "8           Specificity    0.816300    0.801900    0.829400    0.811500   \n",
      "9              F1 score    0.837752    0.841451    0.845011    0.848101   \n",
      "10  F1 score (weighted)    0.829764    0.829601    0.837597    0.839627   \n",
      "11     F1 score (macro)    0.829402    0.828889    0.837225    0.839345   \n",
      "12    Balanced Accuracy    0.829248    0.828312    0.837225    0.838937   \n",
      "13                  MCC    0.658856    0.658243    0.674450    0.679525   \n",
      "14                  NPV    0.825900    0.831300    0.829400    0.850600   \n",
      "15              ROC_AUC    0.829248    0.828312    0.837225    0.838937   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.701272    0.724369    0.657130    0.682070    0.706967    0.707509  \n",
      "1   410.000000  392.000000  398.000000  402.000000  416.000000  408.000000  \n",
      "2   346.000000  350.000000  351.000000  334.000000  352.000000  350.000000  \n",
      "3    83.000000   81.000000   76.000000   84.000000   64.000000   65.000000  \n",
      "4    60.000000   76.000000   74.000000   79.000000   67.000000   76.000000  \n",
      "5     0.840934    0.825362    0.833148    0.818687    0.854283    0.843159  \n",
      "6     0.831643    0.828753    0.839662    0.827160    0.866667    0.862579  \n",
      "7     0.872340    0.837607    0.843220    0.835759    0.861284    0.842975  \n",
      "8     0.806500    0.812100    0.822000    0.799000    0.846200    0.843400  \n",
      "9     0.851506    0.833156    0.841438    0.831437    0.863967    0.852665  \n",
      "10    0.840643    0.825316    0.833128    0.818611    0.854317    0.843283  \n",
      "11    0.840124    0.824980    0.832691    0.817644    0.853540    0.842504  \n",
      "12    0.839434    0.824836    0.832617    0.817401    0.853719    0.843174  \n",
      "13    0.681359    0.650010    0.665390    0.635339    0.707100    0.685261  \n",
      "14    0.852200    0.821600    0.825900    0.808700    0.840100    0.821600  \n",
      "15    0.839434    0.824836    0.832617    0.817401    0.853719    0.843174  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_9_cat = np.where((y_pred_svm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7037\n",
      "\tBest params:\n",
      "\t\tC: 64.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.709371</td>\n",
       "      <td>0.019467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>199.900000</td>\n",
       "      <td>7.460265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>174.400000</td>\n",
       "      <td>7.244922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>38.700000</td>\n",
       "      <td>6.111010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>36.200000</td>\n",
       "      <td>5.202563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.833260</td>\n",
       "      <td>0.018840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.837906</td>\n",
       "      <td>0.025346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.846757</td>\n",
       "      <td>0.020740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.818590</td>\n",
       "      <td>0.026898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.842113</td>\n",
       "      <td>0.018580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.833216</td>\n",
       "      <td>0.018829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.832606</td>\n",
       "      <td>0.018947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.832673</td>\n",
       "      <td>0.019104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.665699</td>\n",
       "      <td>0.038221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.828160</td>\n",
       "      <td>0.024041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.832673</td>\n",
       "      <td>0.019104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.709371     0.019467\n",
       "1                    TP       199.900000     7.460265\n",
       "2                    TN       174.400000     7.244922\n",
       "3                    FP        38.700000     6.111010\n",
       "4                    FN        36.200000     5.202563\n",
       "5              Accuracy         0.833260     0.018840\n",
       "6             Precision         0.837906     0.025346\n",
       "7           Sensitivity         0.846757     0.020740\n",
       "8           Specificity         0.818590     0.026898\n",
       "9              F1 score         0.842113     0.018580\n",
       "10  F1 score (weighted)         0.833216     0.018829\n",
       "11     F1 score (macro)         0.832606     0.018947\n",
       "12    Balanced Accuracy         0.832673     0.019104\n",
       "13                  MCC         0.665699     0.038221\n",
       "14                  NPV         0.828160     0.024041\n",
       "15              ROC_AUC         0.832673     0.019104"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.698260</td>\n",
       "      <td>0.719248</td>\n",
       "      <td>0.692609</td>\n",
       "      <td>0.738147</td>\n",
       "      <td>0.701272</td>\n",
       "      <td>0.724369</td>\n",
       "      <td>0.657130</td>\n",
       "      <td>0.682070</td>\n",
       "      <td>0.706967</td>\n",
       "      <td>0.707509</td>\n",
       "      <td>0.702758</td>\n",
       "      <td>0.022772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>402.700000</td>\n",
       "      <td>7.364328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>348.200000</td>\n",
       "      <td>6.494442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>77.100000</td>\n",
       "      <td>7.519604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>6.306963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.829811</td>\n",
       "      <td>0.829811</td>\n",
       "      <td>0.837597</td>\n",
       "      <td>0.839822</td>\n",
       "      <td>0.840934</td>\n",
       "      <td>0.825362</td>\n",
       "      <td>0.833148</td>\n",
       "      <td>0.818687</td>\n",
       "      <td>0.854283</td>\n",
       "      <td>0.843159</td>\n",
       "      <td>0.835261</td>\n",
       "      <td>0.010106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.845011</td>\n",
       "      <td>0.830579</td>\n",
       "      <td>0.831643</td>\n",
       "      <td>0.828753</td>\n",
       "      <td>0.839662</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862579</td>\n",
       "      <td>0.839396</td>\n",
       "      <td>0.014405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.842217</td>\n",
       "      <td>0.854737</td>\n",
       "      <td>0.845011</td>\n",
       "      <td>0.866379</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.837607</td>\n",
       "      <td>0.843220</td>\n",
       "      <td>0.835759</td>\n",
       "      <td>0.861284</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.850153</td>\n",
       "      <td>0.012711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.816300</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.812100</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.846200</td>\n",
       "      <td>0.843400</td>\n",
       "      <td>0.818830</td>\n",
       "      <td>0.016370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.837752</td>\n",
       "      <td>0.841451</td>\n",
       "      <td>0.845011</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.851506</td>\n",
       "      <td>0.833156</td>\n",
       "      <td>0.841438</td>\n",
       "      <td>0.831437</td>\n",
       "      <td>0.863967</td>\n",
       "      <td>0.852665</td>\n",
       "      <td>0.844648</td>\n",
       "      <td>0.009839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.829764</td>\n",
       "      <td>0.829601</td>\n",
       "      <td>0.837597</td>\n",
       "      <td>0.839627</td>\n",
       "      <td>0.840643</td>\n",
       "      <td>0.825316</td>\n",
       "      <td>0.833128</td>\n",
       "      <td>0.818611</td>\n",
       "      <td>0.854317</td>\n",
       "      <td>0.843283</td>\n",
       "      <td>0.835189</td>\n",
       "      <td>0.010132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.829402</td>\n",
       "      <td>0.828889</td>\n",
       "      <td>0.837225</td>\n",
       "      <td>0.839345</td>\n",
       "      <td>0.840124</td>\n",
       "      <td>0.824980</td>\n",
       "      <td>0.832691</td>\n",
       "      <td>0.817644</td>\n",
       "      <td>0.853540</td>\n",
       "      <td>0.842504</td>\n",
       "      <td>0.834634</td>\n",
       "      <td>0.010135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.829248</td>\n",
       "      <td>0.828312</td>\n",
       "      <td>0.837225</td>\n",
       "      <td>0.838937</td>\n",
       "      <td>0.839434</td>\n",
       "      <td>0.824836</td>\n",
       "      <td>0.832617</td>\n",
       "      <td>0.817401</td>\n",
       "      <td>0.853719</td>\n",
       "      <td>0.843174</td>\n",
       "      <td>0.834490</td>\n",
       "      <td>0.010281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.658856</td>\n",
       "      <td>0.658243</td>\n",
       "      <td>0.674450</td>\n",
       "      <td>0.679525</td>\n",
       "      <td>0.681359</td>\n",
       "      <td>0.650010</td>\n",
       "      <td>0.665390</td>\n",
       "      <td>0.635339</td>\n",
       "      <td>0.707100</td>\n",
       "      <td>0.685261</td>\n",
       "      <td>0.669553</td>\n",
       "      <td>0.020362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.825900</td>\n",
       "      <td>0.831300</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.850600</td>\n",
       "      <td>0.852200</td>\n",
       "      <td>0.821600</td>\n",
       "      <td>0.825900</td>\n",
       "      <td>0.808700</td>\n",
       "      <td>0.840100</td>\n",
       "      <td>0.821600</td>\n",
       "      <td>0.830730</td>\n",
       "      <td>0.013517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.829248</td>\n",
       "      <td>0.828312</td>\n",
       "      <td>0.837225</td>\n",
       "      <td>0.838937</td>\n",
       "      <td>0.839434</td>\n",
       "      <td>0.824836</td>\n",
       "      <td>0.832617</td>\n",
       "      <td>0.817401</td>\n",
       "      <td>0.853719</td>\n",
       "      <td>0.843174</td>\n",
       "      <td>0.834490</td>\n",
       "      <td>0.010281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.698260    0.719248    0.692609    0.738147   \n",
       "1                    TP  395.000000  406.000000  398.000000  402.000000   \n",
       "2                    TN  351.000000  340.000000  355.000000  353.000000   \n",
       "3                    FP   79.000000   84.000000   73.000000   82.000000   \n",
       "4                    FN   74.000000   69.000000   73.000000   62.000000   \n",
       "5              Accuracy    0.829811    0.829811    0.837597    0.839822   \n",
       "6             Precision    0.833333    0.828571    0.845011    0.830579   \n",
       "7           Sensitivity    0.842217    0.854737    0.845011    0.866379   \n",
       "8           Specificity    0.816300    0.801900    0.829400    0.811500   \n",
       "9              F1 score    0.837752    0.841451    0.845011    0.848101   \n",
       "10  F1 score (weighted)    0.829764    0.829601    0.837597    0.839627   \n",
       "11     F1 score (macro)    0.829402    0.828889    0.837225    0.839345   \n",
       "12    Balanced Accuracy    0.829248    0.828312    0.837225    0.838937   \n",
       "13                  MCC    0.658856    0.658243    0.674450    0.679525   \n",
       "14                  NPV    0.825900    0.831300    0.829400    0.850600   \n",
       "15              ROC_AUC    0.829248    0.828312    0.837225    0.838937   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.701272    0.724369    0.657130    0.682070    0.706967    0.707509   \n",
       "1   410.000000  392.000000  398.000000  402.000000  416.000000  408.000000   \n",
       "2   346.000000  350.000000  351.000000  334.000000  352.000000  350.000000   \n",
       "3    83.000000   81.000000   76.000000   84.000000   64.000000   65.000000   \n",
       "4    60.000000   76.000000   74.000000   79.000000   67.000000   76.000000   \n",
       "5     0.840934    0.825362    0.833148    0.818687    0.854283    0.843159   \n",
       "6     0.831643    0.828753    0.839662    0.827160    0.866667    0.862579   \n",
       "7     0.872340    0.837607    0.843220    0.835759    0.861284    0.842975   \n",
       "8     0.806500    0.812100    0.822000    0.799000    0.846200    0.843400   \n",
       "9     0.851506    0.833156    0.841438    0.831437    0.863967    0.852665   \n",
       "10    0.840643    0.825316    0.833128    0.818611    0.854317    0.843283   \n",
       "11    0.840124    0.824980    0.832691    0.817644    0.853540    0.842504   \n",
       "12    0.839434    0.824836    0.832617    0.817401    0.853719    0.843174   \n",
       "13    0.681359    0.650010    0.665390    0.635339    0.707100    0.685261   \n",
       "14    0.852200    0.821600    0.825900    0.808700    0.840100    0.821600   \n",
       "15    0.839434    0.824836    0.832617    0.817401    0.853719    0.843174   \n",
       "\n",
       "           ave       std  \n",
       "0     0.702758  0.022772  \n",
       "1   402.700000  7.364328  \n",
       "2   348.200000  6.494442  \n",
       "3    77.100000  7.519604  \n",
       "4    71.000000  6.306963  \n",
       "5     0.835261  0.010106  \n",
       "6     0.839396  0.014405  \n",
       "7     0.850153  0.012711  \n",
       "8     0.818830  0.016370  \n",
       "9     0.844648  0.009839  \n",
       "10    0.835189  0.010132  \n",
       "11    0.834634  0.010135  \n",
       "12    0.834490  0.010281  \n",
       "13    0.669553  0.020362  \n",
       "14    0.830730  0.013517  \n",
       "15    0.834490  0.010281  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_svm0</th>\n",
       "      <th>y_pred_svm1</th>\n",
       "      <th>y_pred_svm2</th>\n",
       "      <th>y_pred_svm3</th>\n",
       "      <th>y_pred_svm4</th>\n",
       "      <th>y_pred_svm_ave</th>\n",
       "      <th>y_pred_svm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4176702</td>\n",
       "      <td>0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.047977</td>\n",
       "      <td>6.055991</td>\n",
       "      <td>6.127658</td>\n",
       "      <td>6.053485</td>\n",
       "      <td>6.059681</td>\n",
       "      <td>6.140799</td>\n",
       "      <td>0.162896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL272401</td>\n",
       "      <td>1</td>\n",
       "      <td>7.26</td>\n",
       "      <td>6.986258</td>\n",
       "      <td>6.996264</td>\n",
       "      <td>7.014725</td>\n",
       "      <td>6.831147</td>\n",
       "      <td>7.010183</td>\n",
       "      <td>7.016430</td>\n",
       "      <td>0.125841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL118</td>\n",
       "      <td>2</td>\n",
       "      <td>5.93</td>\n",
       "      <td>5.060373</td>\n",
       "      <td>5.031805</td>\n",
       "      <td>4.973583</td>\n",
       "      <td>5.072415</td>\n",
       "      <td>5.081156</td>\n",
       "      <td>5.191555</td>\n",
       "      <td>0.332146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3655939</td>\n",
       "      <td>3</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.818188</td>\n",
       "      <td>6.938552</td>\n",
       "      <td>6.850485</td>\n",
       "      <td>6.852908</td>\n",
       "      <td>7.046855</td>\n",
       "      <td>6.797831</td>\n",
       "      <td>0.243648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3621537</td>\n",
       "      <td>4</td>\n",
       "      <td>5.88</td>\n",
       "      <td>6.207133</td>\n",
       "      <td>6.132287</td>\n",
       "      <td>6.107543</td>\n",
       "      <td>6.102985</td>\n",
       "      <td>6.060675</td>\n",
       "      <td>6.081770</td>\n",
       "      <td>0.100448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL2047606</td>\n",
       "      <td>4487</td>\n",
       "      <td>5.44</td>\n",
       "      <td>5.447351</td>\n",
       "      <td>5.519933</td>\n",
       "      <td>5.489541</td>\n",
       "      <td>5.517867</td>\n",
       "      <td>5.530971</td>\n",
       "      <td>5.490944</td>\n",
       "      <td>0.035742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL217781</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.48</td>\n",
       "      <td>7.125798</td>\n",
       "      <td>7.062058</td>\n",
       "      <td>7.082189</td>\n",
       "      <td>7.055553</td>\n",
       "      <td>7.144547</td>\n",
       "      <td>7.158358</td>\n",
       "      <td>0.147393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL2105763</td>\n",
       "      <td>4489</td>\n",
       "      <td>9.92</td>\n",
       "      <td>6.919959</td>\n",
       "      <td>6.937916</td>\n",
       "      <td>6.970548</td>\n",
       "      <td>6.813810</td>\n",
       "      <td>6.961558</td>\n",
       "      <td>7.420632</td>\n",
       "      <td>1.118935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL3415969</td>\n",
       "      <td>4490</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.659980</td>\n",
       "      <td>5.659900</td>\n",
       "      <td>5.931509</td>\n",
       "      <td>5.660365</td>\n",
       "      <td>5.660245</td>\n",
       "      <td>5.720333</td>\n",
       "      <td>0.099981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL467066</td>\n",
       "      <td>4491</td>\n",
       "      <td>7.55</td>\n",
       "      <td>7.280424</td>\n",
       "      <td>7.350166</td>\n",
       "      <td>7.320265</td>\n",
       "      <td>7.320296</td>\n",
       "      <td>7.425848</td>\n",
       "      <td>7.374500</td>\n",
       "      <td>0.090147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_svm0  y_pred_svm1  \\\n",
       "0         CHEMBL4176702            0     6.50     6.047977     6.055991   \n",
       "1          CHEMBL272401            1     7.26     6.986258     6.996264   \n",
       "2             CHEMBL118            2     5.93     5.060373     5.031805   \n",
       "3         CHEMBL3655939            3     6.28     6.818188     6.938552   \n",
       "4         CHEMBL3621537            4     5.88     6.207133     6.132287   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "4487      CHEMBL2047606         4487     5.44     5.447351     5.519933   \n",
       "4488       CHEMBL217781         4488     7.48     7.125798     7.062058   \n",
       "4489      CHEMBL2105763         4489     9.92     6.919959     6.937916   \n",
       "4490      CHEMBL3415969         4490     5.75     5.659980     5.659900   \n",
       "4491       CHEMBL467066         4491     7.55     7.280424     7.350166   \n",
       "\n",
       "      y_pred_svm2  y_pred_svm3  y_pred_svm4  y_pred_svm_ave  y_pred_svm_std  \n",
       "0        6.127658     6.053485     6.059681        6.140799        0.162896  \n",
       "1        7.014725     6.831147     7.010183        7.016430        0.125841  \n",
       "2        4.973583     5.072415     5.081156        5.191555        0.332146  \n",
       "3        6.850485     6.852908     7.046855        6.797831        0.243648  \n",
       "4        6.107543     6.102985     6.060675        6.081770        0.100448  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "4487     5.489541     5.517867     5.530971        5.490944        0.035742  \n",
       "4488     7.082189     7.055553     7.144547        7.158358        0.147393  \n",
       "4489     6.970548     6.813810     6.961558        7.420632        1.118935  \n",
       "4490     5.931509     5.660365     5.660245        5.720333        0.099981  \n",
       "4491     7.320265     7.320296     7.425848        7.374500        0.090147  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where((y_pred_optimized_svm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id, svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "svm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2f1c2fff-1c92-4518-83ac-c4e880fe4590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.710592</td>\n",
       "      <td>0.035518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.836730</td>\n",
       "      <td>0.016838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.843034</td>\n",
       "      <td>0.021458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.846946</td>\n",
       "      <td>0.024376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.825432</td>\n",
       "      <td>0.023004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.844749</td>\n",
       "      <td>0.017877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.836698</td>\n",
       "      <td>0.016834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.836053</td>\n",
       "      <td>0.016778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.836187</td>\n",
       "      <td>0.016883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.672638</td>\n",
       "      <td>0.033740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.829864</td>\n",
       "      <td>0.024361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.836187</td>\n",
       "      <td>0.016883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.710592     0.035518\n",
       "1              Accuracy         0.836730     0.016838\n",
       "2             Precision         0.843034     0.021458\n",
       "3           Sensitivity         0.846946     0.024376\n",
       "4           Specificity         0.825432     0.023004\n",
       "5              F1 score         0.844749     0.017877\n",
       "6   F1 score (weighted)         0.836698     0.016834\n",
       "7      F1 score (macro)         0.836053     0.016778\n",
       "8     Balanced Accuracy         0.836187     0.016883\n",
       "9                   MCC         0.672638     0.033740\n",
       "10                  NPV         0.829864     0.024361\n",
       "11              ROC_AUC         0.836187     0.016883"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6394fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_svm.to_csv('mat_met_svm_opt.csv')\n",
    "svm_5preds.to_csv('svm_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.6898 with a standard deviation of 0.0256\n",
      "SVM optimized model r2_score 0.7126 with a standard deviation of 0.0196\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm.joblib']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, \"OUTPUT/svm_reg.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6e46b761-0f0c-48c9-9d17-d49420ceb22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "82af877f-1e9a-49d6-a15d-c0324d767704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "108a44a5-038b-4d83-877e-55a291431d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/PredResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    rf_5preds.to_excel(writer, sheet_name=\"RF\", )\n",
    "    lgbm_5preds.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    xgb_5preds.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    knn_5preds.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    svm_5preds.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
