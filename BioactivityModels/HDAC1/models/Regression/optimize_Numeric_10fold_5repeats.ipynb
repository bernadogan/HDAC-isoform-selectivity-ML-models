{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/anaconda3/envs/teachopencadd/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "levels_up = 3\n",
    "HDAC1= HERE.parents[levels_up-1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>n_rot</th>\n",
       "      <th>n_heavy</th>\n",
       "      <th>n_hba</th>\n",
       "      <th>n_hbd</th>\n",
       "      <th>logp</th>\n",
       "      <th>num_ar</th>\n",
       "      <th>num_sa</th>\n",
       "      <th>num_alip</th>\n",
       "      <th>pchembl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4286867</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[6411541, 10511, 137380, 1877151, 4783723, 486...</td>\n",
       "      <td>518.264154</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5083</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3689853</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4035923, 6538203, 2344420, 16829770, 4517655,...</td>\n",
       "      <td>400.247441</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3827056</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[3826379, 2478511, 2013766, 10841999, 1139606,...</td>\n",
       "      <td>611.182805</td>\n",
       "      <td>6.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.4499</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3689883</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1429681, 7687405, 6095547, 12157985, 9830328,...</td>\n",
       "      <td>386.268176</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0635</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2023528</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[371106, 825901, 1055378, 943140, 1666919, 191...</td>\n",
       "      <td>813.322122</td>\n",
       "      <td>14.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.1843</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4286867  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL3689853  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3827056  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3689883  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL2023528  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  molecular_weight  n_rot  \\\n",
       "0  [6411541, 10511, 137380, 1877151, 4783723, 486...        518.264154   12.0   \n",
       "1  [4035923, 6538203, 2344420, 16829770, 4517655,...        400.247441    7.0   \n",
       "2  [3826379, 2478511, 2013766, 10841999, 1139606,...        611.182805    6.0   \n",
       "3  [1429681, 7687405, 6095547, 12157985, 9830328,...        386.268176    9.0   \n",
       "4  [371106, 825901, 1055378, 943140, 1666919, 191...        813.322122   14.0   \n",
       "\n",
       "   n_heavy  n_hba  n_hbd    logp  num_ar  num_sa  num_alip  pchembl  \n",
       "0     38.0    8.0    4.0  4.5083     3.0     1.0       1.0     7.25  \n",
       "1     29.0    6.0    2.0  3.0088     2.0     1.0       1.0     6.43  \n",
       "2     43.0   11.0    2.0  3.4499     4.0     1.0       1.0     7.52  \n",
       "3     28.0    5.0    2.0  4.0635     2.0     0.0       0.0     7.70  \n",
       "4     59.0   16.0    7.0  3.1843     4.0     1.0       3.0     7.27  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1/\"HDAC1_4492compounds_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>pActivity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL327146</td>\n",
       "      <td>O=C(CCCCCC(C(=O)Nc1ccc2ncccc2c1)C(=O)Nc1ccc2nc...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL116620</td>\n",
       "      <td>O=C(/C=C/c1cccc(C(C(=O)Nc2ccccc2)C(=O)Nc2ccccc...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2093007</td>\n",
       "      <td>C/C=C1\\NC(=O)[C@@H](CSC)NC(=O)[C@@H](C(C)C)CC(...</td>\n",
       "      <td>5.20</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL316457</td>\n",
       "      <td>CC(C)c1cc(C(C)C)c(S(=O)(=O)Nc2ccc(/C=C/C(=O)NO...</td>\n",
       "      <td>6.22</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL269692</td>\n",
       "      <td>O=C(NCc1ccc(C(=O)NO)cc1)OCc1cccnc1</td>\n",
       "      <td>5.52</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4649511</td>\n",
       "      <td>O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO</td>\n",
       "      <td>8.48</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL4637976</td>\n",
       "      <td>O=C(CCCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO</td>\n",
       "      <td>6.89</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL4638227</td>\n",
       "      <td>CC(=O)Nc1ccc2c(c1)CN(C(=O)[C@H](N)Cc1ccc(Cl)cc...</td>\n",
       "      <td>5.06</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL4444219</td>\n",
       "      <td>CCCNNC(=O)/C=C/c1ccc(CNCCc2c(C)[nH]c3ccccc23)cc1</td>\n",
       "      <td>8.31</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0          CHEMBL327146  O=C(CCCCCC(C(=O)Nc1ccc2ncccc2c1)C(=O)Nc1ccc2nc...   \n",
       "1          CHEMBL116620  O=C(/C=C/c1cccc(C(C(=O)Nc2ccccc2)C(=O)Nc2ccccc...   \n",
       "2         CHEMBL2093007  C/C=C1\\NC(=O)[C@@H](CSC)NC(=O)[C@@H](C(C)C)CC(...   \n",
       "3          CHEMBL316457  CC(C)c1cc(C(C)C)c(S(=O)(=O)Nc2ccc(/C=C/C(=O)NO...   \n",
       "4          CHEMBL269692                 O=C(NCc1ccc(C(=O)NO)cc1)OCc1cccnc1   \n",
       "...                 ...                                                ...   \n",
       "4487      CHEMBL4649511        O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO   \n",
       "4488      CHEMBL4637976       O=C(CCCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO   \n",
       "4489      CHEMBL4638227  CC(=O)Nc1ccc2c(c1)CN(C(=O)[C@H](N)Cc1ccc(Cl)cc...   \n",
       "4490      CHEMBL4444219   CCCNNC(=O)/C=C/c1ccc(CNCCc2c(C)[nH]c3ccccc23)cc1   \n",
       "4491      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "\n",
       "      pActivity          label  \n",
       "0          9.00  Single points  \n",
       "1          9.00  Single points  \n",
       "2          5.20  Single points  \n",
       "3          6.22  Single points  \n",
       "4          5.52  Single points  \n",
       "...         ...            ...  \n",
       "4487       8.48    Dual-binder  \n",
       "4488       6.89  Single points  \n",
       "4489       5.06  Single points  \n",
       "4490       8.31  Single points  \n",
       "4491       7.55    Dual-binder  \n",
       "\n",
       "[4492 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1/\"HDAC1_4492compounds_withTypes-Ki_newThreshold.csv\", index_col=0)\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>n_rot</th>\n",
       "      <th>n_heavy</th>\n",
       "      <th>n_hba</th>\n",
       "      <th>n_hbd</th>\n",
       "      <th>logp</th>\n",
       "      <th>num_ar</th>\n",
       "      <th>num_sa</th>\n",
       "      <th>num_alip</th>\n",
       "      <th>pchembl</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4464975</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[569161, 3913535, 6718756, 2817337, 6615052, 6...</td>\n",
       "      <td>449.114234</td>\n",
       "      <td>6.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.22660</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.72</td>\n",
       "      <td>hDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL95747</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1260438, 16948577, 1495766, 2469425, 5711363,...</td>\n",
       "      <td>285.172879</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.22830</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL4072618</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[13333824, 12708875, 4142580, 33294862, 115873...</td>\n",
       "      <td>425.177313</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.66800</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL2408692</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4681989, 4118356, 4155872, 616889, 1158735, 9...</td>\n",
       "      <td>359.126991</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.91350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.36</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4226829</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1888269, 941437, 137380, 1352495, 28861, 3075...</td>\n",
       "      <td>692.273368</td>\n",
       "      <td>12.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.49792</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.55</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                           fp_MACCS  \\\n",
       "4487      CHEMBL4464975  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4488        CHEMBL95747  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4489      CHEMBL4072618  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4490      CHEMBL2408692  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4491      CHEMBL4226829  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_Morgan3  \\\n",
       "4487  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4488  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4489  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4490  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "4491  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MorganF  \\\n",
       "4487  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4488  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4489  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4490  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4491  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                                fp_MAP4  molecular_weight  \\\n",
       "4487  [569161, 3913535, 6718756, 2817337, 6615052, 6...        449.114234   \n",
       "4488  [1260438, 16948577, 1495766, 2469425, 5711363,...        285.172879   \n",
       "4489  [13333824, 12708875, 4142580, 33294862, 115873...        425.177313   \n",
       "4490  [4681989, 4118356, 4155872, 616889, 1158735, 9...        359.126991   \n",
       "4491  [1888269, 941437, 137380, 1352495, 28861, 3075...        692.273368   \n",
       "\n",
       "      n_rot  n_heavy  n_hba  n_hbd     logp  num_ar  num_sa  num_alip  \\\n",
       "4487    6.0     32.0    6.0    2.0  3.22660     4.0     0.0       0.0   \n",
       "4488    8.0     21.0    2.0    2.0  4.22830     2.0     0.0       0.0   \n",
       "4489    9.0     30.0    5.0    2.0  3.66800     2.0     0.0       1.0   \n",
       "4490    2.0     27.0    4.0    2.0  1.91350     2.0     0.0       1.0   \n",
       "4491   12.0     46.0   10.0    4.0  5.49792     2.0     2.0       2.0   \n",
       "\n",
       "      pchembl            label  \n",
       "4487     4.72  hDAC6-selective  \n",
       "4488     7.60    Single points  \n",
       "4489     5.19       Non-binder  \n",
       "4490     6.36       Non-binder  \n",
       "4491     5.55    Single points  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>n_rot</th>\n",
       "      <th>n_heavy</th>\n",
       "      <th>n_hba</th>\n",
       "      <th>n_hbd</th>\n",
       "      <th>logp</th>\n",
       "      <th>num_ar</th>\n",
       "      <th>num_sa</th>\n",
       "      <th>num_alip</th>\n",
       "      <th>pchembl</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4286867</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[6411541, 10511, 137380, 1877151, 4783723, 486...</td>\n",
       "      <td>518.264154</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5083</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Semi-selective</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3689853</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4035923, 6538203, 2344420, 16829770, 4517655,...</td>\n",
       "      <td>400.247441</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.43</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3827056</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[3826379, 2478511, 2013766, 10841999, 1139606,...</td>\n",
       "      <td>611.182805</td>\n",
       "      <td>6.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.4499</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.52</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3689883</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1429681, 7687405, 6095547, 12157985, 9830328,...</td>\n",
       "      <td>386.268176</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0635</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4286867  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL3689853  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3827056  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3689883  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  molecular_weight  n_rot  \\\n",
       "0  [6411541, 10511, 137380, 1877151, 4783723, 486...        518.264154   12.0   \n",
       "1  [4035923, 6538203, 2344420, 16829770, 4517655,...        400.247441    7.0   \n",
       "2  [3826379, 2478511, 2013766, 10841999, 1139606,...        611.182805    6.0   \n",
       "3  [1429681, 7687405, 6095547, 12157985, 9830328,...        386.268176    9.0   \n",
       "\n",
       "   n_heavy  n_hba  n_hbd    logp  num_ar  num_sa  num_alip  pchembl  \\\n",
       "0     38.0    8.0    4.0  4.5083     3.0     1.0       1.0     7.25   \n",
       "1     29.0    6.0    2.0  3.0088     2.0     1.0       1.0     6.43   \n",
       "2     43.0   11.0    2.0  3.4499     4.0     1.0       1.0     7.52   \n",
       "3     28.0    5.0    2.0  4.0635     2.0     0.0       0.0     7.70   \n",
       "\n",
       "            label  Class  \n",
       "0  Semi-selective    5.0  \n",
       "1   Single points    0.0  \n",
       "2   Single points    0.0  \n",
       "3   Single points    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for activity\n",
    "df[\"activity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as active if pchembl value is >=6.6 0 otherwise\n",
    "df.loc[df[df.pchembl >= 6.6].index, \"activity\"] = 1.0\n",
    "\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"pchembl\"].values\n",
    "Y_cat =  df[\"activity\"].values\n",
    "Y_class = df['Class'].values\n",
    "indices =  np.array(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['pchembl'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['pchembl'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.676650     0.038483\n",
      "1                    TP       202.400000    12.624491\n",
      "2                    TN       171.400000    10.762073\n",
      "3                    FP        41.600000     9.371351\n",
      "4                    FN        33.800000     6.957011\n",
      "5              Accuracy         0.832151     0.025770\n",
      "6             Precision         0.830039     0.034339\n",
      "7           Sensitivity         0.856388     0.031545\n",
      "8           Specificity         0.804810     0.042830\n",
      "9              F1 score         0.842562     0.025859\n",
      "10  F1 score (weighted)         0.831833     0.025882\n",
      "11     F1 score (macro)         0.831053     0.025784\n",
      "12    Balanced Accuracy         0.830598     0.025722\n",
      "13                  MCC         0.663651     0.051394\n",
      "14                  NPV         0.836080     0.028270\n",
      "15              ROC_AUC         0.830598     0.025722\n",
      "CPU times: user 3min 40s, sys: 29.9 ms, total: 3min 40s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=16,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6) , 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=16, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=16, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 15:33:49,228]\u001b[0m A new study created in memory with name: RFRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:35:04,346]\u001b[0m Trial 0 finished with value: 0.6571303498036795 and parameters: {'n_estimators': 671}. Best is trial 0 with value: 0.6571303498036795.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:36:34,412]\u001b[0m Trial 1 finished with value: 0.6570109513642123 and parameters: {'n_estimators': 802}. Best is trial 0 with value: 0.6571303498036795.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:37:44,271]\u001b[0m Trial 2 finished with value: 0.6568237101063039 and parameters: {'n_estimators': 611}. Best is trial 0 with value: 0.6571303498036795.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:38:56,053]\u001b[0m Trial 3 finished with value: 0.6569091030368981 and parameters: {'n_estimators': 632}. Best is trial 0 with value: 0.6571303498036795.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:39:17,979]\u001b[0m Trial 4 finished with value: 0.657411213901814 and parameters: {'n_estimators': 167}. Best is trial 4 with value: 0.657411213901814.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:39:47,461]\u001b[0m Trial 5 finished with value: 0.6575713839520181 and parameters: {'n_estimators': 208}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:41:54,039]\u001b[0m Trial 6 finished with value: 0.656920609755064 and parameters: {'n_estimators': 931}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:44:02,550]\u001b[0m Trial 7 finished with value: 0.6569572803506398 and parameters: {'n_estimators': 945}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:44:26,043]\u001b[0m Trial 8 finished with value: 0.6570617711563973 and parameters: {'n_estimators': 163}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:44:42,457]\u001b[0m Trial 9 finished with value: 0.6565821428517511 and parameters: {'n_estimators': 111}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:45:33,344]\u001b[0m Trial 10 finished with value: 0.6570999357573459 and parameters: {'n_estimators': 366}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:46:19,075]\u001b[0m Trial 11 finished with value: 0.6574383805390741 and parameters: {'n_estimators': 328}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:47:07,110]\u001b[0m Trial 12 finished with value: 0.6573331070831882 and parameters: {'n_estimators': 345}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:47:58,217]\u001b[0m Trial 13 finished with value: 0.6570363599743484 and parameters: {'n_estimators': 371}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:48:36,575]\u001b[0m Trial 14 finished with value: 0.6574338393480426 and parameters: {'n_estimators': 275}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:49:41,866]\u001b[0m Trial 15 finished with value: 0.6567336476922316 and parameters: {'n_estimators': 463}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:51:28,942]\u001b[0m Trial 16 finished with value: 0.6568906976598019 and parameters: {'n_estimators': 490}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:52:34,749]\u001b[0m Trial 17 finished with value: 0.6570003344651294 and parameters: {'n_estimators': 250}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:53:49,902]\u001b[0m Trial 18 finished with value: 0.6573572129380505 and parameters: {'n_estimators': 280}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:55:11,061]\u001b[0m Trial 19 finished with value: 0.6568941070767379 and parameters: {'n_estimators': 485}. Best is trial 5 with value: 0.6575713839520181.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:55:40,889]\u001b[0m Trial 20 finished with value: 0.657733449524031 and parameters: {'n_estimators': 209}. Best is trial 20 with value: 0.657733449524031.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:56:09,788]\u001b[0m Trial 21 finished with value: 0.6576386274364712 and parameters: {'n_estimators': 205}. Best is trial 20 with value: 0.657733449524031.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:56:37,237]\u001b[0m Trial 22 finished with value: 0.6580002553574176 and parameters: {'n_estimators': 192}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:56:51,959]\u001b[0m Trial 23 finished with value: 0.6549445419370172 and parameters: {'n_estimators': 100}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:57:49,920]\u001b[0m Trial 24 finished with value: 0.6569022547115232 and parameters: {'n_estimators': 416}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:58:20,210]\u001b[0m Trial 25 finished with value: 0.657620518027789 and parameters: {'n_estimators': 214}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:59:01,272]\u001b[0m Trial 26 finished with value: 0.6574858827110559 and parameters: {'n_estimators': 294}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:59:24,703]\u001b[0m Trial 27 finished with value: 0.6570617711563973 and parameters: {'n_estimators': 163}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:01:03,486]\u001b[0m Trial 28 finished with value: 0.6570661652631518 and parameters: {'n_estimators': 720}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:02:02,017]\u001b[0m Trial 29 finished with value: 0.6569352168074098 and parameters: {'n_estimators': 420}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:03:16,900]\u001b[0m Trial 30 finished with value: 0.6566600334287787 and parameters: {'n_estimators': 545}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:03:47,526]\u001b[0m Trial 31 finished with value: 0.6578105423866718 and parameters: {'n_estimators': 215}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:04:16,258]\u001b[0m Trial 32 finished with value: 0.6574402608206424 and parameters: {'n_estimators': 204}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:04:50,733]\u001b[0m Trial 33 finished with value: 0.6570453517670506 and parameters: {'n_estimators': 245}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:05:11,650]\u001b[0m Trial 34 finished with value: 0.6568342146003039 and parameters: {'n_estimators': 144}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:05:56,446]\u001b[0m Trial 35 finished with value: 0.6574637098520963 and parameters: {'n_estimators': 320}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:06:26,200]\u001b[0m Trial 36 finished with value: 0.6576524567110625 and parameters: {'n_estimators': 211}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:06:41,098]\u001b[0m Trial 37 finished with value: 0.6549445419370172 and parameters: {'n_estimators': 100}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:07:58,870]\u001b[0m Trial 38 finished with value: 0.6568075408858466 and parameters: {'n_estimators': 575}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:09:42,762]\u001b[0m Trial 39 finished with value: 0.6570776531483217 and parameters: {'n_estimators': 762}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:11:44,339]\u001b[0m Trial 40 finished with value: 0.6570878584730213 and parameters: {'n_estimators': 859}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:12:27,565]\u001b[0m Trial 41 finished with value: 0.6575601243458726 and parameters: {'n_estimators': 207}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:13:15,488]\u001b[0m Trial 42 finished with value: 0.6578760996123005 and parameters: {'n_estimators': 183}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:13:55,092]\u001b[0m Trial 43 finished with value: 0.6570782058319393 and parameters: {'n_estimators': 142}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:14:59,054]\u001b[0m Trial 44 finished with value: 0.6572478906173136 and parameters: {'n_estimators': 233}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:15:47,516]\u001b[0m Trial 45 finished with value: 0.6575884641329182 and parameters: {'n_estimators': 175}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 16:16:46,753]\u001b[0m Trial 46 finished with value: 0.657435414994503 and parameters: {'n_estimators': 295}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:19:03,605]\u001b[0m Trial 47 finished with value: 0.6568498187546927 and parameters: {'n_estimators': 997}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:19:59,105]\u001b[0m Trial 48 finished with value: 0.6568520302345879 and parameters: {'n_estimators': 400}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:20:19,302]\u001b[0m Trial 49 finished with value: 0.6568237827620792 and parameters: {'n_estimators': 140}. Best is trial 22 with value: 0.6580002553574176.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6580\n",
      "\tBest params:\n",
      "\t\tn_estimators: 192\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.653416\n",
      "1                    TP  396.000000\n",
      "2                    TN  356.000000\n",
      "3                    FP   88.000000\n",
      "4                    FN   59.000000\n",
      "5              Accuracy    0.836485\n",
      "6             Precision    0.818182\n",
      "7           Sensitivity    0.870330\n",
      "8           Specificity    0.801800\n",
      "9              F1 score    0.843450\n",
      "10  F1 score (weighted)    0.836250\n",
      "11     F1 score (macro)    0.836161\n",
      "12    Balanced Accuracy    0.836066\n",
      "13                  MCC    0.674070\n",
      "14                  NPV    0.857800\n",
      "15              ROC_AUC    0.836066\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_0_cat = np.where((y_pred_rf_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 16:20:59,361]\u001b[0m Trial 50 finished with value: 0.664653163396508 and parameters: {'n_estimators': 266}. Best is trial 50 with value: 0.664653163396508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:21:37,309]\u001b[0m Trial 51 finished with value: 0.6645507335747272 and parameters: {'n_estimators': 274}. Best is trial 50 with value: 0.664653163396508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:22:13,977]\u001b[0m Trial 52 finished with value: 0.6646261242437764 and parameters: {'n_estimators': 265}. Best is trial 50 with value: 0.664653163396508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:23:01,309]\u001b[0m Trial 53 finished with value: 0.6655465176109802 and parameters: {'n_estimators': 345}. Best is trial 53 with value: 0.6655465176109802.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:23:50,271]\u001b[0m Trial 54 finished with value: 0.6657349594183993 and parameters: {'n_estimators': 355}. Best is trial 54 with value: 0.6657349594183993.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:24:38,555]\u001b[0m Trial 55 finished with value: 0.6656540181949127 and parameters: {'n_estimators': 351}. Best is trial 54 with value: 0.6657349594183993.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:25:28,804]\u001b[0m Trial 56 finished with value: 0.6659724028693162 and parameters: {'n_estimators': 369}. Best is trial 56 with value: 0.6659724028693162.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:26:17,941]\u001b[0m Trial 57 finished with value: 0.6658658506201504 and parameters: {'n_estimators': 360}. Best is trial 56 with value: 0.6659724028693162.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:27:09,120]\u001b[0m Trial 58 finished with value: 0.6659512409974304 and parameters: {'n_estimators': 373}. Best is trial 56 with value: 0.6659724028693162.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:27:59,759]\u001b[0m Trial 59 finished with value: 0.6659769214004914 and parameters: {'n_estimators': 370}. Best is trial 59 with value: 0.6659769214004914.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:29:01,014]\u001b[0m Trial 60 finished with value: 0.6662894341172731 and parameters: {'n_estimators': 450}. Best is trial 60 with value: 0.6662894341172731.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:30:01,908]\u001b[0m Trial 61 finished with value: 0.6662679878703537 and parameters: {'n_estimators': 449}. Best is trial 60 with value: 0.6662894341172731.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:31:01,803]\u001b[0m Trial 62 finished with value: 0.6662989427426597 and parameters: {'n_estimators': 438}. Best is trial 62 with value: 0.6662989427426597.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:32:01,925]\u001b[0m Trial 63 finished with value: 0.6662102552124123 and parameters: {'n_estimators': 444}. Best is trial 62 with value: 0.6662989427426597.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:33:03,070]\u001b[0m Trial 64 finished with value: 0.6662682515484157 and parameters: {'n_estimators': 448}. Best is trial 62 with value: 0.6662989427426597.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:34:31,062]\u001b[0m Trial 65 finished with value: 0.6662018968277648 and parameters: {'n_estimators': 442}. Best is trial 62 with value: 0.6662989427426597.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:36:27,492]\u001b[0m Trial 66 finished with value: 0.6663554253745585 and parameters: {'n_estimators': 452}. Best is trial 66 with value: 0.6663554253745585.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:38:22,579]\u001b[0m Trial 67 finished with value: 0.6666924060683058 and parameters: {'n_estimators': 525}. Best is trial 67 with value: 0.6666924060683058.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:39:34,012]\u001b[0m Trial 68 finished with value: 0.6667342094570942 and parameters: {'n_estimators': 521}. Best is trial 68 with value: 0.6667342094570942.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:40:44,595]\u001b[0m Trial 69 finished with value: 0.6667075547371041 and parameters: {'n_estimators': 523}. Best is trial 68 with value: 0.6667342094570942.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:41:54,224]\u001b[0m Trial 70 finished with value: 0.6667669745615419 and parameters: {'n_estimators': 517}. Best is trial 70 with value: 0.6667669745615419.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:43:04,981]\u001b[0m Trial 71 finished with value: 0.6667117670642593 and parameters: {'n_estimators': 520}. Best is trial 70 with value: 0.6667669745615419.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:44:15,247]\u001b[0m Trial 72 finished with value: 0.6667904283583896 and parameters: {'n_estimators': 519}. Best is trial 72 with value: 0.6667904283583896.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:45:25,396]\u001b[0m Trial 73 finished with value: 0.6667488314129774 and parameters: {'n_estimators': 514}. Best is trial 72 with value: 0.6667904283583896.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:46:36,714]\u001b[0m Trial 74 finished with value: 0.6666924060683058 and parameters: {'n_estimators': 525}. Best is trial 72 with value: 0.6667904283583896.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:47:48,313]\u001b[0m Trial 75 finished with value: 0.6667577270519531 and parameters: {'n_estimators': 526}. Best is trial 72 with value: 0.6667904283583896.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:49:14,905]\u001b[0m Trial 76 finished with value: 0.6669527523399841 and parameters: {'n_estimators': 637}. Best is trial 76 with value: 0.6669527523399841.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:50:40,170]\u001b[0m Trial 77 finished with value: 0.6669533315247977 and parameters: {'n_estimators': 631}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:52:07,387]\u001b[0m Trial 78 finished with value: 0.66686297171749 and parameters: {'n_estimators': 647}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:53:34,848]\u001b[0m Trial 79 finished with value: 0.6668754930159808 and parameters: {'n_estimators': 644}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:55:06,553]\u001b[0m Trial 80 finished with value: 0.6668939924640298 and parameters: {'n_estimators': 640}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:57:44,550]\u001b[0m Trial 81 finished with value: 0.6668206114207934 and parameters: {'n_estimators': 649}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:00:06,908]\u001b[0m Trial 82 finished with value: 0.6668517388970361 and parameters: {'n_estimators': 662}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:01:34,584]\u001b[0m Trial 83 finished with value: 0.66686297171749 and parameters: {'n_estimators': 647}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:03:02,903]\u001b[0m Trial 84 finished with value: 0.6668447076292028 and parameters: {'n_estimators': 648}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:04:31,578]\u001b[0m Trial 85 finished with value: 0.666799512916159 and parameters: {'n_estimators': 658}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:05:55,187]\u001b[0m Trial 86 finished with value: 0.666850967698336 and parameters: {'n_estimators': 619}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:07:18,041]\u001b[0m Trial 87 finished with value: 0.6669132769737578 and parameters: {'n_estimators': 611}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:08:40,014]\u001b[0m Trial 88 finished with value: 0.6668740347509476 and parameters: {'n_estimators': 605}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:10:15,203]\u001b[0m Trial 89 finished with value: 0.666499143595115 and parameters: {'n_estimators': 704}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:11:34,554]\u001b[0m Trial 90 finished with value: 0.6667595601559397 and parameters: {'n_estimators': 583}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:12:58,186]\u001b[0m Trial 91 finished with value: 0.6668569450011724 and parameters: {'n_estimators': 614}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:14:18,622]\u001b[0m Trial 92 finished with value: 0.6667347183495893 and parameters: {'n_estimators': 594}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:15:53,368]\u001b[0m Trial 93 finished with value: 0.6664767110418844 and parameters: {'n_estimators': 700}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:18:06,680]\u001b[0m Trial 94 finished with value: 0.6666788413765682 and parameters: {'n_estimators': 680}. Best is trial 77 with value: 0.6669533315247977.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:20:49,971]\u001b[0m Trial 95 finished with value: 0.6669588257083872 and parameters: {'n_estimators': 628}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:22:42,086]\u001b[0m Trial 96 finished with value: 0.6665406425851031 and parameters: {'n_estimators': 761}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 17:23:58,351]\u001b[0m Trial 97 finished with value: 0.6668428321083215 and parameters: {'n_estimators': 563}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:25:23,125]\u001b[0m Trial 98 finished with value: 0.6668973966533984 and parameters: {'n_estimators': 622}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:27:04,552]\u001b[0m Trial 99 finished with value: 0.6665041277227564 and parameters: {'n_estimators': 747}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6670\n",
      "\tBest params:\n",
      "\t\tn_estimators: 628\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.653416    0.660333\n",
      "1                    TP  396.000000  409.000000\n",
      "2                    TN  356.000000  333.000000\n",
      "3                    FP   88.000000   83.000000\n",
      "4                    FN   59.000000   74.000000\n",
      "5              Accuracy    0.836485    0.825362\n",
      "6             Precision    0.818182    0.831301\n",
      "7           Sensitivity    0.870330    0.846791\n",
      "8           Specificity    0.801800    0.800500\n",
      "9              F1 score    0.843450    0.838974\n",
      "10  F1 score (weighted)    0.836250    0.825213\n",
      "11     F1 score (macro)    0.836161    0.824104\n",
      "12    Balanced Accuracy    0.836066    0.823636\n",
      "13                  MCC    0.674070    0.648376\n",
      "14                  NPV    0.857800    0.818200\n",
      "15              ROC_AUC    0.836066    0.823636\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_1_cat = np.where((y_pred_rf_1 >= 6.6), 1, 0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 17:28:39,479]\u001b[0m Trial 100 finished with value: 0.6618669751017239 and parameters: {'n_estimators': 631}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:30:01,581]\u001b[0m Trial 101 finished with value: 0.6618543194964597 and parameters: {'n_estimators': 605}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:31:33,382]\u001b[0m Trial 102 finished with value: 0.6619911026466012 and parameters: {'n_estimators': 682}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:32:58,963]\u001b[0m Trial 103 finished with value: 0.6618768288750569 and parameters: {'n_estimators': 628}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:34:21,284]\u001b[0m Trial 104 finished with value: 0.6618493434263647 and parameters: {'n_estimators': 607}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:35:38,599]\u001b[0m Trial 105 finished with value: 0.6619778675542485 and parameters: {'n_estimators': 570}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:36:53,583]\u001b[0m Trial 106 finished with value: 0.66197060130185 and parameters: {'n_estimators': 552}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:38:22,014]\u001b[0m Trial 107 finished with value: 0.661965450087292 and parameters: {'n_estimators': 593}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:41:20,913]\u001b[0m Trial 108 finished with value: 0.6621871561569083 and parameters: {'n_estimators': 720}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:43:26,771]\u001b[0m Trial 109 finished with value: 0.6619213090512812 and parameters: {'n_estimators': 637}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:45:00,545]\u001b[0m Trial 110 finished with value: 0.662045240032296 and parameters: {'n_estimators': 695}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:46:23,607]\u001b[0m Trial 111 finished with value: 0.6618718670336563 and parameters: {'n_estimators': 615}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:47:54,357]\u001b[0m Trial 112 finished with value: 0.6619723436384932 and parameters: {'n_estimators': 668}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:49:21,984]\u001b[0m Trial 113 finished with value: 0.6618701355662403 and parameters: {'n_estimators': 654}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:51:01,361]\u001b[0m Trial 114 finished with value: 0.6621956302172857 and parameters: {'n_estimators': 732}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:52:47,872]\u001b[0m Trial 115 finished with value: 0.6625541214368476 and parameters: {'n_estimators': 789}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:54:14,054]\u001b[0m Trial 116 finished with value: 0.6619258338274714 and parameters: {'n_estimators': 638}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:55:44,278]\u001b[0m Trial 117 finished with value: 0.6619723436384932 and parameters: {'n_estimators': 668}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:57:04,582]\u001b[0m Trial 118 finished with value: 0.6619589229069289 and parameters: {'n_estimators': 594}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:58:36,946]\u001b[0m Trial 119 finished with value: 0.6620080461039126 and parameters: {'n_estimators': 683}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:59:53,445]\u001b[0m Trial 120 finished with value: 0.6619946068791844 and parameters: {'n_estimators': 544}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:02:21,415]\u001b[0m Trial 121 finished with value: 0.6619133788189492 and parameters: {'n_estimators': 626}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:04:50,727]\u001b[0m Trial 122 finished with value: 0.6618718670336563 and parameters: {'n_estimators': 615}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:06:11,606]\u001b[0m Trial 123 finished with value: 0.6620102659437272 and parameters: {'n_estimators': 583}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:07:42,169]\u001b[0m Trial 124 finished with value: 0.6619907257016707 and parameters: {'n_estimators': 663}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:09:05,084]\u001b[0m Trial 125 finished with value: 0.6618644331377338 and parameters: {'n_estimators': 618}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:10:42,209]\u001b[0m Trial 126 finished with value: 0.6622975441806164 and parameters: {'n_estimators': 717}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:12:08,733]\u001b[0m Trial 127 finished with value: 0.6619124466636945 and parameters: {'n_estimators': 643}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:13:25,409]\u001b[0m Trial 128 finished with value: 0.6620378705584073 and parameters: {'n_estimators': 561}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:14:45,714]\u001b[0m Trial 129 finished with value: 0.6619666609657096 and parameters: {'n_estimators': 596}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:16:18,838]\u001b[0m Trial 130 finished with value: 0.662042537715837 and parameters: {'n_estimators': 689}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:17:46,540]\u001b[0m Trial 131 finished with value: 0.6618758657899128 and parameters: {'n_estimators': 650}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:19:11,057]\u001b[0m Trial 132 finished with value: 0.6618685929766639 and parameters: {'n_estimators': 621}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:20:39,625]\u001b[0m Trial 133 finished with value: 0.6619352868387199 and parameters: {'n_estimators': 645}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:22:32,022]\u001b[0m Trial 134 finished with value: 0.6619812832987535 and parameters: {'n_estimators': 666}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:25:02,211]\u001b[0m Trial 135 finished with value: 0.6620293547403706 and parameters: {'n_estimators': 580}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:27:01,103]\u001b[0m Trial 136 finished with value: 0.6618493434263646 and parameters: {'n_estimators': 607}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:28:27,505]\u001b[0m Trial 137 finished with value: 0.6618725764774898 and parameters: {'n_estimators': 633}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:30:04,978]\u001b[0m Trial 138 finished with value: 0.6621662898492452 and parameters: {'n_estimators': 705}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:31:38,796]\u001b[0m Trial 139 finished with value: 0.662014583627957 and parameters: {'n_estimators': 674}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:33:10,383]\u001b[0m Trial 140 finished with value: 0.6618703927147358 and parameters: {'n_estimators': 657}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:34:30,951]\u001b[0m Trial 141 finished with value: 0.6619752961535817 and parameters: {'n_estimators': 572}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:35:48,405]\u001b[0m Trial 142 finished with value: 0.6620037720601274 and parameters: {'n_estimators': 545}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:37:14,410]\u001b[0m Trial 143 finished with value: 0.6618545774741655 and parameters: {'n_estimators': 608}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:38:34,291]\u001b[0m Trial 144 finished with value: 0.6620671873242854 and parameters: {'n_estimators': 559}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:40:04,951]\u001b[0m Trial 145 finished with value: 0.6619213090512811 and parameters: {'n_estimators': 637}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 18:41:28,978]\u001b[0m Trial 146 finished with value: 0.6619869366373615 and parameters: {'n_estimators': 590}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:42:57,776]\u001b[0m Trial 147 finished with value: 0.6618816957862379 and parameters: {'n_estimators': 622}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:44:41,132]\u001b[0m Trial 148 finished with value: 0.6619218438290575 and parameters: {'n_estimators': 492}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:47:31,514]\u001b[0m Trial 149 finished with value: 0.6618758657899126 and parameters: {'n_estimators': 650}. Best is trial 95 with value: 0.6669588257083872.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6670\n",
      "\tBest params:\n",
      "\t\tn_estimators: 628\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.653416    0.660333    0.677432\n",
      "1                    TP  396.000000  409.000000  417.000000\n",
      "2                    TN  356.000000  333.000000  330.000000\n",
      "3                    FP   88.000000   83.000000   93.000000\n",
      "4                    FN   59.000000   74.000000   59.000000\n",
      "5              Accuracy    0.836485    0.825362    0.830923\n",
      "6             Precision    0.818182    0.831301    0.817647\n",
      "7           Sensitivity    0.870330    0.846791    0.876050\n",
      "8           Specificity    0.801800    0.800500    0.780100\n",
      "9              F1 score    0.843450    0.838974    0.845842\n",
      "10  F1 score (weighted)    0.836250    0.825213    0.830299\n",
      "11     F1 score (macro)    0.836161    0.824104    0.829325\n",
      "12    Balanced Accuracy    0.836066    0.823636    0.828096\n",
      "13                  MCC    0.674070    0.648376    0.661066\n",
      "14                  NPV    0.857800    0.818200    0.848300\n",
      "15              ROC_AUC    0.836066    0.823636    0.828096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_2_cat = np.where((y_pred_rf_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 18:49:32,794]\u001b[0m Trial 150 finished with value: 0.669695415816513 and parameters: {'n_estimators': 680}. Best is trial 150 with value: 0.669695415816513.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:51:08,138]\u001b[0m Trial 151 finished with value: 0.6697349065227234 and parameters: {'n_estimators': 678}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:52:44,451]\u001b[0m Trial 152 finished with value: 0.6696864252214587 and parameters: {'n_estimators': 689}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:54:28,072]\u001b[0m Trial 153 finished with value: 0.6697266633888361 and parameters: {'n_estimators': 733}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:56:11,503]\u001b[0m Trial 154 finished with value: 0.6696696629561133 and parameters: {'n_estimators': 739}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:57:54,298]\u001b[0m Trial 155 finished with value: 0.6696818122383852 and parameters: {'n_estimators': 736}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:59:45,873]\u001b[0m Trial 156 finished with value: 0.6695312083064183 and parameters: {'n_estimators': 796}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:01:47,017]\u001b[0m Trial 157 finished with value: 0.6695219944064061 and parameters: {'n_estimators': 869}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:03:41,807]\u001b[0m Trial 158 finished with value: 0.669487215566783 and parameters: {'n_estimators': 823}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:06:16,370]\u001b[0m Trial 159 finished with value: 0.6694523430113166 and parameters: {'n_estimators': 879}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:09:53,600]\u001b[0m Trial 160 finished with value: 0.6694873076952664 and parameters: {'n_estimators': 873}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:11:58,119]\u001b[0m Trial 161 finished with value: 0.6694523430113166 and parameters: {'n_estimators': 879}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:13:58,789]\u001b[0m Trial 162 finished with value: 0.6695615225362707 and parameters: {'n_estimators': 864}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:16:00,218]\u001b[0m Trial 163 finished with value: 0.6694953651649482 and parameters: {'n_estimators': 871}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:18:01,096]\u001b[0m Trial 164 finished with value: 0.6695124407081076 and parameters: {'n_estimators': 867}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:20:06,196]\u001b[0m Trial 165 finished with value: 0.6694247100976919 and parameters: {'n_estimators': 894}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:22:08,929]\u001b[0m Trial 166 finished with value: 0.6694953651649482 and parameters: {'n_estimators': 871}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:24:10,073]\u001b[0m Trial 167 finished with value: 0.6695118930926601 and parameters: {'n_estimators': 870}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:26:07,912]\u001b[0m Trial 168 finished with value: 0.66951096553265 and parameters: {'n_estimators': 847}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:29:11,513]\u001b[0m Trial 169 finished with value: 0.6695596521142532 and parameters: {'n_estimators': 834}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:32:08,697]\u001b[0m Trial 170 finished with value: 0.669487215566783 and parameters: {'n_estimators': 823}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:34:06,814]\u001b[0m Trial 171 finished with value: 0.6695212434075059 and parameters: {'n_estimators': 836}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:36:03,326]\u001b[0m Trial 172 finished with value: 0.6694917646244402 and parameters: {'n_estimators': 840}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:38:02,307]\u001b[0m Trial 173 finished with value: 0.66951096553265 and parameters: {'n_estimators': 847}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:39:59,806]\u001b[0m Trial 174 finished with value: 0.6694978120360366 and parameters: {'n_estimators': 846}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:42:09,047]\u001b[0m Trial 175 finished with value: 0.6694327355058687 and parameters: {'n_estimators': 922}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:44:01,128]\u001b[0m Trial 176 finished with value: 0.6695634418185887 and parameters: {'n_estimators': 800}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:45:51,741]\u001b[0m Trial 177 finished with value: 0.6695779845356225 and parameters: {'n_estimators': 790}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:47:41,236]\u001b[0m Trial 178 finished with value: 0.6695703980505556 and parameters: {'n_estimators': 785}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:49:31,825]\u001b[0m Trial 179 finished with value: 0.6695749883648652 and parameters: {'n_estimators': 788}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:51:21,251]\u001b[0m Trial 180 finished with value: 0.6696489719714332 and parameters: {'n_estimators': 780}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:53:11,952]\u001b[0m Trial 181 finished with value: 0.6695969323086421 and parameters: {'n_estimators': 783}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:55:02,531]\u001b[0m Trial 182 finished with value: 0.6695746538337153 and parameters: {'n_estimators': 789}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:56:53,781]\u001b[0m Trial 183 finished with value: 0.6695628973783798 and parameters: {'n_estimators': 792}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:58:44,037]\u001b[0m Trial 184 finished with value: 0.6696239090033718 and parameters: {'n_estimators': 781}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:00:33,661]\u001b[0m Trial 185 finished with value: 0.6695679706773803 and parameters: {'n_estimators': 787}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:02:23,684]\u001b[0m Trial 186 finished with value: 0.6695779845356223 and parameters: {'n_estimators': 790}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:04:12,549]\u001b[0m Trial 187 finished with value: 0.6696090344184623 and parameters: {'n_estimators': 778}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:06:01,204]\u001b[0m Trial 188 finished with value: 0.6695972354693238 and parameters: {'n_estimators': 774}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:07:50,378]\u001b[0m Trial 189 finished with value: 0.669585757859444 and parameters: {'n_estimators': 777}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:09:38,332]\u001b[0m Trial 190 finished with value: 0.6696229916515952 and parameters: {'n_estimators': 771}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:11:27,076]\u001b[0m Trial 191 finished with value: 0.6695658839005636 and parameters: {'n_estimators': 775}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:13:15,185]\u001b[0m Trial 192 finished with value: 0.6695881411361461 and parameters: {'n_estimators': 773}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:14:59,817]\u001b[0m Trial 193 finished with value: 0.6695822481013181 and parameters: {'n_estimators': 747}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:16:44,968]\u001b[0m Trial 194 finished with value: 0.6695328657270057 and parameters: {'n_estimators': 750}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:18:31,872]\u001b[0m Trial 195 finished with value: 0.6695823009675307 and parameters: {'n_estimators': 768}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 20:20:18,687]\u001b[0m Trial 196 finished with value: 0.6694681837118097 and parameters: {'n_estimators': 764}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:22:03,222]\u001b[0m Trial 197 finished with value: 0.6695765493400405 and parameters: {'n_estimators': 748}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:23:46,527]\u001b[0m Trial 198 finished with value: 0.6697006406461188 and parameters: {'n_estimators': 738}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:25:30,234]\u001b[0m Trial 199 finished with value: 0.6697006406461189 and parameters: {'n_estimators': 738}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6697\n",
      "\tBest params:\n",
      "\t\tn_estimators: 678\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.653416    0.660333    0.677432    0.668564\n",
      "1                    TP  396.000000  409.000000  417.000000  405.000000\n",
      "2                    TN  356.000000  333.000000  330.000000  338.000000\n",
      "3                    FP   88.000000   83.000000   93.000000   88.000000\n",
      "4                    FN   59.000000   74.000000   59.000000   68.000000\n",
      "5              Accuracy    0.836485    0.825362    0.830923    0.826474\n",
      "6             Precision    0.818182    0.831301    0.817647    0.821501\n",
      "7           Sensitivity    0.870330    0.846791    0.876050    0.856237\n",
      "8           Specificity    0.801800    0.800500    0.780100    0.793400\n",
      "9              F1 score    0.843450    0.838974    0.845842    0.838509\n",
      "10  F1 score (weighted)    0.836250    0.825213    0.830299    0.826185\n",
      "11     F1 score (macro)    0.836161    0.824104    0.829325    0.825505\n",
      "12    Balanced Accuracy    0.836066    0.823636    0.828096    0.824832\n",
      "13                  MCC    0.674070    0.648376    0.661066    0.651835\n",
      "14                  NPV    0.857800    0.818200    0.848300    0.832500\n",
      "15              ROC_AUC    0.836066    0.823636    0.828096    0.824832\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_3_cat = np.where((y_pred_rf_3 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 20:27:28,117]\u001b[0m Trial 200 finished with value: 0.6665694200565467 and parameters: {'n_estimators': 737}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:29:18,994]\u001b[0m Trial 201 finished with value: 0.6664420135327204 and parameters: {'n_estimators': 760}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:31:04,769]\u001b[0m Trial 202 finished with value: 0.6665779750612192 and parameters: {'n_estimators': 733}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:32:55,590]\u001b[0m Trial 203 finished with value: 0.6664109711579955 and parameters: {'n_estimators': 766}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:34:53,059]\u001b[0m Trial 204 finished with value: 0.6664207635146703 and parameters: {'n_estimators': 812}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:36:41,813]\u001b[0m Trial 205 finished with value: 0.6665666297351436 and parameters: {'n_estimators': 746}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:38:26,447]\u001b[0m Trial 206 finished with value: 0.6666349752636902 and parameters: {'n_estimators': 724}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:40:15,321]\u001b[0m Trial 207 finished with value: 0.6664878988790418 and parameters: {'n_estimators': 749}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:42:06,100]\u001b[0m Trial 208 finished with value: 0.6664037049876336 and parameters: {'n_estimators': 770}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:44:03,697]\u001b[0m Trial 209 finished with value: 0.6664140029626031 and parameters: {'n_estimators': 811}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:45:47,869]\u001b[0m Trial 210 finished with value: 0.6667014220240131 and parameters: {'n_estimators': 718}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:47:40,737]\u001b[0m Trial 211 finished with value: 0.6664039408902498 and parameters: {'n_estimators': 775}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:49:30,998]\u001b[0m Trial 212 finished with value: 0.6665089847698116 and parameters: {'n_estimators': 751}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:51:24,239]\u001b[0m Trial 213 finished with value: 0.6663352382830229 and parameters: {'n_estimators': 779}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:53:20,969]\u001b[0m Trial 214 finished with value: 0.6664429148882075 and parameters: {'n_estimators': 808}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:55:08,540]\u001b[0m Trial 215 finished with value: 0.6665072464399555 and parameters: {'n_estimators': 740}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:56:52,378]\u001b[0m Trial 216 finished with value: 0.6666844716501028 and parameters: {'n_estimators': 715}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:58:42,769]\u001b[0m Trial 217 finished with value: 0.6664445361687201 and parameters: {'n_estimators': 767}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:00:27,290]\u001b[0m Trial 218 finished with value: 0.6666414550637167 and parameters: {'n_estimators': 726}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:02:17,055]\u001b[0m Trial 219 finished with value: 0.6664484006140305 and parameters: {'n_estimators': 757}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:04:14,140]\u001b[0m Trial 220 finished with value: 0.6664170700835317 and parameters: {'n_estimators': 805}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:06:08,328]\u001b[0m Trial 221 finished with value: 0.6663771073554993 and parameters: {'n_estimators': 785}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:08:00,278]\u001b[0m Trial 222 finished with value: 0.6663711851722288 and parameters: {'n_estimators': 777}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:09:46,916]\u001b[0m Trial 223 finished with value: 0.6665666299497699 and parameters: {'n_estimators': 738}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:11:42,331]\u001b[0m Trial 224 finished with value: 0.6663967958707424 and parameters: {'n_estimators': 795}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:13:23,831]\u001b[0m Trial 225 finished with value: 0.6667765316487626 and parameters: {'n_estimators': 699}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:15:14,160]\u001b[0m Trial 226 finished with value: 0.6664561478831852 and parameters: {'n_estimators': 763}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:17:11,897]\u001b[0m Trial 227 finished with value: 0.666489344231122 and parameters: {'n_estimators': 817}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:19:00,688]\u001b[0m Trial 228 finished with value: 0.6664878988790418 and parameters: {'n_estimators': 749}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:20:54,674]\u001b[0m Trial 229 finished with value: 0.6664103520059365 and parameters: {'n_estimators': 789}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:22:47,495]\u001b[0m Trial 230 finished with value: 0.6664144044780833 and parameters: {'n_estimators': 774}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:24:40,725]\u001b[0m Trial 231 finished with value: 0.6663711851722287 and parameters: {'n_estimators': 777}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:26:36,563]\u001b[0m Trial 232 finished with value: 0.6663745958399587 and parameters: {'n_estimators': 800}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:28:23,112]\u001b[0m Trial 233 finished with value: 0.6665613035009523 and parameters: {'n_estimators': 736}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:30:17,019]\u001b[0m Trial 234 finished with value: 0.6663628361043713 and parameters: {'n_estimators': 782}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:32:06,364]\u001b[0m Trial 235 finished with value: 0.6665033506005227 and parameters: {'n_estimators': 755}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:34:01,188]\u001b[0m Trial 236 finished with value: 0.6664544001435732 and parameters: {'n_estimators': 790}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:35:46,867]\u001b[0m Trial 237 finished with value: 0.6665698491962503 and parameters: {'n_estimators': 729}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:37:29,008]\u001b[0m Trial 238 finished with value: 0.6668140716359463 and parameters: {'n_estimators': 702}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:39:19,294]\u001b[0m Trial 239 finished with value: 0.6664561478831852 and parameters: {'n_estimators': 763}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:41:16,238]\u001b[0m Trial 240 finished with value: 0.6664429148882075 and parameters: {'n_estimators': 808}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:43:10,306]\u001b[0m Trial 241 finished with value: 0.6663924178332908 and parameters: {'n_estimators': 788}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:44:59,421]\u001b[0m Trial 242 finished with value: 0.6665076411560875 and parameters: {'n_estimators': 754}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:46:51,760]\u001b[0m Trial 243 finished with value: 0.6663352382830229 and parameters: {'n_estimators': 779}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:48:51,749]\u001b[0m Trial 244 finished with value: 0.6665211200017671 and parameters: {'n_estimators': 825}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:50:46,504]\u001b[0m Trial 245 finished with value: 0.666406697067162 and parameters: {'n_estimators': 793}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 21:52:30,968]\u001b[0m Trial 246 finished with value: 0.6667033591727333 and parameters: {'n_estimators': 717}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:54:19,075]\u001b[0m Trial 247 finished with value: 0.6665501035462447 and parameters: {'n_estimators': 744}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:56:10,418]\u001b[0m Trial 248 finished with value: 0.6664329406214285 and parameters: {'n_estimators': 771}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:58:07,352]\u001b[0m Trial 249 finished with value: 0.6664154056671958 and parameters: {'n_estimators': 807}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6697\n",
      "\tBest params:\n",
      "\t\tn_estimators: 678\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.653416    0.660333    0.677432    0.668564   \n",
      "1                    TP  396.000000  409.000000  417.000000  405.000000   \n",
      "2                    TN  356.000000  333.000000  330.000000  338.000000   \n",
      "3                    FP   88.000000   83.000000   93.000000   88.000000   \n",
      "4                    FN   59.000000   74.000000   59.000000   68.000000   \n",
      "5              Accuracy    0.836485    0.825362    0.830923    0.826474   \n",
      "6             Precision    0.818182    0.831301    0.817647    0.821501   \n",
      "7           Sensitivity    0.870330    0.846791    0.876050    0.856237   \n",
      "8           Specificity    0.801800    0.800500    0.780100    0.793400   \n",
      "9              F1 score    0.843450    0.838974    0.845842    0.838509   \n",
      "10  F1 score (weighted)    0.836250    0.825213    0.830299    0.826185   \n",
      "11     F1 score (macro)    0.836161    0.824104    0.829325    0.825505   \n",
      "12    Balanced Accuracy    0.836066    0.823636    0.828096    0.824832   \n",
      "13                  MCC    0.674070    0.648376    0.661066    0.651835   \n",
      "14                  NPV    0.857800    0.818200    0.848300    0.832500   \n",
      "15              ROC_AUC    0.836066    0.823636    0.828096    0.824832   \n",
      "\n",
      "          Set4  \n",
      "0     0.646037  \n",
      "1   400.000000  \n",
      "2   348.000000  \n",
      "3    82.000000  \n",
      "4    69.000000  \n",
      "5     0.832036  \n",
      "6     0.829876  \n",
      "7     0.852878  \n",
      "8     0.809300  \n",
      "9     0.841220  \n",
      "10    0.831895  \n",
      "11    0.831472  \n",
      "12    0.831090  \n",
      "13    0.663293  \n",
      "14    0.834500  \n",
      "15    0.831090  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_4_cat = np.where((y_pred_rf_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 22:00:07,277]\u001b[0m Trial 250 finished with value: 0.6602604320274646 and parameters: {'n_estimators': 761}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:01:59,614]\u001b[0m Trial 251 finished with value: 0.6602221878255696 and parameters: {'n_estimators': 788}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:03:45,394]\u001b[0m Trial 252 finished with value: 0.6602645228888255 and parameters: {'n_estimators': 733}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:05:33,785]\u001b[0m Trial 253 finished with value: 0.6602713411369847 and parameters: {'n_estimators': 747}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:07:15,013]\u001b[0m Trial 254 finished with value: 0.6602184817510445 and parameters: {'n_estimators': 710}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:09:04,713]\u001b[0m Trial 255 finished with value: 0.6602580257327804 and parameters: {'n_estimators': 771}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:10:59,200]\u001b[0m Trial 256 finished with value: 0.660218454717193 and parameters: {'n_estimators': 803}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:12:56,945]\u001b[0m Trial 257 finished with value: 0.6601949594392181 and parameters: {'n_estimators': 824}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:14:45,088]\u001b[0m Trial 258 finished with value: 0.6602888518373252 and parameters: {'n_estimators': 760}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:16:36,633]\u001b[0m Trial 259 finished with value: 0.6601944261977805 and parameters: {'n_estimators': 783}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:18:21,591]\u001b[0m Trial 260 finished with value: 0.6602960082542897 and parameters: {'n_estimators': 735}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:20:00,098]\u001b[0m Trial 261 finished with value: 0.6600865371617437 and parameters: {'n_estimators': 684}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:21:53,920]\u001b[0m Trial 262 finished with value: 0.66018446224185 and parameters: {'n_estimators': 798}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:23:42,615]\u001b[0m Trial 263 finished with value: 0.6603089377881467 and parameters: {'n_estimators': 768}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:25:28,992]\u001b[0m Trial 264 finished with value: 0.6603089135175175 and parameters: {'n_estimators': 751}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:27:12,461]\u001b[0m Trial 265 finished with value: 0.660255049957297 and parameters: {'n_estimators': 724}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:29:03,931]\u001b[0m Trial 266 finished with value: 0.6601944261977803 and parameters: {'n_estimators': 783}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:31:00,510]\u001b[0m Trial 267 finished with value: 0.6602071027917041 and parameters: {'n_estimators': 818}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:32:49,568]\u001b[0m Trial 268 finished with value: 0.6602439856585539 and parameters: {'n_estimators': 759}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:34:29,968]\u001b[0m Trial 269 finished with value: 0.6600818649401161 and parameters: {'n_estimators': 703}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:36:24,752]\u001b[0m Trial 270 finished with value: 0.660197099574074 and parameters: {'n_estimators': 794}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:38:09,444]\u001b[0m Trial 271 finished with value: 0.6602960082542897 and parameters: {'n_estimators': 735}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:39:59,747]\u001b[0m Trial 272 finished with value: 0.6602231181337552 and parameters: {'n_estimators': 774}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:41:46,214]\u001b[0m Trial 273 finished with value: 0.6602753419489586 and parameters: {'n_estimators': 745}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:43:41,495]\u001b[0m Trial 274 finished with value: 0.6602143745649448 and parameters: {'n_estimators': 812}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:45:23,753]\u001b[0m Trial 275 finished with value: 0.6601890154818818 and parameters: {'n_estimators': 715}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:47:14,636]\u001b[0m Trial 276 finished with value: 0.6601912915357722 and parameters: {'n_estimators': 780}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:49:02,359]\u001b[0m Trial 277 finished with value: 0.6602521370623902 and parameters: {'n_estimators': 758}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:50:55,823]\u001b[0m Trial 278 finished with value: 0.6602030874650775 and parameters: {'n_estimators': 795}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:52:44,687]\u001b[0m Trial 279 finished with value: 0.6603089377881466 and parameters: {'n_estimators': 768}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:54:23,389]\u001b[0m Trial 280 finished with value: 0.6600772005381959 and parameters: {'n_estimators': 693}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:56:09,203]\u001b[0m Trial 281 finished with value: 0.6602779165745025 and parameters: {'n_estimators': 739}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:58:03,741]\u001b[0m Trial 282 finished with value: 0.660209907726687 and parameters: {'n_estimators': 804}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:59:55,566]\u001b[0m Trial 283 finished with value: 0.6601944261977805 and parameters: {'n_estimators': 783}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:01:39,248]\u001b[0m Trial 284 finished with value: 0.6602851320580667 and parameters: {'n_estimators': 725}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:03:26,086]\u001b[0m Trial 285 finished with value: 0.6603164146184216 and parameters: {'n_estimators': 752}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:05:16,202]\u001b[0m Trial 286 finished with value: 0.6602231181337552 and parameters: {'n_estimators': 774}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:07:02,895]\u001b[0m Trial 287 finished with value: 0.6602625181008219 and parameters: {'n_estimators': 749}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:09:01,306]\u001b[0m Trial 288 finished with value: 0.6602444945453116 and parameters: {'n_estimators': 828}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:10:55,490]\u001b[0m Trial 289 finished with value: 0.6602099183384948 and parameters: {'n_estimators': 800}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:12:39,708]\u001b[0m Trial 290 finished with value: 0.6602259293917478 and parameters: {'n_estimators': 730}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:14:29,671]\u001b[0m Trial 291 finished with value: 0.6602580257327804 and parameters: {'n_estimators': 771}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:16:21,038]\u001b[0m Trial 292 finished with value: 0.6602010222615078 and parameters: {'n_estimators': 787}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:18:17,192]\u001b[0m Trial 293 finished with value: 0.6602018040527925 and parameters: {'n_estimators': 816}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:19:58,639]\u001b[0m Trial 294 finished with value: 0.6601795639054763 and parameters: {'n_estimators': 711}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:21:47,131]\u001b[0m Trial 295 finished with value: 0.6602880120924346 and parameters: {'n_estimators': 757}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 23:23:40,252]\u001b[0m Trial 296 finished with value: 0.6601933457542131 and parameters: {'n_estimators': 792}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:25:17,683]\u001b[0m Trial 297 finished with value: 0.6600878326558065 and parameters: {'n_estimators': 681}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:27:07,660]\u001b[0m Trial 298 finished with value: 0.6603162170061933 and parameters: {'n_estimators': 765}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:28:53,424]\u001b[0m Trial 299 finished with value: 0.6602901217079259 and parameters: {'n_estimators': 740}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6697\n",
      "\tBest params:\n",
      "\t\tn_estimators: 678\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.653416    0.660333    0.677432    0.668564   \n",
      "1                    TP  396.000000  409.000000  417.000000  405.000000   \n",
      "2                    TN  356.000000  333.000000  330.000000  338.000000   \n",
      "3                    FP   88.000000   83.000000   93.000000   88.000000   \n",
      "4                    FN   59.000000   74.000000   59.000000   68.000000   \n",
      "5              Accuracy    0.836485    0.825362    0.830923    0.826474   \n",
      "6             Precision    0.818182    0.831301    0.817647    0.821501   \n",
      "7           Sensitivity    0.870330    0.846791    0.876050    0.856237   \n",
      "8           Specificity    0.801800    0.800500    0.780100    0.793400   \n",
      "9              F1 score    0.843450    0.838974    0.845842    0.838509   \n",
      "10  F1 score (weighted)    0.836250    0.825213    0.830299    0.826185   \n",
      "11     F1 score (macro)    0.836161    0.824104    0.829325    0.825505   \n",
      "12    Balanced Accuracy    0.836066    0.823636    0.828096    0.824832   \n",
      "13                  MCC    0.674070    0.648376    0.661066    0.651835   \n",
      "14                  NPV    0.857800    0.818200    0.848300    0.832500   \n",
      "15              ROC_AUC    0.836066    0.823636    0.828096    0.824832   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.646037    0.664824  \n",
      "1   400.000000  409.000000  \n",
      "2   348.000000  350.000000  \n",
      "3    82.000000   73.000000  \n",
      "4    69.000000   67.000000  \n",
      "5     0.832036    0.844271  \n",
      "6     0.829876    0.848548  \n",
      "7     0.852878    0.859244  \n",
      "8     0.809300    0.827400  \n",
      "9     0.841220    0.853862  \n",
      "10    0.831895    0.844203  \n",
      "11    0.831472    0.843598  \n",
      "12    0.831090    0.843333  \n",
      "13    0.663293    0.687271  \n",
      "14    0.834500    0.839300  \n",
      "15    0.831090    0.843333  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_5_cat = np.where((y_pred_rf_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 23:30:56,958]\u001b[0m Trial 300 finished with value: 0.6609236508897863 and parameters: {'n_estimators': 777}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:32:54,478]\u001b[0m Trial 301 finished with value: 0.6609711910670725 and parameters: {'n_estimators': 811}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:34:39,443]\u001b[0m Trial 302 finished with value: 0.6607667938100265 and parameters: {'n_estimators': 725}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:36:26,907]\u001b[0m Trial 303 finished with value: 0.6607401793206764 and parameters: {'n_estimators': 753}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:38:05,420]\u001b[0m Trial 304 finished with value: 0.6607346584338323 and parameters: {'n_estimators': 707}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:39:56,005]\u001b[0m Trial 305 finished with value: 0.6609463975600953 and parameters: {'n_estimators': 791}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:41:43,611]\u001b[0m Trial 306 finished with value: 0.6608988150870949 and parameters: {'n_estimators': 765}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:43:38,794]\u001b[0m Trial 307 finished with value: 0.6609752950325753 and parameters: {'n_estimators': 830}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:45:57,303]\u001b[0m Trial 308 finished with value: 0.6607862119540795 and parameters: {'n_estimators': 983}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:47:43,064]\u001b[0m Trial 309 finished with value: 0.6607031890021463 and parameters: {'n_estimators': 743}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:49:34,904]\u001b[0m Trial 310 finished with value: 0.660955710014201 and parameters: {'n_estimators': 780}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:51:30,745]\u001b[0m Trial 311 finished with value: 0.6608847567317002 and parameters: {'n_estimators': 802}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:53:11,212]\u001b[0m Trial 312 finished with value: 0.6607768779989163 and parameters: {'n_estimators': 691}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:54:55,930]\u001b[0m Trial 313 finished with value: 0.6607766185061876 and parameters: {'n_estimators': 723}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:56:46,758]\u001b[0m Trial 314 finished with value: 0.6608988150870949 and parameters: {'n_estimators': 765}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:58:39,623]\u001b[0m Trial 315 finished with value: 0.6609447450762207 and parameters: {'n_estimators': 785}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:00:27,222]\u001b[0m Trial 316 finished with value: 0.6606634815567289 and parameters: {'n_estimators': 745}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:02:24,664]\u001b[0m Trial 317 finished with value: 0.6609765582495751 and parameters: {'n_estimators': 810}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:04:14,660]\u001b[0m Trial 318 finished with value: 0.660876068617195 and parameters: {'n_estimators': 762}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:06:00,765]\u001b[0m Trial 319 finished with value: 0.6607107812308235 and parameters: {'n_estimators': 732}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:07:53,346]\u001b[0m Trial 320 finished with value: 0.6609002174138522 and parameters: {'n_estimators': 779}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:09:54,892]\u001b[0m Trial 321 finished with value: 0.6609792336437096 and parameters: {'n_estimators': 839}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:11:37,456]\u001b[0m Trial 322 finished with value: 0.6607281823290936 and parameters: {'n_estimators': 709}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:13:33,310]\u001b[0m Trial 323 finished with value: 0.660922130755091 and parameters: {'n_estimators': 799}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:15:21,499]\u001b[0m Trial 324 finished with value: 0.6607281814660403 and parameters: {'n_estimators': 752}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:17:13,088]\u001b[0m Trial 325 finished with value: 0.6609394160436104 and parameters: {'n_estimators': 774}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:19:09,971]\u001b[0m Trial 326 finished with value: 0.6610294909525033 and parameters: {'n_estimators': 815}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:20:56,141]\u001b[0m Trial 327 finished with value: 0.6606954085784837 and parameters: {'n_estimators': 739}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:22:49,504]\u001b[0m Trial 328 finished with value: 0.6609463975600953 and parameters: {'n_estimators': 791}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:24:38,861]\u001b[0m Trial 329 finished with value: 0.6608760686171948 and parameters: {'n_estimators': 762}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:26:22,578]\u001b[0m Trial 330 finished with value: 0.6608211167158576 and parameters: {'n_estimators': 717}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:28:00,535]\u001b[0m Trial 331 finished with value: 0.6607385178901881 and parameters: {'n_estimators': 678}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:29:48,470]\u001b[0m Trial 332 finished with value: 0.6607195453664088 and parameters: {'n_estimators': 749}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:31:40,273]\u001b[0m Trial 333 finished with value: 0.6609394160436104 and parameters: {'n_estimators': 774}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:33:35,635]\u001b[0m Trial 334 finished with value: 0.6609522647003979 and parameters: {'n_estimators': 796}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:35:21,183]\u001b[0m Trial 335 finished with value: 0.6607107812308234 and parameters: {'n_estimators': 732}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:37:19,925]\u001b[0m Trial 336 finished with value: 0.6609347455917393 and parameters: {'n_estimators': 825}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:39:01,122]\u001b[0m Trial 337 finished with value: 0.6607681288419325 and parameters: {'n_estimators': 701}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:40:50,983]\u001b[0m Trial 338 finished with value: 0.6608277774043989 and parameters: {'n_estimators': 760}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:42:44,445]\u001b[0m Trial 339 finished with value: 0.6609700524520558 and parameters: {'n_estimators': 781}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:44:39,730]\u001b[0m Trial 340 finished with value: 0.6609130364635547 and parameters: {'n_estimators': 805}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:46:27,804]\u001b[0m Trial 341 finished with value: 0.660663481556729 and parameters: {'n_estimators': 745}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:48:19,459]\u001b[0m Trial 342 finished with value: 0.6608971650338183 and parameters: {'n_estimators': 769}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:50:13,086]\u001b[0m Trial 343 finished with value: 0.6609599442858562 and parameters: {'n_estimators': 792}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:51:57,652]\u001b[0m Trial 344 finished with value: 0.6607667938100265 and parameters: {'n_estimators': 725}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:53:56,711]\u001b[0m Trial 345 finished with value: 0.6610132305273925 and parameters: {'n_estimators': 818}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 00:55:45,168]\u001b[0m Trial 346 finished with value: 0.6607167320037552 and parameters: {'n_estimators': 750}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:57:24,929]\u001b[0m Trial 347 finished with value: 0.6607768779989162 and parameters: {'n_estimators': 691}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:59:17,251]\u001b[0m Trial 348 finished with value: 0.6609557100142008 and parameters: {'n_estimators': 780}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:01:07,419]\u001b[0m Trial 349 finished with value: 0.6609283034436211 and parameters: {'n_estimators': 764}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6697\n",
      "\tBest params:\n",
      "\t\tn_estimators: 678\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.653416    0.660333    0.677432    0.668564   \n",
      "1                    TP  396.000000  409.000000  417.000000  405.000000   \n",
      "2                    TN  356.000000  333.000000  330.000000  338.000000   \n",
      "3                    FP   88.000000   83.000000   93.000000   88.000000   \n",
      "4                    FN   59.000000   74.000000   59.000000   68.000000   \n",
      "5              Accuracy    0.836485    0.825362    0.830923    0.826474   \n",
      "6             Precision    0.818182    0.831301    0.817647    0.821501   \n",
      "7           Sensitivity    0.870330    0.846791    0.876050    0.856237   \n",
      "8           Specificity    0.801800    0.800500    0.780100    0.793400   \n",
      "9              F1 score    0.843450    0.838974    0.845842    0.838509   \n",
      "10  F1 score (weighted)    0.836250    0.825213    0.830299    0.826185   \n",
      "11     F1 score (macro)    0.836161    0.824104    0.829325    0.825505   \n",
      "12    Balanced Accuracy    0.836066    0.823636    0.828096    0.824832   \n",
      "13                  MCC    0.674070    0.648376    0.661066    0.651835   \n",
      "14                  NPV    0.857800    0.818200    0.848300    0.832500   \n",
      "15              ROC_AUC    0.836066    0.823636    0.828096    0.824832   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.646037    0.664824    0.662168  \n",
      "1   400.000000  409.000000  403.000000  \n",
      "2   348.000000  350.000000  343.000000  \n",
      "3    82.000000   73.000000   84.000000  \n",
      "4    69.000000   67.000000   69.000000  \n",
      "5     0.832036    0.844271    0.829811  \n",
      "6     0.829876    0.848548    0.827515  \n",
      "7     0.852878    0.859244    0.853814  \n",
      "8     0.809300    0.827400    0.803300  \n",
      "9     0.841220    0.853862    0.840459  \n",
      "10    0.831895    0.844203    0.829621  \n",
      "11    0.831472    0.843598    0.829049  \n",
      "12    0.831090    0.843333    0.828546  \n",
      "13    0.663293    0.687271    0.658564  \n",
      "14    0.834500    0.839300    0.832500  \n",
      "15    0.831090    0.843333    0.828546  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_6_cat = np.where((y_pred_rf_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 01:03:19,836]\u001b[0m Trial 350 finished with value: 0.6689743190130997 and parameters: {'n_estimators': 845}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:05:13,781]\u001b[0m Trial 351 finished with value: 0.6690065856378283 and parameters: {'n_estimators': 800}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:06:56,888]\u001b[0m Trial 352 finished with value: 0.6691071794012439 and parameters: {'n_estimators': 719}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:08:43,145]\u001b[0m Trial 353 finished with value: 0.6691901102580069 and parameters: {'n_estimators': 737}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:10:35,046]\u001b[0m Trial 354 finished with value: 0.6690305901104217 and parameters: {'n_estimators': 782}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:12:24,213]\u001b[0m Trial 355 finished with value: 0.6691079141689844 and parameters: {'n_estimators': 764}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:14:00,381]\u001b[0m Trial 356 finished with value: 0.6692081434295876 and parameters: {'n_estimators': 670}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:15:48,263]\u001b[0m Trial 357 finished with value: 0.6691136304377194 and parameters: {'n_estimators': 753}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:17:43,451]\u001b[0m Trial 358 finished with value: 0.6690539400934159 and parameters: {'n_estimators': 805}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:19:35,574]\u001b[0m Trial 359 finished with value: 0.6690031865254875 and parameters: {'n_estimators': 789}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:21:20,611]\u001b[0m Trial 360 finished with value: 0.669150726561855 and parameters: {'n_estimators': 739}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:23:02,686]\u001b[0m Trial 361 finished with value: 0.6691506806514018 and parameters: {'n_estimators': 713}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:24:59,711]\u001b[0m Trial 362 finished with value: 0.6689587567990879 and parameters: {'n_estimators': 818}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:26:50,604]\u001b[0m Trial 363 finished with value: 0.6690700228115454 and parameters: {'n_estimators': 772}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:28:38,485]\u001b[0m Trial 364 finished with value: 0.6690923264337872 and parameters: {'n_estimators': 759}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:30:32,217]\u001b[0m Trial 365 finished with value: 0.6690406775452729 and parameters: {'n_estimators': 787}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:32:30,699]\u001b[0m Trial 366 finished with value: 0.6689489577904044 and parameters: {'n_estimators': 831}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:34:10,290]\u001b[0m Trial 367 finished with value: 0.6690845156922591 and parameters: {'n_estimators': 698}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:35:54,002]\u001b[0m Trial 368 finished with value: 0.6692078525364579 and parameters: {'n_estimators': 730}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:36:40,425]\u001b[0m Trial 369 finished with value: 0.6682497397591405 and parameters: {'n_estimators': 320}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:38:30,758]\u001b[0m Trial 370 finished with value: 0.6690546498059379 and parameters: {'n_estimators': 771}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:40:17,956]\u001b[0m Trial 371 finished with value: 0.6690916495188104 and parameters: {'n_estimators': 750}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:42:13,268]\u001b[0m Trial 372 finished with value: 0.6689860217004597 and parameters: {'n_estimators': 802}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:44:04,155]\u001b[0m Trial 373 finished with value: 0.6690435254334052 and parameters: {'n_estimators': 779}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:45:50,755]\u001b[0m Trial 374 finished with value: 0.6691319449680055 and parameters: {'n_estimators': 742}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:47:32,887]\u001b[0m Trial 375 finished with value: 0.6691218981990248 and parameters: {'n_estimators': 718}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:49:26,167]\u001b[0m Trial 376 finished with value: 0.668995504816341 and parameters: {'n_estimators': 797}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:51:15,672]\u001b[0m Trial 377 finished with value: 0.6690318417558079 and parameters: {'n_estimators': 770}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:53:02,493]\u001b[0m Trial 378 finished with value: 0.669075041008188 and parameters: {'n_estimators': 751}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:54:58,089]\u001b[0m Trial 379 finished with value: 0.6690534826551169 and parameters: {'n_estimators': 809}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:56:50,898]\u001b[0m Trial 380 finished with value: 0.6689852030008019 and parameters: {'n_estimators': 790}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:58:30,009]\u001b[0m Trial 381 finished with value: 0.6690832438649486 and parameters: {'n_estimators': 692}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:00:14,052]\u001b[0m Trial 382 finished with value: 0.6691932383011316 and parameters: {'n_estimators': 729}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:02:03,986]\u001b[0m Trial 383 finished with value: 0.6690855433214775 and parameters: {'n_estimators': 767}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:04:00,856]\u001b[0m Trial 384 finished with value: 0.6689531132599331 and parameters: {'n_estimators': 821}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:05:09,655]\u001b[0m Trial 385 finished with value: 0.6686804244763634 and parameters: {'n_estimators': 474}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:06:45,803]\u001b[0m Trial 386 finished with value: 0.6692038217280135 and parameters: {'n_estimators': 671}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:08:33,084]\u001b[0m Trial 387 finished with value: 0.6691071198899868 and parameters: {'n_estimators': 754}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:09:33,206]\u001b[0m Trial 388 finished with value: 0.6681863980509097 and parameters: {'n_estimators': 417}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:11:25,257]\u001b[0m Trial 389 finished with value: 0.6690397636747872 and parameters: {'n_estimators': 784}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:13:10,690]\u001b[0m Trial 390 finished with value: 0.6691901102580069 and parameters: {'n_estimators': 737}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:14:51,163]\u001b[0m Trial 391 finished with value: 0.6691297231666181 and parameters: {'n_estimators': 705}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:16:40,879]\u001b[0m Trial 392 finished with value: 0.6691079141689844 and parameters: {'n_estimators': 764}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:18:34,025]\u001b[0m Trial 393 finished with value: 0.6690180391659422 and parameters: {'n_estimators': 796}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:20:25,329]\u001b[0m Trial 394 finished with value: 0.6690633502249737 and parameters: {'n_estimators': 775}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:22:21,486]\u001b[0m Trial 395 finished with value: 0.6690551036982255 and parameters: {'n_estimators': 812}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 02:24:21,988]\u001b[0m Trial 396 finished with value: 0.6689574397792365 and parameters: {'n_estimators': 841}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:26:05,051]\u001b[0m Trial 397 finished with value: 0.6690813514002975 and parameters: {'n_estimators': 720}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:27:52,748]\u001b[0m Trial 398 finished with value: 0.6690916495188104 and parameters: {'n_estimators': 750}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:29:37,691]\u001b[0m Trial 399 finished with value: 0.6691875757048007 and parameters: {'n_estimators': 738}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6697\n",
      "\tBest params:\n",
      "\t\tn_estimators: 678\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.653416    0.660333    0.677432    0.668564   \n",
      "1                    TP  396.000000  409.000000  417.000000  405.000000   \n",
      "2                    TN  356.000000  333.000000  330.000000  338.000000   \n",
      "3                    FP   88.000000   83.000000   93.000000   88.000000   \n",
      "4                    FN   59.000000   74.000000   59.000000   68.000000   \n",
      "5              Accuracy    0.836485    0.825362    0.830923    0.826474   \n",
      "6             Precision    0.818182    0.831301    0.817647    0.821501   \n",
      "7           Sensitivity    0.870330    0.846791    0.876050    0.856237   \n",
      "8           Specificity    0.801800    0.800500    0.780100    0.793400   \n",
      "9              F1 score    0.843450    0.838974    0.845842    0.838509   \n",
      "10  F1 score (weighted)    0.836250    0.825213    0.830299    0.826185   \n",
      "11     F1 score (macro)    0.836161    0.824104    0.829325    0.825505   \n",
      "12    Balanced Accuracy    0.836066    0.823636    0.828096    0.824832   \n",
      "13                  MCC    0.674070    0.648376    0.661066    0.651835   \n",
      "14                  NPV    0.857800    0.818200    0.848300    0.832500   \n",
      "15              ROC_AUC    0.836066    0.823636    0.828096    0.824832   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.646037    0.664824    0.662168    0.673141  \n",
      "1   400.000000  409.000000  403.000000  402.000000  \n",
      "2   348.000000  350.000000  343.000000  333.000000  \n",
      "3    82.000000   73.000000   84.000000   83.000000  \n",
      "4    69.000000   67.000000   69.000000   81.000000  \n",
      "5     0.832036    0.844271    0.829811    0.817575  \n",
      "6     0.829876    0.848548    0.827515    0.828866  \n",
      "7     0.852878    0.859244    0.853814    0.832298  \n",
      "8     0.809300    0.827400    0.803300    0.800500  \n",
      "9     0.841220    0.853862    0.840459    0.830579  \n",
      "10    0.831895    0.844203    0.829621    0.817544  \n",
      "11    0.831472    0.843598    0.829049    0.816494  \n",
      "12    0.831090    0.843333    0.828546    0.816389  \n",
      "13    0.663293    0.687271    0.658564    0.632996  \n",
      "14    0.834500    0.839300    0.832500    0.804300  \n",
      "15    0.831090    0.843333    0.828546    0.816389  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_7_cat = np.where((y_pred_rf_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 02:31:39,998]\u001b[0m Trial 400 finished with value: 0.656102365809055 and parameters: {'n_estimators': 782}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:33:26,319]\u001b[0m Trial 401 finished with value: 0.6559980316718026 and parameters: {'n_estimators': 756}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:35:18,181]\u001b[0m Trial 402 finished with value: 0.6561942694817057 and parameters: {'n_estimators': 796}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:37:15,056]\u001b[0m Trial 403 finished with value: 0.6564381350124247 and parameters: {'n_estimators': 828}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:39:03,701]\u001b[0m Trial 404 finished with value: 0.656027734014099 and parameters: {'n_estimators': 774}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:40:43,968]\u001b[0m Trial 405 finished with value: 0.6556922834579957 and parameters: {'n_estimators': 712}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:42:31,868]\u001b[0m Trial 406 finished with value: 0.6560553263007604 and parameters: {'n_estimators': 761}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:44:15,081]\u001b[0m Trial 407 finished with value: 0.6557937051473204 and parameters: {'n_estimators': 732}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:45:52,404]\u001b[0m Trial 408 finished with value: 0.6554452913154261 and parameters: {'n_estimators': 688}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:47:45,415]\u001b[0m Trial 409 finished with value: 0.656315053690803 and parameters: {'n_estimators': 805}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:49:36,452]\u001b[0m Trial 410 finished with value: 0.6560574555759531 and parameters: {'n_estimators': 788}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:51:21,554]\u001b[0m Trial 411 finished with value: 0.6559314625372271 and parameters: {'n_estimators': 746}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:53:10,632]\u001b[0m Trial 412 finished with value: 0.6560696594563471 and parameters: {'n_estimators': 770}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:55:05,175]\u001b[0m Trial 413 finished with value: 0.6563799094152143 and parameters: {'n_estimators': 817}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:56:47,417]\u001b[0m Trial 414 finished with value: 0.6557736059069119 and parameters: {'n_estimators': 722}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:58:37,776]\u001b[0m Trial 415 finished with value: 0.6560596815014876 and parameters: {'n_estimators': 785}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:00:24,923]\u001b[0m Trial 416 finished with value: 0.6561000406610138 and parameters: {'n_estimators': 764}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:01:59,335]\u001b[0m Trial 417 finished with value: 0.6552269606761455 and parameters: {'n_estimators': 667}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:02:17,112]\u001b[0m Trial 418 finished with value: 0.6511079129196028 and parameters: {'n_estimators': 117}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:04:01,918]\u001b[0m Trial 419 finished with value: 0.655839114963738 and parameters: {'n_estimators': 742}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:05:54,760]\u001b[0m Trial 420 finished with value: 0.656190706967318 and parameters: {'n_estimators': 800}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:07:33,350]\u001b[0m Trial 421 finished with value: 0.6556131748020297 and parameters: {'n_estimators': 704}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:09:19,463]\u001b[0m Trial 422 finished with value: 0.6559985639402066 and parameters: {'n_estimators': 758}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:11:09,603]\u001b[0m Trial 423 finished with value: 0.6560559458175503 and parameters: {'n_estimators': 783}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:12:53,701]\u001b[0m Trial 424 finished with value: 0.6557408579759778 and parameters: {'n_estimators': 736}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:14:53,286]\u001b[0m Trial 425 finished with value: 0.6565371258274509 and parameters: {'n_estimators': 848}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:16:47,074]\u001b[0m Trial 426 finished with value: 0.6564295451288833 and parameters: {'n_estimators': 811}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:18:36,510]\u001b[0m Trial 427 finished with value: 0.6560860470599804 and parameters: {'n_estimators': 769}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:20:22,285]\u001b[0m Trial 428 finished with value: 0.6559409798202246 and parameters: {'n_estimators': 751}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:22:11,561]\u001b[0m Trial 429 finished with value: 0.656102365809055 and parameters: {'n_estimators': 782}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:23:53,914]\u001b[0m Trial 430 finished with value: 0.6557599388971841 and parameters: {'n_estimators': 724}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:25:50,031]\u001b[0m Trial 431 finished with value: 0.6564566924992435 and parameters: {'n_estimators': 829}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:27:42,977]\u001b[0m Trial 432 finished with value: 0.6562632894235564 and parameters: {'n_estimators': 802}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:29:30,433]\u001b[0m Trial 433 finished with value: 0.6559754928251494 and parameters: {'n_estimators': 760}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:31:20,286]\u001b[0m Trial 434 finished with value: 0.6561046098033453 and parameters: {'n_estimators': 781}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:32:58,764]\u001b[0m Trial 435 finished with value: 0.6555406488368585 and parameters: {'n_estimators': 699}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:34:35,993]\u001b[0m Trial 436 finished with value: 0.6553051176823035 and parameters: {'n_estimators': 682}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:36:21,007]\u001b[0m Trial 437 finished with value: 0.655883056426355 and parameters: {'n_estimators': 744}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:38:12,177]\u001b[0m Trial 438 finished with value: 0.6561820007652269 and parameters: {'n_estimators': 794}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:40:00,406]\u001b[0m Trial 439 finished with value: 0.6560474431113118 and parameters: {'n_estimators': 768}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:41:41,733]\u001b[0m Trial 440 finished with value: 0.6558007519517662 and parameters: {'n_estimators': 720}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:43:37,264]\u001b[0m Trial 441 finished with value: 0.6563725507935507 and parameters: {'n_estimators': 815}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:45:20,897]\u001b[0m Trial 442 finished with value: 0.6557269367655909 and parameters: {'n_estimators': 735}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:46:16,386]\u001b[0m Trial 443 finished with value: 0.6547169998691572 and parameters: {'n_estimators': 388}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:48:02,308]\u001b[0m Trial 444 finished with value: 0.6560289128557467 and parameters: {'n_estimators': 755}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:49:52,695]\u001b[0m Trial 445 finished with value: 0.6560596815014875 and parameters: {'n_estimators': 785}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 03:51:39,478]\u001b[0m Trial 446 finished with value: 0.6560543995952726 and parameters: {'n_estimators': 767}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:53:30,840]\u001b[0m Trial 447 finished with value: 0.6561853785806029 and parameters: {'n_estimators': 795}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:55:03,792]\u001b[0m Trial 448 finished with value: 0.6552202924488977 and parameters: {'n_estimators': 660}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:56:44,087]\u001b[0m Trial 449 finished with value: 0.6556121645303272 and parameters: {'n_estimators': 710}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6697\n",
      "\tBest params:\n",
      "\t\tn_estimators: 678\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.653416    0.660333    0.677432    0.668564   \n",
      "1                    TP  396.000000  409.000000  417.000000  405.000000   \n",
      "2                    TN  356.000000  333.000000  330.000000  338.000000   \n",
      "3                    FP   88.000000   83.000000   93.000000   88.000000   \n",
      "4                    FN   59.000000   74.000000   59.000000   68.000000   \n",
      "5              Accuracy    0.836485    0.825362    0.830923    0.826474   \n",
      "6             Precision    0.818182    0.831301    0.817647    0.821501   \n",
      "7           Sensitivity    0.870330    0.846791    0.876050    0.856237   \n",
      "8           Specificity    0.801800    0.800500    0.780100    0.793400   \n",
      "9              F1 score    0.843450    0.838974    0.845842    0.838509   \n",
      "10  F1 score (weighted)    0.836250    0.825213    0.830299    0.826185   \n",
      "11     F1 score (macro)    0.836161    0.824104    0.829325    0.825505   \n",
      "12    Balanced Accuracy    0.836066    0.823636    0.828096    0.824832   \n",
      "13                  MCC    0.674070    0.648376    0.661066    0.651835   \n",
      "14                  NPV    0.857800    0.818200    0.848300    0.832500   \n",
      "15              ROC_AUC    0.836066    0.823636    0.828096    0.824832   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.646037    0.664824    0.662168    0.673141    0.654957  \n",
      "1   400.000000  409.000000  403.000000  402.000000  414.000000  \n",
      "2   348.000000  350.000000  343.000000  333.000000  328.000000  \n",
      "3    82.000000   73.000000   84.000000   83.000000   88.000000  \n",
      "4    69.000000   67.000000   69.000000   81.000000   69.000000  \n",
      "5     0.832036    0.844271    0.829811    0.817575    0.825362  \n",
      "6     0.829876    0.848548    0.827515    0.828866    0.824701  \n",
      "7     0.852878    0.859244    0.853814    0.832298    0.857143  \n",
      "8     0.809300    0.827400    0.803300    0.800500    0.788500  \n",
      "9     0.841220    0.853862    0.840459    0.830579    0.840609  \n",
      "10    0.831895    0.844203    0.829621    0.817544    0.825005  \n",
      "11    0.831472    0.843598    0.829049    0.816494    0.823749  \n",
      "12    0.831090    0.843333    0.828546    0.816389    0.822802  \n",
      "13    0.663293    0.687271    0.658564    0.632996    0.648246  \n",
      "14    0.834500    0.839300    0.832500    0.804300    0.826200  \n",
      "15    0.831090    0.843333    0.828546    0.816389    0.822802  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_8_cat = np.where((y_pred_rf_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 03:58:41,144]\u001b[0m Trial 450 finished with value: 0.655619046269556 and parameters: {'n_estimators': 746}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:00:31,646]\u001b[0m Trial 451 finished with value: 0.6556758240515085 and parameters: {'n_estimators': 777}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:02:29,391]\u001b[0m Trial 452 finished with value: 0.6555048469864881 and parameters: {'n_estimators': 827}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:04:12,737]\u001b[0m Trial 453 finished with value: 0.6555639186838045 and parameters: {'n_estimators': 729}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:06:05,527]\u001b[0m Trial 454 finished with value: 0.6555794788217397 and parameters: {'n_estimators': 799}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:07:53,255]\u001b[0m Trial 455 finished with value: 0.6555165595831205 and parameters: {'n_estimators': 758}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:09:42,620]\u001b[0m Trial 456 finished with value: 0.6556029912130593 and parameters: {'n_estimators': 773}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:11:36,334]\u001b[0m Trial 457 finished with value: 0.6555278171377033 and parameters: {'n_estimators': 809}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:13:21,214]\u001b[0m Trial 458 finished with value: 0.6556554570242502 and parameters: {'n_estimators': 741}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:15:12,117]\u001b[0m Trial 459 finished with value: 0.6556683404135422 and parameters: {'n_estimators': 783}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:16:54,157]\u001b[0m Trial 460 finished with value: 0.6553163669036444 and parameters: {'n_estimators': 714}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:18:32,915]\u001b[0m Trial 461 finished with value: 0.6552720359371105 and parameters: {'n_estimators': 694}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:20:19,758]\u001b[0m Trial 462 finished with value: 0.6555783012421982 and parameters: {'n_estimators': 755}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:22:13,367]\u001b[0m Trial 463 finished with value: 0.6556131432937085 and parameters: {'n_estimators': 796}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:22:56,312]\u001b[0m Trial 464 finished with value: 0.6538012564867139 and parameters: {'n_estimators': 298}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:24:44,787]\u001b[0m Trial 465 finished with value: 0.6555569183663252 and parameters: {'n_estimators': 763}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:26:28,646]\u001b[0m Trial 466 finished with value: 0.6556001055247875 and parameters: {'n_estimators': 730}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:28:24,475]\u001b[0m Trial 467 finished with value: 0.6555147828707871 and parameters: {'n_estimators': 818}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:30:13,409]\u001b[0m Trial 468 finished with value: 0.6555971002429086 and parameters: {'n_estimators': 767}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:32:14,349]\u001b[0m Trial 469 finished with value: 0.6555278613085572 and parameters: {'n_estimators': 849}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:34:05,254]\u001b[0m Trial 470 finished with value: 0.6556610764559894 and parameters: {'n_estimators': 785}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:36:00,406]\u001b[0m Trial 471 finished with value: 0.6555278171377031 and parameters: {'n_estimators': 809}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:37:45,818]\u001b[0m Trial 472 finished with value: 0.655619046269556 and parameters: {'n_estimators': 746}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:39:19,644]\u001b[0m Trial 473 finished with value: 0.6551730276723179 and parameters: {'n_estimators': 681}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:41:05,625]\u001b[0m Trial 474 finished with value: 0.6556609423280466 and parameters: {'n_estimators': 776}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:43:10,201]\u001b[0m Trial 475 finished with value: 0.6556381395808847 and parameters: {'n_estimators': 901}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:44:49,651]\u001b[0m Trial 476 finished with value: 0.655537260931439 and parameters: {'n_estimators': 728}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:46:32,564]\u001b[0m Trial 477 finished with value: 0.6556036759675502 and parameters: {'n_estimators': 752}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:48:22,696]\u001b[0m Trial 478 finished with value: 0.6555785685408614 and parameters: {'n_estimators': 793}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:50:17,570]\u001b[0m Trial 479 finished with value: 0.6555158892174143 and parameters: {'n_estimators': 828}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:51:57,568]\u001b[0m Trial 480 finished with value: 0.6552961988957048 and parameters: {'n_estimators': 708}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:53:47,919]\u001b[0m Trial 481 finished with value: 0.6556317150697943 and parameters: {'n_estimators': 775}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:55:32,429]\u001b[0m Trial 482 finished with value: 0.6556292802801518 and parameters: {'n_estimators': 738}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:57:25,795]\u001b[0m Trial 483 finished with value: 0.6555228818679752 and parameters: {'n_estimators': 801}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:59:12,809]\u001b[0m Trial 484 finished with value: 0.6555783012421982 and parameters: {'n_estimators': 755}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:01:02,620]\u001b[0m Trial 485 finished with value: 0.6556317150697943 and parameters: {'n_estimators': 775}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:02:53,270]\u001b[0m Trial 486 finished with value: 0.6555928646063606 and parameters: {'n_estimators': 788}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:04:41,767]\u001b[0m Trial 487 finished with value: 0.6555489364052406 and parameters: {'n_estimators': 765}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:06:23,092]\u001b[0m Trial 488 finished with value: 0.6553575108586892 and parameters: {'n_estimators': 716}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:08:07,418]\u001b[0m Trial 489 finished with value: 0.6556430411747977 and parameters: {'n_estimators': 734}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:10:01,782]\u001b[0m Trial 490 finished with value: 0.6555001553599131 and parameters: {'n_estimators': 811}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:11:34,768]\u001b[0m Trial 491 finished with value: 0.6552447727847772 and parameters: {'n_estimators': 654}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:13:20,825]\u001b[0m Trial 492 finished with value: 0.6556521660181024 and parameters: {'n_estimators': 749}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:15:35,924]\u001b[0m Trial 493 finished with value: 0.6556850550431241 and parameters: {'n_estimators': 961}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:17:14,012]\u001b[0m Trial 494 finished with value: 0.6552190634304866 and parameters: {'n_estimators': 690}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:18:25,537]\u001b[0m Trial 495 finished with value: 0.6546971308581631 and parameters: {'n_estimators': 502}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 05:20:18,453]\u001b[0m Trial 496 finished with value: 0.6556133146841943 and parameters: {'n_estimators': 790}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:22:07,639]\u001b[0m Trial 497 finished with value: 0.6555412522277313 and parameters: {'n_estimators': 770}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:23:49,877]\u001b[0m Trial 498 finished with value: 0.6554905024089036 and parameters: {'n_estimators': 726}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:25:47,627]\u001b[0m Trial 499 finished with value: 0.6555331845971452 and parameters: {'n_estimators': 825}. Best is trial 151 with value: 0.6697349065227234.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6697\n",
      "\tBest params:\n",
      "\t\tn_estimators: 678\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.653416    0.660333    0.677432    0.668564   \n",
      "1                    TP  396.000000  409.000000  417.000000  405.000000   \n",
      "2                    TN  356.000000  333.000000  330.000000  338.000000   \n",
      "3                    FP   88.000000   83.000000   93.000000   88.000000   \n",
      "4                    FN   59.000000   74.000000   59.000000   68.000000   \n",
      "5              Accuracy    0.836485    0.825362    0.830923    0.826474   \n",
      "6             Precision    0.818182    0.831301    0.817647    0.821501   \n",
      "7           Sensitivity    0.870330    0.846791    0.876050    0.856237   \n",
      "8           Specificity    0.801800    0.800500    0.780100    0.793400   \n",
      "9              F1 score    0.843450    0.838974    0.845842    0.838509   \n",
      "10  F1 score (weighted)    0.836250    0.825213    0.830299    0.826185   \n",
      "11     F1 score (macro)    0.836161    0.824104    0.829325    0.825505   \n",
      "12    Balanced Accuracy    0.836066    0.823636    0.828096    0.824832   \n",
      "13                  MCC    0.674070    0.648376    0.661066    0.651835   \n",
      "14                  NPV    0.857800    0.818200    0.848300    0.832500   \n",
      "15              ROC_AUC    0.836066    0.823636    0.828096    0.824832   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.646037    0.664824    0.662168    0.673141    0.654957    0.681367  \n",
      "1   400.000000  409.000000  403.000000  402.000000  414.000000  397.000000  \n",
      "2   348.000000  350.000000  343.000000  333.000000  328.000000  341.000000  \n",
      "3    82.000000   73.000000   84.000000   83.000000   88.000000   88.000000  \n",
      "4    69.000000   67.000000   69.000000   81.000000   69.000000   73.000000  \n",
      "5     0.832036    0.844271    0.829811    0.817575    0.825362    0.820912  \n",
      "6     0.829876    0.848548    0.827515    0.828866    0.824701    0.818557  \n",
      "7     0.852878    0.859244    0.853814    0.832298    0.857143    0.844681  \n",
      "8     0.809300    0.827400    0.803300    0.800500    0.788500    0.794900  \n",
      "9     0.841220    0.853862    0.840459    0.830579    0.840609    0.831414  \n",
      "10    0.831895    0.844203    0.829621    0.817544    0.825005    0.820725  \n",
      "11    0.831472    0.843598    0.829049    0.816494    0.823749    0.820215  \n",
      "12    0.831090    0.843333    0.828546    0.816389    0.822802    0.819776  \n",
      "13    0.663293    0.687271    0.658564    0.632996    0.648246    0.640889  \n",
      "14    0.834500    0.839300    0.832500    0.804300    0.826200    0.823700  \n",
      "15    0.831090    0.843333    0.828546    0.816389    0.822802    0.819776  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_9_cat = np.where((y_pred_rf_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6697\n",
      "\tBest params:\n",
      "\t\tn_estimators: 678\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.679479</td>\n",
       "      <td>0.037395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>202.300000</td>\n",
       "      <td>11.411982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>172.300000</td>\n",
       "      <td>11.126046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>40.700000</td>\n",
       "      <td>10.231216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>33.900000</td>\n",
       "      <td>6.154492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.833930</td>\n",
       "      <td>0.025812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.833340</td>\n",
       "      <td>0.037969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.856120</td>\n",
       "      <td>0.027339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.809130</td>\n",
       "      <td>0.046963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.844077</td>\n",
       "      <td>0.024980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.833650</td>\n",
       "      <td>0.025949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.832904</td>\n",
       "      <td>0.025946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.832620</td>\n",
       "      <td>0.026241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.667327</td>\n",
       "      <td>0.051789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.836100</td>\n",
       "      <td>0.025134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.832620</td>\n",
       "      <td>0.026241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.679479     0.037395\n",
       "1                    TP       202.300000    11.411982\n",
       "2                    TN       172.300000    11.126046\n",
       "3                    FP        40.700000    10.231216\n",
       "4                    FN        33.900000     6.154492\n",
       "5              Accuracy         0.833930     0.025812\n",
       "6             Precision         0.833340     0.037969\n",
       "7           Sensitivity         0.856120     0.027339\n",
       "8           Specificity         0.809130     0.046963\n",
       "9              F1 score         0.844077     0.024980\n",
       "10  F1 score (weighted)         0.833650     0.025949\n",
       "11     F1 score (macro)         0.832904     0.025946\n",
       "12    Balanced Accuracy         0.832620     0.026241\n",
       "13                  MCC         0.667327     0.051789\n",
       "14                  NPV         0.836100     0.025134\n",
       "15              ROC_AUC         0.832620     0.026241"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.653416</td>\n",
       "      <td>0.660333</td>\n",
       "      <td>0.677432</td>\n",
       "      <td>0.668564</td>\n",
       "      <td>0.646037</td>\n",
       "      <td>0.664824</td>\n",
       "      <td>0.662168</td>\n",
       "      <td>0.673141</td>\n",
       "      <td>0.654957</td>\n",
       "      <td>0.681367</td>\n",
       "      <td>0.664224</td>\n",
       "      <td>0.011155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>405.200000</td>\n",
       "      <td>6.988880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>9.285592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>5.395471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>68.800000</td>\n",
       "      <td>6.579429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.836485</td>\n",
       "      <td>0.825362</td>\n",
       "      <td>0.830923</td>\n",
       "      <td>0.826474</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.844271</td>\n",
       "      <td>0.829811</td>\n",
       "      <td>0.817575</td>\n",
       "      <td>0.825362</td>\n",
       "      <td>0.820912</td>\n",
       "      <td>0.828921</td>\n",
       "      <td>0.007685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.831301</td>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.821501</td>\n",
       "      <td>0.829876</td>\n",
       "      <td>0.848548</td>\n",
       "      <td>0.827515</td>\n",
       "      <td>0.828866</td>\n",
       "      <td>0.824701</td>\n",
       "      <td>0.818557</td>\n",
       "      <td>0.826669</td>\n",
       "      <td>0.009224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.870330</td>\n",
       "      <td>0.846791</td>\n",
       "      <td>0.876050</td>\n",
       "      <td>0.856237</td>\n",
       "      <td>0.852878</td>\n",
       "      <td>0.859244</td>\n",
       "      <td>0.853814</td>\n",
       "      <td>0.832298</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.844681</td>\n",
       "      <td>0.854947</td>\n",
       "      <td>0.012461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.801800</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.780100</td>\n",
       "      <td>0.793400</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.827400</td>\n",
       "      <td>0.803300</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.788500</td>\n",
       "      <td>0.794900</td>\n",
       "      <td>0.799970</td>\n",
       "      <td>0.012692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.843450</td>\n",
       "      <td>0.838974</td>\n",
       "      <td>0.845842</td>\n",
       "      <td>0.838509</td>\n",
       "      <td>0.841220</td>\n",
       "      <td>0.853862</td>\n",
       "      <td>0.840459</td>\n",
       "      <td>0.830579</td>\n",
       "      <td>0.840609</td>\n",
       "      <td>0.831414</td>\n",
       "      <td>0.840492</td>\n",
       "      <td>0.006697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.836250</td>\n",
       "      <td>0.825213</td>\n",
       "      <td>0.830299</td>\n",
       "      <td>0.826185</td>\n",
       "      <td>0.831895</td>\n",
       "      <td>0.844203</td>\n",
       "      <td>0.829621</td>\n",
       "      <td>0.817544</td>\n",
       "      <td>0.825005</td>\n",
       "      <td>0.820725</td>\n",
       "      <td>0.828694</td>\n",
       "      <td>0.007682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.836161</td>\n",
       "      <td>0.824104</td>\n",
       "      <td>0.829325</td>\n",
       "      <td>0.825505</td>\n",
       "      <td>0.831472</td>\n",
       "      <td>0.843598</td>\n",
       "      <td>0.829049</td>\n",
       "      <td>0.816494</td>\n",
       "      <td>0.823749</td>\n",
       "      <td>0.820215</td>\n",
       "      <td>0.827967</td>\n",
       "      <td>0.007869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.823636</td>\n",
       "      <td>0.828096</td>\n",
       "      <td>0.824832</td>\n",
       "      <td>0.831090</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.828546</td>\n",
       "      <td>0.816389</td>\n",
       "      <td>0.822802</td>\n",
       "      <td>0.819776</td>\n",
       "      <td>0.827457</td>\n",
       "      <td>0.007928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.674070</td>\n",
       "      <td>0.648376</td>\n",
       "      <td>0.661066</td>\n",
       "      <td>0.651835</td>\n",
       "      <td>0.663293</td>\n",
       "      <td>0.687271</td>\n",
       "      <td>0.658564</td>\n",
       "      <td>0.632996</td>\n",
       "      <td>0.648246</td>\n",
       "      <td>0.640889</td>\n",
       "      <td>0.656661</td>\n",
       "      <td>0.015911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.857800</td>\n",
       "      <td>0.818200</td>\n",
       "      <td>0.848300</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.839300</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.804300</td>\n",
       "      <td>0.826200</td>\n",
       "      <td>0.823700</td>\n",
       "      <td>0.831730</td>\n",
       "      <td>0.015096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.823636</td>\n",
       "      <td>0.828096</td>\n",
       "      <td>0.824832</td>\n",
       "      <td>0.831090</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.828546</td>\n",
       "      <td>0.816389</td>\n",
       "      <td>0.822802</td>\n",
       "      <td>0.819776</td>\n",
       "      <td>0.827457</td>\n",
       "      <td>0.007928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.653416    0.660333    0.677432    0.668564   \n",
       "1                    TP  396.000000  409.000000  417.000000  405.000000   \n",
       "2                    TN  356.000000  333.000000  330.000000  338.000000   \n",
       "3                    FP   88.000000   83.000000   93.000000   88.000000   \n",
       "4                    FN   59.000000   74.000000   59.000000   68.000000   \n",
       "5              Accuracy    0.836485    0.825362    0.830923    0.826474   \n",
       "6             Precision    0.818182    0.831301    0.817647    0.821501   \n",
       "7           Sensitivity    0.870330    0.846791    0.876050    0.856237   \n",
       "8           Specificity    0.801800    0.800500    0.780100    0.793400   \n",
       "9              F1 score    0.843450    0.838974    0.845842    0.838509   \n",
       "10  F1 score (weighted)    0.836250    0.825213    0.830299    0.826185   \n",
       "11     F1 score (macro)    0.836161    0.824104    0.829325    0.825505   \n",
       "12    Balanced Accuracy    0.836066    0.823636    0.828096    0.824832   \n",
       "13                  MCC    0.674070    0.648376    0.661066    0.651835   \n",
       "14                  NPV    0.857800    0.818200    0.848300    0.832500   \n",
       "15              ROC_AUC    0.836066    0.823636    0.828096    0.824832   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.646037    0.664824    0.662168    0.673141    0.654957    0.681367   \n",
       "1   400.000000  409.000000  403.000000  402.000000  414.000000  397.000000   \n",
       "2   348.000000  350.000000  343.000000  333.000000  328.000000  341.000000   \n",
       "3    82.000000   73.000000   84.000000   83.000000   88.000000   88.000000   \n",
       "4    69.000000   67.000000   69.000000   81.000000   69.000000   73.000000   \n",
       "5     0.832036    0.844271    0.829811    0.817575    0.825362    0.820912   \n",
       "6     0.829876    0.848548    0.827515    0.828866    0.824701    0.818557   \n",
       "7     0.852878    0.859244    0.853814    0.832298    0.857143    0.844681   \n",
       "8     0.809300    0.827400    0.803300    0.800500    0.788500    0.794900   \n",
       "9     0.841220    0.853862    0.840459    0.830579    0.840609    0.831414   \n",
       "10    0.831895    0.844203    0.829621    0.817544    0.825005    0.820725   \n",
       "11    0.831472    0.843598    0.829049    0.816494    0.823749    0.820215   \n",
       "12    0.831090    0.843333    0.828546    0.816389    0.822802    0.819776   \n",
       "13    0.663293    0.687271    0.658564    0.632996    0.648246    0.640889   \n",
       "14    0.834500    0.839300    0.832500    0.804300    0.826200    0.823700   \n",
       "15    0.831090    0.843333    0.828546    0.816389    0.822802    0.819776   \n",
       "\n",
       "           ave       std  \n",
       "0     0.664224  0.011155  \n",
       "1   405.200000  6.988880  \n",
       "2   340.000000  9.285592  \n",
       "3    85.000000  5.395471  \n",
       "4    68.800000  6.579429  \n",
       "5     0.828921  0.007685  \n",
       "6     0.826669  0.009224  \n",
       "7     0.854947  0.012461  \n",
       "8     0.799970  0.012692  \n",
       "9     0.840492  0.006697  \n",
       "10    0.828694  0.007682  \n",
       "11    0.827967  0.007869  \n",
       "12    0.827457  0.007928  \n",
       "13    0.656661  0.015911  \n",
       "14    0.831730  0.015096  \n",
       "15    0.827457  0.007928  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_rf0</th>\n",
       "      <th>y_pred_rf1</th>\n",
       "      <th>y_pred_rf2</th>\n",
       "      <th>y_pred_rf3</th>\n",
       "      <th>y_pred_rf4</th>\n",
       "      <th>y_pred_rf_ave</th>\n",
       "      <th>y_pred_rf_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4286867</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>6.770472</td>\n",
       "      <td>6.820855</td>\n",
       "      <td>6.807082</td>\n",
       "      <td>6.811681</td>\n",
       "      <td>6.727493</td>\n",
       "      <td>6.864597</td>\n",
       "      <td>0.175221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3689853</td>\n",
       "      <td>1</td>\n",
       "      <td>6.43</td>\n",
       "      <td>6.518850</td>\n",
       "      <td>6.580664</td>\n",
       "      <td>6.639204</td>\n",
       "      <td>6.579277</td>\n",
       "      <td>6.644410</td>\n",
       "      <td>6.565401</td>\n",
       "      <td>0.073728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3827056</td>\n",
       "      <td>2</td>\n",
       "      <td>7.52</td>\n",
       "      <td>8.729277</td>\n",
       "      <td>8.774720</td>\n",
       "      <td>8.773584</td>\n",
       "      <td>8.780487</td>\n",
       "      <td>8.786003</td>\n",
       "      <td>8.560678</td>\n",
       "      <td>0.465773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3689883</td>\n",
       "      <td>3</td>\n",
       "      <td>7.70</td>\n",
       "      <td>7.436563</td>\n",
       "      <td>7.415206</td>\n",
       "      <td>7.419130</td>\n",
       "      <td>7.373378</td>\n",
       "      <td>7.421268</td>\n",
       "      <td>7.460924</td>\n",
       "      <td>0.108647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2023528</td>\n",
       "      <td>4</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.607625</td>\n",
       "      <td>7.669848</td>\n",
       "      <td>7.513798</td>\n",
       "      <td>7.355310</td>\n",
       "      <td>7.400442</td>\n",
       "      <td>7.469504</td>\n",
       "      <td>0.140706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4464975</td>\n",
       "      <td>4487</td>\n",
       "      <td>4.72</td>\n",
       "      <td>5.176844</td>\n",
       "      <td>5.182434</td>\n",
       "      <td>5.128319</td>\n",
       "      <td>5.207729</td>\n",
       "      <td>5.284779</td>\n",
       "      <td>5.116684</td>\n",
       "      <td>0.183476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL95747</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.984038</td>\n",
       "      <td>7.025012</td>\n",
       "      <td>7.189293</td>\n",
       "      <td>7.151203</td>\n",
       "      <td>7.129022</td>\n",
       "      <td>7.179761</td>\n",
       "      <td>0.200971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL4072618</td>\n",
       "      <td>4489</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.157171</td>\n",
       "      <td>5.164853</td>\n",
       "      <td>5.118481</td>\n",
       "      <td>5.171903</td>\n",
       "      <td>5.123584</td>\n",
       "      <td>5.154332</td>\n",
       "      <td>0.025594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL2408692</td>\n",
       "      <td>4490</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.554935</td>\n",
       "      <td>6.462968</td>\n",
       "      <td>6.458560</td>\n",
       "      <td>6.526234</td>\n",
       "      <td>5.966246</td>\n",
       "      <td>6.388157</td>\n",
       "      <td>0.198425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4226829</td>\n",
       "      <td>4491</td>\n",
       "      <td>5.55</td>\n",
       "      <td>5.945518</td>\n",
       "      <td>5.930015</td>\n",
       "      <td>5.921015</td>\n",
       "      <td>5.934267</td>\n",
       "      <td>5.933007</td>\n",
       "      <td>5.868970</td>\n",
       "      <td>0.142829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_rf0  y_pred_rf1  \\\n",
       "0         CHEMBL4286867            0     7.25    6.770472    6.820855   \n",
       "1         CHEMBL3689853            1     6.43    6.518850    6.580664   \n",
       "2         CHEMBL3827056            2     7.52    8.729277    8.774720   \n",
       "3         CHEMBL3689883            3     7.70    7.436563    7.415206   \n",
       "4         CHEMBL2023528            4     7.27    7.607625    7.669848   \n",
       "...                 ...          ...      ...         ...         ...   \n",
       "4487      CHEMBL4464975         4487     4.72    5.176844    5.182434   \n",
       "4488        CHEMBL95747         4488     7.60    6.984038    7.025012   \n",
       "4489      CHEMBL4072618         4489     5.19    5.157171    5.164853   \n",
       "4490      CHEMBL2408692         4490     6.36    6.554935    6.462968   \n",
       "4491      CHEMBL4226829         4491     5.55    5.945518    5.930015   \n",
       "\n",
       "      y_pred_rf2  y_pred_rf3  y_pred_rf4  y_pred_rf_ave  y_pred_rf_std  \n",
       "0       6.807082    6.811681    6.727493       6.864597       0.175221  \n",
       "1       6.639204    6.579277    6.644410       6.565401       0.073728  \n",
       "2       8.773584    8.780487    8.786003       8.560678       0.465773  \n",
       "3       7.419130    7.373378    7.421268       7.460924       0.108647  \n",
       "4       7.513798    7.355310    7.400442       7.469504       0.140706  \n",
       "...          ...         ...         ...            ...            ...  \n",
       "4487    5.128319    5.207729    5.284779       5.116684       0.183476  \n",
       "4488    7.189293    7.151203    7.129022       7.179761       0.200971  \n",
       "4489    5.118481    5.171903    5.123584       5.154332       0.025594  \n",
       "4490    6.458560    6.526234    5.966246       6.388157       0.198425  \n",
       "4491    5.921015    5.934267    5.933007       5.868970       0.142829  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=1121218, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where((y_pred_optimized_rf >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "rf_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47203ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.685121</td>\n",
       "      <td>0.024404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.834731</td>\n",
       "      <td>0.017151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.834629</td>\n",
       "      <td>0.026817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.855663</td>\n",
       "      <td>0.021652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.812030</td>\n",
       "      <td>0.030238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.844657</td>\n",
       "      <td>0.016971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.834576</td>\n",
       "      <td>0.017240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.833875</td>\n",
       "      <td>0.017247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.833850</td>\n",
       "      <td>0.017418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.668863</td>\n",
       "      <td>0.034306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.835406</td>\n",
       "      <td>0.024017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.833850</td>\n",
       "      <td>0.017418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.685121     0.024404\n",
       "1              Accuracy         0.834731     0.017151\n",
       "2             Precision         0.834629     0.026817\n",
       "3           Sensitivity         0.855663     0.021652\n",
       "4           Specificity         0.812030     0.030238\n",
       "5              F1 score         0.844657     0.016971\n",
       "6   F1 score (weighted)         0.834576     0.017240\n",
       "7      F1 score (macro)         0.833875     0.017247\n",
       "8     Balanced Accuracy         0.833850     0.017418\n",
       "9                   MCC         0.668863     0.034306\n",
       "10                  NPV         0.835406     0.024017\n",
       "11              ROC_AUC         0.833850     0.017418"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d030d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_5preds.to_csv('rf_5test_CV_result.csv')\n",
    "mat_met_optimized_rf.to_csv('mat_met_rf_opt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.6796 with a standard deviation of 0.0298\n",
      "RF optimized model r2_score 0.6841 with a standard deviation of 0.0294\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_rf.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_reg, \"./rf_reg.joblib\")\n",
    "#joblib.dump(optimized_rf, \"./optimized_rf.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, \"./optimizedCV_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.663239     0.035815\n",
      "1                    TP       194.000000    10.263203\n",
      "2                    TN       171.100000     8.723531\n",
      "3                    FP        41.900000     6.349978\n",
      "4                    FN        42.200000     5.711587\n",
      "5              Accuracy         0.812777     0.019324\n",
      "6             Precision         0.822440     0.025358\n",
      "7           Sensitivity         0.821092     0.025171\n",
      "8           Specificity         0.803310     0.029021\n",
      "9              F1 score         0.821528     0.020437\n",
      "10  F1 score (weighted)         0.812745     0.019347\n",
      "11     F1 score (macro)         0.812051     0.019379\n",
      "12    Balanced Accuracy         0.812205     0.019462\n",
      "13                  MCC         0.624621     0.038934\n",
      "14                  NPV         0.802390     0.023526\n",
      "15              ROC_AUC         0.812205     0.019462\n",
      "CPU times: user 38.6 s, sys: 132 ms, total: 38.8 s\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=16,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=16,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=16,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 05:40:46,445]\u001b[0m A new study created in memory with name: lgbmRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:40:51,276]\u001b[0m Trial 0 finished with value: 0.675112720226063 and parameters: {'n_estimators': 778, 'learning_rate': 0.09921044350826577, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 144}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:40:54,743]\u001b[0m Trial 1 finished with value: 0.6678738850391156 and parameters: {'n_estimators': 756, 'learning_rate': 0.18992323196099356, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 80}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:03,114]\u001b[0m Trial 2 finished with value: 0.6425647795501651 and parameters: {'n_estimators': 900, 'learning_rate': 0.017764731310748526, 'max_depth': 6, 'max_bin': 196, 'num_leaves': 39}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:11,699]\u001b[0m Trial 3 finished with value: 0.6681211470874444 and parameters: {'n_estimators': 794, 'learning_rate': 0.031666327914765084, 'max_depth': 8, 'max_bin': 243, 'num_leaves': 710}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:15,663]\u001b[0m Trial 4 finished with value: 0.3371875767914855 and parameters: {'n_estimators': 116, 'learning_rate': 0.005200551320265432, 'max_depth': 11, 'max_bin': 183, 'num_leaves': 748}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:18,081]\u001b[0m Trial 5 finished with value: 0.6524669729996739 and parameters: {'n_estimators': 230, 'learning_rate': 0.1654333130866744, 'max_depth': 6, 'max_bin': 227, 'num_leaves': 61}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:19,740]\u001b[0m Trial 6 finished with value: 0.5016844693546687 and parameters: {'n_estimators': 257, 'learning_rate': 0.044336110605717516, 'max_depth': 3, 'max_bin': 241, 'num_leaves': 215}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:33,938]\u001b[0m Trial 7 finished with value: 0.526378332247797 and parameters: {'n_estimators': 701, 'learning_rate': 0.002735017372173275, 'max_depth': 9, 'max_bin': 151, 'num_leaves': 629}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:38,484]\u001b[0m Trial 8 finished with value: 0.6699482842660357 and parameters: {'n_estimators': 496, 'learning_rate': 0.14142469093053614, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 415}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:42,423]\u001b[0m Trial 9 finished with value: 0.6607955947748159 and parameters: {'n_estimators': 859, 'learning_rate': 0.18809865390760774, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 569}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:45,795]\u001b[0m Trial 10 finished with value: 0.6347669912611493 and parameters: {'n_estimators': 613, 'learning_rate': 0.08584798415322173, 'max_depth': 3, 'max_bin': 286, 'num_leaves': 292}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:50,501]\u001b[0m Trial 11 finished with value: 0.6680798193857328 and parameters: {'n_estimators': 463, 'learning_rate': 0.13042697749667856, 'max_depth': 12, 'max_bin': 199, 'num_leaves': 460}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:41:55,382]\u001b[0m Trial 12 finished with value: 0.6711777141783559 and parameters: {'n_estimators': 498, 'learning_rate': 0.1091096160281232, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 387}. Best is trial 0 with value: 0.675112720226063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:00,296]\u001b[0m Trial 13 finished with value: 0.6758218018592639 and parameters: {'n_estimators': 507, 'learning_rate': 0.08756648411479998, 'max_depth': 10, 'max_bin': 174, 'num_leaves': 267}. Best is trial 13 with value: 0.6758218018592639.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:06,057]\u001b[0m Trial 14 finished with value: 0.6702278459292201 and parameters: {'n_estimators': 625, 'learning_rate': 0.06685168279604774, 'max_depth': 8, 'max_bin': 162, 'num_leaves': 184}. Best is trial 13 with value: 0.6758218018592639.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:11,525]\u001b[0m Trial 15 finished with value: 0.6691455557308961 and parameters: {'n_estimators': 386, 'learning_rate': 0.08743950127074428, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 190}. Best is trial 13 with value: 0.6758218018592639.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:16,859]\u001b[0m Trial 16 finished with value: 0.6626200542543059 and parameters: {'n_estimators': 618, 'learning_rate': 0.05638514329197092, 'max_depth': 6, 'max_bin': 260, 'num_leaves': 269}. Best is trial 13 with value: 0.6758218018592639.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:21,338]\u001b[0m Trial 17 finished with value: 0.6707551623786141 and parameters: {'n_estimators': 349, 'learning_rate': 0.10836370975215502, 'max_depth': 9, 'max_bin': 170, 'num_leaves': 334}. Best is trial 13 with value: 0.6758218018592639.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:22,663]\u001b[0m Trial 18 finished with value: 0.635470462306383 and parameters: {'n_estimators': 51, 'learning_rate': 0.12839411900948752, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 129}. Best is trial 13 with value: 0.6758218018592639.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:27,782]\u001b[0m Trial 19 finished with value: 0.6682415796833703 and parameters: {'n_estimators': 693, 'learning_rate': 0.07549260738829429, 'max_depth': 7, 'max_bin': 298, 'num_leaves': 147}. Best is trial 13 with value: 0.6758218018592639.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:33,315]\u001b[0m Trial 20 finished with value: 0.6766005316495283 and parameters: {'n_estimators': 559, 'learning_rate': 0.09887616230112278, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 484}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:38,390]\u001b[0m Trial 21 finished with value: 0.6714129176617277 and parameters: {'n_estimators': 551, 'learning_rate': 0.09721290989020023, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 474}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:43,302]\u001b[0m Trial 22 finished with value: 0.6672955008926049 and parameters: {'n_estimators': 418, 'learning_rate': 0.1144120490142944, 'max_depth': 9, 'max_bin': 263, 'num_leaves': 553}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:47,062]\u001b[0m Trial 23 finished with value: 0.668248043168609 and parameters: {'n_estimators': 558, 'learning_rate': 0.1473507359930648, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 249}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:52,031]\u001b[0m Trial 24 finished with value: 0.6687304115048163 and parameters: {'n_estimators': 319, 'learning_rate': 0.06605319120045819, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 336}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:57,650]\u001b[0m Trial 25 finished with value: 0.67540194877396 and parameters: {'n_estimators': 707, 'learning_rate': 0.09472226521221777, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 512}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:43:06,261]\u001b[0m Trial 26 finished with value: 0.6753690262906756 and parameters: {'n_estimators': 694, 'learning_rate': 0.04368795177860734, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 547}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:43:12,056]\u001b[0m Trial 27 finished with value: 0.6753390122787526 and parameters: {'n_estimators': 547, 'learning_rate': 0.08249221348140341, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 493}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:43:15,034]\u001b[0m Trial 28 finished with value: 0.649053598112616 and parameters: {'n_estimators': 439, 'learning_rate': 0.12202812759581305, 'max_depth': 4, 'max_bin': 283, 'num_leaves': 647}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:43:20,582]\u001b[0m Trial 29 finished with value: 0.6739790797571718 and parameters: {'n_estimators': 791, 'learning_rate': 0.09616529977256115, 'max_depth': 11, 'max_bin': 234, 'num_leaves': 387}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:43:24,553]\u001b[0m Trial 30 finished with value: 0.6682469449584418 and parameters: {'n_estimators': 647, 'learning_rate': 0.1434733934796773, 'max_depth': 9, 'max_bin': 250, 'num_leaves': 437}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 05:43:33,006]\u001b[0m Trial 31 finished with value: 0.6744037367047286 and parameters: {'n_estimators': 706, 'learning_rate': 0.04976702553275919, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 529}. Best is trial 20 with value: 0.6766005316495283.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:43:42,566]\u001b[0m Trial 32 finished with value: 0.6779922538261174 and parameters: {'n_estimators': 751, 'learning_rate': 0.03536050165542186, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 608}. Best is trial 32 with value: 0.6779922538261174.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:43:52,435]\u001b[0m Trial 33 finished with value: 0.6689957483359311 and parameters: {'n_estimators': 751, 'learning_rate': 0.02707010157744374, 'max_depth': 10, 'max_bin': 284, 'num_leaves': 606}. Best is trial 32 with value: 0.6779922538261174.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:43:58,845]\u001b[0m Trial 34 finished with value: 0.6725689042347143 and parameters: {'n_estimators': 865, 'learning_rate': 0.07170788088149549, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 674}. Best is trial 32 with value: 0.6779922538261174.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:44:11,395]\u001b[0m Trial 35 finished with value: 0.6676814163908329 and parameters: {'n_estimators': 825, 'learning_rate': 0.018468772204593933, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 506}. Best is trial 32 with value: 0.6779922538261174.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:44:19,468]\u001b[0m Trial 36 finished with value: 0.6792151005138968 and parameters: {'n_estimators': 741, 'learning_rate': 0.057562431791606014, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 689}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:44:28,027]\u001b[0m Trial 37 finished with value: 0.6658811117725653 and parameters: {'n_estimators': 763, 'learning_rate': 0.03450099425283143, 'max_depth': 8, 'max_bin': 267, 'num_leaves': 719}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:44:35,765]\u001b[0m Trial 38 finished with value: 0.6736918058414594 and parameters: {'n_estimators': 593, 'learning_rate': 0.05202876022564553, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 600}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:44:42,825]\u001b[0m Trial 39 finished with value: 0.6737300261827163 and parameters: {'n_estimators': 892, 'learning_rate': 0.06389004831055153, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 679}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:44:46,306]\u001b[0m Trial 40 finished with value: 0.6274512349351515 and parameters: {'n_estimators': 197, 'learning_rate': 0.03586942514106474, 'max_depth': 9, 'max_bin': 281, 'num_leaves': 750}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:44:51,841]\u001b[0m Trial 41 finished with value: 0.6772033516432071 and parameters: {'n_estimators': 743, 'learning_rate': 0.09659112896471951, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 573}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:44:57,842]\u001b[0m Trial 42 finished with value: 0.672797486008872 and parameters: {'n_estimators': 832, 'learning_rate': 0.08203875777897145, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 590}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:45:05,546]\u001b[0m Trial 43 finished with value: 0.6747027770857883 and parameters: {'n_estimators': 660, 'learning_rate': 0.055835633664076255, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 636}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:45:17,975]\u001b[0m Trial 44 finished with value: 0.6692513008038989 and parameters: {'n_estimators': 744, 'learning_rate': 0.017631565091718135, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 679}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:45:23,235]\u001b[0m Trial 45 finished with value: 0.6753648004881215 and parameters: {'n_estimators': 505, 'learning_rate': 0.10591630081161235, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 572}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:45:28,949]\u001b[0m Trial 46 finished with value: 0.6744856188066948 and parameters: {'n_estimators': 586, 'learning_rate': 0.0903004135341812, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 713}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:45:33,730]\u001b[0m Trial 47 finished with value: 0.6586002184359049 and parameters: {'n_estimators': 806, 'learning_rate': 0.07812685161556931, 'max_depth': 5, 'max_bin': 278, 'num_leaves': 422}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:45:37,420]\u001b[0m Trial 48 finished with value: 0.6626221469236248 and parameters: {'n_estimators': 656, 'learning_rate': 0.1990828630483893, 'max_depth': 10, 'max_bin': 268, 'num_leaves': 352}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:45:42,687]\u001b[0m Trial 49 finished with value: 0.6791249405071427 and parameters: {'n_estimators': 523, 'learning_rate': 0.11950257547550686, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 624}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6792\n",
      "\tBest params:\n",
      "\t\tn_estimators: 741\n",
      "\t\tlearning_rate: 0.057562431791606014\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 266\n",
      "\t\tnum_leaves: 689\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.665298\n",
      "1                    TP  377.000000\n",
      "2                    TN  373.000000\n",
      "3                    FP   71.000000\n",
      "4                    FN   78.000000\n",
      "5              Accuracy    0.834260\n",
      "6             Precision    0.841518\n",
      "7           Sensitivity    0.828571\n",
      "8           Specificity    0.840100\n",
      "9              F1 score    0.834994\n",
      "10  F1 score (weighted)    0.834266\n",
      "11     F1 score (macro)    0.834257\n",
      "12    Balanced Accuracy    0.834331\n",
      "13                  MCC    0.668615\n",
      "14                  NPV    0.827100\n",
      "15              ROC_AUC    0.834331\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_0_cat = np.where((y_pred_lgbm_0>= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 05:45:47,853]\u001b[0m Trial 50 finished with value: 0.6736989269765725 and parameters: {'n_estimators': 475, 'learning_rate': 0.16279299883594786, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 621}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:45:52,680]\u001b[0m Trial 51 finished with value: 0.6753306314513534 and parameters: {'n_estimators': 523, 'learning_rate': 0.13420239813345208, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 644}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:45:58,247]\u001b[0m Trial 52 finished with value: 0.6771947859999427 and parameters: {'n_estimators': 395, 'learning_rate': 0.11551332634211546, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 566}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:02,721]\u001b[0m Trial 53 finished with value: 0.6755687711378006 and parameters: {'n_estimators': 306, 'learning_rate': 0.12013490735259412, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 579}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:08,205]\u001b[0m Trial 54 finished with value: 0.6787339233429651 and parameters: {'n_estimators': 393, 'learning_rate': 0.10751731150330042, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 534}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:12,455]\u001b[0m Trial 55 finished with value: 0.6743455550883065 and parameters: {'n_estimators': 375, 'learning_rate': 0.15534918621788027, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 539}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:16,754]\u001b[0m Trial 56 finished with value: 0.6742030534651864 and parameters: {'n_estimators': 250, 'learning_rate': 0.11519714436663576, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 666}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:21,215]\u001b[0m Trial 57 finished with value: 0.6727184426375239 and parameters: {'n_estimators': 323, 'learning_rate': 0.12664859304434314, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 696}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:24,851]\u001b[0m Trial 58 finished with value: 0.6767710182686846 and parameters: {'n_estimators': 188, 'learning_rate': 0.13292649718836563, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 608}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:29,080]\u001b[0m Trial 59 finished with value: 0.6713827579883442 and parameters: {'n_estimators': 424, 'learning_rate': 0.10571490289921652, 'max_depth': 7, 'max_bin': 254, 'num_leaves': 457}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:34,240]\u001b[0m Trial 60 finished with value: 0.6753711060884484 and parameters: {'n_estimators': 291, 'learning_rate': 0.11294413739811365, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 564}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:38,096]\u001b[0m Trial 61 finished with value: 0.674831136966416 and parameters: {'n_estimators': 180, 'learning_rate': 0.1328664096808128, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 617}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:40,984]\u001b[0m Trial 62 finished with value: 0.6666423606627973 and parameters: {'n_estimators': 119, 'learning_rate': 0.17634784212754384, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 653}. Best is trial 36 with value: 0.6792151005138968.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:46,148]\u001b[0m Trial 63 finished with value: 0.6793945685328551 and parameters: {'n_estimators': 365, 'learning_rate': 0.13699211789402646, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 521}. Best is trial 63 with value: 0.6793945685328551.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:50,792]\u001b[0m Trial 64 finished with value: 0.6778478115843267 and parameters: {'n_estimators': 402, 'learning_rate': 0.15151904168709718, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 524}. Best is trial 63 with value: 0.6793945685328551.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:55,134]\u001b[0m Trial 65 finished with value: 0.67211493329291 and parameters: {'n_estimators': 358, 'learning_rate': 0.15130227728244983, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 530}. Best is trial 63 with value: 0.6793945685328551.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:46:58,932]\u001b[0m Trial 66 finished with value: 0.6737797421943503 and parameters: {'n_estimators': 724, 'learning_rate': 0.14133151453646545, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 513}. Best is trial 63 with value: 0.6793945685328551.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:47:02,718]\u001b[0m Trial 67 finished with value: 0.6771838519499824 and parameters: {'n_estimators': 404, 'learning_rate': 0.17926917922962626, 'max_depth': 10, 'max_bin': 282, 'num_leaves': 470}. Best is trial 63 with value: 0.6793945685328551.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:47:06,991]\u001b[0m Trial 68 finished with value: 0.6726096399529934 and parameters: {'n_estimators': 451, 'learning_rate': 0.15845622918275445, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 725}. Best is trial 63 with value: 0.6793945685328551.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:47:11,595]\u001b[0m Trial 69 finished with value: 0.6758262330896231 and parameters: {'n_estimators': 477, 'learning_rate': 0.16811851394299365, 'max_depth': 10, 'max_bin': 280, 'num_leaves': 592}. Best is trial 63 with value: 0.6793945685328551.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:47:15,986]\u001b[0m Trial 70 finished with value: 0.6746155561207114 and parameters: {'n_estimators': 363, 'learning_rate': 0.14035053193149782, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 441}. Best is trial 63 with value: 0.6793945685328551.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:47:20,707]\u001b[0m Trial 71 finished with value: 0.6724890137059819 and parameters: {'n_estimators': 339, 'learning_rate': 0.12363915364379394, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 563}. Best is trial 63 with value: 0.6793945685328551.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:47:26,569]\u001b[0m Trial 72 finished with value: 0.679568978189598 and parameters: {'n_estimators': 406, 'learning_rate': 0.10134200702934384, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 498}. Best is trial 72 with value: 0.679568978189598.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:47:34,235]\u001b[0m Trial 73 finished with value: 0.5822183613951317 and parameters: {'n_estimators': 286, 'learning_rate': 0.007460537288942429, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 496}. Best is trial 72 with value: 0.679568978189598.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:47:43,432]\u001b[0m Trial 74 finished with value: 0.6778073123976678 and parameters: {'n_estimators': 775, 'learning_rate': 0.043018289361666844, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 538}. Best is trial 72 with value: 0.679568978189598.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:47:51,329]\u001b[0m Trial 75 finished with value: 0.6728389990387755 and parameters: {'n_estimators': 417, 'learning_rate': 0.04369634334547624, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 517}. Best is trial 72 with value: 0.679568978189598.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:48:02,471]\u001b[0m Trial 76 finished with value: 0.6840252980239433 and parameters: {'n_estimators': 779, 'learning_rate': 0.03857672323037514, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 537}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:48:08,214]\u001b[0m Trial 77 finished with value: 0.6810607227734484 and parameters: {'n_estimators': 435, 'learning_rate': 0.101098456815599, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 626}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:48:13,245]\u001b[0m Trial 78 finished with value: 0.6762338822544789 and parameters: {'n_estimators': 456, 'learning_rate': 0.1006385092025224, 'max_depth': 9, 'max_bin': 293, 'num_leaves': 627}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:48:23,987]\u001b[0m Trial 79 finished with value: 0.6751879295135363 and parameters: {'n_estimators': 674, 'learning_rate': 0.028047871099950793, 'max_depth': 10, 'max_bin': 285, 'num_leaves': 662}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:48:29,724]\u001b[0m Trial 80 finished with value: 0.6808967789708766 and parameters: {'n_estimators': 854, 'learning_rate': 0.09029023947450479, 'max_depth': 10, 'max_bin': 297, 'num_leaves': 406}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 05:48:34,912]\u001b[0m Trial 81 finished with value: 0.6807255681661799 and parameters: {'n_estimators': 883, 'learning_rate': 0.10310506794850463, 'max_depth': 10, 'max_bin': 299, 'num_leaves': 371}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:48:40,550]\u001b[0m Trial 82 finished with value: 0.6817487576121151 and parameters: {'n_estimators': 869, 'learning_rate': 0.10258563285401393, 'max_depth': 9, 'max_bin': 300, 'num_leaves': 367}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:48:45,366]\u001b[0m Trial 83 finished with value: 0.6778641766858182 and parameters: {'n_estimators': 866, 'learning_rate': 0.1023312987826321, 'max_depth': 9, 'max_bin': 300, 'num_leaves': 373}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:48:51,749]\u001b[0m Trial 84 finished with value: 0.678467936930477 and parameters: {'n_estimators': 900, 'learning_rate': 0.09002709155753047, 'max_depth': 10, 'max_bin': 298, 'num_leaves': 299}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:48:58,357]\u001b[0m Trial 85 finished with value: 0.6757906268927844 and parameters: {'n_estimators': 840, 'learning_rate': 0.07101084787526876, 'max_depth': 9, 'max_bin': 293, 'num_leaves': 400}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:04,051]\u001b[0m Trial 86 finished with value: 0.678111945774924 and parameters: {'n_estimators': 871, 'learning_rate': 0.09218973991880824, 'max_depth': 8, 'max_bin': 300, 'num_leaves': 321}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:09,600]\u001b[0m Trial 87 finished with value: 0.6782048322482277 and parameters: {'n_estimators': 815, 'learning_rate': 0.08256247393444877, 'max_depth': 10, 'max_bin': 289, 'num_leaves': 366}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:13,904]\u001b[0m Trial 88 finished with value: 0.6780463247816817 and parameters: {'n_estimators': 833, 'learning_rate': 0.11974932437544812, 'max_depth': 8, 'max_bin': 284, 'num_leaves': 420}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:19,003]\u001b[0m Trial 89 finished with value: 0.6763656777995232 and parameters: {'n_estimators': 843, 'learning_rate': 0.11082248724465897, 'max_depth': 9, 'max_bin': 296, 'num_leaves': 399}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:26,487]\u001b[0m Trial 90 finished with value: 0.6752828891895704 and parameters: {'n_estimators': 787, 'learning_rate': 0.05992259785067193, 'max_depth': 10, 'max_bin': 290, 'num_leaves': 301}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:32,175]\u001b[0m Trial 91 finished with value: 0.6839650785944617 and parameters: {'n_estimators': 884, 'learning_rate': 0.10338143226659816, 'max_depth': 10, 'max_bin': 294, 'num_leaves': 488}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:37,600]\u001b[0m Trial 92 finished with value: 0.677080765799202 and parameters: {'n_estimators': 880, 'learning_rate': 0.10150909803783825, 'max_depth': 10, 'max_bin': 297, 'num_leaves': 441}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:44,317]\u001b[0m Trial 93 finished with value: 0.679573967055005 and parameters: {'n_estimators': 886, 'learning_rate': 0.08648801224140204, 'max_depth': 10, 'max_bin': 293, 'num_leaves': 486}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:50,266]\u001b[0m Trial 94 finished with value: 0.6801301674057807 and parameters: {'n_estimators': 886, 'learning_rate': 0.07654030824873803, 'max_depth': 10, 'max_bin': 293, 'num_leaves': 487}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:55,965]\u001b[0m Trial 95 finished with value: 0.6815344358971737 and parameters: {'n_estimators': 856, 'learning_rate': 0.08461772069822684, 'max_depth': 9, 'max_bin': 293, 'num_leaves': 480}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:50:02,550]\u001b[0m Trial 96 finished with value: 0.6799812282182753 and parameters: {'n_estimators': 855, 'learning_rate': 0.07707434360013168, 'max_depth': 9, 'max_bin': 293, 'num_leaves': 478}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:50:08,744]\u001b[0m Trial 97 finished with value: 0.6800721611280732 and parameters: {'n_estimators': 846, 'learning_rate': 0.0847739213420544, 'max_depth': 9, 'max_bin': 300, 'num_leaves': 479}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:50:14,423]\u001b[0m Trial 98 finished with value: 0.6778937762787917 and parameters: {'n_estimators': 858, 'learning_rate': 0.07609846738749558, 'max_depth': 9, 'max_bin': 299, 'num_leaves': 461}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:50:20,086]\u001b[0m Trial 99 finished with value: 0.6755553628321638 and parameters: {'n_estimators': 805, 'learning_rate': 0.08023447090866863, 'max_depth': 9, 'max_bin': 286, 'num_leaves': 430}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6840\n",
      "\tBest params:\n",
      "\t\tn_estimators: 779\n",
      "\t\tlearning_rate: 0.03857672323037514\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 285\n",
      "\t\tnum_leaves: 537\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.665298    0.676487\n",
      "1                    TP  377.000000  387.000000\n",
      "2                    TN  373.000000  343.000000\n",
      "3                    FP   71.000000   73.000000\n",
      "4                    FN   78.000000   96.000000\n",
      "5              Accuracy    0.834260    0.812013\n",
      "6             Precision    0.841518    0.841304\n",
      "7           Sensitivity    0.828571    0.801242\n",
      "8           Specificity    0.840100    0.824500\n",
      "9              F1 score    0.834994    0.820785\n",
      "10  F1 score (weighted)    0.834266    0.812249\n",
      "11     F1 score (macro)    0.834257    0.811562\n",
      "12    Balanced Accuracy    0.834331    0.812881\n",
      "13                  MCC    0.668615    0.624192\n",
      "14                  NPV    0.827100    0.781300\n",
      "15              ROC_AUC    0.834331    0.812881\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_1_cat = np.where((y_pred_lgbm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 05:50:26,078]\u001b[0m Trial 100 finished with value: 0.678936779818315 and parameters: {'n_estimators': 845, 'learning_rate': 0.0950801419258824, 'max_depth': 9, 'max_bin': 293, 'num_leaves': 411}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:50:31,654]\u001b[0m Trial 101 finished with value: 0.6783370886727816 and parameters: {'n_estimators': 891, 'learning_rate': 0.08588507406243572, 'max_depth': 8, 'max_bin': 293, 'num_leaves': 467}. Best is trial 76 with value: 0.6840252980239433.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:50:38,287]\u001b[0m Trial 102 finished with value: 0.6841701186532203 and parameters: {'n_estimators': 883, 'learning_rate': 0.07127269544987658, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 481}. Best is trial 102 with value: 0.6841701186532203.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:50:45,917]\u001b[0m Trial 103 finished with value: 0.6827542206253279 and parameters: {'n_estimators': 854, 'learning_rate': 0.07171381772988254, 'max_depth': 9, 'max_bin': 300, 'num_leaves': 449}. Best is trial 102 with value: 0.6841701186532203.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:50:52,514]\u001b[0m Trial 104 finished with value: 0.6822860168015639 and parameters: {'n_estimators': 818, 'learning_rate': 0.0697790859306481, 'max_depth': 9, 'max_bin': 299, 'num_leaves': 455}. Best is trial 102 with value: 0.6841701186532203.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:50:59,143]\u001b[0m Trial 105 finished with value: 0.6859009993773388 and parameters: {'n_estimators': 816, 'learning_rate': 0.066663294424514, 'max_depth': 10, 'max_bin': 297, 'num_leaves': 449}. Best is trial 105 with value: 0.6859009993773388.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:51:05,297]\u001b[0m Trial 106 finished with value: 0.6785685382178474 and parameters: {'n_estimators': 817, 'learning_rate': 0.064161114307195, 'max_depth': 7, 'max_bin': 297, 'num_leaves': 379}. Best is trial 105 with value: 0.6859009993773388.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:51:10,951]\u001b[0m Trial 107 finished with value: 0.6770462454125636 and parameters: {'n_estimators': 799, 'learning_rate': 0.06734671599954378, 'max_depth': 8, 'max_bin': 288, 'num_leaves': 347}. Best is trial 105 with value: 0.6859009993773388.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:51:19,150]\u001b[0m Trial 108 finished with value: 0.6856564034936776 and parameters: {'n_estimators': 770, 'learning_rate': 0.049352054796229075, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 443}. Best is trial 105 with value: 0.6859009993773388.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:51:26,572]\u001b[0m Trial 109 finished with value: 0.6873710208270969 and parameters: {'n_estimators': 775, 'learning_rate': 0.07179922820115858, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 453}. Best is trial 109 with value: 0.6873710208270969.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:51:33,937]\u001b[0m Trial 110 finished with value: 0.682763659121871 and parameters: {'n_estimators': 772, 'learning_rate': 0.048860633459388805, 'max_depth': 9, 'max_bin': 205, 'num_leaves': 446}. Best is trial 109 with value: 0.6873710208270969.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:51:41,339]\u001b[0m Trial 111 finished with value: 0.6786570971134614 and parameters: {'n_estimators': 776, 'learning_rate': 0.05293173854910095, 'max_depth': 9, 'max_bin': 197, 'num_leaves': 450}. Best is trial 109 with value: 0.6873710208270969.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:51:47,979]\u001b[0m Trial 112 finished with value: 0.6774284658813713 and parameters: {'n_estimators': 724, 'learning_rate': 0.060921545645888454, 'max_depth': 9, 'max_bin': 204, 'num_leaves': 432}. Best is trial 109 with value: 0.6873710208270969.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:51:56,536]\u001b[0m Trial 113 finished with value: 0.6824058760186346 and parameters: {'n_estimators': 825, 'learning_rate': 0.0475210402072183, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 448}. Best is trial 109 with value: 0.6873710208270969.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:52:03,761]\u001b[0m Trial 114 finished with value: 0.6714901399688488 and parameters: {'n_estimators': 768, 'learning_rate': 0.04019913314534918, 'max_depth': 8, 'max_bin': 208, 'num_leaves': 451}. Best is trial 109 with value: 0.6873710208270969.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:52:11,920]\u001b[0m Trial 115 finished with value: 0.6837509573576082 and parameters: {'n_estimators': 824, 'learning_rate': 0.047998068544653806, 'max_depth': 9, 'max_bin': 187, 'num_leaves': 499}. Best is trial 109 with value: 0.6873710208270969.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:52:18,701]\u001b[0m Trial 116 finished with value: 0.6879382205326637 and parameters: {'n_estimators': 788, 'learning_rate': 0.07004956473034307, 'max_depth': 10, 'max_bin': 190, 'num_leaves': 504}. Best is trial 116 with value: 0.6879382205326637.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:52:26,226]\u001b[0m Trial 117 finished with value: 0.6803664113824099 and parameters: {'n_estimators': 786, 'learning_rate': 0.051675675254540844, 'max_depth': 10, 'max_bin': 174, 'num_leaves': 505}. Best is trial 116 with value: 0.6879382205326637.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:52:35,102]\u001b[0m Trial 118 finished with value: 0.6846828964214877 and parameters: {'n_estimators': 815, 'learning_rate': 0.04829883747482344, 'max_depth': 10, 'max_bin': 192, 'num_leaves': 445}. Best is trial 116 with value: 0.6879382205326637.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:52:44,141]\u001b[0m Trial 119 finished with value: 0.6814041589979747 and parameters: {'n_estimators': 757, 'learning_rate': 0.03960605391467806, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 551}. Best is trial 116 with value: 0.6879382205326637.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:52:53,513]\u001b[0m Trial 120 finished with value: 0.6874267045273725 and parameters: {'n_estimators': 799, 'learning_rate': 0.04812935363967228, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 426}. Best is trial 116 with value: 0.6879382205326637.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:53:03,848]\u001b[0m Trial 121 finished with value: 0.6817787928166974 and parameters: {'n_estimators': 725, 'learning_rate': 0.03264971842612381, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 419}. Best is trial 116 with value: 0.6879382205326637.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:53:11,426]\u001b[0m Trial 122 finished with value: 0.6833297772881936 and parameters: {'n_estimators': 799, 'learning_rate': 0.0499419442607307, 'max_depth': 10, 'max_bin': 180, 'num_leaves': 443}. Best is trial 116 with value: 0.6879382205326637.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:53:20,170]\u001b[0m Trial 123 finished with value: 0.6893565917321014 and parameters: {'n_estimators': 799, 'learning_rate': 0.04811474628046, 'max_depth': 10, 'max_bin': 182, 'num_leaves': 496}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:53:28,774]\u001b[0m Trial 124 finished with value: 0.6832447668877373 and parameters: {'n_estimators': 792, 'learning_rate': 0.048243491088959534, 'max_depth': 10, 'max_bin': 180, 'num_leaves': 504}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:53:37,558]\u001b[0m Trial 125 finished with value: 0.6841627245790324 and parameters: {'n_estimators': 797, 'learning_rate': 0.04636350087387421, 'max_depth': 10, 'max_bin': 182, 'num_leaves': 500}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:53:46,799]\u001b[0m Trial 126 finished with value: 0.6816824775610182 and parameters: {'n_estimators': 804, 'learning_rate': 0.03793669406298678, 'max_depth': 10, 'max_bin': 181, 'num_leaves': 463}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:53:57,788]\u001b[0m Trial 127 finished with value: 0.6767105859331259 and parameters: {'n_estimators': 736, 'learning_rate': 0.028025462281839893, 'max_depth': 10, 'max_bin': 169, 'num_leaves': 500}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:54:04,360]\u001b[0m Trial 128 finished with value: 0.679512770680769 and parameters: {'n_estimators': 829, 'learning_rate': 0.05528683537548111, 'max_depth': 10, 'max_bin': 185, 'num_leaves': 41}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:54:12,595]\u001b[0m Trial 129 finished with value: 0.6834824606978407 and parameters: {'n_estimators': 757, 'learning_rate': 0.044937409740104184, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 471}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:54:19,333]\u001b[0m Trial 130 finished with value: 0.6828301503415494 and parameters: {'n_estimators': 689, 'learning_rate': 0.059627869667031444, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 512}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 05:54:28,697]\u001b[0m Trial 131 finished with value: 0.6834961633919445 and parameters: {'n_estimators': 756, 'learning_rate': 0.04314206528517416, 'max_depth': 10, 'max_bin': 176, 'num_leaves': 475}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:54:36,495]\u001b[0m Trial 132 finished with value: 0.6749993232942944 and parameters: {'n_estimators': 752, 'learning_rate': 0.04229785364738951, 'max_depth': 10, 'max_bin': 174, 'num_leaves': 491}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:54:48,857]\u001b[0m Trial 133 finished with value: 0.6774350731475556 and parameters: {'n_estimators': 767, 'learning_rate': 0.02437459548001957, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 471}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:54:56,497]\u001b[0m Trial 134 finished with value: 0.6816701131860639 and parameters: {'n_estimators': 783, 'learning_rate': 0.05643629268195372, 'max_depth': 10, 'max_bin': 184, 'num_leaves': 539}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:55:04,885]\u001b[0m Trial 135 finished with value: 0.6846220774253234 and parameters: {'n_estimators': 744, 'learning_rate': 0.04488395927970178, 'max_depth': 10, 'max_bin': 168, 'num_leaves': 521}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:55:14,268]\u001b[0m Trial 136 finished with value: 0.6790328364997243 and parameters: {'n_estimators': 813, 'learning_rate': 0.03454666171151569, 'max_depth': 10, 'max_bin': 172, 'num_leaves': 524}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:55:21,927]\u001b[0m Trial 137 finished with value: 0.6855574161099883 and parameters: {'n_estimators': 701, 'learning_rate': 0.06379338511393311, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 553}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:55:28,972]\u001b[0m Trial 138 finished with value: 0.6889742637531754 and parameters: {'n_estimators': 714, 'learning_rate': 0.06638515083420461, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 543}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:55:35,615]\u001b[0m Trial 139 finished with value: 0.6871212772147122 and parameters: {'n_estimators': 705, 'learning_rate': 0.06446881241673035, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 585}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:55:43,189]\u001b[0m Trial 140 finished with value: 0.6837359054468636 and parameters: {'n_estimators': 706, 'learning_rate': 0.06139748530251786, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 543}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:55:50,251]\u001b[0m Trial 141 finished with value: 0.6837718743817056 and parameters: {'n_estimators': 722, 'learning_rate': 0.06717315260653114, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 558}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:55:57,650]\u001b[0m Trial 142 finished with value: 0.6841306363426392 and parameters: {'n_estimators': 678, 'learning_rate': 0.0729272845105556, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 576}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:56:04,558]\u001b[0m Trial 143 finished with value: 0.6865080003963826 and parameters: {'n_estimators': 647, 'learning_rate': 0.0731861326262469, 'max_depth': 11, 'max_bin': 156, 'num_leaves': 581}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:56:10,467]\u001b[0m Trial 144 finished with value: 0.6846315364270078 and parameters: {'n_estimators': 677, 'learning_rate': 0.07391004800834002, 'max_depth': 11, 'max_bin': 152, 'num_leaves': 585}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:56:17,814]\u001b[0m Trial 145 finished with value: 0.6877013787228614 and parameters: {'n_estimators': 645, 'learning_rate': 0.0644684349728765, 'max_depth': 11, 'max_bin': 152, 'num_leaves': 589}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:56:24,550]\u001b[0m Trial 146 finished with value: 0.6842318680189003 and parameters: {'n_estimators': 622, 'learning_rate': 0.06401943314235457, 'max_depth': 11, 'max_bin': 152, 'num_leaves': 589}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:56:31,430]\u001b[0m Trial 147 finished with value: 0.68308398805121 and parameters: {'n_estimators': 632, 'learning_rate': 0.06260147022392809, 'max_depth': 11, 'max_bin': 150, 'num_leaves': 571}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:56:38,329]\u001b[0m Trial 148 finished with value: 0.681977184500228 and parameters: {'n_estimators': 593, 'learning_rate': 0.06617180939852232, 'max_depth': 11, 'max_bin': 155, 'num_leaves': 596}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:56:46,087]\u001b[0m Trial 149 finished with value: 0.6865487508242174 and parameters: {'n_estimators': 649, 'learning_rate': 0.0570319870175049, 'max_depth': 11, 'max_bin': 154, 'num_leaves': 586}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6894\n",
      "\tBest params:\n",
      "\t\tn_estimators: 799\n",
      "\t\tlearning_rate: 0.04811474628046\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 182\n",
      "\t\tnum_leaves: 496\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.665298    0.676487    0.697708\n",
      "1                    TP  377.000000  387.000000  413.000000\n",
      "2                    TN  373.000000  343.000000  342.000000\n",
      "3                    FP   71.000000   73.000000   81.000000\n",
      "4                    FN   78.000000   96.000000   63.000000\n",
      "5              Accuracy    0.834260    0.812013    0.839822\n",
      "6             Precision    0.841518    0.841304    0.836032\n",
      "7           Sensitivity    0.828571    0.801242    0.867647\n",
      "8           Specificity    0.840100    0.824500    0.808500\n",
      "9              F1 score    0.834994    0.820785    0.851546\n",
      "10  F1 score (weighted)    0.834266    0.812249    0.839567\n",
      "11     F1 score (macro)    0.834257    0.811562    0.838817\n",
      "12    Balanced Accuracy    0.834331    0.812881    0.838079\n",
      "13                  MCC    0.668615    0.624192    0.678314\n",
      "14                  NPV    0.827100    0.781300    0.844400\n",
      "15              ROC_AUC    0.834331    0.812881    0.838079\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_2_cat = np.where((y_pred_lgbm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 05:56:55,651]\u001b[0m Trial 150 finished with value: 0.6851436982809034 and parameters: {'n_estimators': 654, 'learning_rate': 0.055478623271072766, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 608}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:57:04,584]\u001b[0m Trial 151 finished with value: 0.6861996405524856 and parameters: {'n_estimators': 654, 'learning_rate': 0.05485774583312016, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 582}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:57:12,659]\u001b[0m Trial 152 finished with value: 0.6846643320602197 and parameters: {'n_estimators': 644, 'learning_rate': 0.055605895262674156, 'max_depth': 11, 'max_bin': 155, 'num_leaves': 599}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:57:20,782]\u001b[0m Trial 153 finished with value: 0.686209488163444 and parameters: {'n_estimators': 654, 'learning_rate': 0.05427093732221989, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 609}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:57:27,923]\u001b[0m Trial 154 finished with value: 0.683694277521256 and parameters: {'n_estimators': 646, 'learning_rate': 0.058161606158327876, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 636}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:57:36,274]\u001b[0m Trial 155 finished with value: 0.6851123420566865 and parameters: {'n_estimators': 608, 'learning_rate': 0.053129497055721, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 553}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:57:43,854]\u001b[0m Trial 156 finished with value: 0.6822126643910686 and parameters: {'n_estimators': 608, 'learning_rate': 0.05493466685673721, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 604}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:57:51,591]\u001b[0m Trial 157 finished with value: 0.6875179104128916 and parameters: {'n_estimators': 669, 'learning_rate': 0.06696848603243598, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 557}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:57:58,887]\u001b[0m Trial 158 finished with value: 0.6849431954116403 and parameters: {'n_estimators': 665, 'learning_rate': 0.06823537819898355, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 618}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:58:07,747]\u001b[0m Trial 159 finished with value: 0.6888985464768984 and parameters: {'n_estimators': 685, 'learning_rate': 0.05970429693114719, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 580}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:58:14,952]\u001b[0m Trial 160 finished with value: 0.6853077524341378 and parameters: {'n_estimators': 701, 'learning_rate': 0.0627631773337832, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 573}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:58:22,308]\u001b[0m Trial 161 finished with value: 0.6876493272043159 and parameters: {'n_estimators': 696, 'learning_rate': 0.06300411446029378, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 584}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:58:29,525]\u001b[0m Trial 162 finished with value: 0.6838236266943264 and parameters: {'n_estimators': 692, 'learning_rate': 0.058744985537128584, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 584}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:58:36,990]\u001b[0m Trial 163 finished with value: 0.683304210434316 and parameters: {'n_estimators': 636, 'learning_rate': 0.06631600265291648, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 558}. Best is trial 123 with value: 0.6893565917321014.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:58:44,562]\u001b[0m Trial 164 finished with value: 0.6896201597307303 and parameters: {'n_estimators': 670, 'learning_rate': 0.06898342563248124, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 567}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:58:51,555]\u001b[0m Trial 165 finished with value: 0.6850299611224957 and parameters: {'n_estimators': 661, 'learning_rate': 0.08019909621868611, 'max_depth': 11, 'max_bin': 202, 'num_leaves': 584}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:58:58,593]\u001b[0m Trial 166 finished with value: 0.686147274894327 and parameters: {'n_estimators': 576, 'learning_rate': 0.06824503323815764, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 613}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:59:03,407]\u001b[0m Trial 167 finished with value: 0.6717445563490585 and parameters: {'n_estimators': 576, 'learning_rate': 0.07019804681143393, 'max_depth': 6, 'max_bin': 216, 'num_leaves': 614}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:59:10,342]\u001b[0m Trial 168 finished with value: 0.6852180837367449 and parameters: {'n_estimators': 676, 'learning_rate': 0.07484558043257201, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 654}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:59:13,638]\u001b[0m Trial 169 finished with value: 0.6189516885999857 and parameters: {'n_estimators': 619, 'learning_rate': 0.06852167934347142, 'max_depth': 3, 'max_bin': 223, 'num_leaves': 637}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:59:21,988]\u001b[0m Trial 170 finished with value: 0.6873843301528415 and parameters: {'n_estimators': 712, 'learning_rate': 0.05940458120556516, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 567}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:59:29,358]\u001b[0m Trial 171 finished with value: 0.6856717511253041 and parameters: {'n_estimators': 711, 'learning_rate': 0.062065747016441616, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 567}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:59:37,935]\u001b[0m Trial 172 finished with value: 0.6865685226003164 and parameters: {'n_estimators': 681, 'learning_rate': 0.059180250170689355, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 587}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:59:47,175]\u001b[0m Trial 173 finished with value: 0.6885777630282954 and parameters: {'n_estimators': 686, 'learning_rate': 0.05910782559033724, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 587}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:59:55,485]\u001b[0m Trial 174 finished with value: 0.6875319338328538 and parameters: {'n_estimators': 680, 'learning_rate': 0.05776540462297111, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 595}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:00:03,769]\u001b[0m Trial 175 finished with value: 0.6846991413904923 and parameters: {'n_estimators': 687, 'learning_rate': 0.05851173108061819, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 598}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:00:12,072]\u001b[0m Trial 176 finished with value: 0.6866903419175321 and parameters: {'n_estimators': 716, 'learning_rate': 0.05983487245893851, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 568}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:00:17,465]\u001b[0m Trial 177 finished with value: 0.6629554632987733 and parameters: {'n_estimators': 726, 'learning_rate': 0.06003696802390154, 'max_depth': 5, 'max_bin': 199, 'num_leaves': 569}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:00:24,366]\u001b[0m Trial 178 finished with value: 0.6874449035748372 and parameters: {'n_estimators': 713, 'learning_rate': 0.07348422976338036, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 545}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:00:32,047]\u001b[0m Trial 179 finished with value: 0.6889748843342316 and parameters: {'n_estimators': 711, 'learning_rate': 0.06365249348236145, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 543}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:00:38,372]\u001b[0m Trial 180 finished with value: 0.6823524244717877 and parameters: {'n_estimators': 714, 'learning_rate': 0.0777911340042067, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 549}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:00:46,776]\u001b[0m Trial 181 finished with value: 0.686876963489185 and parameters: {'n_estimators': 689, 'learning_rate': 0.063175703168476, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 534}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:00:54,603]\u001b[0m Trial 182 finished with value: 0.6855797458181394 and parameters: {'n_estimators': 689, 'learning_rate': 0.06374672100499543, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 532}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:01:02,681]\u001b[0m Trial 183 finished with value: 0.6880421790929836 and parameters: {'n_estimators': 706, 'learning_rate': 0.06097883250558945, 'max_depth': 12, 'max_bin': 198, 'num_leaves': 530}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:01:09,927]\u001b[0m Trial 184 finished with value: 0.6825622527080196 and parameters: {'n_estimators': 708, 'learning_rate': 0.06395702494684449, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 541}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:01:17,725]\u001b[0m Trial 185 finished with value: 0.68692770049226 and parameters: {'n_estimators': 736, 'learning_rate': 0.07031971029900902, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 529}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:01:25,193]\u001b[0m Trial 186 finished with value: 0.6866795367281162 and parameters: {'n_estimators': 731, 'learning_rate': 0.07360854069891104, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 527}. Best is trial 164 with value: 0.6896201597307303.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:01:33,096]\u001b[0m Trial 187 finished with value: 0.6901569662717765 and parameters: {'n_estimators': 699, 'learning_rate': 0.06968993674967647, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 556}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:01:39,398]\u001b[0m Trial 188 finished with value: 0.6840996327557805 and parameters: {'n_estimators': 737, 'learning_rate': 0.06995281985860059, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 556}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:01:47,166]\u001b[0m Trial 189 finished with value: 0.6891819310156462 and parameters: {'n_estimators': 669, 'learning_rate': 0.07857344806033223, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 541}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:01:54,184]\u001b[0m Trial 190 finished with value: 0.6789734617561406 and parameters: {'n_estimators': 673, 'learning_rate': 0.07782782993894512, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 546}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:02:00,755]\u001b[0m Trial 191 finished with value: 0.6830464121808624 and parameters: {'n_estimators': 692, 'learning_rate': 0.07096616912749999, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 514}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:02:08,784]\u001b[0m Trial 192 finished with value: 0.6839158697470893 and parameters: {'n_estimators': 707, 'learning_rate': 0.06882998901756901, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 562}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:02:16,461]\u001b[0m Trial 193 finished with value: 0.6835930932183467 and parameters: {'n_estimators': 739, 'learning_rate': 0.06631476180302293, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 103}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:02:23,372]\u001b[0m Trial 194 finished with value: 0.6854890054917265 and parameters: {'n_estimators': 669, 'learning_rate': 0.07444322035957242, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 523}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:02:31,102]\u001b[0m Trial 195 finished with value: 0.6850031663679494 and parameters: {'n_estimators': 704, 'learning_rate': 0.06598526750905036, 'max_depth': 12, 'max_bin': 200, 'num_leaves': 540}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:02:37,969]\u001b[0m Trial 196 finished with value: 0.6871584235104221 and parameters: {'n_estimators': 722, 'learning_rate': 0.08054743527609427, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 556}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:02:44,207]\u001b[0m Trial 197 finished with value: 0.6835610599668993 and parameters: {'n_estimators': 671, 'learning_rate': 0.08069307009501578, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 556}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:02:50,740]\u001b[0m Trial 198 finished with value: 0.6829868302286187 and parameters: {'n_estimators': 724, 'learning_rate': 0.07683015673416546, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 567}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:02:58,601]\u001b[0m Trial 199 finished with value: 0.6856230736402168 and parameters: {'n_estimators': 697, 'learning_rate': 0.051714680666258915, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 548}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6902\n",
      "\tBest params:\n",
      "\t\tn_estimators: 699\n",
      "\t\tlearning_rate: 0.06968993674967647\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 202\n",
      "\t\tnum_leaves: 556\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.665298    0.676487    0.697708    0.684076\n",
      "1                    TP  377.000000  387.000000  413.000000  387.000000\n",
      "2                    TN  373.000000  343.000000  342.000000  354.000000\n",
      "3                    FP   71.000000   73.000000   81.000000   72.000000\n",
      "4                    FN   78.000000   96.000000   63.000000   86.000000\n",
      "5              Accuracy    0.834260    0.812013    0.839822    0.824249\n",
      "6             Precision    0.841518    0.841304    0.836032    0.843137\n",
      "7           Sensitivity    0.828571    0.801242    0.867647    0.818182\n",
      "8           Specificity    0.840100    0.824500    0.808500    0.831000\n",
      "9              F1 score    0.834994    0.820785    0.851546    0.830472\n",
      "10  F1 score (weighted)    0.834266    0.812249    0.839567    0.824350\n",
      "11     F1 score (macro)    0.834257    0.811562    0.838817    0.824012\n",
      "12    Balanced Accuracy    0.834331    0.812881    0.838079    0.824584\n",
      "13                  MCC    0.668615    0.624192    0.678314    0.648425\n",
      "14                  NPV    0.827100    0.781300    0.844400    0.804500\n",
      "15              ROC_AUC    0.834331    0.812881    0.838079    0.824584\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_3_cat = np.where((y_pred_lgbm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:03:07,028]\u001b[0m Trial 200 finished with value: 0.6859511497570535 and parameters: {'n_estimators': 706, 'learning_rate': 0.06132227670711708, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 571}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:03:13,420]\u001b[0m Trial 201 finished with value: 0.6837037697988789 and parameters: {'n_estimators': 728, 'learning_rate': 0.07230189886585353, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 517}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:03:18,411]\u001b[0m Trial 202 finished with value: 0.6824376987867623 and parameters: {'n_estimators': 744, 'learning_rate': 0.08231454582755346, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 540}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:03:25,469]\u001b[0m Trial 203 finished with value: 0.6822665192909122 and parameters: {'n_estimators': 751, 'learning_rate': 0.07004056698323881, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 527}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:03:32,452]\u001b[0m Trial 204 finished with value: 0.6815204898991539 and parameters: {'n_estimators': 684, 'learning_rate': 0.06589144231702164, 'max_depth': 12, 'max_bin': 190, 'num_leaves': 557}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:03:39,331]\u001b[0m Trial 205 finished with value: 0.6814072007355076 and parameters: {'n_estimators': 719, 'learning_rate': 0.0728965632003636, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 598}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:03:46,889]\u001b[0m Trial 206 finished with value: 0.6807184962606619 and parameters: {'n_estimators': 666, 'learning_rate': 0.062477867023226996, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 577}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:03:53,195]\u001b[0m Trial 207 finished with value: 0.6803177611966824 and parameters: {'n_estimators': 696, 'learning_rate': 0.07676840338797161, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 509}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:04:00,243]\u001b[0m Trial 208 finished with value: 0.6819975815314835 and parameters: {'n_estimators': 738, 'learning_rate': 0.05768738353767187, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 544}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:04:07,541]\u001b[0m Trial 209 finished with value: 0.6833538529989666 and parameters: {'n_estimators': 761, 'learning_rate': 0.06921670909436459, 'max_depth': 11, 'max_bin': 193, 'num_leaves': 564}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:04:15,384]\u001b[0m Trial 210 finished with value: 0.6829230778949581 and parameters: {'n_estimators': 714, 'learning_rate': 0.06550201564451923, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 587}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:04:22,964]\u001b[0m Trial 211 finished with value: 0.6860142244254117 and parameters: {'n_estimators': 682, 'learning_rate': 0.062450397260547595, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 541}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:04:31,825]\u001b[0m Trial 212 finished with value: 0.6865623573890106 and parameters: {'n_estimators': 692, 'learning_rate': 0.0517112335921516, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 529}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:04:38,866]\u001b[0m Trial 213 finished with value: 0.6824164121179679 and parameters: {'n_estimators': 662, 'learning_rate': 0.06544063861666884, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 222}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:04:46,333]\u001b[0m Trial 214 finished with value: 0.6838570272004485 and parameters: {'n_estimators': 698, 'learning_rate': 0.05904073978554592, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 533}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:04:53,330]\u001b[0m Trial 215 finished with value: 0.6806275842938362 and parameters: {'n_estimators': 715, 'learning_rate': 0.07168393996847852, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 557}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:05:01,537]\u001b[0m Trial 216 finished with value: 0.6875685932122548 and parameters: {'n_estimators': 675, 'learning_rate': 0.061983935436702955, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 575}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:05:09,981]\u001b[0m Trial 217 finished with value: 0.687495657953875 and parameters: {'n_estimators': 672, 'learning_rate': 0.05300495867635166, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 597}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:05:16,953]\u001b[0m Trial 218 finished with value: 0.6816845716557713 and parameters: {'n_estimators': 668, 'learning_rate': 0.052271565632253957, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 596}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:05:25,188]\u001b[0m Trial 219 finished with value: 0.6849936421637375 and parameters: {'n_estimators': 630, 'learning_rate': 0.055194328474252, 'max_depth': 11, 'max_bin': 195, 'num_leaves': 582}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:05:32,396]\u001b[0m Trial 220 finished with value: 0.6869742246796552 and parameters: {'n_estimators': 639, 'learning_rate': 0.05845479947848237, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 599}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:05:39,000]\u001b[0m Trial 221 finished with value: 0.684685173224097 and parameters: {'n_estimators': 658, 'learning_rate': 0.05763124495147129, 'max_depth': 11, 'max_bin': 186, 'num_leaves': 601}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:05:46,746]\u001b[0m Trial 222 finished with value: 0.6839214997059059 and parameters: {'n_estimators': 643, 'learning_rate': 0.06082162509549331, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 618}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:05:54,788]\u001b[0m Trial 223 finished with value: 0.6858900634198204 and parameters: {'n_estimators': 684, 'learning_rate': 0.05132214821031509, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 571}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:06:01,511]\u001b[0m Trial 224 finished with value: 0.6856440852942992 and parameters: {'n_estimators': 676, 'learning_rate': 0.05671272080748031, 'max_depth': 11, 'max_bin': 191, 'num_leaves': 592}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:06:07,515]\u001b[0m Trial 225 finished with value: 0.6785640347873302 and parameters: {'n_estimators': 638, 'learning_rate': 0.06496453692707128, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 576}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:06:15,570]\u001b[0m Trial 226 finished with value: 0.6826527046665875 and parameters: {'n_estimators': 702, 'learning_rate': 0.060820406236730074, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 623}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:06:22,918]\u001b[0m Trial 227 finished with value: 0.679973009917577 and parameters: {'n_estimators': 664, 'learning_rate': 0.06757484443664835, 'max_depth': 11, 'max_bin': 177, 'num_leaves': 598}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:06:32,575]\u001b[0m Trial 228 finished with value: 0.6855118315546233 and parameters: {'n_estimators': 688, 'learning_rate': 0.048202203121574194, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 559}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:06:39,990]\u001b[0m Trial 229 finished with value: 0.6824120372297916 and parameters: {'n_estimators': 710, 'learning_rate': 0.056383434703103384, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 578}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:06:46,587]\u001b[0m Trial 230 finished with value: 0.6815706552607411 and parameters: {'n_estimators': 675, 'learning_rate': 0.06352996493598209, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 606}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:06:53,128]\u001b[0m Trial 231 finished with value: 0.6830378907152049 and parameters: {'n_estimators': 722, 'learning_rate': 0.0697363152548285, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 552}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:06:59,938]\u001b[0m Trial 232 finished with value: 0.6839093321611405 and parameters: {'n_estimators': 728, 'learning_rate': 0.07282334478957699, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 562}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:07:05,670]\u001b[0m Trial 233 finished with value: 0.6796782359709684 and parameters: {'n_estimators': 785, 'learning_rate': 0.07894694088532102, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 514}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:07:11,404]\u001b[0m Trial 234 finished with value: 0.6786854641262726 and parameters: {'n_estimators': 740, 'learning_rate': 0.06885502894635648, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 591}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:07:18,627]\u001b[0m Trial 235 finished with value: 0.6840690182350642 and parameters: {'n_estimators': 698, 'learning_rate': 0.06047022873675269, 'max_depth': 11, 'max_bin': 150, 'num_leaves': 575}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:07:25,789]\u001b[0m Trial 236 finished with value: 0.6833912431927829 and parameters: {'n_estimators': 654, 'learning_rate': 0.06574074007780917, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 548}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:07:32,520]\u001b[0m Trial 237 finished with value: 0.6833900656961307 and parameters: {'n_estimators': 714, 'learning_rate': 0.07455383682928216, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 527}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:07:40,287]\u001b[0m Trial 238 finished with value: 0.6827547971891367 and parameters: {'n_estimators': 768, 'learning_rate': 0.052450278597363204, 'max_depth': 11, 'max_bin': 171, 'num_leaves': 567}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:07:47,738]\u001b[0m Trial 239 finished with value: 0.6822453002654285 and parameters: {'n_estimators': 674, 'learning_rate': 0.06187105549397685, 'max_depth': 11, 'max_bin': 192, 'num_leaves': 546}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:07:54,678]\u001b[0m Trial 240 finished with value: 0.684635035123737 and parameters: {'n_estimators': 692, 'learning_rate': 0.06913475944376449, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 589}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:08:02,256]\u001b[0m Trial 241 finished with value: 0.6834260796384999 and parameters: {'n_estimators': 682, 'learning_rate': 0.06411178072610454, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 534}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:08:09,643]\u001b[0m Trial 242 finished with value: 0.6836622592531085 and parameters: {'n_estimators': 699, 'learning_rate': 0.05744282305859716, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 534}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:08:16,641]\u001b[0m Trial 243 finished with value: 0.6827800722257381 and parameters: {'n_estimators': 688, 'learning_rate': 0.06225263041271713, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 516}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:08:22,562]\u001b[0m Trial 244 finished with value: 0.6791517083065101 and parameters: {'n_estimators': 731, 'learning_rate': 0.06634717064879707, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 560}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:08:30,366]\u001b[0m Trial 245 finished with value: 0.6873533914137262 and parameters: {'n_estimators': 715, 'learning_rate': 0.05401639140923084, 'max_depth': 11, 'max_bin': 163, 'num_leaves': 578}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:08:39,412]\u001b[0m Trial 246 finished with value: 0.686079449944929 and parameters: {'n_estimators': 714, 'learning_rate': 0.05356923714710489, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 580}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:08:48,354]\u001b[0m Trial 247 finished with value: 0.6892023924407811 and parameters: {'n_estimators': 737, 'learning_rate': 0.04818507590922007, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 609}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:08:57,311]\u001b[0m Trial 248 finished with value: 0.6862918496813196 and parameters: {'n_estimators': 758, 'learning_rate': 0.046040142997155635, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 610}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:09:05,907]\u001b[0m Trial 249 finished with value: 0.6869119579142298 and parameters: {'n_estimators': 665, 'learning_rate': 0.04789274070159237, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 636}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6902\n",
      "\tBest params:\n",
      "\t\tn_estimators: 699\n",
      "\t\tlearning_rate: 0.06968993674967647\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 202\n",
      "\t\tnum_leaves: 556\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665298    0.676487    0.697708    0.684076   \n",
      "1                    TP  377.000000  387.000000  413.000000  387.000000   \n",
      "2                    TN  373.000000  343.000000  342.000000  354.000000   \n",
      "3                    FP   71.000000   73.000000   81.000000   72.000000   \n",
      "4                    FN   78.000000   96.000000   63.000000   86.000000   \n",
      "5              Accuracy    0.834260    0.812013    0.839822    0.824249   \n",
      "6             Precision    0.841518    0.841304    0.836032    0.843137   \n",
      "7           Sensitivity    0.828571    0.801242    0.867647    0.818182   \n",
      "8           Specificity    0.840100    0.824500    0.808500    0.831000   \n",
      "9              F1 score    0.834994    0.820785    0.851546    0.830472   \n",
      "10  F1 score (weighted)    0.834266    0.812249    0.839567    0.824350   \n",
      "11     F1 score (macro)    0.834257    0.811562    0.838817    0.824012   \n",
      "12    Balanced Accuracy    0.834331    0.812881    0.838079    0.824584   \n",
      "13                  MCC    0.668615    0.624192    0.678314    0.648425   \n",
      "14                  NPV    0.827100    0.781300    0.844400    0.804500   \n",
      "15              ROC_AUC    0.834331    0.812881    0.838079    0.824584   \n",
      "\n",
      "          Set4  \n",
      "0     0.650299  \n",
      "1   385.000000  \n",
      "2   357.000000  \n",
      "3    73.000000  \n",
      "4    84.000000  \n",
      "5     0.825362  \n",
      "6     0.840611  \n",
      "7     0.820896  \n",
      "8     0.830200  \n",
      "9     0.830636  \n",
      "10    0.825428  \n",
      "11    0.825192  \n",
      "12    0.825564  \n",
      "13    0.650631  \n",
      "14    0.809500  \n",
      "15    0.825564  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_4_cat = np.where((y_pred_lgbm_4 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:09:14,650]\u001b[0m Trial 250 finished with value: 0.677804399818378 and parameters: {'n_estimators': 630, 'learning_rate': 0.05181769938267676, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 603}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:09:24,028]\u001b[0m Trial 251 finished with value: 0.6773765881707163 and parameters: {'n_estimators': 794, 'learning_rate': 0.04064060690112253, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 590}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:09:28,338]\u001b[0m Trial 252 finished with value: 0.6436174082170605 and parameters: {'n_estimators': 708, 'learning_rate': 0.05663994966726927, 'max_depth': 4, 'max_bin': 236, 'num_leaves': 574}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:09:36,315]\u001b[0m Trial 253 finished with value: 0.6768128191740658 and parameters: {'n_estimators': 650, 'learning_rate': 0.050303413037865154, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 626}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:09:43,199]\u001b[0m Trial 254 finished with value: 0.6764951838252471 and parameters: {'n_estimators': 746, 'learning_rate': 0.05846968548140006, 'max_depth': 11, 'max_bin': 185, 'num_leaves': 610}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:09:51,768]\u001b[0m Trial 255 finished with value: 0.6775588663699839 and parameters: {'n_estimators': 718, 'learning_rate': 0.045018418900437826, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 580}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:09:53,810]\u001b[0m Trial 256 finished with value: 0.6047498709426813 and parameters: {'n_estimators': 73, 'learning_rate': 0.053788166325162395, 'max_depth': 11, 'max_bin': 153, 'num_leaves': 562}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:10:00,632]\u001b[0m Trial 257 finished with value: 0.6735233503792029 and parameters: {'n_estimators': 699, 'learning_rate': 0.06113911076184884, 'max_depth': 11, 'max_bin': 189, 'num_leaves': 596}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:10:07,310]\u001b[0m Trial 258 finished with value: 0.6743283664059188 and parameters: {'n_estimators': 675, 'learning_rate': 0.07559050101701416, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 568}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:10:16,085]\u001b[0m Trial 259 finished with value: 0.6830094555801601 and parameters: {'n_estimators': 774, 'learning_rate': 0.05823046176662445, 'max_depth': 10, 'max_bin': 163, 'num_leaves': 547}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:10:25,026]\u001b[0m Trial 260 finished with value: 0.6791471743230485 and parameters: {'n_estimators': 658, 'learning_rate': 0.06473190884551427, 'max_depth': 11, 'max_bin': 193, 'num_leaves': 588}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:10:33,616]\u001b[0m Trial 261 finished with value: 0.6791272568866447 and parameters: {'n_estimators': 718, 'learning_rate': 0.05533346180695846, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 555}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:10:41,990]\u001b[0m Trial 262 finished with value: 0.6781318577726972 and parameters: {'n_estimators': 732, 'learning_rate': 0.05034938697390608, 'max_depth': 11, 'max_bin': 154, 'num_leaves': 614}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:10:47,702]\u001b[0m Trial 263 finished with value: 0.6712881551969181 and parameters: {'n_estimators': 681, 'learning_rate': 0.06763450501950366, 'max_depth': 7, 'max_bin': 167, 'num_leaves': 576}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:10:53,396]\u001b[0m Trial 264 finished with value: 0.6753557388901961 and parameters: {'n_estimators': 694, 'learning_rate': 0.08106797287729432, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 392}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:10:59,790]\u001b[0m Trial 265 finished with value: 0.6745151034108183 and parameters: {'n_estimators': 750, 'learning_rate': 0.07303924794370145, 'max_depth': 10, 'max_bin': 159, 'num_leaves': 603}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:11:07,278]\u001b[0m Trial 266 finished with value: 0.6770348640539501 and parameters: {'n_estimators': 707, 'learning_rate': 0.0601877645710075, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 570}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:11:14,978]\u001b[0m Trial 267 finished with value: 0.6755843952529323 and parameters: {'n_estimators': 639, 'learning_rate': 0.05386016015944531, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 550}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:11:21,873]\u001b[0m Trial 268 finished with value: 0.6781979259517666 and parameters: {'n_estimators': 667, 'learning_rate': 0.06718227216005083, 'max_depth': 11, 'max_bin': 183, 'num_leaves': 492}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:11:30,377]\u001b[0m Trial 269 finished with value: 0.6772575395626766 and parameters: {'n_estimators': 730, 'learning_rate': 0.04363165805987013, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 586}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:11:38,009]\u001b[0m Trial 270 finished with value: 0.6775862314851665 and parameters: {'n_estimators': 529, 'learning_rate': 0.06234347716781865, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 625}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:11:46,021]\u001b[0m Trial 271 finished with value: 0.6786561174454533 and parameters: {'n_estimators': 685, 'learning_rate': 0.07022792691339715, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 595}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:11:54,570]\u001b[0m Trial 272 finished with value: 0.6784550465648224 and parameters: {'n_estimators': 785, 'learning_rate': 0.05951698927810156, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 505}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:12:01,698]\u001b[0m Trial 273 finished with value: 0.6761731519549412 and parameters: {'n_estimators': 706, 'learning_rate': 0.07505521044625393, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 560}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:12:10,805]\u001b[0m Trial 274 finished with value: 0.6775567593831655 and parameters: {'n_estimators': 607, 'learning_rate': 0.04759165314568777, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 548}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:12:17,965]\u001b[0m Trial 275 finished with value: 0.6797040097677143 and parameters: {'n_estimators': 807, 'learning_rate': 0.06388299911510124, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 582}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:12:23,381]\u001b[0m Trial 276 finished with value: 0.676805017014461 and parameters: {'n_estimators': 655, 'learning_rate': 0.08380654282463476, 'max_depth': 10, 'max_bin': 226, 'num_leaves': 575}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:12:31,650]\u001b[0m Trial 277 finished with value: 0.6767542469566139 and parameters: {'n_estimators': 683, 'learning_rate': 0.05731115461824487, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 604}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:12:38,972]\u001b[0m Trial 278 finished with value: 0.6790709122838876 and parameters: {'n_estimators': 760, 'learning_rate': 0.07101247819803462, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 544}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:12:47,278]\u001b[0m Trial 279 finished with value: 0.6798912208587325 and parameters: {'n_estimators': 703, 'learning_rate': 0.05371458244662927, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 563}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:12:54,875]\u001b[0m Trial 280 finished with value: 0.6793422209464357 and parameters: {'n_estimators': 724, 'learning_rate': 0.07817884490540757, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 516}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:13:02,046]\u001b[0m Trial 281 finished with value: 0.6805036685253664 and parameters: {'n_estimators': 670, 'learning_rate': 0.06659917755936925, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 593}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:13:08,993]\u001b[0m Trial 282 finished with value: 0.6779102137144362 and parameters: {'n_estimators': 743, 'learning_rate': 0.06134235734776732, 'max_depth': 11, 'max_bin': 171, 'num_leaves': 621}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:13:18,347]\u001b[0m Trial 283 finished with value: 0.6764915277076049 and parameters: {'n_estimators': 695, 'learning_rate': 0.04955230082341185, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 535}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:13:27,142]\u001b[0m Trial 284 finished with value: 0.6789694481114931 and parameters: {'n_estimators': 620, 'learning_rate': 0.05654107302746379, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 564}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:13:34,930]\u001b[0m Trial 285 finished with value: 0.6751601915337887 and parameters: {'n_estimators': 643, 'learning_rate': 0.06612530439486004, 'max_depth': 12, 'max_bin': 177, 'num_leaves': 579}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:13:42,003]\u001b[0m Trial 286 finished with value: 0.6760305190876783 and parameters: {'n_estimators': 722, 'learning_rate': 0.07360076680657474, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 526}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:13:49,635]\u001b[0m Trial 287 finished with value: 0.6776750601289958 and parameters: {'n_estimators': 674, 'learning_rate': 0.06293973967101159, 'max_depth': 11, 'max_bin': 152, 'num_leaves': 642}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:13:56,722]\u001b[0m Trial 288 finished with value: 0.6800079391487083 and parameters: {'n_estimators': 709, 'learning_rate': 0.05853205838506222, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 431}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:14:03,790]\u001b[0m Trial 289 finished with value: 0.6758675241480239 and parameters: {'n_estimators': 769, 'learning_rate': 0.06935469794074979, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 553}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:14:13,246]\u001b[0m Trial 290 finished with value: 0.6797985834403348 and parameters: {'n_estimators': 801, 'learning_rate': 0.05263924667864929, 'max_depth': 11, 'max_bin': 155, 'num_leaves': 607}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:14:21,941]\u001b[0m Trial 291 finished with value: 0.6747118523090697 and parameters: {'n_estimators': 658, 'learning_rate': 0.04323464229384908, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 591}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:14:29,235]\u001b[0m Trial 292 finished with value: 0.6761682675747915 and parameters: {'n_estimators': 688, 'learning_rate': 0.0645006374039408, 'max_depth': 12, 'max_bin': 198, 'num_leaves': 409}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:14:35,357]\u001b[0m Trial 293 finished with value: 0.676523463963742 and parameters: {'n_estimators': 741, 'learning_rate': 0.08862639579810357, 'max_depth': 11, 'max_bin': 168, 'num_leaves': 570}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:14:42,360]\u001b[0m Trial 294 finished with value: 0.6776265274256643 and parameters: {'n_estimators': 719, 'learning_rate': 0.07797710581080905, 'max_depth': 11, 'max_bin': 180, 'num_leaves': 504}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:14:51,403]\u001b[0m Trial 295 finished with value: 0.6809030709915229 and parameters: {'n_estimators': 696, 'learning_rate': 0.055355400215922626, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 540}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:15:01,319]\u001b[0m Trial 296 finished with value: 0.6780342695657862 and parameters: {'n_estimators': 784, 'learning_rate': 0.037946109471954786, 'max_depth': 11, 'max_bin': 202, 'num_leaves': 576}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:15:08,527]\u001b[0m Trial 297 finished with value: 0.6791718353383723 and parameters: {'n_estimators': 671, 'learning_rate': 0.07175150408799001, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 558}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:15:16,348]\u001b[0m Trial 298 finished with value: 0.6807017407718321 and parameters: {'n_estimators': 637, 'learning_rate': 0.0594839494091725, 'max_depth': 11, 'max_bin': 187, 'num_leaves': 617}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:15:24,610]\u001b[0m Trial 299 finished with value: 0.6753215031428559 and parameters: {'n_estimators': 704, 'learning_rate': 0.06683307348482277, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 594}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6902\n",
      "\tBest params:\n",
      "\t\tn_estimators: 699\n",
      "\t\tlearning_rate: 0.06968993674967647\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 202\n",
      "\t\tnum_leaves: 556\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665298    0.676487    0.697708    0.684076   \n",
      "1                    TP  377.000000  387.000000  413.000000  387.000000   \n",
      "2                    TN  373.000000  343.000000  342.000000  354.000000   \n",
      "3                    FP   71.000000   73.000000   81.000000   72.000000   \n",
      "4                    FN   78.000000   96.000000   63.000000   86.000000   \n",
      "5              Accuracy    0.834260    0.812013    0.839822    0.824249   \n",
      "6             Precision    0.841518    0.841304    0.836032    0.843137   \n",
      "7           Sensitivity    0.828571    0.801242    0.867647    0.818182   \n",
      "8           Specificity    0.840100    0.824500    0.808500    0.831000   \n",
      "9              F1 score    0.834994    0.820785    0.851546    0.830472   \n",
      "10  F1 score (weighted)    0.834266    0.812249    0.839567    0.824350   \n",
      "11     F1 score (macro)    0.834257    0.811562    0.838817    0.824012   \n",
      "12    Balanced Accuracy    0.834331    0.812881    0.838079    0.824584   \n",
      "13                  MCC    0.668615    0.624192    0.678314    0.648425   \n",
      "14                  NPV    0.827100    0.781300    0.844400    0.804500   \n",
      "15              ROC_AUC    0.834331    0.812881    0.838079    0.824584   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.650299    0.681476  \n",
      "1   385.000000  392.000000  \n",
      "2   357.000000  360.000000  \n",
      "3    73.000000   63.000000  \n",
      "4    84.000000   84.000000  \n",
      "5     0.825362    0.836485  \n",
      "6     0.840611    0.861538  \n",
      "7     0.820896    0.823529  \n",
      "8     0.830200    0.851100  \n",
      "9     0.830636    0.842105  \n",
      "10    0.825428    0.836621  \n",
      "11    0.825192    0.836278  \n",
      "12    0.825564    0.837297  \n",
      "13    0.650631    0.673470  \n",
      "14    0.809500    0.810800  \n",
      "15    0.825564    0.837297  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_5_cat = np.where((y_pred_lgbm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:15:34,201]\u001b[0m Trial 300 finished with value: 0.6780569616169377 and parameters: {'n_estimators': 753, 'learning_rate': 0.04835416550079981, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 525}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:15:42,419]\u001b[0m Trial 301 finished with value: 0.6854957129633397 and parameters: {'n_estimators': 732, 'learning_rate': 0.06250988550007719, 'max_depth': 12, 'max_bin': 198, 'num_leaves': 545}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:15:49,566]\u001b[0m Trial 302 finished with value: 0.6823280493878654 and parameters: {'n_estimators': 681, 'learning_rate': 0.06878172281704575, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 582}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:15:57,679]\u001b[0m Trial 303 finished with value: 0.6803996690539166 and parameters: {'n_estimators': 662, 'learning_rate': 0.052555570476690475, 'max_depth': 11, 'max_bin': 150, 'num_leaves': 561}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:16:05,375]\u001b[0m Trial 304 finished with value: 0.6857610755359975 and parameters: {'n_estimators': 835, 'learning_rate': 0.059346144365141816, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 607}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:16:13,204]\u001b[0m Trial 305 finished with value: 0.6796409993256178 and parameters: {'n_estimators': 715, 'learning_rate': 0.07407988723095643, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 496}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:16:19,892]\u001b[0m Trial 306 finished with value: 0.6812800498624949 and parameters: {'n_estimators': 690, 'learning_rate': 0.06428187850388863, 'max_depth': 11, 'max_bin': 174, 'num_leaves': 570}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:16:23,588]\u001b[0m Trial 307 finished with value: 0.6718317876452591 and parameters: {'n_estimators': 654, 'learning_rate': 0.19977096726668914, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 538}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:16:30,778]\u001b[0m Trial 308 finished with value: 0.6749456621664575 and parameters: {'n_estimators': 731, 'learning_rate': 0.04565288718806123, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 590}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:16:37,426]\u001b[0m Trial 309 finished with value: 0.6802253719409616 and parameters: {'n_estimators': 700, 'learning_rate': 0.05532130950911521, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 520}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:16:43,240]\u001b[0m Trial 310 finished with value: 0.6781569982307655 and parameters: {'n_estimators': 681, 'learning_rate': 0.06967669284250395, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 552}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:16:47,429]\u001b[0m Trial 311 finished with value: 0.673233071112163 and parameters: {'n_estimators': 777, 'learning_rate': 0.19035775976713692, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 628}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:16:53,224]\u001b[0m Trial 312 finished with value: 0.6781864065544706 and parameters: {'n_estimators': 751, 'learning_rate': 0.0796385367224332, 'max_depth': 11, 'max_bin': 190, 'num_leaves': 600}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:17:00,956]\u001b[0m Trial 313 finished with value: 0.6802884270919579 and parameters: {'n_estimators': 719, 'learning_rate': 0.0499940373332542, 'max_depth': 12, 'max_bin': 195, 'num_leaves': 485}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:17:07,275]\u001b[0m Trial 314 finished with value: 0.6809320982114586 and parameters: {'n_estimators': 671, 'learning_rate': 0.05966750530093306, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 424}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:17:13,730]\u001b[0m Trial 315 finished with value: 0.6807201264457344 and parameters: {'n_estimators': 627, 'learning_rate': 0.06485575337743518, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 577}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:17:18,966]\u001b[0m Trial 316 finished with value: 0.6764256960607887 and parameters: {'n_estimators': 808, 'learning_rate': 0.07638548283252579, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 466}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:17:23,611]\u001b[0m Trial 317 finished with value: 0.6608128329107633 and parameters: {'n_estimators': 699, 'learning_rate': 0.056751031205666755, 'max_depth': 5, 'max_bin': 154, 'num_leaves': 555}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:17:29,088]\u001b[0m Trial 318 finished with value: 0.6785777049581259 and parameters: {'n_estimators': 647, 'learning_rate': 0.0718330873745826, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 567}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:17:35,522]\u001b[0m Trial 319 finished with value: 0.6827506642890269 and parameters: {'n_estimators': 485, 'learning_rate': 0.061814484611371553, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 539}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:17:41,339]\u001b[0m Trial 320 finished with value: 0.6811598851132207 and parameters: {'n_estimators': 714, 'learning_rate': 0.06676134683042867, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 580}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:17:47,186]\u001b[0m Trial 321 finished with value: 0.6789024749375987 and parameters: {'n_estimators': 685, 'learning_rate': 0.08604885944514075, 'max_depth': 12, 'max_bin': 199, 'num_leaves': 604}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:17:55,469]\u001b[0m Trial 322 finished with value: 0.6844051854625227 and parameters: {'n_estimators': 741, 'learning_rate': 0.05454032337516207, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 514}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:18:01,771]\u001b[0m Trial 323 finished with value: 0.6794204722769064 and parameters: {'n_estimators': 665, 'learning_rate': 0.062136975123049276, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 592}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:18:10,382]\u001b[0m Trial 324 finished with value: 0.6799359158009487 and parameters: {'n_estimators': 703, 'learning_rate': 0.041575589613642196, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 616}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:18:17,943]\u001b[0m Trial 325 finished with value: 0.681190838571559 and parameters: {'n_estimators': 725, 'learning_rate': 0.050126099730514476, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 561}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:18:23,857]\u001b[0m Trial 326 finished with value: 0.6758572579693081 and parameters: {'n_estimators': 797, 'learning_rate': 0.06828993755369776, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 546}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:18:28,545]\u001b[0m Trial 327 finished with value: 0.6767124980242822 and parameters: {'n_estimators': 227, 'learning_rate': 0.05862081918747441, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 584}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:18:34,842]\u001b[0m Trial 328 finished with value: 0.6792536576704882 and parameters: {'n_estimators': 769, 'learning_rate': 0.07189874197081292, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 536}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:18:40,668]\u001b[0m Trial 329 finished with value: 0.6787967690397569 and parameters: {'n_estimators': 684, 'learning_rate': 0.08223130442628342, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 575}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:18:46,401]\u001b[0m Trial 330 finished with value: 0.6765077303490075 and parameters: {'n_estimators': 658, 'learning_rate': 0.06463650481246375, 'max_depth': 11, 'max_bin': 192, 'num_leaves': 524}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:18:54,197]\u001b[0m Trial 331 finished with value: 0.6761660810473055 and parameters: {'n_estimators': 706, 'learning_rate': 0.04601736982138191, 'max_depth': 10, 'max_bin': 224, 'num_leaves': 562}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:19:00,152]\u001b[0m Trial 332 finished with value: 0.6782712875030995 and parameters: {'n_estimators': 818, 'learning_rate': 0.07600483279142987, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 598}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:19:07,142]\u001b[0m Trial 333 finished with value: 0.6802660649279775 and parameters: {'n_estimators': 758, 'learning_rate': 0.05274977089285111, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 551}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:19:14,396]\u001b[0m Trial 334 finished with value: 0.6810623313726518 and parameters: {'n_estimators': 674, 'learning_rate': 0.060254012741224675, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 569}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:19:21,660]\u001b[0m Trial 335 finished with value: 0.6831487878681148 and parameters: {'n_estimators': 639, 'learning_rate': 0.06699562002252712, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 587}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:19:28,196]\u001b[0m Trial 336 finished with value: 0.6787715483476986 and parameters: {'n_estimators': 737, 'learning_rate': 0.05629737129470181, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 162}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:19:33,951]\u001b[0m Trial 337 finished with value: 0.6784588963152335 and parameters: {'n_estimators': 692, 'learning_rate': 0.07226125719623351, 'max_depth': 11, 'max_bin': 236, 'num_leaves': 505}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:19:40,579]\u001b[0m Trial 338 finished with value: 0.6824856715981774 and parameters: {'n_estimators': 717, 'learning_rate': 0.06343010705135292, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 650}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:19:45,714]\u001b[0m Trial 339 finished with value: 0.6652875424526034 and parameters: {'n_estimators': 606, 'learning_rate': 0.05047883942149568, 'max_depth': 6, 'max_bin': 166, 'num_leaves': 615}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:19:51,491]\u001b[0m Trial 340 finished with value: 0.6786929542835487 and parameters: {'n_estimators': 693, 'learning_rate': 0.06950632972837781, 'max_depth': 11, 'max_bin': 155, 'num_leaves': 528}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:19:58,459]\u001b[0m Trial 341 finished with value: 0.6844338095798527 and parameters: {'n_estimators': 784, 'learning_rate': 0.05842275474185295, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 548}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:20:05,303]\u001b[0m Trial 342 finished with value: 0.6779085584990338 and parameters: {'n_estimators': 670, 'learning_rate': 0.061553498810262584, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 582}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:20:11,382]\u001b[0m Trial 343 finished with value: 0.6793620243662905 and parameters: {'n_estimators': 729, 'learning_rate': 0.09402728577360647, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 630}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:20:20,446]\u001b[0m Trial 344 finished with value: 0.6830412168306582 and parameters: {'n_estimators': 645, 'learning_rate': 0.05459423837747053, 'max_depth': 11, 'max_bin': 195, 'num_leaves': 570}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:20:39,266]\u001b[0m Trial 345 finished with value: 0.5477688245915936 and parameters: {'n_estimators': 707, 'learning_rate': 0.0024934780187931133, 'max_depth': 12, 'max_bin': 201, 'num_leaves': 603}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:20:45,281]\u001b[0m Trial 346 finished with value: 0.6783115925818766 and parameters: {'n_estimators': 680, 'learning_rate': 0.07565880626117116, 'max_depth': 11, 'max_bin': 179, 'num_leaves': 484}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:20:51,534]\u001b[0m Trial 347 finished with value: 0.6785732492440337 and parameters: {'n_estimators': 745, 'learning_rate': 0.06799665930533108, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 556}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:20:57,327]\u001b[0m Trial 348 finished with value: 0.6750290412275805 and parameters: {'n_estimators': 659, 'learning_rate': 0.06468333951581987, 'max_depth': 8, 'max_bin': 172, 'num_leaves': 539}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:21:05,960]\u001b[0m Trial 349 finished with value: 0.6803486975977503 and parameters: {'n_estimators': 712, 'learning_rate': 0.0467121868837644, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 590}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.690157\n",
      "\tBest params:\n",
      "\t\tn_estimators: 699\n",
      "\t\tlearning_rate: 0.06968993674967647\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 202\n",
      "\t\tnum_leaves: 556\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665298    0.676487    0.697708    0.684076   \n",
      "1                    TP  377.000000  387.000000  413.000000  387.000000   \n",
      "2                    TN  373.000000  343.000000  342.000000  354.000000   \n",
      "3                    FP   71.000000   73.000000   81.000000   72.000000   \n",
      "4                    FN   78.000000   96.000000   63.000000   86.000000   \n",
      "5              Accuracy    0.834260    0.812013    0.839822    0.824249   \n",
      "6             Precision    0.841518    0.841304    0.836032    0.843137   \n",
      "7           Sensitivity    0.828571    0.801242    0.867647    0.818182   \n",
      "8           Specificity    0.840100    0.824500    0.808500    0.831000   \n",
      "9              F1 score    0.834994    0.820785    0.851546    0.830472   \n",
      "10  F1 score (weighted)    0.834266    0.812249    0.839567    0.824350   \n",
      "11     F1 score (macro)    0.834257    0.811562    0.838817    0.824012   \n",
      "12    Balanced Accuracy    0.834331    0.812881    0.838079    0.824584   \n",
      "13                  MCC    0.668615    0.624192    0.678314    0.648425   \n",
      "14                  NPV    0.827100    0.781300    0.844400    0.804500   \n",
      "15              ROC_AUC    0.834331    0.812881    0.838079    0.824584   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.650299    0.681476    0.707530  \n",
      "1   385.000000  392.000000  401.000000  \n",
      "2   357.000000  360.000000  355.000000  \n",
      "3    73.000000   63.000000   72.000000  \n",
      "4    84.000000   84.000000   71.000000  \n",
      "5     0.825362    0.836485    0.840934  \n",
      "6     0.840611    0.861538    0.847780  \n",
      "7     0.820896    0.823529    0.849576  \n",
      "8     0.830200    0.851100    0.831400  \n",
      "9     0.830636    0.842105    0.848677  \n",
      "10    0.825428    0.836621    0.840925  \n",
      "11    0.825192    0.836278    0.840517  \n",
      "12    0.825564    0.837297    0.840479  \n",
      "13    0.650631    0.673470    0.681036  \n",
      "14    0.809500    0.810800    0.833300  \n",
      "15    0.825564    0.837297    0.840479  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_6_cat = np.where((y_pred_lgbm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:21:13,915]\u001b[0m Trial 350 finished with value: 0.6839852697217117 and parameters: {'n_estimators': 695, 'learning_rate': 0.0592148085872625, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 385}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:21:22,056]\u001b[0m Trial 351 finished with value: 0.6845148941129162 and parameters: {'n_estimators': 766, 'learning_rate': 0.052341198265722186, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 512}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:21:28,392]\u001b[0m Trial 352 finished with value: 0.6842880346199786 and parameters: {'n_estimators': 729, 'learning_rate': 0.08050893422001745, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 564}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:21:33,472]\u001b[0m Trial 353 finished with value: 0.679128705282659 and parameters: {'n_estimators': 625, 'learning_rate': 0.07248872419311943, 'max_depth': 7, 'max_bin': 157, 'num_leaves': 611}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:21:40,555]\u001b[0m Trial 354 finished with value: 0.684559220747739 and parameters: {'n_estimators': 827, 'learning_rate': 0.0646182258181654, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 460}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:21:48,729]\u001b[0m Trial 355 finished with value: 0.6860556054426002 and parameters: {'n_estimators': 793, 'learning_rate': 0.05619007497528864, 'max_depth': 12, 'max_bin': 198, 'num_leaves': 579}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:21:54,718]\u001b[0m Trial 356 finished with value: 0.6799062486950105 and parameters: {'n_estimators': 679, 'learning_rate': 0.06790827763882831, 'max_depth': 10, 'max_bin': 169, 'num_leaves': 529}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:22:02,782]\u001b[0m Trial 357 finished with value: 0.6823151672080914 and parameters: {'n_estimators': 701, 'learning_rate': 0.04161312865483802, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 594}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:22:09,570]\u001b[0m Trial 358 finished with value: 0.6836450136995119 and parameters: {'n_estimators': 659, 'learning_rate': 0.06077533649570524, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 551}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:22:17,345]\u001b[0m Trial 359 finished with value: 0.6844857737963291 and parameters: {'n_estimators': 718, 'learning_rate': 0.04945182655339147, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 571}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:22:23,499]\u001b[0m Trial 360 finished with value: 0.6809243377623311 and parameters: {'n_estimators': 752, 'learning_rate': 0.07073941336370637, 'max_depth': 10, 'max_bin': 185, 'num_leaves': 540}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:22:31,096]\u001b[0m Trial 361 finished with value: 0.6849421113454796 and parameters: {'n_estimators': 683, 'learning_rate': 0.05601247532701894, 'max_depth': 11, 'max_bin': 151, 'num_leaves': 603}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:22:34,331]\u001b[0m Trial 362 finished with value: 0.6639595724945013 and parameters: {'n_estimators': 160, 'learning_rate': 0.06433814829520941, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 497}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:22:41,166]\u001b[0m Trial 363 finished with value: 0.6847234140923069 and parameters: {'n_estimators': 695, 'learning_rate': 0.0754632072135704, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 630}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:22:48,726]\u001b[0m Trial 364 finished with value: 0.6844133024797567 and parameters: {'n_estimators': 646, 'learning_rate': 0.0598630555893704, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 560}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:22:55,965]\u001b[0m Trial 365 finished with value: 0.6863324380981919 and parameters: {'n_estimators': 737, 'learning_rate': 0.05267729885559424, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 584}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:23:02,711]\u001b[0m Trial 366 finished with value: 0.6831547038148221 and parameters: {'n_estimators': 668, 'learning_rate': 0.06687669205761686, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 520}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:23:09,058]\u001b[0m Trial 367 finished with value: 0.6838595782607007 and parameters: {'n_estimators': 718, 'learning_rate': 0.07884861546066471, 'max_depth': 12, 'max_bin': 189, 'num_leaves': 573}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:23:21,299]\u001b[0m Trial 368 finished with value: 0.6742570566761708 and parameters: {'n_estimators': 699, 'learning_rate': 0.013198536353120946, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 279}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:23:28,871]\u001b[0m Trial 369 finished with value: 0.6817637186628248 and parameters: {'n_estimators': 779, 'learning_rate': 0.04530926533718405, 'max_depth': 9, 'max_bin': 175, 'num_leaves': 550}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:23:35,785]\u001b[0m Trial 370 finished with value: 0.6802988330819126 and parameters: {'n_estimators': 681, 'learning_rate': 0.06257073490402293, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 620}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:23:40,533]\u001b[0m Trial 371 finished with value: 0.6550893327562326 and parameters: {'n_estimators': 710, 'learning_rate': 0.058111843224038486, 'max_depth': 4, 'max_bin': 202, 'num_leaves': 592}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:23:50,657]\u001b[0m Trial 372 finished with value: 0.6895008771477916 and parameters: {'n_estimators': 804, 'learning_rate': 0.035794701792120354, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 413}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:23:57,307]\u001b[0m Trial 373 finished with value: 0.6841329492838393 and parameters: {'n_estimators': 830, 'learning_rate': 0.06952958400116102, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 399}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:24:04,607]\u001b[0m Trial 374 finished with value: 0.6835275703491666 and parameters: {'n_estimators': 813, 'learning_rate': 0.07387548959192998, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 426}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:24:15,372]\u001b[0m Trial 375 finished with value: 0.684899132550643 and parameters: {'n_estimators': 800, 'learning_rate': 0.03325574791579149, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 435}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:24:26,921]\u001b[0m Trial 376 finished with value: 0.6863959238750265 and parameters: {'n_estimators': 786, 'learning_rate': 0.026755585337156926, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 424}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:24:37,598]\u001b[0m Trial 377 finished with value: 0.6858613984228279 and parameters: {'n_estimators': 839, 'learning_rate': 0.0301590634878186, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 442}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:24:47,748]\u001b[0m Trial 378 finished with value: 0.687663405452345 and parameters: {'n_estimators': 802, 'learning_rate': 0.03902204009628548, 'max_depth': 12, 'max_bin': 158, 'num_leaves': 529}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:24:57,842]\u001b[0m Trial 379 finished with value: 0.6874388047180272 and parameters: {'n_estimators': 793, 'learning_rate': 0.03595943038805509, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 408}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:25:07,534]\u001b[0m Trial 380 finished with value: 0.6850927078156526 and parameters: {'n_estimators': 800, 'learning_rate': 0.03556731775706712, 'max_depth': 12, 'max_bin': 156, 'num_leaves': 409}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:25:21,132]\u001b[0m Trial 381 finished with value: 0.688948116573035 and parameters: {'n_estimators': 788, 'learning_rate': 0.021888863037258413, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 400}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:25:30,055]\u001b[0m Trial 382 finished with value: 0.6868708981307887 and parameters: {'n_estimators': 811, 'learning_rate': 0.03869871280878874, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 393}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:25:40,542]\u001b[0m Trial 383 finished with value: 0.6867252840340493 and parameters: {'n_estimators': 793, 'learning_rate': 0.031071976661754517, 'max_depth': 12, 'max_bin': 156, 'num_leaves': 416}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:25:55,379]\u001b[0m Trial 384 finished with value: 0.6853604972004701 and parameters: {'n_estimators': 820, 'learning_rate': 0.016242981460268757, 'max_depth': 12, 'max_bin': 151, 'num_leaves': 400}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:26:05,476]\u001b[0m Trial 385 finished with value: 0.6863314719090329 and parameters: {'n_estimators': 777, 'learning_rate': 0.03534808145880947, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 403}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:26:13,779]\u001b[0m Trial 386 finished with value: 0.6847905235134166 and parameters: {'n_estimators': 807, 'learning_rate': 0.038404708552391864, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 417}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:26:23,402]\u001b[0m Trial 387 finished with value: 0.6846088841327296 and parameters: {'n_estimators': 768, 'learning_rate': 0.03310747614918536, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 357}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:26:35,885]\u001b[0m Trial 388 finished with value: 0.6839114139600089 and parameters: {'n_estimators': 825, 'learning_rate': 0.02006154467050539, 'max_depth': 12, 'max_bin': 158, 'num_leaves': 402}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:26:44,510]\u001b[0m Trial 389 finished with value: 0.6865516756884962 and parameters: {'n_estimators': 790, 'learning_rate': 0.040598218902461625, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 378}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:26:52,974]\u001b[0m Trial 390 finished with value: 0.6825597832721195 and parameters: {'n_estimators': 796, 'learning_rate': 0.041263533061452765, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 378}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:27:06,622]\u001b[0m Trial 391 finished with value: 0.6876761408575549 and parameters: {'n_estimators': 838, 'learning_rate': 0.02254791755938615, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 417}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:27:18,967]\u001b[0m Trial 392 finished with value: 0.6859912616172792 and parameters: {'n_estimators': 838, 'learning_rate': 0.023524878448195362, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 368}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:27:32,057]\u001b[0m Trial 393 finished with value: 0.6874598680223525 and parameters: {'n_estimators': 855, 'learning_rate': 0.02081029344824463, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 416}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:27:41,814]\u001b[0m Trial 394 finished with value: 0.6872754293427337 and parameters: {'n_estimators': 882, 'learning_rate': 0.03021099785146726, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 388}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:27:53,974]\u001b[0m Trial 395 finished with value: 0.6832765842281796 and parameters: {'n_estimators': 850, 'learning_rate': 0.022200086182500058, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 418}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:28:08,071]\u001b[0m Trial 396 finished with value: 0.6831401191996056 and parameters: {'n_estimators': 822, 'learning_rate': 0.018132021562983255, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 404}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:28:24,359]\u001b[0m Trial 397 finished with value: 0.6629717612017766 and parameters: {'n_estimators': 867, 'learning_rate': 0.007715271317429488, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 436}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:28:36,137]\u001b[0m Trial 398 finished with value: 0.6883085182512131 and parameters: {'n_estimators': 861, 'learning_rate': 0.026159667825285886, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 418}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:28:47,586]\u001b[0m Trial 399 finished with value: 0.6888041609599032 and parameters: {'n_estimators': 864, 'learning_rate': 0.027756279966851177, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 387}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6901570\n",
      "\tBest params:\n",
      "\t\tn_estimators: 699\n",
      "\t\tlearning_rate: 0.06968993674967647\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 202\n",
      "\t\tnum_leaves: 556\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665298    0.676487    0.697708    0.684076   \n",
      "1                    TP  377.000000  387.000000  413.000000  387.000000   \n",
      "2                    TN  373.000000  343.000000  342.000000  354.000000   \n",
      "3                    FP   71.000000   73.000000   81.000000   72.000000   \n",
      "4                    FN   78.000000   96.000000   63.000000   86.000000   \n",
      "5              Accuracy    0.834260    0.812013    0.839822    0.824249   \n",
      "6             Precision    0.841518    0.841304    0.836032    0.843137   \n",
      "7           Sensitivity    0.828571    0.801242    0.867647    0.818182   \n",
      "8           Specificity    0.840100    0.824500    0.808500    0.831000   \n",
      "9              F1 score    0.834994    0.820785    0.851546    0.830472   \n",
      "10  F1 score (weighted)    0.834266    0.812249    0.839567    0.824350   \n",
      "11     F1 score (macro)    0.834257    0.811562    0.838817    0.824012   \n",
      "12    Balanced Accuracy    0.834331    0.812881    0.838079    0.824584   \n",
      "13                  MCC    0.668615    0.624192    0.678314    0.648425   \n",
      "14                  NPV    0.827100    0.781300    0.844400    0.804500   \n",
      "15              ROC_AUC    0.834331    0.812881    0.838079    0.824584   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.650299    0.681476    0.707530    0.679296  \n",
      "1   385.000000  392.000000  401.000000  394.000000  \n",
      "2   357.000000  360.000000  355.000000  330.000000  \n",
      "3    73.000000   63.000000   72.000000   86.000000  \n",
      "4    84.000000   84.000000   71.000000   89.000000  \n",
      "5     0.825362    0.836485    0.840934    0.805339  \n",
      "6     0.840611    0.861538    0.847780    0.820833  \n",
      "7     0.820896    0.823529    0.849576    0.815735  \n",
      "8     0.830200    0.851100    0.831400    0.793300  \n",
      "9     0.830636    0.842105    0.848677    0.818276  \n",
      "10    0.825428    0.836621    0.840925    0.805386  \n",
      "11    0.825192    0.836278    0.840517    0.804348  \n",
      "12    0.825564    0.837297    0.840479    0.804502  \n",
      "13    0.650631    0.673470    0.681036    0.608713  \n",
      "14    0.809500    0.810800    0.833300    0.787600  \n",
      "15    0.825564    0.837297    0.840479    0.804502  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_7_cat = np.where((y_pred_lgbm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:28:59,169]\u001b[0m Trial 400 finished with value: 0.6729890719570946 and parameters: {'n_estimators': 852, 'learning_rate': 0.025794928872415725, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 391}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:29:11,388]\u001b[0m Trial 401 finished with value: 0.6722713697610188 and parameters: {'n_estimators': 856, 'learning_rate': 0.025288414509698407, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 385}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:29:25,351]\u001b[0m Trial 402 finished with value: 0.674379403069187 and parameters: {'n_estimators': 856, 'learning_rate': 0.020131560911274785, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 322}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:29:40,983]\u001b[0m Trial 403 finished with value: 0.6690987384803891 and parameters: {'n_estimators': 873, 'learning_rate': 0.01302241171775836, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 420}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:29:55,812]\u001b[0m Trial 404 finished with value: 0.6727409711798893 and parameters: {'n_estimators': 873, 'learning_rate': 0.01508955345653392, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 375}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:30:07,918]\u001b[0m Trial 405 finished with value: 0.6732047751927567 and parameters: {'n_estimators': 899, 'learning_rate': 0.0214689677097995, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 367}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:30:18,847]\u001b[0m Trial 406 finished with value: 0.6763932999181345 and parameters: {'n_estimators': 844, 'learning_rate': 0.026488978474368995, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 393}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:30:30,269]\u001b[0m Trial 407 finished with value: 0.6736719349925064 and parameters: {'n_estimators': 841, 'learning_rate': 0.02736761679637387, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 334}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:30:46,434]\u001b[0m Trial 408 finished with value: 0.6660544294402898 and parameters: {'n_estimators': 872, 'learning_rate': 0.012122096906691625, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 508}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:30:58,858]\u001b[0m Trial 409 finished with value: 0.6732791302782615 and parameters: {'n_estimators': 863, 'learning_rate': 0.022366618185336928, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 360}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:31:09,418]\u001b[0m Trial 410 finished with value: 0.6771072125195889 and parameters: {'n_estimators': 857, 'learning_rate': 0.02983280533518571, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 352}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:31:21,257]\u001b[0m Trial 411 finished with value: 0.6706511490473459 and parameters: {'n_estimators': 837, 'learning_rate': 0.02265364031422786, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 521}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:31:32,865]\u001b[0m Trial 412 finished with value: 0.6757129986521798 and parameters: {'n_estimators': 878, 'learning_rate': 0.029531152325756826, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 414}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:31:48,997]\u001b[0m Trial 413 finished with value: 0.674706292657141 and parameters: {'n_estimators': 833, 'learning_rate': 0.018422517785810423, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 533}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:31:59,331]\u001b[0m Trial 414 finished with value: 0.6788022395981589 and parameters: {'n_estimators': 845, 'learning_rate': 0.032816657247458574, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 426}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:32:11,140]\u001b[0m Trial 415 finished with value: 0.6727402973859433 and parameters: {'n_estimators': 869, 'learning_rate': 0.02448151663127959, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 531}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:32:24,605]\u001b[0m Trial 416 finished with value: 0.6726499273225438 and parameters: {'n_estimators': 827, 'learning_rate': 0.01853149248484039, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 387}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:32:35,372]\u001b[0m Trial 417 finished with value: 0.6742120794888465 and parameters: {'n_estimators': 899, 'learning_rate': 0.026517069436123024, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 436}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:32:47,000]\u001b[0m Trial 418 finished with value: 0.6385495925958693 and parameters: {'n_estimators': 555, 'learning_rate': 0.009326947185136054, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 502}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:32:59,992]\u001b[0m Trial 419 finished with value: 0.6714984218565838 and parameters: {'n_estimators': 820, 'learning_rate': 0.02233859049366497, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 411}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:33:11,585]\u001b[0m Trial 420 finished with value: 0.6749072847330466 and parameters: {'n_estimators': 856, 'learning_rate': 0.02916260915334527, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 513}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:33:24,728]\u001b[0m Trial 421 finished with value: 0.6680961439272667 and parameters: {'n_estimators': 677, 'learning_rate': 0.017529926870628695, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 540}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:33:35,152]\u001b[0m Trial 422 finished with value: 0.6766513468054949 and parameters: {'n_estimators': 811, 'learning_rate': 0.03157531806727742, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 241}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:33:44,973]\u001b[0m Trial 423 finished with value: 0.677316500028567 and parameters: {'n_estimators': 662, 'learning_rate': 0.036658208931437467, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 454}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:33:57,220]\u001b[0m Trial 424 finished with value: 0.6711523890506134 and parameters: {'n_estimators': 818, 'learning_rate': 0.02348402183697224, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 491}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:34:09,257]\u001b[0m Trial 425 finished with value: 0.6748039428430794 and parameters: {'n_estimators': 846, 'learning_rate': 0.027393068965342042, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 397}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:34:19,103]\u001b[0m Trial 426 finished with value: 0.6784161927641149 and parameters: {'n_estimators': 652, 'learning_rate': 0.0345657632105508, 'max_depth': 12, 'max_bin': 201, 'num_leaves': 525}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:34:33,196]\u001b[0m Trial 427 finished with value: 0.6701119865200761 and parameters: {'n_estimators': 887, 'learning_rate': 0.01611629965861375, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 472}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:34:41,075]\u001b[0m Trial 428 finished with value: 0.6595742234242914 and parameters: {'n_estimators': 671, 'learning_rate': 0.024142912752015258, 'max_depth': 8, 'max_bin': 158, 'num_leaves': 75}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:34:53,059]\u001b[0m Trial 429 finished with value: 0.6690631413002484 and parameters: {'n_estimators': 687, 'learning_rate': 0.019237594669730497, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 552}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:35:03,213]\u001b[0m Trial 430 finished with value: 0.6754113218191756 and parameters: {'n_estimators': 839, 'learning_rate': 0.03135406991261364, 'max_depth': 12, 'max_bin': 200, 'num_leaves': 537}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:35:11,725]\u001b[0m Trial 431 finished with value: 0.6790914039637246 and parameters: {'n_estimators': 868, 'learning_rate': 0.044216006940535325, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 430}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:35:16,812]\u001b[0m Trial 432 finished with value: 0.6398154110652869 and parameters: {'n_estimators': 657, 'learning_rate': 0.03666138190722279, 'max_depth': 5, 'max_bin': 205, 'num_leaves': 665}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:35:24,321]\u001b[0m Trial 433 finished with value: 0.6756600095168269 and parameters: {'n_estimators': 691, 'learning_rate': 0.06783667899892537, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 520}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:35:31,175]\u001b[0m Trial 434 finished with value: 0.6705862411558255 and parameters: {'n_estimators': 533, 'learning_rate': 0.06524590348580213, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 545}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:35:39,936]\u001b[0m Trial 435 finished with value: 0.6756747274191721 and parameters: {'n_estimators': 626, 'learning_rate': 0.03996387955778706, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 411}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:35:51,002]\u001b[0m Trial 436 finished with value: 0.673660250278609 and parameters: {'n_estimators': 806, 'learning_rate': 0.02646598080334084, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 561}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:35:57,717]\u001b[0m Trial 437 finished with value: 0.674049710776519 and parameters: {'n_estimators': 764, 'learning_rate': 0.06305148302485104, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 377}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:36:10,418]\u001b[0m Trial 438 finished with value: 0.6710878830991385 and parameters: {'n_estimators': 686, 'learning_rate': 0.02049221454228385, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 534}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:36:23,224]\u001b[0m Trial 439 finished with value: 0.6518581935381889 and parameters: {'n_estimators': 668, 'learning_rate': 0.011062891216860646, 'max_depth': 11, 'max_bin': 150, 'num_leaves': 510}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:36:27,069]\u001b[0m Trial 440 finished with value: 0.6613231019847251 and parameters: {'n_estimators': 642, 'learning_rate': 0.17525223585929328, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 401}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:36:33,878]\u001b[0m Trial 441 finished with value: 0.6751888400229819 and parameters: {'n_estimators': 861, 'learning_rate': 0.07135344508027432, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 566}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:36:38,610]\u001b[0m Trial 442 finished with value: 0.6685992415394653 and parameters: {'n_estimators': 442, 'learning_rate': 0.14548973823401345, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 612}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:36:46,020]\u001b[0m Trial 443 finished with value: 0.6745964983347961 and parameters: {'n_estimators': 835, 'learning_rate': 0.062138778518787935, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 550}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:36:59,546]\u001b[0m Trial 444 finished with value: 0.6657605231802535 and parameters: {'n_estimators': 672, 'learning_rate': 0.015040978966587522, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 576}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:37:07,327]\u001b[0m Trial 445 finished with value: 0.6726467039019429 and parameters: {'n_estimators': 687, 'learning_rate': 0.04757049250366442, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 645}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:37:18,164]\u001b[0m Trial 446 finished with value: 0.6751238403854427 and parameters: {'n_estimators': 822, 'learning_rate': 0.029168127169080446, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 448}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:37:24,805]\u001b[0m Trial 447 finished with value: 0.6733286438508596 and parameters: {'n_estimators': 885, 'learning_rate': 0.06795329055510055, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 340}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:37:34,132]\u001b[0m Trial 448 finished with value: 0.6773853676292597 and parameters: {'n_estimators': 750, 'learning_rate': 0.04360663528255941, 'max_depth': 12, 'max_bin': 195, 'num_leaves': 597}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:37:40,769]\u001b[0m Trial 449 finished with value: 0.6732873339153397 and parameters: {'n_estimators': 699, 'learning_rate': 0.06189979923699591, 'max_depth': 9, 'max_bin': 212, 'num_leaves': 526}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.69015697\n",
      "\tBest params:\n",
      "\t\tn_estimators: 699\n",
      "\t\tlearning_rate: 0.06968993674967647\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 202\n",
      "\t\tnum_leaves: 556\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665298    0.676487    0.697708    0.684076   \n",
      "1                    TP  377.000000  387.000000  413.000000  387.000000   \n",
      "2                    TN  373.000000  343.000000  342.000000  354.000000   \n",
      "3                    FP   71.000000   73.000000   81.000000   72.000000   \n",
      "4                    FN   78.000000   96.000000   63.000000   86.000000   \n",
      "5              Accuracy    0.834260    0.812013    0.839822    0.824249   \n",
      "6             Precision    0.841518    0.841304    0.836032    0.843137   \n",
      "7           Sensitivity    0.828571    0.801242    0.867647    0.818182   \n",
      "8           Specificity    0.840100    0.824500    0.808500    0.831000   \n",
      "9              F1 score    0.834994    0.820785    0.851546    0.830472   \n",
      "10  F1 score (weighted)    0.834266    0.812249    0.839567    0.824350   \n",
      "11     F1 score (macro)    0.834257    0.811562    0.838817    0.824012   \n",
      "12    Balanced Accuracy    0.834331    0.812881    0.838079    0.824584   \n",
      "13                  MCC    0.668615    0.624192    0.678314    0.648425   \n",
      "14                  NPV    0.827100    0.781300    0.844400    0.804500   \n",
      "15              ROC_AUC    0.834331    0.812881    0.838079    0.824584   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.650299    0.681476    0.707530    0.679296    0.669725  \n",
      "1   385.000000  392.000000  401.000000  394.000000  394.000000  \n",
      "2   357.000000  360.000000  355.000000  330.000000  337.000000  \n",
      "3    73.000000   63.000000   72.000000   86.000000   79.000000  \n",
      "4    84.000000   84.000000   71.000000   89.000000   89.000000  \n",
      "5     0.825362    0.836485    0.840934    0.805339    0.813126  \n",
      "6     0.840611    0.861538    0.847780    0.820833    0.832981  \n",
      "7     0.820896    0.823529    0.849576    0.815735    0.815735  \n",
      "8     0.830200    0.851100    0.831400    0.793300    0.810100  \n",
      "9     0.830636    0.842105    0.848677    0.818276    0.824268  \n",
      "10    0.825428    0.836621    0.840925    0.805386    0.813258  \n",
      "11    0.825192    0.836278    0.840517    0.804348    0.812371  \n",
      "12    0.825564    0.837297    0.840479    0.804502    0.812916  \n",
      "13    0.650631    0.673470    0.681036    0.608713    0.624945  \n",
      "14    0.809500    0.810800    0.833300    0.787600    0.791100  \n",
      "15    0.825564    0.837297    0.840479    0.804502    0.812916  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_8_cat = np.where((y_pred_lgbm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:37:49,125]\u001b[0m Trial 450 finished with value: 0.6758453118284062 and parameters: {'n_estimators': 780, 'learning_rate': 0.05606218654588787, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 561}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:38:00,506]\u001b[0m Trial 451 finished with value: 0.6739099383707216 and parameters: {'n_estimators': 659, 'learning_rate': 0.03380331349527867, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 695}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:38:10,785]\u001b[0m Trial 452 finished with value: 0.6655240412918318 and parameters: {'n_estimators': 591, 'learning_rate': 0.022829981157401324, 'max_depth': 11, 'max_bin': 152, 'num_leaves': 541}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:38:16,615]\u001b[0m Trial 453 finished with value: 0.6721975647624211 and parameters: {'n_estimators': 337, 'learning_rate': 0.06613343389115273, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 587}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:38:23,328]\u001b[0m Trial 454 finished with value: 0.6739595808447495 and parameters: {'n_estimators': 851, 'learning_rate': 0.07209989451702531, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 389}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:38:31,410]\u001b[0m Trial 455 finished with value: 0.6727139749707218 and parameters: {'n_estimators': 813, 'learning_rate': 0.05839685555333579, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 421}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:38:39,830]\u001b[0m Trial 456 finished with value: 0.6763129289615273 and parameters: {'n_estimators': 699, 'learning_rate': 0.05293662940438937, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 496}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:38:47,547]\u001b[0m Trial 457 finished with value: 0.6756030366562875 and parameters: {'n_estimators': 737, 'learning_rate': 0.06910481235914519, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 550}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:38:52,843]\u001b[0m Trial 458 finished with value: 0.6658041351073126 and parameters: {'n_estimators': 290, 'learning_rate': 0.07533477987128853, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 607}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:39:00,943]\u001b[0m Trial 459 finished with value: 0.6765061238497434 and parameters: {'n_estimators': 500, 'learning_rate': 0.06440211978233833, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 568}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:39:10,794]\u001b[0m Trial 460 finished with value: 0.6762472319632329 and parameters: {'n_estimators': 644, 'learning_rate': 0.04984914527825016, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 128}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:39:22,716]\u001b[0m Trial 461 finished with value: 0.6717135902536947 and parameters: {'n_estimators': 672, 'learning_rate': 0.026919410541825728, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 632}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:39:30,617]\u001b[0m Trial 462 finished with value: 0.6745544349533044 and parameters: {'n_estimators': 760, 'learning_rate': 0.06065245202068057, 'max_depth': 11, 'max_bin': 156, 'num_leaves': 526}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:39:41,631]\u001b[0m Trial 463 finished with value: 0.6759816869928648 and parameters: {'n_estimators': 724, 'learning_rate': 0.03823250751487128, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 408}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:39:45,430]\u001b[0m Trial 464 finished with value: 0.6158300265402277 and parameters: {'n_estimators': 704, 'learning_rate': 0.06792504986941975, 'max_depth': 3, 'max_bin': 200, 'num_leaves': 576}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:39:50,439]\u001b[0m Trial 465 finished with value: 0.6421776069998538 and parameters: {'n_estimators': 805, 'learning_rate': 0.05632720792553293, 'max_depth': 4, 'max_bin': 220, 'num_leaves': 435}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:39:59,667]\u001b[0m Trial 466 finished with value: 0.6718192274794321 and parameters: {'n_estimators': 681, 'learning_rate': 0.04479708013472491, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 507}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:40:10,970]\u001b[0m Trial 467 finished with value: 0.672743817448786 and parameters: {'n_estimators': 830, 'learning_rate': 0.03257349879698905, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 481}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:40:18,666]\u001b[0m Trial 468 finished with value: 0.6769181395507402 and parameters: {'n_estimators': 778, 'learning_rate': 0.06203616183985357, 'max_depth': 11, 'max_bin': 153, 'num_leaves': 539}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:40:25,958]\u001b[0m Trial 469 finished with value: 0.6768224480417959 and parameters: {'n_estimators': 711, 'learning_rate': 0.07144193476081927, 'max_depth': 12, 'max_bin': 197, 'num_leaves': 595}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:40:37,458]\u001b[0m Trial 470 finished with value: 0.6652147196855303 and parameters: {'n_estimators': 659, 'learning_rate': 0.02105551135769749, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 549}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:40:51,081]\u001b[0m Trial 471 finished with value: 0.6675824518711619 and parameters: {'n_estimators': 688, 'learning_rate': 0.01630696152397115, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 621}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:40:59,980]\u001b[0m Trial 472 finished with value: 0.6777782939808461 and parameters: {'n_estimators': 883, 'learning_rate': 0.06524573914646963, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 585}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:41:07,475]\u001b[0m Trial 473 finished with value: 0.6719282839660834 and parameters: {'n_estimators': 851, 'learning_rate': 0.050562451210756226, 'max_depth': 9, 'max_bin': 202, 'num_leaves': 366}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:41:13,250]\u001b[0m Trial 474 finished with value: 0.6717892050995319 and parameters: {'n_estimators': 381, 'learning_rate': 0.07610293000876786, 'max_depth': 11, 'max_bin': 163, 'num_leaves': 196}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:41:20,754]\u001b[0m Trial 475 finished with value: 0.6775143691791208 and parameters: {'n_estimators': 732, 'learning_rate': 0.05705898262829591, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 566}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:41:31,311]\u001b[0m Trial 476 finished with value: 0.6711499538932502 and parameters: {'n_estimators': 636, 'learning_rate': 0.025314604244533766, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 519}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:41:39,773]\u001b[0m Trial 477 finished with value: 0.679560840974134 and parameters: {'n_estimators': 676, 'learning_rate': 0.06206649595498088, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 556}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:41:45,322]\u001b[0m Trial 478 finished with value: 0.6680836136754835 and parameters: {'n_estimators': 796, 'learning_rate': 0.10811370352112434, 'max_depth': 11, 'max_bin': 195, 'num_leaves': 602}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:41:52,278]\u001b[0m Trial 479 finished with value: 0.6758998812900373 and parameters: {'n_estimators': 709, 'learning_rate': 0.06881253727285527, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 386}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:42:03,626]\u001b[0m Trial 480 finished with value: 0.6715951984071343 and parameters: {'n_estimators': 694, 'learning_rate': 0.028325260487950057, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 581}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:42:11,581]\u001b[0m Trial 481 finished with value: 0.6766807819876783 and parameters: {'n_estimators': 620, 'learning_rate': 0.06578565789407176, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 540}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:42:18,412]\u001b[0m Trial 482 finished with value: 0.6657704039737077 and parameters: {'n_estimators': 865, 'learning_rate': 0.05420685769380061, 'max_depth': 7, 'max_bin': 154, 'num_leaves': 415}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:42:29,258]\u001b[0m Trial 483 finished with value: 0.6785375340588595 and parameters: {'n_estimators': 750, 'learning_rate': 0.040426442740228656, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 399}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:42:37,085]\u001b[0m Trial 484 finished with value: 0.6782033425101199 and parameters: {'n_estimators': 827, 'learning_rate': 0.059753206537665506, 'max_depth': 11, 'max_bin': 202, 'num_leaves': 528}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:42:42,234]\u001b[0m Trial 485 finished with value: 0.6709992009812942 and parameters: {'n_estimators': 654, 'learning_rate': 0.12481180206120472, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 566}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:42:52,139]\u001b[0m Trial 486 finished with value: 0.6755181468686684 and parameters: {'n_estimators': 675, 'learning_rate': 0.047398066280649756, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 457}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:43:00,494]\u001b[0m Trial 487 finished with value: 0.675873351382948 and parameters: {'n_estimators': 723, 'learning_rate': 0.0704233980985201, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 606}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:43:13,304]\u001b[0m Trial 488 finished with value: 0.6694876442719983 and parameters: {'n_estimators': 782, 'learning_rate': 0.01879492667858169, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 553}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:43:23,702]\u001b[0m Trial 489 finished with value: 0.6728319100645932 and parameters: {'n_estimators': 840, 'learning_rate': 0.03567862440240343, 'max_depth': 12, 'max_bin': 188, 'num_leaves': 588}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:43:31,929]\u001b[0m Trial 490 finished with value: 0.6726358129382535 and parameters: {'n_estimators': 698, 'learning_rate': 0.053322260594572075, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 431}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:43:39,490]\u001b[0m Trial 491 finished with value: 0.6753814873987218 and parameters: {'n_estimators': 667, 'learning_rate': 0.07349851758866381, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 506}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:43:47,205]\u001b[0m Trial 492 finished with value: 0.6759571553012125 and parameters: {'n_estimators': 805, 'learning_rate': 0.06548904443308157, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 572}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:43:55,500]\u001b[0m Trial 493 finished with value: 0.6736671104424532 and parameters: {'n_estimators': 770, 'learning_rate': 0.06033729856652083, 'max_depth': 11, 'max_bin': 163, 'num_leaves': 618}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:44:02,504]\u001b[0m Trial 494 finished with value: 0.6738670696450746 and parameters: {'n_estimators': 685, 'learning_rate': 0.07867100063851072, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 375}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:44:14,441]\u001b[0m Trial 495 finished with value: 0.6713544986169354 and parameters: {'n_estimators': 740, 'learning_rate': 0.024140401464252947, 'max_depth': 11, 'max_bin': 182, 'num_leaves': 535}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:44:25,760]\u001b[0m Trial 496 finished with value: 0.67588218507527 and parameters: {'n_estimators': 709, 'learning_rate': 0.03203589184880319, 'max_depth': 12, 'max_bin': 199, 'num_leaves': 563}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:44:35,540]\u001b[0m Trial 497 finished with value: 0.6750205482247705 and parameters: {'n_estimators': 723, 'learning_rate': 0.043296386327667684, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 519}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:44:41,483]\u001b[0m Trial 498 finished with value: 0.6728193166067985 and parameters: {'n_estimators': 900, 'learning_rate': 0.08337122558439661, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 494}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:44:46,663]\u001b[0m Trial 499 finished with value: 0.6649265054962612 and parameters: {'n_estimators': 261, 'learning_rate': 0.057908047804835015, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 582}. Best is trial 187 with value: 0.6901569662717765.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.690156966\n",
      "\tBest params:\n",
      "\t\tn_estimators: 699\n",
      "\t\tlearning_rate: 0.06968993674967647\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 202\n",
      "\t\tnum_leaves: 556\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665298    0.676487    0.697708    0.684076   \n",
      "1                    TP  377.000000  387.000000  413.000000  387.000000   \n",
      "2                    TN  373.000000  343.000000  342.000000  354.000000   \n",
      "3                    FP   71.000000   73.000000   81.000000   72.000000   \n",
      "4                    FN   78.000000   96.000000   63.000000   86.000000   \n",
      "5              Accuracy    0.834260    0.812013    0.839822    0.824249   \n",
      "6             Precision    0.841518    0.841304    0.836032    0.843137   \n",
      "7           Sensitivity    0.828571    0.801242    0.867647    0.818182   \n",
      "8           Specificity    0.840100    0.824500    0.808500    0.831000   \n",
      "9              F1 score    0.834994    0.820785    0.851546    0.830472   \n",
      "10  F1 score (weighted)    0.834266    0.812249    0.839567    0.824350   \n",
      "11     F1 score (macro)    0.834257    0.811562    0.838817    0.824012   \n",
      "12    Balanced Accuracy    0.834331    0.812881    0.838079    0.824584   \n",
      "13                  MCC    0.668615    0.624192    0.678314    0.648425   \n",
      "14                  NPV    0.827100    0.781300    0.844400    0.804500   \n",
      "15              ROC_AUC    0.834331    0.812881    0.838079    0.824584   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.650299    0.681476    0.707530    0.679296    0.669725    0.712637  \n",
      "1   385.000000  392.000000  401.000000  394.000000  394.000000  408.000000  \n",
      "2   357.000000  360.000000  355.000000  330.000000  337.000000  351.000000  \n",
      "3    73.000000   63.000000   72.000000   86.000000   79.000000   78.000000  \n",
      "4    84.000000   84.000000   71.000000   89.000000   89.000000   62.000000  \n",
      "5     0.825362    0.836485    0.840934    0.805339    0.813126    0.844271  \n",
      "6     0.840611    0.861538    0.847780    0.820833    0.832981    0.839506  \n",
      "7     0.820896    0.823529    0.849576    0.815735    0.815735    0.868085  \n",
      "8     0.830200    0.851100    0.831400    0.793300    0.810100    0.818200  \n",
      "9     0.830636    0.842105    0.848677    0.818276    0.824268    0.853556  \n",
      "10    0.825428    0.836621    0.840925    0.805386    0.813258    0.844095  \n",
      "11    0.825192    0.836278    0.840517    0.804348    0.812371    0.843643  \n",
      "12    0.825564    0.837297    0.840479    0.804502    0.812916    0.843133  \n",
      "13    0.650631    0.673470    0.681036    0.608713    0.624945    0.687824  \n",
      "14    0.809500    0.810800    0.833300    0.787600    0.791100    0.849900  \n",
      "15    0.825564    0.837297    0.840479    0.804502    0.812916    0.843133  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_9_cat = np.where((y_pred_lgbm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                           np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAEaCAYAAACSFRnbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/MUlEQVR4nO3deVhUZf8/8PcMw4AIKDiaIZsK4l4ugYoIGtoTtthqG4qBabhk5pKZ4ZIolj5pYRkqZGZpfqksbZEUQQUX3BIUEFcShAEVlWWYmfv3hz/P4wjKgKzj+3VdXhdnu8/nPoO85+wyIYQAERGRCZE3dAFERES1jeFGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmh+FGDSooKAj+/v6VTpPJZFi/fn09V/RgCgkJgZ+fX52uY+7cuXBzc6vTddQGhUKBmJiYhi6D7hPDjagK5eXlqMtnHWg0mjpruyE01f401bqpcgw3ahJGjx6NYcOGVRg/ePBgBAUFAfjfnsGGDRvQoUMHWFpawt/fH2fOnDFYZvv27fD29kazZs3Qrl07jBkzBgUFBdL0W3uTn3/+OVxdXWFhYYEbN27Az88Pb775Jt5//32oVCrY2toiJCQEJSUlBm37+fnB3t4eLVq0gK+vL/bv32+wfplMhhUrVuC1115DixYt8PrrrwMAZs+ejS5dusDKygpOTk4YP348rl69Ki0XExMDhUKBnTt3okePHmjWrBl8fX1x8eJFJCQkoFevXmjevDn8/f3x77//Gt3nuXPnYs2aNdi1axdkMhlkMpm053L9+nW88847aNeuHaysrNCrVy/ExsZK7Z49exYymQzfffcdAgIC0Lx5c3zwwQdGfaa3Pq9NmzbB3d0dVlZWGDFiBIqKihAbGwsPDw/Y2NjgxRdfNNgOtz6fZcuWSXW98MILUKvV0jxCCHz66afo0KEDlEolOnbsiM8++8xg/a6urvjwww8RGhqKVq1awdvbG66urtDpdBgzZoy0LQDg8uXLeOONN+Ds7IxmzZrBw8MDS5cuNfjSc6uur7/+Gi4uLrC1tcWzzz6L/Px8g/XGxcXBx8cHVlZW0u9IVlaWNP2HH37Ao48+CktLS7i6umLq1Km4ceOGNH337t3w9vaGjY0NbGxs8Mgjj+DPP/80aps/UARRAxo9erR4/PHHK50GQHz77bdCCCH27t0rZDKZOH36tDT91KlTQiaTid27dwshhAgLCxNWVlbC29tb7N+/X+zfv194enqKnj17Cr1eL4QQ4u+//xbNmjUTK1asEBkZGWL//v3Cz89P+Pj4SPOMHj1a2NjYiBEjRojDhw+LY8eOifLycuHr6ytsbGxESEiISEtLE1u2bBGtW7cWkyZNkmqKjY0VmzZtEunp6eL48eMiODhY2NnZCbVabdAve3t7sWLFCnHq1CmRnp4uhBBiwYIFIiEhQZw5c0bExcUJDw8PMWrUKGm56OhoIZPJhK+vr0hOThYpKSnCzc1NDBw4UPj6+oqkpCRx6NAh4eHhIV5++WVpuar6fO3aNfHaa6+J/v37i5ycHJGTkyOKi4uFXq8Xfn5+wtfXVyQmJoqsrCyxatUqYW5uLuLi4oQQQpw5c0YAEO3atRPffvutyMrKMviMbhcWFiY6duxoMGxlZSUCAgLE0aNHRXx8vFCpVGLo0KHiySefFEeOHBEJCQmiTZs2YsaMGQa/MzY2NuLpp58Wx44dEzt37hRubm7i6aeflub54osvhKWlpVi1apXIyMgQX375pbCwsBCrV6+W5nFxcRE2NjYiLCxMpKeni9TUVJGXlyfMzMzEZ599Jm0LIYTIyckRixcvFikpKeL06dPi22+/Fc2bNxdr1641qMvW1la88sor4p9//hF79uwRzs7OBp/h9u3bhVwuF++88444cuSIOHHihFi9erU4ceKE9Bm3bNlSrFu3TmRlZYldu3aJHj16iDfeeEMIIYRWqxV2dnbi3XffFRkZGSIjI0PExsaKhISESrf5g4zhRg1q9OjRwszMTDRv3rzCv9vDTQghevToIWbPni0Nv//++6Jr167ScFhYmAAgMjMzpXHp6ekCgNi+fbsQQghfX18xc+ZMgxrOnTsnAIjDhw9LNbVo0UJcu3bNYD5fX1/h4uIitFqtNG7VqlVCqVSK69evV9o/nU4nWrZsKdavXy+NAyDefPPNKrdNbGysUCqVQqfTCSFu/uG7vU4hhFiyZIkAIA4ePCiNW7ZsmWjVqpVB3VX1OTg4WPj6+hrMs3PnTmFhYSGuXLliMH7MmDHi2WefFUL8L9zmz59fZX8qCzczMzORn58vjQsNDRVyuVzk5eVJ4yZPniz69OkjDY8ePVo0b97coK4///xTABAZGRlCCCEcHR3F9OnTDdY/ZcoU0b59e2nYxcVFDBkypEKdZmZmIjo6usr+TJ48Wfj7+xvUpVKpRGlpqTRu0aJFom3bttLwwIEDxfDhw+/apouLi/jyyy8Nxu3atUsAEIWFhaKwsFAAEDt37qyyvgcdD0tSg/Py8sKRI0cq/LvTuHHjEB0dDZ1OB61Wi5iYGIwdO9ZgntatWxtctNCpUyeoVCqkpaUBAA4cOIDPPvsM1tbW0r+uXbsCADIzM6XlunTpAmtr6wo1eHp6wszMTBr29vaGRqORDiudOXMGgYGBcHNzg62tLWxtbXH16lWcO3euQjt3io2NxaBBg+Dg4ABra2u8/vrr0Gg0yM3NleaRyWTo0aOHNNy2bVsAQM+ePQ3GFRQUQKfTVavPdzpw4AA0Gg3atWtnsOz69esrLFdZf4zRrl07qFQqg9rbtm2L1q1bG4zLy8szWK5r165o0aKFNOzt7Q0AOHHiBIqKipCdnY1BgwYZLOPr64uzZ8+iuLi42nXr9XosXrwYjz76KFQqFaytrfHVV19V+Fy7dOkCCwsLg/5dunRJGk5JSan08DoA5Ofn49y5c5g6darB9n7yyScBAKdOnYKdnR1CQkLwxBNP4Mknn8TixYuRnp5uVB8eNIqGLoCoWbNmRl1FFxgYiJkzZ2Lr1q3Q6/W4fPkyRo0aVeVy4rbzInq9HjNnzkRgYGCF+W4FBQA0b97cqNrFHReaPPXUU1CpVIiMjISTkxOUSiUGDhxY4WKFO9vft28fXnrpJcyaNQuffPIJ7OzskJycjNGjRxssK5fLDcL11jkhc3PzCuNu1WZsn++k1+vRokULHDhwoMI0pVJ5z/4Y6/a6gZu1VzZOr9dXu+1b2+GWOz8rwPi6ly5dikWLFmHZsmXo3bs3bGxs8N///hdbt241mO/O7SKTySqs9866brnVx+XLl2Pw4MEVpjs6OgIAoqKi8M477+Cvv/7C9u3bMWfOHHzxxRcYN26cUX15UDDcqMmwtbXFK6+8gqioKOj1erzwwguwt7c3mCc/Px9ZWVno2LEjACAjIwMFBQXo0qULAKBv375ITU2t8SXpBw4cgE6nkwImKSlJumChoKAAaWlp2LZtG5544gkAQHZ2doW9jsrs3r0bKpUKH3/8sTRu8+bNNarxTsb0WalUSnt6ty935coVlJaWonv37rVSS225tYdma2sLANi7dy+Am3tOtra2cHR0xK5duzB8+HBpmYSEBLRv3x5WVlb3bLuybZGQkID//Oc/CA4Olsbda6/3bvr06YM///wTkyZNqjDtoYcegpOTE9LT0ysckbhT9+7d0b17d0ydOhXjx4/H119/zXC7Aw9LUpMybtw4/P777/jzzz/x1ltvVZhuZWWFMWPGICUlBQcPHsTo0aPRo0cP6V66+fPn45dffsG7776LI0eOICsrC3/88QeCg4MNrnq8m4KCAkyYMAEnTpzA1q1bMWfOHIwdOxbNmzeHnZ0dWrdujaioKGRkZCApKQmvvvoqmjVrVmW7Hh4eyM/Px5o1a3D69GmsW7cOK1eurP4GqoQxfW7fvj1OnjyJ1NRUqNVqlJWVYciQIfD398fzzz+Pn376CadPn0ZKSgo+//xzREVF1UptNSWTyTBq1CgcP34cCQkJmDBhAoYPHw53d3cAwKxZs6Q6MzMzsWrVKnz55ZdGXcnZvn177Ny5ExcvXpSuwPTw8EB8fDx27tyJjIwMfPjhh9i3b1+1654zZw5+//13TJkyBceOHUN6ejpiYmKkQ4sLFy7EihUr8PHHH+P48eNIT0/Hzz//LAXXqVOnMHPmTOzevRvnzp1DUlISEhMTpcPM9D8MN2pSHnvsMfTo0QMdO3aEr69vhekPP/ww3nrrLbzwwgvSpe8//fSTdCho8ODB2LFjB/755x/4+PigZ8+eePfdd2FjY1PhcFhlXnzxRdjY2GDgwIF45ZVXEBAQgCVLlgC4ecjwxx9/RFZWFnr27ImgoCBMmTIFDz/8cJXtPvXUU5g9ezY++OAD9OjRAz/88AM++eSTam6dyhnT5+DgYDz22GMYMGAAWrduje+//x4ymQxbtmzB888/j6lTp6Jz584YPnw4tm7dKu0ZNxRPT08MHDgQQ4cOxRNPPIFu3bohOjpamv72229j/vz5CA8PR9euXREREYHFixcb7HndzdKlS5GSkoL27dtL5/7mzJkDX19fPPvss+jfvz8uX76MyZMnV7vuYcOGYdu2bdi3bx+8vLzg6emJb775RvocAgMDsWnTJmzduhWenp547LHHMHfuXLRr1w7AzcOomZmZeOWVV9CpUye88MILGDBgAL744otq12LqZKKyA9FEjZRWq4WLiwumTp2K9957z2Da3LlzsX79epw6dapO1u3n5wc3NzesXr26Tton4wQFBSE7OxtxcXENXQo1YjznRk2CXq9HXl4eVq1ahevXryMkJKShSyKiRozhRk3C+fPn0b59ezz88MOIjo42uAyciOhOPCxJREQmhxeUEBGRyWG4ERGRyeE5t0bk4sWLDV1CrVKpVAZPajcF7FPTwD41DffbJwcHh7tO454bERGZHIYbERGZHIYbERGZHIYbERGZHIYbERGZHIYbERGZHIYbERGZHIYbERGZHN7E3Yg8teZkQ5dARFRvkmcOrLO2uedGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmh+FGREQmp17CLTAwsM7X8ddff2HXrl11vp7KxMfHo7CwsEHWTUREFTWpV97o9XrI5ZXn8bBhwxps3fHx8XBycoK9vX2d1kBERMap93DbsmULkpKSUF5eDk9PT7z88ssAgCVLlqCgoADl5eUICAiAv78/gJt7fU899RSOHj2KUaNGYeHChQgICMChQ4egVCoxffp0tGzZEps2bYKlpSWeeeYZzJ07F25ubkhNTUVxcTHGjx+PLl26oKysDJGRkbh48SLatWuH/Px8BAcHo2PHjpXWeue6jx8/jpSUFGg0GnTq1AlvvfUW9u3bh6ysLKxYsQJKpRILFy5EdnY2vvnmG5SWlsLW1hahoaGws7Ort21MRPSgq9dwO3r0KHJychAeHg4hBJYsWYK0tDR07doVoaGhsLa2hkajwaxZs+Dl5QUbGxuUlZXByckJI0eOBACUlZXB3d0dr776KtavX4+///4bL7zwQoV16fV6LFq0CIcOHcLmzZsxZ84c/Pnnn7C2tsann36K8+fPY8aMGfes9851Ozo64sUXXwQAfP7550hJSUG/fv3wxx9/IDAwEB07doRWq8XatWsxY8YM2NraYu/evfj+++8RGhpaof24uDjExcUBABYvXnxf25aIqKlRKBRQqVR103adtHoXR48exbFjx6RQKS0tRW5uLrp27Ypt27bhwIEDAAC1Wo2cnBzY2NhALpejX79+/ytYoUCfPn0AAB06dMCxY8cqXZenp6c0T15eHgDg5MmTCAgIAAA4OzvDxcXlnvXeue7jx49jy5YtKCsrw/Xr1+Hk5IS+ffsaLHPx4kVcuHABCxYsAHAzZO+21+bv7y/toRIRPWi0Wi3UanWNl3dwcLjrtHo/LDlixAgMHTrUYFxqair++ecffPzxx7CwsMDcuXNRXl4OADA3Nzc412VmZgaZTAbgZvjodLpK12Nubi7No9fra1Tr7evWaDRYs2YNFi1aBJVKhU2bNkGj0VS6nKOjIxYuXFijdRIR0f2r11sBHnnkEezcuROlpaUAgMLCQly9ehXFxcVo3rw5LCws8O+//yIzM7NO1t+5c2ckJSUBALKzs3H+/Hmjl70Vtra2tigtLcW+ffukaZaWligpKQFw85tEUVERMjIyANz8ZnLhwoXa6gIRERmhXvfcHnnkEfz777+YPXs2gJuhMGnSJDz66KPYvn07pk2bBgcHB7i7u9fJ+ocNG4bIyEhMmzYNrq6ucHZ2hpWVlVHLNm/eHI8//jjee+89tGnTxuAiFD8/P0RFRUkXlLz33nuIjo5GcXExdDodAgIC4OTkVCd9IiKiimRCCNHQRdQXvV4PrVYLpVKJ3NxcLFiwAMuXL4dC0TjuiOi9YEdDl0BEVG+SZw40nXNuDamsrAzz5s2DTqeDEAIhISGNJtiIiKj2PFB/2Zs1a1bpJfcffPCBdE7tlkmTJsHZ2bm+SiMiolr0QIXb3YSHhzd0CUREVIv44GQiIjI5DDciIjI5DDciIjI5DDciIjI5DDciIjI5DDciIjI5DDciIjI5vM+tEfktuHNDl1CrVCrVfT1apzFin5oG9om450ZERCaH4UZERCaH4UZERCaH4UZERCaH4UZERCaH4UZERCaH4UZERCaH97k1Ik+tOdnQJVAjYGr3OxI1BO65ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyTE63PR6fV3WQUREVGuMCje9Xo/AwECUl5fXdT1ERET3zahwk8vlcHBwwLVr1+q6HiIiovtm9FsBBg4ciIiICDz55JNo1aoVZDKZNK179+51UhwREVFNGB1uf/31FwDgxx9/NBgvk8nwxRdf1G5VDWDChAlYtGgRbG1tq71sfHw8evbsCXt7+/tui4iI7p/R4RYZGVmXdTRp8fHxcHJyksKNiIgaVrVeVqrVapGZmYnLly9jwIABKC0tBQBYWlrWWkF5eXkIDw9H586dkZmZCRcXF/j5+eHHH3/E1atXMXnyZABATEwMNBoNlEolQkND4eDggN9++w3nz59HaGgozp8/j+XLlyM8PBwWFhYV1nPt2jUsX74cRUVFcHNzgxBCmpaQkIDff/8dWq0W7u7uCAkJgVwuR2BgIIYOHYrU1FQ0b94cU6ZMQVpaGrKysrBixQoolUosXLgQAPDHH38gJSUFWq0WU6dORbt27SrUEBcXh7i4OADA4sWLa20bUtOmUqnqdX0KhaLe11nX2KemoS77ZHS4nT9/HhERETA3N0dBQQEGDBiAtLQ07Nq1C++++26tFpWbm4upU6fC0dERs2bNwu7duzF//nwcPHgQsbGxmDhxIubNmwczMzMcO3YMGzZswLRp0xAQEIB58+Zh//79iI2NxdixYysNNuDm4dXOnTvjxRdfxKFDh6SQyc7Oxt69e7FgwQIoFAqsXr0aiYmJ8PX1RVlZGdq3b49Ro0Zh8+bN+PHHHxEcHIw//vgDgYGB6Nixo9S+jY0NIiIi8Oeff+LXX3/F+PHjK9Tg7+8Pf3//Wt121PSp1ep6XZ9Kpar3ddY19qlpuN8+OTg43HWa0eEWFRWFkSNHYtCgQRgzZgwAoGvXrli1alWNC7ubNm3awNnZGQDg5OSEHj16QCaTwdnZGfn5+SguLkZkZCRyc3MBADqdDsDNqzpDQ0Mxbdo0DB06FJ07d77rOk6cOIFp06YBAHr37o3mzZsDAI4fP44zZ85g1qxZAACNRiOdO5PJZBgwYAAAwMfHB59++uld2/fy8gIAdOjQAfv376/xtiAiouozOtyys7Ph4+NjMM7S0hIajabWizI3N5d+lslk0rBMJoNer8fGjRvRrVs3TJ8+HXl5eZg3b540f05ODiwtLVFYWFjlem6/4vMWIQR8fX3x2muv1Wj5WxSKm5tWLpdL4UtERPXD6CeUtG7dGqdPnzYYd+rUKbRt27bWi6pKcXGxdPFGfHy8wfiYmBjMmzcP169fR3Jy8l3b6NKlCxITEwEAhw8fxo0bNwAAPXr0QHJyMq5evQoAuH79OvLz8wHcDL5bbe7evVvaM7S0tERJSUntdpKIiGrM6D23kSNHYvHixRg6dCi0Wi1++uknbN++HePGjavL+ir17LPPIjIyElu3bkW3bt2k8TExMRg2bBgcHBwwfvx4zJs3D126dEGLFi0qtPHSSy9h+fLlmDlzJrp06SKd1HR0dMQrr7yCjz/+GEIImJmZITg4GK1bt4aFhQUuXLiAmTNnwsrKSjrX6Ofnh6ioKIMLSoiIqOHIxO2XCVbh9OnT2LFjB/Lz89GqVSv4+/ujQ4cOdVlfoxIYGIhvv/22ztrvvWBHnbVNTcdvwXc/V1wXeKFC08A+VVQrF5QkJSWhf//+FcIsOTkZ/fr1q3FxREREtc3ocPvqq6/Qv3//CuNXrVrVqMNt586d2LZtm8E4Dw8PhISEVLututxrIyKi2lNluF26dAnAzTcD5OXlGdzsfOnSJSiVyrqrrhYMHjwYgwcPbugyiIioHlUZbreeCAIAkyZNMpjWsmVLvPTSS7VfFRER0X2oMtw2btwIAAgLCzO4n4yIiKixMvo+t1vBplarkZGRUWcFERER3S+jLyhRq9VYvnw5zp49C+DmxRXJyck4cuRIpc9NJCIiaihG77l9/fXX6NWrF7755hvp0VI9e/bEsWPH6qw4IiKimjA63E6dOoURI0ZALv/fIlZWViguLq6TwoiIiGrK6MOSLVq0QG5ursEd4dnZ2Sb3fqGGVN9PpqhrfKICETUUo8Pt6aefRkREBEaMGAG9Xo/du3fjp59+wogRI+qwPCIiouozOtyGDBkCa2tr/P3332jVqhV27dqFkSNHwtPTsy7rIyIiqjajww0APD09GWZERNToVSvcTpw4gTNnzqC0tNRg/PPPP1+rRREREd0Po8Nt7dq1SEpKQufOnQ2eJ3mvt1ETERE1BKPDLTExEUuXLpXegE1ERNRYGX2fm0qlgrm5eV3WQkREVCuM3nMbP348Vq1aBW9vb7Ro0cJgWteuXWu9MCIiopoyOtxOnz6Nw4cP48SJExXe4fbll1/WemEPoqfWnGzoEhoFU7uZnYjqn9Hh9v3332PmzJno2bNnXdZDRER034w+52ZhYcHDj0RE1CQYHW4jR45ETEwMrly5Ar1eb/CPiIioMTH6sOSt82rbt2+vMO3W27qJiIgaA6PD7YsvvqjLOoiIiGqN0eHWunXruqyDiIio1lTr2ZIHDx5EWloaioqKDMZPnDixVosiIiK6H0ZfUPLjjz/i66+/hl6vR3JyMqytrXH06FFYWVnVZX1ERETVZvSe286dO/Hhhx/C2dkZ8fHxCAoKwsCBA/F///d/dVkfERFRtRm953bjxg04OzsDABQKBbRaLdzc3JCWllZnxREREdWE0Xtubdu2xYULF+Dk5AQnJyf89ddfsLa2hrW1dV3WR0REVG1Gh9vIkSNx7do1AMDrr7+O5cuXo7S0FCEhIXVWHBERUU0YFW56vR5KpRKdOnUCALi5ueHzzz+v08KIiIhqyqhzbnK5HEuWLIFCUa07B+rN2bNncejQIWn44MGD+Pnnn2ul7a1bt6KsrKxW2iIiovph9AUlXbp0QUZGRl3WUmNnz57F4cOHpeG+fftixIgRtdL2tm3bqh1ufN4mEVHDqtYTShYtWoS+ffuiVatWkMlk0rSRI0ca1UZeXh4WLVoEDw8PZGRkwN7eHjNmzKjwfjgAyM3NxZo1a1BUVAQLCwuMGzcO7dq1Q1JSEjZv3gy5XA4rKyvMmTMHGzduhEajwcmTJ/Hcc89Bo9EgKysLwcHBiIyMhFKpxMWLF5Gfn4/Q0FDEx8cjMzMTbm5umDBhAgAgKioKWVlZ0Gg06NevH15++WVs27YNhYWFmDdvHmxtbREWFobdu3fjp59+AgD06tULb7zxBgAgMDAQTz31FI4ePYpRo0YhJSUFBw8ehJmZGXr27IlRo0ZV6GNcXBzi4uIAAIsXLzb2ozB5KpWqoUu4K4VC0ajrqwn2qWlgn6rZtrEzajQaPPbYYwCAwsLCGq8wJycH77zzDsaPH49ly5YhOTkZgwYNqjDf119/jbFjx+Lhhx9GZmYmVq9ejbCwMGzevBmzZ8+Gvb09bty4AYVCgZEjR0phBgDx8fEGbd24cQMfffQRDh48iIiICCxYsACOjo6YNWsWzp49C1dXV7z66quwtraGXq/H/Pnzce7cOQQEBGDr1q0ICwuDra0tCgsL8d133yEiIgLNmzfHxx9/jP3798PT0xNlZWVwcnLCyJEjcf36dXz55Zf47LPPIJPJcOPGjUq3hb+/P/z9/Wu8LU2VWq1u6BLuSqVSNer6aoJ9ahrYp4ocHBzuOs3ocAsNDa1xAbdr06YNXF1dAQAdOnRAfn5+hXlKS0uRnp6OZcuWSeO0Wi0AwMPDA5GRkejfvz+8vLyMWmefPn0gk8ng7OyMFi1aSPfrOTk5IS8vD66urti7dy/+/vtv6HQ6XL58GdnZ2XBxcTFoJysrC926dYOtrS0AwMfHBydOnICnpyfkcjn69esHAGjWrBmUSiW++uor9O7dG3369KneRiIiovtS7StESkpKcO3aNQghpHEPPfSQ0cubm5tLP8vlcmg0mgrz6PV6NG/eHJ988kmFaW+99RYyMzNx6NAhzJgxA0uWLDF6nTKZzGD9MpkMer0eeXl5+PXXX7Fo0SJYW1sjMjIS5eXlFdq5vc+VrUMuv3kK08zMDOHh4fjnn3+wd+9e/PHHHwgLC6uyTiIiqh1Gh1t2djZWrFiBc+fOVZhW2+9zs7KyQps2bZCUlIT+/ftDCIFz587B1dUVubm5cHd3h7u7O1JSUlBQUABLS0uUlJTUeH3FxcWwtLSElZUVrly5giNHjqBbt24AAEtLS5SWlsLW1hbu7u6IiYlBUVERrK2tsWfPHvznP/+p0F5paSnKysrQu3dvdOrUCZMmTapxbUREVH1Gh9vq1avRrVs3hIWFYeLEiYiMjMSGDRuke99q2+TJkxEVFYXY2FhotVp4e3vD1dUV69evR05ODgCge/fucHFxgUqlwi+//ILp06fjueeeq/a6XF1d4erqivfeew9t2rSBh4eHNM3f3x/h4eGws7NDWFgYXnvtNcybNw/AzQtKbp2HvF1JSQmWLFmC8vJyCCEwevToGm4FIiKqCZm417G224wZMwZRUVFQKBQICgpCTEwMSktL8d577yEyMrKu63wg9F6wo6FLaBR+C+7c0CXcFU/qNw3sU9NQlxeUGH2fm7m5OXQ6HQDAxsYGarUaQghcv369xoURERHVBaMPS3bu3BlJSUnw8/NDv379EB4eDnNzc+ncVE2tXr0a6enpBuMCAgIwePDg+2qXiIgeXEaH29SpU6WfX331VTg5OaG0tLTSe9Sqgw9eJiKi2lbtWwFuHYr08fExeEoJERFRY2F0uN24cQNr165FcnIytFotFAoF+vXrhzFjxvCdbkRE1KgYfUHJypUrodFoEBERgXXr1iEiIgLl5eVYuXJlXdZHRERUbUaHW2pqKiZNmgRHR0dYWFjA0dEREyZMQFpaWl3WR0REVG1Gh5uDgwPy8vIMxqnV6nveZ0BERNQQjD7n1r17dyxcuBA+Pj7SjXeJiYkYNGgQduz4383HQ4YMqZNCiYiIjGV0uGVmZqJt27bIzMxEZmYmAKBt27bIyMgweIkpw42IiBqaUeEmhMD48eOhUqlgZmZW1zU9sBrzY6dqwhQfF0RETYNR59xkMhmmTZvG+9qIiKhJMPqCEldXV+lp/ERERI2Z0efcunXrhvDwcPj6+kKlUhlM43k2IiJqTIwOt/T0dLRp0wYnTpyoMI3hRkREjYnR4RYWFlaXdRAREdUao8+5AcC1a9eQkJCALVu2AAAKCwtRUFBQJ4URERHVlNHhlpaWhilTpiAxMRGbN28GAOTm5iIqKqrOiiMiIqoJow9LxsTEYMqUKejRowfGjBkDAHBzc0NWVladFfegeWrNSaPnNbV74oiIapPRe275+fno0aOHwTiFQgGdTlfrRREREd0Po8PN0dERR44cMRj3zz//wNnZubZrIiIiui9GH5YMDAxEREQEevXqBY1Gg6+//hopKSmYPn16XdZHRERUbUaHW6dOnfDJJ58gMTERlpaWUKlUCA8PR6tWreqyPiIiomozOtwAwN7eHs888wyuXbsGGxsbPmuSiIgaJaPD7caNG1i7di2Sk5Oh1WqhUCjQr18/jBkzBtbW1nVZIxERUbUYfUHJypUrodFoEBERgXXr1iEiIgLl5eVYuXJlXdZHRERUbUaHW2pqKiZNmgRHR0dYWFjA0dEREyZMQFpaWl3WR0REVG1Gh5uDgwPy8vIMxqnVajg4ONR6UURERPfD6HNu3bt3x8KFC+Hj4yO9YTkxMRGDBg3Cjh07pPn4hgAiImpoRodbZmYm2rZti8zMTGRmZgIA2rZti4yMDGRkZEjzMdyIiKih8ZU3RERkcow+5/bNN9/g7NmzdVgKERFR7TB6z02n02HhwoWwtbWFj48PfHx8mtzTSSIjI9GnTx/069evoUshIqI6ZHS4vfnmmwgKCsLhw4eRmJiI2NhYuLu7Y9CgQfDy8oKlpWVd1klERGQ0mRBC1GTBCxcuYMWKFTh//jyUSiW8vb3x8ssvw97evlrt5OXlYdGiRfDw8EBGRgbs7e0xY8YMhIeHIzAwEB07dkRRURFmzZqFyMhIxMfHY//+/dDr9bhw4QKefvppaLVaJCQkwNzcHLNmzbrrE1Nu33M7ffo0vvnmG5SWlsLW1hahoaGws7NDXFwc/v77b2i1Wjz00EOYNGkSdDodpk+fjs8//xxyuRxlZWWYMmUKPv/8c6jVaqxZswZFRUWwsLDAuHHj0K5dOyQlJWHz5s2Qy+WwsrLCvHnzKtQTFxeHuLg4AMDixYvRe8GOCvPcTfLMgdXazg1BoVBAq9U2dBm1in1qGtinpuF++6RUKu/ednUaKi4uRnJyMhITE3Hu3Dl4eXkhODgYKpUKv/32G8LDw/Hpp59Wu8CcnBy88847GD9+PJYtW4bk5OR7zn/hwgUsWbIE5eXlmDRpEl5//XUsWbIEMTEx2LVrF4YPH37P5bVaLdauXYsZM2bA1tYWe/fuxffff4/Q0FB4eXnB398fAPDDDz9gx44dePLJJ+Hi4oK0tDR0794dKSkpeOSRR6BQKPD1119j7NixePjhh5GZmYnVq1cjLCwMmzdvxuzZs2Fvb48bN25UWoe/v7+0rupSq9U1Wq4+3bplxJSwT00D+9Q03G+f7nWftdHhtnTpUhw5cgRdu3bF0KFD8dhjj8Hc3FyaPmrUKAQFBdWowDZt2sDV1RUA0KFDB+Tn599z/m7duqFZs2Zo1qwZrKys0LdvXwCAs7Mzzp8/X+X6Ll68iAsXLmDBggUAAL1eDzs7OwA3g/OHH37AjRs3UFpaikceeQQAMGDAAOzduxfdu3fHnj178MQTT6C0tBTp6elYtmyZ1PatbyEeHh6IjIxE//794eXlVb0NQkRE98XocHN3d0dwcDBatmxZ6XS5XI6oqKgaFXF7SMrlcmg0GpiZmeHWEdPy8vJ7zq9QKKSfjX0zuKOjIxYuXFhhfGRkJKZPnw5XV1fEx8cjNTUVANC3b19s2LAB169fx+nTp9G9e3eUlpaiefPm+OSTTyq089ZbbyEzMxOHDh3CjBkzsGTJEtjY2BhVGxER3Z8qw+2jjz6SXm2TkpJS6Ty3zidZWFjUWmGtW7fG6dOn4ebmVuVhyupycHBAUVERMjIy0KlTJ2i1WuTk5MDJyQmlpaWws7ODVqtFYmKidA7R0tISbm5uiI6ORp8+faRzaW3atEFSUhL69+8PIQTOnTsHV1dX5Obmwt3dHe7u7khJSUFBQQHDjYionlQZbnc+cWTNmjUIDg6us4Juefrpp/Hf//4XCQkJ6N69e622rVAo8N577yE6OhrFxcXQ6XQICAiAk5MTRo4ciQ8++ACtW7eGs7MzSkpKpOUGDBiAZcuWYe7cudK4yZMnIyoqCrGxsdBqtfD29oarqyvWr1+PnJwcADcfXebi4lKrfSAiorur9tWSY8aMQXR0dF3V80CrztWSvwV3rsNKagdPgDcN7FPTwD5VdK8LSox+QgkREVFTUa1bAZqK1atXIz093WBcQEAABg8e3EAVERFRfaoy3I4fP24wrNfrK4yr7XNi9yskJKShSyAiogZUZbh9+eWXBsPW1tYG42QyGb744ovar4yIiKiGqgy3yMjI+qiDiIio1vCCEiIiMjkMNyIiMjkMNyIiMjkMNyIiMjkMNyIiMjkmeRN3U9UUHqlFRNQUcM+NiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsONiIhMDsOtGjZt2oQtW7ZUGF9YWIilS5c2QEVERFQZhlstsLe3x3vvvdfQZRAR0f8nE0KIhi7ifuXl5SE8PBydO3dGZmYmXFxc4Ofnhx9//BFXr17F5MmTAQAxMTHQaDRQKpUIDQ2Fg4MDfvvtN5w/fx6hoaE4f/48li9fjvDwcFhYWFRYz6ZNm3Dp0iUUFhaioKAAzzzzDPz9/ZGXl4eIiAgsXboU8fHxOHjwIMrKynDp0iV4enrijTfeqLTuuLg4xMXFAQAWL14MjUZTdxupASgUCmi12oYuo1axT00D+9Q03G+flErl3duucauNTG5uLqZOnQpHR0fMmjULu3fvxvz583Hw4EHExsZi4sSJmDdvHszMzHDs2DFs2LAB06ZNQ0BAAObNm4f9+/cjNjYWY8eOrTTYbjl//jwWLlyI0tJSzJw5E717964wz9mzZ7FkyRIoFApMmTIF//nPf6BSqSrM5+/vD39/f2lYrVbXzsZoJFQqFfvUBLBPTQP7VJGDg8Ndp5lMuLVp0wbOzs4AACcnJ/To0QMymQzOzs7Iz89HcXExIiMjkZubCwDQ6XQAALlcjtDQUEybNg1Dhw5F586d77mevn37QqlUQqlUolu3bjh16hRcXV0N5unevTusrKwAAI6OjlCr1ZWGGxER1Q2TOedmbm4u/SyTyaRhmUwGvV6PjRs3olu3bli6dClmzpyJ8vJyaf6cnBxYWlqisLCwyvXIZLJ7Dt9Zi1wul4KUiIjqh8mEW1WKi4thb28PAIiPjzcYHxMTg3nz5uH69etITk6+ZzsHDhyARqPBtWvXkJqaio4dO9Zl2UREVAMPTLg9++yz+P777zFnzhzo9XppfExMDIYNGwYHBweMHz8e3333Ha5evXrXdtzc3LB48WLMnj0bL7zwghSYRETUeJjE1ZKm4uLFiw1dQq3iCfCmgX1qGtiniu51QckDs+dGREQPDpO5WrI27dy5E9u2bTMY5+HhgZCQkAaqiIiIqoPhVonBgwdj8ODBDV0GERHVEA9LEhGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyWG4ERGRyZEJIURDF0FERFSbuOfWSLz//vsNXUKtY5+aBvapaWCfqofhRkREJofhRkREJofh1kj4+/s3dAm1jn1qGtinpoF9qh5eUEJERCaHe25ERGRyGG5ERGRyFA1dwIPkyJEjiI6Ohl6vx+OPP44RI0YYTBdCIDo6GocPH4aFhQVCQ0PRoUOHhinWSFX16d9//8XKlStx5swZvPLKK3jmmWcaptBqqKpPiYmJ+OWXXwAAlpaWCAkJgaura/0XWg1V9enAgQPYuHEjZDIZzMzMEBQUhM6dOzdMsdVQVb9uOXXqFGbPno13330X/fr1q98iq6mqPqWmpmLJkiVo06YNAMDLywsvvvhiA1RqPGM+p9TUVMTExECn08HGxgbz5s27v5UKqhc6nU5MnDhR5ObmivLycjFt2jRx4cIFg3lSUlLEwoULhV6vF+np6WLWrFkNVK1xjOnTlStXRGZmptiwYYP45ZdfGqhS4xnTp5MnT4pr164JIYQ4dOiQSXxOJSUlQq/XCyGEOHv2rHjnnXcaoNLqMaZft+abO3euCA8PF0lJSQ1QqfGM6dPx48fFokWLGqjC6jOmT9evXxdTpkwR+fn5QoibfzfuFw9L1pNTp06hbdu2eOihh6BQKDBgwAAcOHDAYJ6DBw9i0KBBkMlk6NSpE27cuIHLly83UMVVM6ZPLVq0gJubG8zMzBqoyuoxpk8eHh6wtrYGALi7u6OgoKAhSjWaMX2ytLSETCYDAJSVlUk/N2bG9AsAfv/9d3h5ecHW1rYBqqweY/vUlBjTp927d8PLywsqlQrAzb8b94vhVk8KCwvRqlUrabhVq1YoLCysMM+tD/du8zQmxvSpqalun3bs2IFevXrVR2k1Zmyf9u/fjylTpmDRokV4++2367PEGjH2/9T+/fsxbNiw+i6vRoz9rDIyMjB9+nSEh4fjwoUL9VlitRnTp5ycHFy/fh1z587FzJkzsWvXrvteL8+51RNRyR0Xd347NmaexqSp1WuM6vTp+PHj2LlzJ+bPn1/XZd0XY/vk6ekJT09PpKWlYePGjZgzZ059lFdjxvQrJiYGr7/+OuTypvE93pg+tW/fHitXroSlpSUOHTqETz75BCtWrKivEqvNmD7pdDqcOXMGc+bMgUajwYcffgh3d3c4ODjUeL0Mt3rSqlUrg8NXBQUFsLOzqzCPWq2+5zyNiTF9amqM7dO5c+ewatUqzJo1CzY2NvVZYrVV93Pq2rUrIiMjUVRU1KgP5RnTr6ysLCxfvhwAUFRUhMOHD0Mul8PT07NeazWWMX2ysrKSfu7duzfWrFnTqD8rY//22djYwNLSEpaWlujSpQvOnTt3X+HWNL7OmICOHTsiJycHeXl50Gq12Lt3L/r27WswT9++fZGQkAAhBDIyMmBlZdWow8KYPjU1xvRJrVbj008/xcSJE+/rP199MaZPubm50jfs06dPQ6vVNvrQNqZfkZGR0r9+/fohJCSk0QYbYFyfrly5In1Wp06dgl6vb9SflbF/+06ePAmdToeysjKcOnUK7dq1u6/18gkl9ejQoUP45ptvoNfrMXjwYDz//PP466+/AADDhg2DEAJr1qzB0aNHoVQqERoaio4dOzZw1fdWVZ+uXLmC999/HyUlJZDJZLC0tMSyZcsMvn02NlX16auvvsK+ffuk86NmZmZYvHhxQ5Zcpar69PPPPyMhIQFmZmZQKpUIDAxsErcCVNWv20VGRqJPnz6N/laAqvr0xx9/4K+//pI+q1GjRsHDw6OBq743Yz6nLVu2YOfOnZDL5RgyZAiGDx9+X+tkuBERkcnhYUkiIjI5DDciIjI5DDciIjI5DDciIjI5DDciIjI5DDciE7N//368/fbbCAwMxJkzZ+plnfHx8fd8okl4eDji4+Nrfb111W5N5eXl4eWXX4ZOp2voUh54fEIJNSkTJkzAuHHj0LNnz4YuBXPnzoWPjw8ef/zxhi7FwLfffos333wTjz32WK21mZKSgs2bNyM7Oxvm5uZ49NFH8frrrxs8M/BePvjgg/uuYdOmTcjNzcXkyZNrtd07TZkyBc888wyGDBliMH7btm1ISEho9Pc00k3ccyOqJiEE9Hp9Q5dxV/n5+XBycqrRspX1Kzk5GStWrEBAQADWrFmDZcuWQaFQ4KOPPsL169fvt9xGx9fXFwkJCRXGJyQkwNfXtwEqoprgnhs1WfHx8fj777/RsWNHxMfHw9raGpMmTUJOTg42btyI8vJyvPHGG/Dz8wNw8wkV5ubmuHTpEjIzM9G+fXtMnDgRrVu3BgCkp6cjJiYGFy9ehIODA4KCgqQnP8ydOxceHh5IS0vD6dOn4eXlhRMnTiAzMxMxMTHw8/NDcHAwoqOjsX//fhQXF6Nt27YICgpCly5dANzc88jOzoZSqcT+/fuhUqkwYcIE6Sk0arUaMTExOHHiBIQQ8Pb2RnBwMICbbx/49ddfceXKFbi5ueGtt96S6r6lvLwcb775JvR6PaZPn46WLVvi888/R3Z2NlavXo2zZ8/C3t4er732mvT4o8jISCiVSqjVaqSlpWH69OkGe8VCCKxbtw7PP/88fHx8AABKpRLjx4/H9OnTsXXrVowcOVKaf+3atdi1axfs7OwQHByMHj16SNvv9r3ce/XnwoULiImJwenTp6FQKPDkk0+iQ4cO+OmnnwDcfLFq27Zt8cknn0jtDho0CGPHjsX8+fPh7OwM4OazJN9++22sXLkSLVq0QEpKCn744Qfk5+fD0dERY8eOhYuLS4Xfq0GDBmHjxo3Iz8+XasrOzsa5c+fg7e2NQ4cO4YcffsClS5dgZWWFwYMH4+WXX670d/TOIw137n1mZGRg3bp1yM7ORuvWrREUFIRu3bpV/gtP1XPfb4QjqkehoaHi6NGjQgghdu7cKUaOHCl27NghdDqd+P7778X48eNFVFSU0Gg04siRIyIwMFCUlJQIIYT44osvRGBgoEhNTRUajUasXbtWfPjhh0IIIa5duyaCgoLErl27hFarFYmJiSIoKEgUFRUJIYQICwsT48ePF+fPnxdarVaUl5eLsLAwERcXZ1Dfrl27RFFRkdBqtWLLli0iJCRElJWVCSGE2Lhxo3jttddESkqK0Ol04rvvvhMffPCBEOLmCx2nTZsmoqOjRUlJiSgrKxMnTpwQQgixb98+MXHiRHHhwgWh1WrF5s2bxezZs++6jV566SWRk5MjhBCivLxcTJw4Ufzf//2fKC8vF//8848IDAwU//77r7RNRo0aJU6cOCF0Op1U6y3Z2dnipZdeEpcuXaqwno0bN0r13/osfv31V1FeXi727NkjRo0aJb3U9fZtda/+FBcXi7Fjx4otW7aIsrIyUVxcLDIyMqT1LV++3KCG29uNjIwUGzZskKb9/vvv4uOPPxZCCJGVlSWCg4NFRkaG0Ol0YufOnSI0NFRoNJpKt+H8+fPF5s2bpeHvvvtORERECCFuviz03LlzQqfTibNnz4qQkBCxb98+IYQQly5dEi+99JLQarVCCMPf1zv7UFBQIMaMGSP9Phw9elSMGTNGXL16tdKaqHp4WJKatDZt2mDw4MGQy+UYMGAACgoK8OKLL8Lc3ByPPPIIFAoFcnNzpfl79+6Nrl27wtzcHK+++ioyMjKgVqtx6NAhtG3bFoMGDYKZmRkGDhwIBwcHpKSkSMv6+fnByckJZmZmUCgqP+gxaNAg2NjYwMzMDE8//TS0Wi0uXrwoTe/cuTN69+4NuVyOQYMG4ezZswBuPgC3sLAQgYGBsLS0hFKplJ7tGBcXh+eeew6Ojo4wMzPDc889h7NnzyI/P7/K7ZOZmYnS0lKMGDECCoUC3bt3R+/evbF7925pnsceewydO3eGXC6HUqk0WP7atWsAgJYtW1Zou2XLltJ04OYLJocPHy69kNLBwQGHDh2qsNy9+pOSkoKWLVvi6aefhlKpRLNmzeDu7l5lPwFg4MCB2LNnjzS8Z88eDBw4EADw999/w9/fH+7u7pDL5fDz84NCoUBmZmalbd1+aFKv1yMxMVE6AtCtWzc4OztDLpfDxcUF3t7eSEtLM6rG2yUkJKBXr17S70PPnj3RsWPHSrcZVR8PS1KTdvsbe2/9Yb79D7FSqURpaak0fPsFEJaWlrC2tsbly5dRWFhY4TBf69atDV6qaMzFE7/++it27NiBwsJCyGQylJSUVAiA22srLy+HTqeDWq1G69atK31jeX5+PqKjo7Fu3TppnBCi0prvdPnyZahUKoP3mVWnX7eeNn/lyhW0adPGYNqVK1cMnkZvb29v8J6uO9djTH8KCgrw0EMP3bNPd9O9e3doNBpkZmaiZcuWOHv2rPQGALVajV27duGPP/6Q5tdqtXd9Ea2XlxfWrFmDjIwMaDQaaDQa9O7dG8DNLwwbNmzA+fPnodVqodVqa/QwZrVajeTkZIMvUDqdjoclawnDjR4ot79XqrS0FNevX4ednR3s7e2xb98+g3nVajUeffRRafjOFyzeOXzixAn88ssv+Oijj+Do6Ai5XI4xY8ZU+rLGO6lUKqjVauh0ugoBp1KpDM55VYednR3UajX0er0UcGq1Gg8//PBd+3E7BwcHtGrVCklJSXj22Wel8Xq9Hvv27TO4IrOwsBBCCKk9tVpd6SuQ7tWf/Px8g72v21X1Ily5XI7+/ftjz549aNGiBXr37o1mzZoBuBngzz//PJ5//vl7tnGLhYUFvLy8kJCQAI1GgwEDBkh76ytWrMATTzyBWbNmQalUIiYmBkVFRXdtR6PRSMNXrlyRfm7VqhV8fHwwfvx4o2qi6uFhSXqgHD58GCdPnoRWq8UPP/wAd3d3qFQq9OrVCzk5Odi9ezd0Oh327t2L7Oxs6dt6ZVq0aIFLly5JwyUlJTAzM4OtrS30ej02b96M4uJio+pyc3ODnZ0dvvvuO5SWlkKj0eDkyZMAgKFDh+Lnn3/GhQsXAADFxcVISkoyql13d3dYWlpiy5Yt0Gq1SE1NRUpKCry9vY1aXiaTITAwELGxsdi9ezc0Gg2uXLmCr776CsXFxQavJbl69Sp+//13aLVaJCUl4d9//0WvXr0qtHmv/vTp0wdXrlzB1q1bUV5ejpKSEunQYYsWLZCfn3/PK1UHDhyIvXv3Yvfu3dIhSQB4/PHHsX37dmRmZkIIgdLSUhw6dAglJSV3bcvPzw979+7Fvn37DK6SLCkpgbW1NZRKJU6dOmVwiPdOrq6u2LNnD7RaLbKysgy+QPn4+CAlJQVHjhyBXq+HRqNBamqqwRcwqjnuudEDxdvbGz/++CMyMjLQoUMH6ao1GxsbvP/++4iOjkZUVBTatm2L999//55vNw4ICEBkZCS2b98OHx8fBAUF4dFHH8U777wDCwsLDB8+XHrnW1XkcjlmzpyJtWvXIjQ0FDKZDN7e3ujcuTM8PT1RWlqKzz77DGq1GlZWVujRowf69+9fZbsKhQIzZszA6tWr8dNPP8He3h4TJ06s1osgBwwYAHNzc8TGxmLVqlVQKBR45JFHsGDBAoPDku7u7sjJyUFwcDBatmyJqVOnVvoSzXv1p1mzZvjwww8RExODzZs3Q6FQYPjw4XB3d0f//v2RmJiI4OBgtGnTBhERERXadnd3h4WFBQoLCw2CtWPHjhg3bhzWrl2LnJwc6ZzmrStZK9OlSxdYWVnB3Nwcbm5u0viQkBCsW7cOa9euRdeuXdG/f3/cuHGj0jZGjhyJ5cuXY8yYMejatSu8vb2l2ydUKhVmzJiB9evXY/ny5ZDL5XBzc8PYsWOr/lCoSnyfGz0wIiMj0apVK7zyyisNXcoDJywsDEOGDOF9YlRveFiSiOpUWVkZLl26VOGCFKK6xHAjojpz9epVvPXWW+jatat0awNRfeBhSSIiMjnccyMiIpPDcCMiIpPDcCMiIpPDcCMiIpPDcCMiIpPz/wBAV79xDrs1mAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.697969     0.035125\n",
      "1                    TP       197.500000    11.787470\n",
      "2                    TN       173.700000     9.877584\n",
      "3                    FP        39.300000     7.587270\n",
      "4                    FN        38.700000     7.484057\n",
      "5              Accuracy         0.826360     0.026869\n",
      "6             Precision         0.834108     0.030552\n",
      "7           Sensitivity         0.835831     0.033150\n",
      "8           Specificity         0.815490     0.035580\n",
      "9              F1 score         0.834691     0.027440\n",
      "10  F1 score (weighted)         0.826291     0.026904\n",
      "11     F1 score (macro)         0.825607     0.027092\n",
      "12    Balanced Accuracy         0.825658     0.027160\n",
      "13                  MCC         0.651824     0.054374\n",
      "14                  NPV         0.818230     0.032056\n",
      "15              ROC_AUC         0.825658     0.027160\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.665298</td>\n",
       "      <td>0.676487</td>\n",
       "      <td>0.697708</td>\n",
       "      <td>0.684076</td>\n",
       "      <td>0.650299</td>\n",
       "      <td>0.681476</td>\n",
       "      <td>0.707530</td>\n",
       "      <td>0.679296</td>\n",
       "      <td>0.669725</td>\n",
       "      <td>0.712637</td>\n",
       "      <td>0.682453</td>\n",
       "      <td>0.019183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>393.800000</td>\n",
       "      <td>10.942273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>350.200000</td>\n",
       "      <td>12.479316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>74.800000</td>\n",
       "      <td>6.390966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>80.200000</td>\n",
       "      <td>11.467829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.834260</td>\n",
       "      <td>0.812013</td>\n",
       "      <td>0.839822</td>\n",
       "      <td>0.824249</td>\n",
       "      <td>0.825362</td>\n",
       "      <td>0.836485</td>\n",
       "      <td>0.840934</td>\n",
       "      <td>0.805339</td>\n",
       "      <td>0.813126</td>\n",
       "      <td>0.844271</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.013714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.841518</td>\n",
       "      <td>0.841304</td>\n",
       "      <td>0.836032</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.840611</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.847780</td>\n",
       "      <td>0.820833</td>\n",
       "      <td>0.832981</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.840524</td>\n",
       "      <td>0.010374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.801242</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.849576</td>\n",
       "      <td>0.815735</td>\n",
       "      <td>0.815735</td>\n",
       "      <td>0.868085</td>\n",
       "      <td>0.830920</td>\n",
       "      <td>0.022956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.840100</td>\n",
       "      <td>0.824500</td>\n",
       "      <td>0.808500</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.830200</td>\n",
       "      <td>0.851100</td>\n",
       "      <td>0.831400</td>\n",
       "      <td>0.793300</td>\n",
       "      <td>0.810100</td>\n",
       "      <td>0.818200</td>\n",
       "      <td>0.823840</td>\n",
       "      <td>0.016817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.834994</td>\n",
       "      <td>0.820785</td>\n",
       "      <td>0.851546</td>\n",
       "      <td>0.830472</td>\n",
       "      <td>0.830636</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.848677</td>\n",
       "      <td>0.818276</td>\n",
       "      <td>0.824268</td>\n",
       "      <td>0.853556</td>\n",
       "      <td>0.835532</td>\n",
       "      <td>0.012864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.834266</td>\n",
       "      <td>0.812249</td>\n",
       "      <td>0.839567</td>\n",
       "      <td>0.824350</td>\n",
       "      <td>0.825428</td>\n",
       "      <td>0.836621</td>\n",
       "      <td>0.840925</td>\n",
       "      <td>0.805386</td>\n",
       "      <td>0.813258</td>\n",
       "      <td>0.844095</td>\n",
       "      <td>0.827615</td>\n",
       "      <td>0.013617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.834257</td>\n",
       "      <td>0.811562</td>\n",
       "      <td>0.838817</td>\n",
       "      <td>0.824012</td>\n",
       "      <td>0.825192</td>\n",
       "      <td>0.836278</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.812371</td>\n",
       "      <td>0.843643</td>\n",
       "      <td>0.827100</td>\n",
       "      <td>0.013807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.812881</td>\n",
       "      <td>0.838079</td>\n",
       "      <td>0.824584</td>\n",
       "      <td>0.825564</td>\n",
       "      <td>0.837297</td>\n",
       "      <td>0.840479</td>\n",
       "      <td>0.804502</td>\n",
       "      <td>0.812916</td>\n",
       "      <td>0.843133</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.013478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.668615</td>\n",
       "      <td>0.624192</td>\n",
       "      <td>0.678314</td>\n",
       "      <td>0.648425</td>\n",
       "      <td>0.650631</td>\n",
       "      <td>0.673470</td>\n",
       "      <td>0.681036</td>\n",
       "      <td>0.608713</td>\n",
       "      <td>0.624945</td>\n",
       "      <td>0.687824</td>\n",
       "      <td>0.654617</td>\n",
       "      <td>0.027651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.827100</td>\n",
       "      <td>0.781300</td>\n",
       "      <td>0.844400</td>\n",
       "      <td>0.804500</td>\n",
       "      <td>0.809500</td>\n",
       "      <td>0.810800</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.787600</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.849900</td>\n",
       "      <td>0.813950</td>\n",
       "      <td>0.023964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.812881</td>\n",
       "      <td>0.838079</td>\n",
       "      <td>0.824584</td>\n",
       "      <td>0.825564</td>\n",
       "      <td>0.837297</td>\n",
       "      <td>0.840479</td>\n",
       "      <td>0.804502</td>\n",
       "      <td>0.812916</td>\n",
       "      <td>0.843133</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.013478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.665298    0.676487    0.697708    0.684076   \n",
       "1                    TP  377.000000  387.000000  413.000000  387.000000   \n",
       "2                    TN  373.000000  343.000000  342.000000  354.000000   \n",
       "3                    FP   71.000000   73.000000   81.000000   72.000000   \n",
       "4                    FN   78.000000   96.000000   63.000000   86.000000   \n",
       "5              Accuracy    0.834260    0.812013    0.839822    0.824249   \n",
       "6             Precision    0.841518    0.841304    0.836032    0.843137   \n",
       "7           Sensitivity    0.828571    0.801242    0.867647    0.818182   \n",
       "8           Specificity    0.840100    0.824500    0.808500    0.831000   \n",
       "9              F1 score    0.834994    0.820785    0.851546    0.830472   \n",
       "10  F1 score (weighted)    0.834266    0.812249    0.839567    0.824350   \n",
       "11     F1 score (macro)    0.834257    0.811562    0.838817    0.824012   \n",
       "12    Balanced Accuracy    0.834331    0.812881    0.838079    0.824584   \n",
       "13                  MCC    0.668615    0.624192    0.678314    0.648425   \n",
       "14                  NPV    0.827100    0.781300    0.844400    0.804500   \n",
       "15              ROC_AUC    0.834331    0.812881    0.838079    0.824584   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.650299    0.681476    0.707530    0.679296    0.669725    0.712637   \n",
       "1   385.000000  392.000000  401.000000  394.000000  394.000000  408.000000   \n",
       "2   357.000000  360.000000  355.000000  330.000000  337.000000  351.000000   \n",
       "3    73.000000   63.000000   72.000000   86.000000   79.000000   78.000000   \n",
       "4    84.000000   84.000000   71.000000   89.000000   89.000000   62.000000   \n",
       "5     0.825362    0.836485    0.840934    0.805339    0.813126    0.844271   \n",
       "6     0.840611    0.861538    0.847780    0.820833    0.832981    0.839506   \n",
       "7     0.820896    0.823529    0.849576    0.815735    0.815735    0.868085   \n",
       "8     0.830200    0.851100    0.831400    0.793300    0.810100    0.818200   \n",
       "9     0.830636    0.842105    0.848677    0.818276    0.824268    0.853556   \n",
       "10    0.825428    0.836621    0.840925    0.805386    0.813258    0.844095   \n",
       "11    0.825192    0.836278    0.840517    0.804348    0.812371    0.843643   \n",
       "12    0.825564    0.837297    0.840479    0.804502    0.812916    0.843133   \n",
       "13    0.650631    0.673470    0.681036    0.608713    0.624945    0.687824   \n",
       "14    0.809500    0.810800    0.833300    0.787600    0.791100    0.849900   \n",
       "15    0.825564    0.837297    0.840479    0.804502    0.812916    0.843133   \n",
       "\n",
       "           ave        std  \n",
       "0     0.682453   0.019183  \n",
       "1   393.800000  10.942273  \n",
       "2   350.200000  12.479316  \n",
       "3    74.800000   6.390966  \n",
       "4    80.200000  11.467829  \n",
       "5     0.827586   0.013714  \n",
       "6     0.840524   0.010374  \n",
       "7     0.830920   0.022956  \n",
       "8     0.823840   0.016817  \n",
       "9     0.835532   0.012864  \n",
       "10    0.827615   0.013617  \n",
       "11    0.827100   0.013807  \n",
       "12    0.827377   0.013478  \n",
       "13    0.654617   0.027651  \n",
       "14    0.813950   0.023964  \n",
       "15    0.827377   0.013478  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_lgbm0</th>\n",
       "      <th>y_pred_lgbm1</th>\n",
       "      <th>y_pred_lgbm2</th>\n",
       "      <th>y_pred_lgbm3</th>\n",
       "      <th>y_pred_lgbm4</th>\n",
       "      <th>y_pred_lgbm_ave</th>\n",
       "      <th>y_pred_lgbm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4286867</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.061109</td>\n",
       "      <td>7.128216</td>\n",
       "      <td>6.904595</td>\n",
       "      <td>7.081291</td>\n",
       "      <td>7.047927</td>\n",
       "      <td>7.078856</td>\n",
       "      <td>0.102771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3689853</td>\n",
       "      <td>1</td>\n",
       "      <td>6.43</td>\n",
       "      <td>6.623541</td>\n",
       "      <td>6.824560</td>\n",
       "      <td>6.457572</td>\n",
       "      <td>6.613510</td>\n",
       "      <td>6.708559</td>\n",
       "      <td>6.609624</td>\n",
       "      <td>0.136369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3827056</td>\n",
       "      <td>2</td>\n",
       "      <td>7.52</td>\n",
       "      <td>9.014687</td>\n",
       "      <td>8.888470</td>\n",
       "      <td>8.851263</td>\n",
       "      <td>8.746736</td>\n",
       "      <td>8.625055</td>\n",
       "      <td>8.607702</td>\n",
       "      <td>0.501085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3689883</td>\n",
       "      <td>3</td>\n",
       "      <td>7.70</td>\n",
       "      <td>7.380153</td>\n",
       "      <td>7.400202</td>\n",
       "      <td>7.412950</td>\n",
       "      <td>7.322333</td>\n",
       "      <td>7.421699</td>\n",
       "      <td>7.439556</td>\n",
       "      <td>0.120883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2023528</td>\n",
       "      <td>4</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.286109</td>\n",
       "      <td>7.420156</td>\n",
       "      <td>7.576086</td>\n",
       "      <td>7.212505</td>\n",
       "      <td>7.241474</td>\n",
       "      <td>7.334388</td>\n",
       "      <td>0.126319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4464975</td>\n",
       "      <td>4487</td>\n",
       "      <td>4.72</td>\n",
       "      <td>5.158414</td>\n",
       "      <td>4.985077</td>\n",
       "      <td>5.165894</td>\n",
       "      <td>5.066597</td>\n",
       "      <td>4.943778</td>\n",
       "      <td>5.006627</td>\n",
       "      <td>0.151971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL95747</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.60</td>\n",
       "      <td>7.122615</td>\n",
       "      <td>7.123870</td>\n",
       "      <td>7.054640</td>\n",
       "      <td>7.041531</td>\n",
       "      <td>7.025064</td>\n",
       "      <td>7.161287</td>\n",
       "      <td>0.199848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL4072618</td>\n",
       "      <td>4489</td>\n",
       "      <td>5.19</td>\n",
       "      <td>4.924537</td>\n",
       "      <td>5.039298</td>\n",
       "      <td>5.178789</td>\n",
       "      <td>5.049264</td>\n",
       "      <td>5.131079</td>\n",
       "      <td>5.085494</td>\n",
       "      <td>0.092247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL2408692</td>\n",
       "      <td>4490</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.407742</td>\n",
       "      <td>6.494178</td>\n",
       "      <td>6.452008</td>\n",
       "      <td>6.657210</td>\n",
       "      <td>5.458403</td>\n",
       "      <td>6.304924</td>\n",
       "      <td>0.389820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4226829</td>\n",
       "      <td>4491</td>\n",
       "      <td>5.55</td>\n",
       "      <td>5.733736</td>\n",
       "      <td>5.634876</td>\n",
       "      <td>5.744934</td>\n",
       "      <td>5.739001</td>\n",
       "      <td>5.823179</td>\n",
       "      <td>5.704288</td>\n",
       "      <td>0.088017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_lgbm0  y_pred_lgbm1  \\\n",
       "0         CHEMBL4286867            0     7.25      7.061109      7.128216   \n",
       "1         CHEMBL3689853            1     6.43      6.623541      6.824560   \n",
       "2         CHEMBL3827056            2     7.52      9.014687      8.888470   \n",
       "3         CHEMBL3689883            3     7.70      7.380153      7.400202   \n",
       "4         CHEMBL2023528            4     7.27      7.286109      7.420156   \n",
       "...                 ...          ...      ...           ...           ...   \n",
       "4487      CHEMBL4464975         4487     4.72      5.158414      4.985077   \n",
       "4488        CHEMBL95747         4488     7.60      7.122615      7.123870   \n",
       "4489      CHEMBL4072618         4489     5.19      4.924537      5.039298   \n",
       "4490      CHEMBL2408692         4490     6.36      6.407742      6.494178   \n",
       "4491      CHEMBL4226829         4491     5.55      5.733736      5.634876   \n",
       "\n",
       "      y_pred_lgbm2  y_pred_lgbm3  y_pred_lgbm4  y_pred_lgbm_ave  \\\n",
       "0         6.904595      7.081291      7.047927         7.078856   \n",
       "1         6.457572      6.613510      6.708559         6.609624   \n",
       "2         8.851263      8.746736      8.625055         8.607702   \n",
       "3         7.412950      7.322333      7.421699         7.439556   \n",
       "4         7.576086      7.212505      7.241474         7.334388   \n",
       "...            ...           ...           ...              ...   \n",
       "4487      5.165894      5.066597      4.943778         5.006627   \n",
       "4488      7.054640      7.041531      7.025064         7.161287   \n",
       "4489      5.178789      5.049264      5.131079         5.085494   \n",
       "4490      6.452008      6.657210      5.458403         6.304924   \n",
       "4491      5.744934      5.739001      5.823179         5.704288   \n",
       "\n",
       "      y_pred_lgbm_std  \n",
       "0            0.102771  \n",
       "1            0.136369  \n",
       "2            0.501085  \n",
       "3            0.120883  \n",
       "4            0.126319  \n",
       "...               ...  \n",
       "4487         0.151971  \n",
       "4488         0.199848  \n",
       "4489         0.092247  \n",
       "4490         0.389820  \n",
       "4491         0.088017  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where((y_pred_optimized_lgbm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "lgbm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c16510fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_lgbm.to_csv('mat_met_lgbm_opt.csv')\n",
    "lgbm_5preds.to_csv('lgbm_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.6680 with a standard deviation of 0.0370\n",
      "LightGBM optimized model r2_score 0.7003 with a standard deviation of 0.0320\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_lgbm.joblib']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"./lgbm_reg.joblib\")\n",
    "#joblib.dump(optimized_lgbm, \"./optimized_lgbm.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"./optimizedCV_lgbm.joblib\") \n",
    "#loaded_rf = joblib.load(\"./optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.646608     0.035959\n",
      "1                    TP       189.900000     9.791720\n",
      "2                    TN       172.600000     9.167576\n",
      "3                    FP        40.400000     7.336363\n",
      "4                    FN        46.300000     5.850926\n",
      "5              Accuracy         0.806990     0.022677\n",
      "6             Precision         0.824755     0.029509\n",
      "7           Sensitivity         0.803790     0.025493\n",
      "8           Specificity         0.810450     0.032703\n",
      "9              F1 score         0.813890     0.023227\n",
      "10  F1 score (weighted)         0.807059     0.022694\n",
      "11     F1 score (macro)         0.806478     0.022775\n",
      "12    Balanced Accuracy         0.807114     0.022903\n",
      "13                  MCC         0.613757     0.045576\n",
      "14                  NPV         0.788530     0.024919\n",
      "15              ROC_AUC         0.807114     0.022903\n",
      "CPU times: user 2h 34min 27s, sys: 4.91 s, total: 2h 34min 32s\n",
      "Wall time: 6min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=16, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=16, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:53:01,158]\u001b[0m A new study created in memory with name: XGBRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:53:03,365]\u001b[0m Trial 0 finished with value: 0.3933044590210316 and parameters: {'n_estimators': 110, 'eta': 0.0324185013819833, 'max_depth': 6, 'alpha': 0.6084, 'lambda': 23.81515403566228, 'max_bin': 456}. Best is trial 0 with value: 0.3933044590210316.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:53:18,363]\u001b[0m Trial 1 finished with value: 0.6706421978846683 and parameters: {'n_estimators': 448, 'eta': 0.029788046074910094, 'max_depth': 9, 'alpha': 0.0636, 'lambda': 10.88362860208099, 'max_bin': 291}. Best is trial 1 with value: 0.6706421978846683.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:53:36,398]\u001b[0m Trial 2 finished with value: 0.6799466856044281 and parameters: {'n_estimators': 837, 'eta': 0.0489358236101397, 'max_depth': 7, 'alpha': 0.0635, 'lambda': 21.723505655577757, 'max_bin': 273}. Best is trial 2 with value: 0.6799466856044281.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:53:50,517]\u001b[0m Trial 3 finished with value: 0.6571482523531347 and parameters: {'n_estimators': 711, 'eta': 0.026633313319286027, 'max_depth': 6, 'alpha': 0.7273000000000001, 'lambda': 13.255154163850348, 'max_bin': 441}. Best is trial 2 with value: 0.6799466856044281.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:53:59,845]\u001b[0m Trial 4 finished with value: 0.6756799723269354 and parameters: {'n_estimators': 584, 'eta': 0.08225412813386579, 'max_depth': 5, 'alpha': 0.4445, 'lambda': 4.950031557610346, 'max_bin': 415}. Best is trial 2 with value: 0.6799466856044281.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:54:31,323]\u001b[0m Trial 5 finished with value: 0.6832102471783179 and parameters: {'n_estimators': 766, 'eta': 0.03613327808494588, 'max_depth': 11, 'alpha': 0.48560000000000003, 'lambda': 31.24429572919936, 'max_bin': 341}. Best is trial 5 with value: 0.6832102471783179.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:54:55,516]\u001b[0m Trial 6 finished with value: 0.6826721754672065 and parameters: {'n_estimators': 756, 'eta': 0.05822123542048248, 'max_depth': 11, 'alpha': 0.0205, 'lambda': 31.3793460844739, 'max_bin': 477}. Best is trial 5 with value: 0.6832102471783179.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:55:03,142]\u001b[0m Trial 7 finished with value: 0.6701836722940071 and parameters: {'n_estimators': 171, 'eta': 0.09438508295468277, 'max_depth': 11, 'alpha': 0.5022, 'lambda': 13.546832573135319, 'max_bin': 318}. Best is trial 5 with value: 0.6832102471783179.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:55:13,327]\u001b[0m Trial 8 finished with value: 0.6737769376780847 and parameters: {'n_estimators': 383, 'eta': 0.04572259275025737, 'max_depth': 8, 'alpha': 0.5916, 'lambda': 5.319895655313589, 'max_bin': 464}. Best is trial 5 with value: 0.6832102471783179.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:55:15,504]\u001b[0m Trial 9 finished with value: 0.5395829783007164 and parameters: {'n_estimators': 77, 'eta': 0.07049915732967325, 'max_depth': 7, 'alpha': 0.6097, 'lambda': 33.25355903747002, 'max_bin': 411}. Best is trial 5 with value: 0.6832102471783179.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:55:22,263]\u001b[0m Trial 10 finished with value: -3.4348961222459535 and parameters: {'n_estimators': 891, 'eta': 0.001228319511446939, 'max_depth': 12, 'alpha': 0.9670000000000001, 'lambda': 38.95882389172303, 'max_bin': 346}. Best is trial 5 with value: 0.6832102471783179.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:55:45,067]\u001b[0m Trial 11 finished with value: 0.6793742934365378 and parameters: {'n_estimators': 691, 'eta': 0.06699793480774491, 'max_depth': 10, 'alpha': 0.2766, 'lambda': 29.52056908882904, 'max_bin': 500}. Best is trial 5 with value: 0.6832102471783179.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:56:17,767]\u001b[0m Trial 12 finished with value: 0.6441345242672394 and parameters: {'n_estimators': 736, 'eta': 0.009424562292238807, 'max_depth': 12, 'alpha': 0.21930000000000002, 'lambda': 29.094060125804706, 'max_bin': 373}. Best is trial 5 with value: 0.6832102471783179.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:56:39,530]\u001b[0m Trial 13 finished with value: 0.682024798225679 and parameters: {'n_estimators': 568, 'eta': 0.06234112329977176, 'max_depth': 10, 'alpha': 0.8799, 'lambda': 39.43770915167525, 'max_bin': 367}. Best is trial 5 with value: 0.6832102471783179.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:57:14,197]\u001b[0m Trial 14 finished with value: 0.6867425306792625 and parameters: {'n_estimators': 814, 'eta': 0.03989304728958867, 'max_depth': 11, 'alpha': 0.2738, 'lambda': 32.91837008271521, 'max_bin': 329}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:57:25,679]\u001b[0m Trial 15 finished with value: 0.5998397924423217 and parameters: {'n_estimators': 311, 'eta': 0.017291978064863045, 'max_depth': 10, 'alpha': 0.3252, 'lambda': 25.78194848111502, 'max_bin': 326}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:57:53,345]\u001b[0m Trial 16 finished with value: 0.6852570589215194 and parameters: {'n_estimators': 592, 'eta': 0.03994029334329424, 'max_depth': 11, 'alpha': 0.4035, 'lambda': 35.608911148731025, 'max_bin': 308}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:58:16,368]\u001b[0m Trial 17 finished with value: 0.6781249113522902 and parameters: {'n_estimators': 601, 'eta': 0.04025946804291156, 'max_depth': 9, 'alpha': 0.2058, 'lambda': 36.724567445469546, 'max_bin': 252}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:58:42,613]\u001b[0m Trial 18 finished with value: 0.6683894098834707 and parameters: {'n_estimators': 534, 'eta': 0.01860413683635593, 'max_depth': 12, 'alpha': 0.3466, 'lambda': 17.941063401859093, 'max_bin': 293}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:59:09,123]\u001b[0m Trial 19 finished with value: 0.6823569613269338 and parameters: {'n_estimators': 645, 'eta': 0.05439713809014933, 'max_depth': 11, 'alpha': 0.1351, 'lambda': 35.36381731393199, 'max_bin': 307}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:59:25,311]\u001b[0m Trial 20 finished with value: 0.677409876248656 and parameters: {'n_estimators': 475, 'eta': 0.04386898078032836, 'max_depth': 9, 'alpha': 0.37670000000000003, 'lambda': 26.375343559126513, 'max_bin': 405}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:00:01,491]\u001b[0m Trial 21 finished with value: 0.6848107364616465 and parameters: {'n_estimators': 821, 'eta': 0.036348431216841566, 'max_depth': 11, 'alpha': 0.4748, 'lambda': 34.170569073057074, 'max_bin': 344}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:00:34,548]\u001b[0m Trial 22 finished with value: 0.6758388877245799 and parameters: {'n_estimators': 838, 'eta': 0.02200983366542223, 'max_depth': 10, 'alpha': 0.41910000000000003, 'lambda': 34.736252967128515, 'max_bin': 353}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:01:11,432]\u001b[0m Trial 23 finished with value: 0.684136295832323 and parameters: {'n_estimators': 834, 'eta': 0.036166624459146, 'max_depth': 12, 'alpha': 0.5184, 'lambda': 39.89726483528299, 'max_bin': 325}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:01:38,838]\u001b[0m Trial 24 finished with value: 0.684654130779202 and parameters: {'n_estimators': 892, 'eta': 0.052414611044093125, 'max_depth': 11, 'alpha': 0.73, 'lambda': 28.046315097516977, 'max_bin': 389}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:01:58,460]\u001b[0m Trial 25 finished with value: 0.6827187733373157 and parameters: {'n_estimators': 659, 'eta': 0.0746337832691654, 'max_depth': 10, 'alpha': 0.23520000000000002, 'lambda': 33.18917824012915, 'max_bin': 275}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:02:23,070]\u001b[0m Trial 26 finished with value: 0.6252912894844878 and parameters: {'n_estimators': 809, 'eta': 0.011008837891129677, 'max_depth': 8, 'alpha': 0.1364, 'lambda': 36.38033099143051, 'max_bin': 302}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:02:35,074]\u001b[0m Trial 27 finished with value: 0.6689243979536814 and parameters: {'n_estimators': 258, 'eta': 0.04109963199437054, 'max_depth': 11, 'alpha': 0.3022, 'lambda': 18.686704618831214, 'max_bin': 356}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:02:59,828]\u001b[0m Trial 28 finished with value: 0.6832085870155665 and parameters: {'n_estimators': 520, 'eta': 0.026512745817265752, 'max_depth': 12, 'alpha': 0.3805, 'lambda': 1.5221307929684293, 'max_bin': 340}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 07:03:21,070]\u001b[0m Trial 29 finished with value: 0.6816714185597629 and parameters: {'n_estimators': 645, 'eta': 0.033951962129802636, 'max_depth': 9, 'alpha': 0.5656, 'lambda': 24.54820906129855, 'max_bin': 391}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:03:35,955]\u001b[0m Trial 30 finished with value: 0.6779162729156475 and parameters: {'n_estimators': 410, 'eta': 0.04887872289827061, 'max_depth': 10, 'alpha': 0.6776, 'lambda': 22.079614223551864, 'max_bin': 279}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:04:07,386]\u001b[0m Trial 31 finished with value: 0.6857034132433294 and parameters: {'n_estimators': 806, 'eta': 0.050130120042862394, 'max_depth': 11, 'alpha': 0.7516, 'lambda': 28.057862778218603, 'max_bin': 394}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:04:41,408]\u001b[0m Trial 32 finished with value: 0.6838294795252668 and parameters: {'n_estimators': 780, 'eta': 0.030925509658017495, 'max_depth': 11, 'alpha': 0.8997, 'lambda': 31.705365324081725, 'max_bin': 432}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:05:13,250]\u001b[0m Trial 33 finished with value: 0.6848066091354426 and parameters: {'n_estimators': 892, 'eta': 0.05684035344796102, 'max_depth': 12, 'alpha': 0.8008000000000001, 'lambda': 37.34606820635376, 'max_bin': 387}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:05:45,430]\u001b[0m Trial 34 finished with value: 0.6841930704416265 and parameters: {'n_estimators': 695, 'eta': 0.039655230096163605, 'max_depth': 11, 'alpha': 0.6666000000000001, 'lambda': 33.80296699633384, 'max_bin': 313}. Best is trial 14 with value: 0.6867425306792625.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:06:16,040]\u001b[0m Trial 35 finished with value: 0.6872135450076245 and parameters: {'n_estimators': 803, 'eta': 0.0493516347528431, 'max_depth': 10, 'alpha': 0.437, 'lambda': 29.310301632685523, 'max_bin': 362}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:06:38,878]\u001b[0m Trial 36 finished with value: 0.6821622709985239 and parameters: {'n_estimators': 742, 'eta': 0.06292096838522374, 'max_depth': 9, 'alpha': 0.4203, 'lambda': 27.927188711944776, 'max_bin': 370}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:07:07,079]\u001b[0m Trial 37 finished with value: 0.6832631011121012 and parameters: {'n_estimators': 792, 'eta': 0.04654064024282671, 'max_depth': 10, 'alpha': 0.17250000000000001, 'lambda': 23.21532348396287, 'max_bin': 423}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:07:18,069]\u001b[0m Trial 38 finished with value: 0.6551358600870703 and parameters: {'n_estimators': 616, 'eta': 0.051026582308243015, 'max_depth': 5, 'alpha': 0.5472, 'lambda': 30.603519535110756, 'max_bin': 261}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:07:39,588]\u001b[0m Trial 39 finished with value: 0.6792125072476543 and parameters: {'n_estimators': 729, 'eta': 0.028612729382084204, 'max_depth': 8, 'alpha': 0.2576, 'lambda': 20.410264240652307, 'max_bin': 335}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:08:01,469]\u001b[0m Trial 40 finished with value: 0.6822093746616704 and parameters: {'n_estimators': 865, 'eta': 0.07822609554189483, 'max_depth': 11, 'alpha': 0.4534, 'lambda': 26.7144292807781, 'max_bin': 293}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:08:35,898]\u001b[0m Trial 41 finished with value: 0.6834506030321992 and parameters: {'n_estimators': 810, 'eta': 0.03679247922114794, 'max_depth': 11, 'alpha': 0.4792, 'lambda': 31.4855219811107, 'max_bin': 363}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:09:09,629]\u001b[0m Trial 42 finished with value: 0.6859530441664267 and parameters: {'n_estimators': 845, 'eta': 0.04574890014205266, 'max_depth': 11, 'alpha': 0.3997, 'lambda': 33.00414843511568, 'max_bin': 332}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:09:36,778]\u001b[0m Trial 43 finished with value: 0.6840808382265056 and parameters: {'n_estimators': 767, 'eta': 0.05954715981907466, 'max_depth': 10, 'alpha': 0.3734, 'lambda': 32.58332936194999, 'max_bin': 327}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:10:11,496]\u001b[0m Trial 44 finished with value: 0.6838196978069926 and parameters: {'n_estimators': 858, 'eta': 0.04309906741200528, 'max_depth': 12, 'alpha': 0.30160000000000003, 'lambda': 29.66417997096707, 'max_bin': 379}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:10:41,769]\u001b[0m Trial 45 finished with value: 0.6869045179754357 and parameters: {'n_estimators': 772, 'eta': 0.048578571343898175, 'max_depth': 11, 'alpha': 0.41090000000000004, 'lambda': 36.70815267431279, 'max_bin': 449}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:11:08,245]\u001b[0m Trial 46 finished with value: 0.6848123617187063 and parameters: {'n_estimators': 705, 'eta': 0.048603994721508594, 'max_depth': 10, 'alpha': 0.6065, 'lambda': 38.42718657315615, 'max_bin': 461}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:11:29,545]\u001b[0m Trial 47 finished with value: 0.6768194664997794 and parameters: {'n_estimators': 788, 'eta': 0.08928522545795908, 'max_depth': 12, 'alpha': 0.34500000000000003, 'lambda': 37.71573841775478, 'max_bin': 448}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:11:48,910]\u001b[0m Trial 48 finished with value: 0.6801795727453468 and parameters: {'n_estimators': 855, 'eta': 0.06873985482530125, 'max_depth': 11, 'alpha': 0.5427000000000001, 'lambda': 13.315190666969777, 'max_bin': 482}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:12:07,696]\u001b[0m Trial 49 finished with value: 0.6803219095709657 and parameters: {'n_estimators': 746, 'eta': 0.055606394793412825, 'max_depth': 7, 'alpha': 0.0613, 'lambda': 27.969512770791834, 'max_bin': 408}. Best is trial 35 with value: 0.6872135450076245.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6872\n",
      "\tBest params:\n",
      "\t\tn_estimators: 803\n",
      "\t\teta: 0.0493516347528431\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.437\n",
      "\t\tlambda: 29.310301632685523\n",
      "\t\tmax_bin: 362\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.683009\n",
      "1                    TP  385.000000\n",
      "2                    TN  373.000000\n",
      "3                    FP   71.000000\n",
      "4                    FN   70.000000\n",
      "5              Accuracy    0.843159\n",
      "6             Precision    0.844298\n",
      "7           Sensitivity    0.846154\n",
      "8           Specificity    0.840100\n",
      "9              F1 score    0.845225\n",
      "10  F1 score (weighted)    0.843157\n",
      "11     F1 score (macro)    0.843131\n",
      "12    Balanced Accuracy    0.843122\n",
      "13                  MCC    0.686264\n",
      "14                  NPV    0.842000\n",
      "15              ROC_AUC    0.843122\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "y_pred_xgb_0_cat = np.where((y_pred_xgb_0 >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 07:12:32,191]\u001b[0m Trial 50 finished with value: 0.6918179401348288 and parameters: {'n_estimators': 678, 'eta': 0.06501879732566876, 'max_depth': 10, 'alpha': 0.4495, 'lambda': 29.99590391766522, 'max_bin': 422}. Best is trial 50 with value: 0.6918179401348288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:12:59,532]\u001b[0m Trial 51 finished with value: 0.6931828127033265 and parameters: {'n_estimators': 679, 'eta': 0.04585326886760678, 'max_depth': 10, 'alpha': 0.44320000000000004, 'lambda': 30.274823458625328, 'max_bin': 439}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:13:19,681]\u001b[0m Trial 52 finished with value: 0.6895220884456548 and parameters: {'n_estimators': 685, 'eta': 0.06388160759565564, 'max_depth': 9, 'alpha': 0.4292, 'lambda': 30.315867173782124, 'max_bin': 445}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:13:41,153]\u001b[0m Trial 53 finished with value: 0.6928414335679727 and parameters: {'n_estimators': 664, 'eta': 0.06350604704831454, 'max_depth': 9, 'alpha': 0.45180000000000003, 'lambda': 30.071385377321214, 'max_bin': 450}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:13:59,758]\u001b[0m Trial 54 finished with value: 0.6878729932298184 and parameters: {'n_estimators': 541, 'eta': 0.06488687205374886, 'max_depth': 9, 'alpha': 0.5066, 'lambda': 29.77229905022534, 'max_bin': 440}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:14:17,250]\u001b[0m Trial 55 finished with value: 0.6874179372985882 and parameters: {'n_estimators': 549, 'eta': 0.06392357451456486, 'max_depth': 9, 'alpha': 0.5077, 'lambda': 25.289401469154704, 'max_bin': 430}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:14:34,327]\u001b[0m Trial 56 finished with value: 0.6851279114101503 and parameters: {'n_estimators': 548, 'eta': 0.0648454909593534, 'max_depth': 8, 'alpha': 0.5179, 'lambda': 24.54020428705069, 'max_bin': 472}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:14:50,562]\u001b[0m Trial 57 finished with value: 0.6857980432289864 and parameters: {'n_estimators': 493, 'eta': 0.07266211819225311, 'max_depth': 9, 'alpha': 0.6459, 'lambda': 30.415976862839578, 'max_bin': 433}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:15:09,545]\u001b[0m Trial 58 finished with value: 0.6898319782283224 and parameters: {'n_estimators': 664, 'eta': 0.07904251581231798, 'max_depth': 9, 'alpha': 0.468, 'lambda': 25.998706859596336, 'max_bin': 437}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:15:26,697]\u001b[0m Trial 59 finished with value: 0.6913175365585104 and parameters: {'n_estimators': 678, 'eta': 0.09191525207188275, 'max_depth': 8, 'alpha': 0.467, 'lambda': 27.2084421898414, 'max_bin': 445}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:15:42,376]\u001b[0m Trial 60 finished with value: 0.6894864266688445 and parameters: {'n_estimators': 663, 'eta': 0.09986225541736787, 'max_depth': 8, 'alpha': 0.4656, 'lambda': 26.654842071991524, 'max_bin': 422}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:15:58,486]\u001b[0m Trial 61 finished with value: 0.6916099980742972 and parameters: {'n_estimators': 671, 'eta': 0.09779991475979757, 'max_depth': 8, 'alpha': 0.4501, 'lambda': 26.036858571603396, 'max_bin': 421}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:16:11,559]\u001b[0m Trial 62 finished with value: 0.6871151546689138 and parameters: {'n_estimators': 627, 'eta': 0.08390652905206288, 'max_depth': 8, 'alpha': 0.5742, 'lambda': 10.702263243722047, 'max_bin': 488}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:16:27,190]\u001b[0m Trial 63 finished with value: 0.6906908428125217 and parameters: {'n_estimators': 673, 'eta': 0.08926848744947405, 'max_depth': 8, 'alpha': 0.4461, 'lambda': 22.605211136300408, 'max_bin': 453}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:16:40,160]\u001b[0m Trial 64 finished with value: 0.6864571916441764 and parameters: {'n_estimators': 580, 'eta': 0.09127583181349688, 'max_depth': 7, 'alpha': 0.3602, 'lambda': 23.18963655908295, 'max_bin': 456}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:16:53,565]\u001b[0m Trial 65 finished with value: 0.6872063058765319 and parameters: {'n_estimators': 668, 'eta': 0.09978283033634187, 'max_depth': 7, 'alpha': 0.4767, 'lambda': 20.467019026105785, 'max_bin': 467}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:17:09,158]\u001b[0m Trial 66 finished with value: 0.6877599539970924 and parameters: {'n_estimators': 629, 'eta': 0.08492179240897484, 'max_depth': 8, 'alpha': 0.3194, 'lambda': 21.816971949415215, 'max_bin': 416}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:17:24,559]\u001b[0m Trial 67 finished with value: 0.6895208161910705 and parameters: {'n_estimators': 718, 'eta': 0.07845465398927325, 'max_depth': 7, 'alpha': 0.544, 'lambda': 25.555585250786404, 'max_bin': 454}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:17:39,224]\u001b[0m Trial 68 finished with value: 0.6837850630639698 and parameters: {'n_estimators': 600, 'eta': 0.09559287192579488, 'max_depth': 8, 'alpha': 0.44830000000000003, 'lambda': 16.591279908382674, 'max_bin': 401}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:17:56,680]\u001b[0m Trial 69 finished with value: 0.6906081218061835 and parameters: {'n_estimators': 677, 'eta': 0.08738174113372596, 'max_depth': 9, 'alpha': 0.4903, 'lambda': 27.230193079526792, 'max_bin': 437}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:18:13,570]\u001b[0m Trial 70 finished with value: 0.6883619525878706 and parameters: {'n_estimators': 689, 'eta': 0.0880146358061973, 'max_depth': 8, 'alpha': 0.6179, 'lambda': 28.432434525302106, 'max_bin': 423}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:18:30,557]\u001b[0m Trial 71 finished with value: 0.6905572378479815 and parameters: {'n_estimators': 648, 'eta': 0.09573452341631707, 'max_depth': 9, 'alpha': 0.49410000000000004, 'lambda': 23.861313187216737, 'max_bin': 439}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:18:48,270]\u001b[0m Trial 72 finished with value: 0.6893953040852375 and parameters: {'n_estimators': 624, 'eta': 0.09377467220130581, 'max_depth': 9, 'alpha': 0.39880000000000004, 'lambda': 27.21167267288945, 'max_bin': 466}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:19:04,155]\u001b[0m Trial 73 finished with value: 0.6862705572346732 and parameters: {'n_estimators': 716, 'eta': 0.09586159464124144, 'max_depth': 8, 'alpha': 0.526, 'lambda': 23.80720103906356, 'max_bin': 443}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:19:22,443]\u001b[0m Trial 74 finished with value: 0.6915978227667808 and parameters: {'n_estimators': 644, 'eta': 0.08681584321349613, 'max_depth': 9, 'alpha': 0.4884, 'lambda': 18.83557984210777, 'max_bin': 412}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:19:35,581]\u001b[0m Trial 75 finished with value: 0.6825496077276656 and parameters: {'n_estimators': 596, 'eta': 0.08828103356027932, 'max_depth': 6, 'alpha': 0.5836, 'lambda': 19.265509593564687, 'max_bin': 413}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:19:53,893]\u001b[0m Trial 76 finished with value: 0.6871119082799234 and parameters: {'n_estimators': 680, 'eta': 0.0815866260584568, 'max_depth': 10, 'alpha': 0.4439, 'lambda': 17.12797511256786, 'max_bin': 428}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:20:09,716]\u001b[0m Trial 77 finished with value: 0.6888128296347122 and parameters: {'n_estimators': 748, 'eta': 0.09205507362936077, 'max_depth': 8, 'alpha': 0.3925, 'lambda': 14.660143846654792, 'max_bin': 453}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:20:26,889]\u001b[0m Trial 78 finished with value: 0.6890686423751179 and parameters: {'n_estimators': 563, 'eta': 0.08581798689346729, 'max_depth': 9, 'alpha': 0.342, 'lambda': 22.47768614178384, 'max_bin': 403}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 07:20:48,664]\u001b[0m Trial 79 finished with value: 0.6911240061173898 and parameters: {'n_estimators': 706, 'eta': 0.07192533012329191, 'max_depth': 10, 'alpha': 0.4359, 'lambda': 31.601377632963235, 'max_bin': 415}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:21:11,289]\u001b[0m Trial 80 finished with value: 0.6909901666844849 and parameters: {'n_estimators': 706, 'eta': 0.07125661453648656, 'max_depth': 10, 'alpha': 0.43860000000000005, 'lambda': 31.52020243480227, 'max_bin': 400}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:21:32,502]\u001b[0m Trial 81 finished with value: 0.6905438711864591 and parameters: {'n_estimators': 705, 'eta': 0.07421371599175755, 'max_depth': 10, 'alpha': 0.43, 'lambda': 31.41012828903905, 'max_bin': 416}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:21:37,865]\u001b[0m Trial 82 finished with value: 0.639117340777059 and parameters: {'n_estimators': 130, 'eta': 0.05972873216623293, 'max_depth': 10, 'alpha': 0.4544, 'lambda': 34.62059776844496, 'max_bin': 381}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:21:58,549]\u001b[0m Trial 83 finished with value: 0.6883134395420678 and parameters: {'n_estimators': 646, 'eta': 0.0704161756431488, 'max_depth': 10, 'alpha': 0.41340000000000005, 'lambda': 32.164482783021725, 'max_bin': 397}. Best is trial 51 with value: 0.6931828127033265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:22:20,574]\u001b[0m Trial 84 finished with value: 0.6941865246155858 and parameters: {'n_estimators': 729, 'eta': 0.08179573134756025, 'max_depth': 10, 'alpha': 0.36000000000000004, 'lambda': 28.49504598331064, 'max_bin': 420}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:22:42,480]\u001b[0m Trial 85 finished with value: 0.6872904164210427 and parameters: {'n_estimators': 729, 'eta': 0.07488564065589631, 'max_depth': 10, 'alpha': 0.3634, 'lambda': 28.599867475509146, 'max_bin': 421}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:23:06,199]\u001b[0m Trial 86 finished with value: 0.6918086396761586 and parameters: {'n_estimators': 755, 'eta': 0.06799524123094003, 'max_depth': 10, 'alpha': 0.38870000000000005, 'lambda': 35.64091813760442, 'max_bin': 407}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:23:25,993]\u001b[0m Trial 87 finished with value: 0.6870924228540117 and parameters: {'n_estimators': 759, 'eta': 0.08050415902591146, 'max_depth': 10, 'alpha': 0.38920000000000005, 'lambda': 33.5331995338735, 'max_bin': 411}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:23:48,332]\u001b[0m Trial 88 finished with value: 0.6880693174986108 and parameters: {'n_estimators': 733, 'eta': 0.07553655427731942, 'max_depth': 10, 'alpha': 0.3296, 'lambda': 29.232821819009693, 'max_bin': 426}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:24:08,110]\u001b[0m Trial 89 finished with value: 0.6903083487853873 and parameters: {'n_estimators': 610, 'eta': 0.06800824059466104, 'max_depth': 9, 'alpha': 0.2872, 'lambda': 35.21151881283496, 'max_bin': 417}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:24:24,268]\u001b[0m Trial 90 finished with value: 0.687607303444124 and parameters: {'n_estimators': 411, 'eta': 0.08307264039243482, 'max_depth': 10, 'alpha': 0.2549, 'lambda': 30.583524347678363, 'max_bin': 409}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:24:49,412]\u001b[0m Trial 91 finished with value: 0.6930400449692089 and parameters: {'n_estimators': 710, 'eta': 0.07215116773994913, 'max_depth': 10, 'alpha': 0.37570000000000003, 'lambda': 35.89663469875407, 'max_bin': 400}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:25:16,973]\u001b[0m Trial 92 finished with value: 0.6918064698036137 and parameters: {'n_estimators': 696, 'eta': 0.060946815327425935, 'max_depth': 10, 'alpha': 0.37270000000000003, 'lambda': 33.960547255744075, 'max_bin': 386}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:25:42,749]\u001b[0m Trial 93 finished with value: 0.6904939154403067 and parameters: {'n_estimators': 728, 'eta': 0.05960351331072159, 'max_depth': 10, 'alpha': 0.3649, 'lambda': 38.32887000833826, 'max_bin': 387}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:26:08,266]\u001b[0m Trial 94 finished with value: 0.6927070841273807 and parameters: {'n_estimators': 761, 'eta': 0.0529123179489107, 'max_depth': 9, 'alpha': 0.3815, 'lambda': 36.142190947146794, 'max_bin': 394}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:26:33,462]\u001b[0m Trial 95 finished with value: 0.6916936656642807 and parameters: {'n_estimators': 768, 'eta': 0.06134496612054079, 'max_depth': 9, 'alpha': 0.3783, 'lambda': 36.66714466129171, 'max_bin': 380}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:27:01,432]\u001b[0m Trial 96 finished with value: 0.690866246048446 and parameters: {'n_estimators': 778, 'eta': 0.05202902463873141, 'max_depth': 10, 'alpha': 0.31320000000000003, 'lambda': 35.98981527385369, 'max_bin': 394}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:27:27,982]\u001b[0m Trial 97 finished with value: 0.6904147323123464 and parameters: {'n_estimators': 828, 'eta': 0.061346879295963226, 'max_depth': 10, 'alpha': 0.3472, 'lambda': 39.83647473828273, 'max_bin': 377}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:27:53,209]\u001b[0m Trial 98 finished with value: 0.6935376490665304 and parameters: {'n_estimators': 753, 'eta': 0.06676128047599715, 'max_depth': 9, 'alpha': 0.38280000000000003, 'lambda': 37.21992886566344, 'max_bin': 385}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:28:17,810]\u001b[0m Trial 99 finished with value: 0.6919979937885222 and parameters: {'n_estimators': 799, 'eta': 0.06661473452420572, 'max_depth': 9, 'alpha': 0.2792, 'lambda': 37.17434869347075, 'max_bin': 382}. Best is trial 84 with value: 0.6941865246155858.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6942\n",
      "\tBest params:\n",
      "\t\tn_estimators: 729\n",
      "\t\teta: 0.08179573134756025\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.36000000000000004\n",
      "\t\tlambda: 28.49504598331064\n",
      "\t\tmax_bin: 420\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.683009    0.703984\n",
      "1                    TP  385.000000  407.000000\n",
      "2                    TN  373.000000  343.000000\n",
      "3                    FP   71.000000   73.000000\n",
      "4                    FN   70.000000   76.000000\n",
      "5              Accuracy    0.843159    0.834260\n",
      "6             Precision    0.844298    0.847917\n",
      "7           Sensitivity    0.846154    0.842650\n",
      "8           Specificity    0.840100    0.824500\n",
      "9              F1 score    0.845225    0.845275\n",
      "10  F1 score (weighted)    0.843157    0.834300\n",
      "11     F1 score (macro)    0.843131    0.833416\n",
      "12    Balanced Accuracy    0.843122    0.833585\n",
      "13                  MCC    0.686264    0.666851\n",
      "14                  NPV    0.842000    0.818600\n",
      "15              ROC_AUC    0.843122    0.833585\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_1_cat = np.where((y_pred_xgb_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 07:28:45,726]\u001b[0m Trial 100 finished with value: 0.7015417450179974 and parameters: {'n_estimators': 752, 'eta': 0.06690975119101743, 'max_depth': 9, 'alpha': 0.2737, 'lambda': 37.62009230106907, 'max_bin': 386}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:29:10,972]\u001b[0m Trial 101 finished with value: 0.6986022220691381 and parameters: {'n_estimators': 753, 'eta': 0.06638121055587809, 'max_depth': 9, 'alpha': 0.2101, 'lambda': 34.20873814279258, 'max_bin': 370}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:29:36,968]\u001b[0m Trial 102 finished with value: 0.6995692989274334 and parameters: {'n_estimators': 804, 'eta': 0.06720113338409964, 'max_depth': 9, 'alpha': 0.18330000000000002, 'lambda': 38.9555799285147, 'max_bin': 363}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:30:04,033]\u001b[0m Trial 103 finished with value: 0.6997204320036376 and parameters: {'n_estimators': 793, 'eta': 0.06550885036921923, 'max_depth': 9, 'alpha': 0.19840000000000002, 'lambda': 37.64672918474319, 'max_bin': 353}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:30:32,189]\u001b[0m Trial 104 finished with value: 0.7003003120827402 and parameters: {'n_estimators': 788, 'eta': 0.06723787667124198, 'max_depth': 9, 'alpha': 0.18810000000000002, 'lambda': 38.87202955903121, 'max_bin': 350}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:31:00,693]\u001b[0m Trial 105 finished with value: 0.6955950702816732 and parameters: {'n_estimators': 831, 'eta': 0.05331986947548592, 'max_depth': 9, 'alpha': 0.19340000000000002, 'lambda': 39.10778196789021, 'max_bin': 351}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:31:29,969]\u001b[0m Trial 106 finished with value: 0.6992286960965337 and parameters: {'n_estimators': 866, 'eta': 0.057045188929747456, 'max_depth': 9, 'alpha': 0.1799, 'lambda': 38.875233608267436, 'max_bin': 350}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:31:59,886]\u001b[0m Trial 107 finished with value: 0.6998388861964877 and parameters: {'n_estimators': 878, 'eta': 0.05567926128397701, 'max_depth': 9, 'alpha': 0.19390000000000002, 'lambda': 39.245024330967006, 'max_bin': 355}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:32:28,882]\u001b[0m Trial 108 finished with value: 0.6982004172982504 and parameters: {'n_estimators': 875, 'eta': 0.055460418285819824, 'max_depth': 9, 'alpha': 0.183, 'lambda': 38.971064339780455, 'max_bin': 350}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:33:00,210]\u001b[0m Trial 109 finished with value: 0.699732513789029 and parameters: {'n_estimators': 898, 'eta': 0.056578911777967915, 'max_depth': 9, 'alpha': 0.194, 'lambda': 39.01452031876173, 'max_bin': 352}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:33:29,942]\u001b[0m Trial 110 finished with value: 0.6978919905746785 and parameters: {'n_estimators': 877, 'eta': 0.05725598035484816, 'max_depth': 9, 'alpha': 0.1887, 'lambda': 39.14567794086087, 'max_bin': 341}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:33:58,079]\u001b[0m Trial 111 finished with value: 0.698592599682317 and parameters: {'n_estimators': 869, 'eta': 0.05660457788775263, 'max_depth': 9, 'alpha': 0.18530000000000002, 'lambda': 38.80974144870613, 'max_bin': 352}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:34:27,041]\u001b[0m Trial 112 finished with value: 0.6994640085871131 and parameters: {'n_estimators': 876, 'eta': 0.05656573214525576, 'max_depth': 9, 'alpha': 0.1887, 'lambda': 39.01861164359078, 'max_bin': 350}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:34:53,960]\u001b[0m Trial 113 finished with value: 0.6962245596939519 and parameters: {'n_estimators': 877, 'eta': 0.057156770213505875, 'max_depth': 9, 'alpha': 0.1376, 'lambda': 38.36069056958854, 'max_bin': 361}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:35:23,290]\u001b[0m Trial 114 finished with value: 0.6989240596271202 and parameters: {'n_estimators': 879, 'eta': 0.05605311338834924, 'max_depth': 9, 'alpha': 0.16190000000000002, 'lambda': 39.9703183124175, 'max_bin': 341}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:35:50,875]\u001b[0m Trial 115 finished with value: 0.6971992506302699 and parameters: {'n_estimators': 845, 'eta': 0.05520443087353129, 'max_depth': 9, 'alpha': 0.1565, 'lambda': 39.95770658806628, 'max_bin': 348}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:36:16,478]\u001b[0m Trial 116 finished with value: 0.6974188545197242 and parameters: {'n_estimators': 895, 'eta': 0.05491996791357521, 'max_depth': 9, 'alpha': 0.0801, 'lambda': 37.9445471812903, 'max_bin': 370}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:36:43,992]\u001b[0m Trial 117 finished with value: 0.6988926185460262 and parameters: {'n_estimators': 872, 'eta': 0.05777907852412806, 'max_depth': 9, 'alpha': 0.2238, 'lambda': 38.855387099197756, 'max_bin': 356}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:37:11,662]\u001b[0m Trial 118 finished with value: 0.6994888622259652 and parameters: {'n_estimators': 865, 'eta': 0.058097733082266394, 'max_depth': 9, 'alpha': 0.22080000000000002, 'lambda': 37.847913848041046, 'max_bin': 357}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:37:38,370]\u001b[0m Trial 119 finished with value: 0.7003045208388654 and parameters: {'n_estimators': 820, 'eta': 0.05939785491370136, 'max_depth': 9, 'alpha': 0.2235, 'lambda': 37.56768092357052, 'max_bin': 358}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:38:07,459]\u001b[0m Trial 120 finished with value: 0.7001293377179195 and parameters: {'n_estimators': 850, 'eta': 0.0579739671038857, 'max_depth': 9, 'alpha': 0.23290000000000002, 'lambda': 37.49048454559324, 'max_bin': 357}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:38:38,347]\u001b[0m Trial 121 finished with value: 0.6994194273939345 and parameters: {'n_estimators': 899, 'eta': 0.05822145520411593, 'max_depth': 9, 'alpha': 0.2298, 'lambda': 37.49571951227622, 'max_bin': 358}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:39:07,645]\u001b[0m Trial 122 finished with value: 0.700542076710487 and parameters: {'n_estimators': 850, 'eta': 0.05945425144987154, 'max_depth': 9, 'alpha': 0.09870000000000001, 'lambda': 37.71392618442577, 'max_bin': 338}. Best is trial 100 with value: 0.7015417450179974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:39:36,514]\u001b[0m Trial 123 finished with value: 0.7017614311612046 and parameters: {'n_estimators': 856, 'eta': 0.062441520953215444, 'max_depth': 9, 'alpha': 0.0873, 'lambda': 37.68024688260858, 'max_bin': 335}. Best is trial 123 with value: 0.7017614311612046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:40:02,992]\u001b[0m Trial 124 finished with value: 0.7021007319355244 and parameters: {'n_estimators': 819, 'eta': 0.06277008013326321, 'max_depth': 9, 'alpha': 0.08020000000000001, 'lambda': 37.92158311954217, 'max_bin': 336}. Best is trial 124 with value: 0.7021007319355244.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:40:27,967]\u001b[0m Trial 125 finished with value: 0.6971819836035763 and parameters: {'n_estimators': 820, 'eta': 0.06327759582720174, 'max_depth': 9, 'alpha': 0.0998, 'lambda': 37.69596001554913, 'max_bin': 323}. Best is trial 124 with value: 0.7021007319355244.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:40:53,823]\u001b[0m Trial 126 finished with value: 0.6985713066971995 and parameters: {'n_estimators': 850, 'eta': 0.06935147893108094, 'max_depth': 9, 'alpha': 0.0323, 'lambda': 36.81310755334831, 'max_bin': 332}. Best is trial 124 with value: 0.7021007319355244.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:41:21,695]\u001b[0m Trial 127 finished with value: 0.7008108677980504 and parameters: {'n_estimators': 809, 'eta': 0.062171647255937774, 'max_depth': 9, 'alpha': 0.09910000000000001, 'lambda': 37.98696604204351, 'max_bin': 365}. Best is trial 124 with value: 0.7021007319355244.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:41:49,111]\u001b[0m Trial 128 finished with value: 0.7003903574175563 and parameters: {'n_estimators': 810, 'eta': 0.062133079467832905, 'max_depth': 9, 'alpha': 0.10880000000000001, 'lambda': 35.24993235075733, 'max_bin': 365}. Best is trial 124 with value: 0.7021007319355244.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 07:42:16,334]\u001b[0m Trial 129 finished with value: 0.6996807052497354 and parameters: {'n_estimators': 813, 'eta': 0.06232687803233269, 'max_depth': 9, 'alpha': 0.1102, 'lambda': 35.04444697379234, 'max_bin': 336}. Best is trial 124 with value: 0.7021007319355244.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:42:42,940]\u001b[0m Trial 130 finished with value: 0.7016251904565507 and parameters: {'n_estimators': 840, 'eta': 0.06226466206084178, 'max_depth': 9, 'alpha': 0.11030000000000001, 'lambda': 34.952879904648505, 'max_bin': 344}. Best is trial 124 with value: 0.7021007319355244.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:43:10,936]\u001b[0m Trial 131 finished with value: 0.700529257693196 and parameters: {'n_estimators': 837, 'eta': 0.06201166855566055, 'max_depth': 9, 'alpha': 0.11670000000000001, 'lambda': 36.50814192361044, 'max_bin': 336}. Best is trial 124 with value: 0.7021007319355244.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:43:38,634]\u001b[0m Trial 132 finished with value: 0.6968813831963028 and parameters: {'n_estimators': 841, 'eta': 0.06501392349761387, 'max_depth': 9, 'alpha': 0.0424, 'lambda': 36.467657485798966, 'max_bin': 317}. Best is trial 124 with value: 0.7021007319355244.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:44:06,850]\u001b[0m Trial 133 finished with value: 0.700667965454712 and parameters: {'n_estimators': 792, 'eta': 0.06032620042216556, 'max_depth': 9, 'alpha': 0.012700000000000001, 'lambda': 37.095400248814784, 'max_bin': 344}. Best is trial 124 with value: 0.7021007319355244.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:44:34,727]\u001b[0m Trial 134 finished with value: 0.7024991310525436 and parameters: {'n_estimators': 828, 'eta': 0.06002597326956303, 'max_depth': 9, 'alpha': 0.10490000000000001, 'lambda': 34.622143745336565, 'max_bin': 336}. Best is trial 134 with value: 0.7024991310525436.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:45:02,190]\u001b[0m Trial 135 finished with value: 0.7004751214892824 and parameters: {'n_estimators': 830, 'eta': 0.06002500454981414, 'max_depth': 9, 'alpha': 0.0098, 'lambda': 35.31466357879945, 'max_bin': 345}. Best is trial 134 with value: 0.7024991310525436.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:45:30,845]\u001b[0m Trial 136 finished with value: 0.7013399627524559 and parameters: {'n_estimators': 831, 'eta': 0.05982946030537568, 'max_depth': 9, 'alpha': 0.004, 'lambda': 34.77527246026737, 'max_bin': 343}. Best is trial 134 with value: 0.7024991310525436.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:45:55,843]\u001b[0m Trial 137 finished with value: 0.6983670963126901 and parameters: {'n_estimators': 821, 'eta': 0.06015467238636592, 'max_depth': 8, 'alpha': 0.0102, 'lambda': 35.03830900060687, 'max_bin': 337}. Best is trial 134 with value: 0.7024991310525436.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:46:21,183]\u001b[0m Trial 138 finished with value: 0.7020502482286984 and parameters: {'n_estimators': 781, 'eta': 0.06187163524049949, 'max_depth': 9, 'alpha': 0.061500000000000006, 'lambda': 32.96134409867018, 'max_bin': 345}. Best is trial 134 with value: 0.7024991310525436.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:46:48,503]\u001b[0m Trial 139 finished with value: 0.7020238354332883 and parameters: {'n_estimators': 835, 'eta': 0.06229315260041755, 'max_depth': 9, 'alpha': 0.052000000000000005, 'lambda': 33.19535531694404, 'max_bin': 330}. Best is trial 134 with value: 0.7024991310525436.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:46:58,048]\u001b[0m Trial 140 finished with value: 0.6838347767565223 and parameters: {'n_estimators': 290, 'eta': 0.06313558917993371, 'max_depth': 9, 'alpha': 0.0005, 'lambda': 32.761906636798884, 'max_bin': 323}. Best is trial 134 with value: 0.7024991310525436.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:47:25,168]\u001b[0m Trial 141 finished with value: 0.7034098027765595 and parameters: {'n_estimators': 836, 'eta': 0.06181888233125367, 'max_depth': 9, 'alpha': 0.054900000000000004, 'lambda': 34.38702115768485, 'max_bin': 345}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:47:51,997]\u001b[0m Trial 142 finished with value: 0.6982019631746598 and parameters: {'n_estimators': 830, 'eta': 0.06219979244727293, 'max_depth': 9, 'alpha': 0.053700000000000005, 'lambda': 34.4657811966, 'max_bin': 344}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:48:17,851]\u001b[0m Trial 143 finished with value: 0.7008871804956082 and parameters: {'n_estimators': 780, 'eta': 0.061175854686364765, 'max_depth': 9, 'alpha': 0.0858, 'lambda': 33.64708075558071, 'max_bin': 329}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:48:44,709]\u001b[0m Trial 144 finished with value: 0.7002119937341573 and parameters: {'n_estimators': 787, 'eta': 0.06081969222128037, 'max_depth': 9, 'alpha': 0.0806, 'lambda': 32.96632104492224, 'max_bin': 331}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:48:58,651]\u001b[0m Trial 145 finished with value: 0.6762218633947013 and parameters: {'n_estimators': 778, 'eta': 0.0639872975894054, 'max_depth': 5, 'alpha': 0.024300000000000002, 'lambda': 33.88569192051405, 'max_bin': 308}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:49:23,400]\u001b[0m Trial 146 finished with value: 0.6983483058562407 and parameters: {'n_estimators': 831, 'eta': 0.06922035784612585, 'max_depth': 8, 'alpha': 0.0806, 'lambda': 35.95582719857673, 'max_bin': 318}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:49:48,127]\u001b[0m Trial 147 finished with value: 0.6998391919738595 and parameters: {'n_estimators': 847, 'eta': 0.05979759265470466, 'max_depth': 9, 'alpha': 0.058800000000000005, 'lambda': 34.53559064075455, 'max_bin': 326}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:50:12,115]\u001b[0m Trial 148 finished with value: 0.698987252003571 and parameters: {'n_estimators': 799, 'eta': 0.06453115350234764, 'max_depth': 9, 'alpha': 0.1246, 'lambda': 32.18554998585743, 'max_bin': 344}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:50:38,570]\u001b[0m Trial 149 finished with value: 0.7011036997015998 and parameters: {'n_estimators': 839, 'eta': 0.06149900889879633, 'max_depth': 9, 'alpha': 0.041100000000000005, 'lambda': 33.51631821864004, 'max_bin': 335}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7034\n",
      "\tBest params:\n",
      "\t\tn_estimators: 836\n",
      "\t\teta: 0.06181888233125367\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.054900000000000004\n",
      "\t\tlambda: 34.38702115768485\n",
      "\t\tmax_bin: 345\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.683009    0.703984    0.706733\n",
      "1                    TP  385.000000  407.000000  407.000000\n",
      "2                    TN  373.000000  343.000000  332.000000\n",
      "3                    FP   71.000000   73.000000   91.000000\n",
      "4                    FN   70.000000   76.000000   69.000000\n",
      "5              Accuracy    0.843159    0.834260    0.822024\n",
      "6             Precision    0.844298    0.847917    0.817269\n",
      "7           Sensitivity    0.846154    0.842650    0.855042\n",
      "8           Specificity    0.840100    0.824500    0.784900\n",
      "9              F1 score    0.845225    0.845275    0.835729\n",
      "10  F1 score (weighted)    0.843157    0.834300    0.821659\n",
      "11     F1 score (macro)    0.843131    0.833416    0.820777\n",
      "12    Balanced Accuracy    0.843122    0.833585    0.819956\n",
      "13                  MCC    0.686264    0.666851    0.642550\n",
      "14                  NPV    0.842000    0.818600    0.827900\n",
      "15              ROC_AUC    0.843122    0.833585    0.819956\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_2_cat = np.where((y_pred_xgb_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 07:51:08,564]\u001b[0m Trial 150 finished with value: 0.702867324868417 and parameters: {'n_estimators': 855, 'eta': 0.05039183627414838, 'max_depth': 9, 'alpha': 0.09240000000000001, 'lambda': 33.3901550861454, 'max_bin': 337}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:51:37,287]\u001b[0m Trial 151 finished with value: 0.6992231119455814 and parameters: {'n_estimators': 852, 'eta': 0.05326286163953722, 'max_depth': 9, 'alpha': 0.08990000000000001, 'lambda': 33.28801772374054, 'max_bin': 336}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:52:01,576]\u001b[0m Trial 152 finished with value: 0.7030991490622149 and parameters: {'n_estimators': 797, 'eta': 0.06289220026948508, 'max_depth': 9, 'alpha': 0.0454, 'lambda': 36.35600499263145, 'max_bin': 328}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:52:30,112]\u001b[0m Trial 153 finished with value: 0.7003993983318993 and parameters: {'n_estimators': 804, 'eta': 0.05011977553176638, 'max_depth': 9, 'alpha': 0.043500000000000004, 'lambda': 33.49471212284353, 'max_bin': 328}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:52:54,914]\u001b[0m Trial 154 finished with value: 0.7013814028083236 and parameters: {'n_estimators': 781, 'eta': 0.06294771286161219, 'max_depth': 9, 'alpha': 0.0693, 'lambda': 32.38430031288904, 'max_bin': 331}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:53:16,354]\u001b[0m Trial 155 finished with value: 0.7005687176709016 and parameters: {'n_estimators': 771, 'eta': 0.06576516884345841, 'max_depth': 9, 'alpha': 0.0678, 'lambda': 32.45794170806672, 'max_bin': 319}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:53:38,401]\u001b[0m Trial 156 finished with value: 0.7005928446633305 and parameters: {'n_estimators': 786, 'eta': 0.06370533253509647, 'max_depth': 9, 'alpha': 0.025500000000000002, 'lambda': 34.255062239629815, 'max_bin': 331}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:54:02,363]\u001b[0m Trial 157 finished with value: 0.699832935351717 and parameters: {'n_estimators': 809, 'eta': 0.06833363174924373, 'max_depth': 9, 'alpha': 0.1452, 'lambda': 33.90067275754569, 'max_bin': 340}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:54:22,949]\u001b[0m Trial 158 finished with value: 0.7000764847752132 and parameters: {'n_estimators': 793, 'eta': 0.07025329028633188, 'max_depth': 8, 'alpha': 0.0654, 'lambda': 35.68843907968562, 'max_bin': 301}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:54:46,328]\u001b[0m Trial 159 finished with value: 0.6995460102821509 and parameters: {'n_estimators': 817, 'eta': 0.06559961013634594, 'max_depth': 9, 'alpha': 0.0378, 'lambda': 31.990454028823667, 'max_bin': 328}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:55:10,894]\u001b[0m Trial 160 finished with value: 0.7024192790910936 and parameters: {'n_estimators': 773, 'eta': 0.062163790811861956, 'max_depth': 9, 'alpha': 0.07060000000000001, 'lambda': 32.948796328950564, 'max_bin': 315}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:55:29,875]\u001b[0m Trial 161 finished with value: 0.6982446040632803 and parameters: {'n_estimators': 773, 'eta': 0.06168262318161785, 'max_depth': 9, 'alpha': 0.061900000000000004, 'lambda': 7.3601017975707315, 'max_bin': 322}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:55:50,779]\u001b[0m Trial 162 finished with value: 0.7010854451933669 and parameters: {'n_estimators': 801, 'eta': 0.06370043629070157, 'max_depth': 9, 'alpha': 0.08710000000000001, 'lambda': 30.962175039638478, 'max_bin': 312}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:56:15,685]\u001b[0m Trial 163 finished with value: 0.6991899990895873 and parameters: {'n_estimators': 828, 'eta': 0.06336938347152461, 'max_depth': 9, 'alpha': 0.0854, 'lambda': 32.860109119014176, 'max_bin': 315}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:56:39,140]\u001b[0m Trial 164 finished with value: 0.7004896687226632 and parameters: {'n_estimators': 855, 'eta': 0.06569564238368654, 'max_depth': 9, 'alpha': 0.131, 'lambda': 33.23173276750298, 'max_bin': 311}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:57:05,089]\u001b[0m Trial 165 finished with value: 0.7017907550049325 and parameters: {'n_estimators': 811, 'eta': 0.06329641290929165, 'max_depth': 9, 'alpha': 0.0504, 'lambda': 31.274482520755818, 'max_bin': 330}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:57:27,642]\u001b[0m Trial 166 finished with value: 0.6965252150092954 and parameters: {'n_estimators': 773, 'eta': 0.067604255964621, 'max_depth': 9, 'alpha': 0.0454, 'lambda': 31.313566037092457, 'max_bin': 303}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:57:51,868]\u001b[0m Trial 167 finished with value: 0.7007051461524565 and parameters: {'n_estimators': 745, 'eta': 0.05878329496302391, 'max_depth': 9, 'alpha': 0.0755, 'lambda': 32.33350762707477, 'max_bin': 333}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:58:16,517]\u001b[0m Trial 168 finished with value: 0.698647866843372 and parameters: {'n_estimators': 839, 'eta': 0.0648590939079909, 'max_depth': 9, 'alpha': 0.0506, 'lambda': 30.91396662511596, 'max_bin': 326}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:58:39,316]\u001b[0m Trial 169 finished with value: 0.697628103742348 and parameters: {'n_estimators': 816, 'eta': 0.06336697767828096, 'max_depth': 8, 'alpha': 0.0292, 'lambda': 34.64612451424872, 'max_bin': 312}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:59:03,988]\u001b[0m Trial 170 finished with value: 0.7021563974279312 and parameters: {'n_estimators': 798, 'eta': 0.06089582204242563, 'max_depth': 9, 'alpha': 0.11950000000000001, 'lambda': 33.74283626647758, 'max_bin': 322}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:59:27,823]\u001b[0m Trial 171 finished with value: 0.6985188663207448 and parameters: {'n_estimators': 790, 'eta': 0.061114912784965086, 'max_depth': 9, 'alpha': 0.12050000000000001, 'lambda': 33.34377596943495, 'max_bin': 321}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:59:54,989]\u001b[0m Trial 172 finished with value: 0.7013429624997845 and parameters: {'n_estimators': 859, 'eta': 0.05871265390168144, 'max_depth': 9, 'alpha': 0.09390000000000001, 'lambda': 34.28257587535737, 'max_bin': 332}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:00:19,618]\u001b[0m Trial 173 finished with value: 0.7013734629983801 and parameters: {'n_estimators': 859, 'eta': 0.05886559837638915, 'max_depth': 9, 'alpha': 0.0682, 'lambda': 31.940173705075022, 'max_bin': 332}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:00:45,457]\u001b[0m Trial 174 finished with value: 0.7001963029564172 and parameters: {'n_estimators': 861, 'eta': 0.05412422935266601, 'max_depth': 9, 'alpha': 0.0616, 'lambda': 34.51901215935933, 'max_bin': 333}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:01:10,273]\u001b[0m Trial 175 finished with value: 0.6991292935533684 and parameters: {'n_estimators': 837, 'eta': 0.05863728114146255, 'max_depth': 9, 'alpha': 0.15030000000000002, 'lambda': 31.900796290752723, 'max_bin': 335}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:01:34,764]\u001b[0m Trial 176 finished with value: 0.7023213149358201 and parameters: {'n_estimators': 859, 'eta': 0.0585217462958197, 'max_depth': 9, 'alpha': 0.0002, 'lambda': 35.40972524680526, 'max_bin': 340}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:01:42,375]\u001b[0m Trial 177 finished with value: 0.6641588736241058 and parameters: {'n_estimators': 217, 'eta': 0.051037677274848176, 'max_depth': 9, 'alpha': 0.0043, 'lambda': 36.15832779070636, 'max_bin': 341}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:02:13,730]\u001b[0m Trial 178 finished with value: 0.6159910778963227 and parameters: {'n_estimators': 889, 'eta': 0.006996650785633357, 'max_depth': 9, 'alpha': 0.11460000000000001, 'lambda': 35.60783489745814, 'max_bin': 324}. Best is trial 141 with value: 0.7034098027765595.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 08:02:40,791]\u001b[0m Trial 179 finished with value: 0.7035213683286923 and parameters: {'n_estimators': 863, 'eta': 0.058708045610131716, 'max_depth': 9, 'alpha': 0.0258, 'lambda': 35.28482831938927, 'max_bin': 341}. Best is trial 179 with value: 0.7035213683286923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:03:07,443]\u001b[0m Trial 180 finished with value: 0.6998048109399246 and parameters: {'n_estimators': 860, 'eta': 0.05453793544173374, 'max_depth': 9, 'alpha': 0.065, 'lambda': 32.629285200732426, 'max_bin': 328}. Best is trial 179 with value: 0.7035213683286923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:03:32,920]\u001b[0m Trial 181 finished with value: 0.7019776243124336 and parameters: {'n_estimators': 886, 'eta': 0.05867374762031155, 'max_depth': 9, 'alpha': 0.0199, 'lambda': 35.12594390093309, 'max_bin': 345}. Best is trial 179 with value: 0.7035213683286923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:03:56,226]\u001b[0m Trial 182 finished with value: 0.7026521930645451 and parameters: {'n_estimators': 891, 'eta': 0.0582819390378929, 'max_depth': 9, 'alpha': 0.019700000000000002, 'lambda': 35.282938790656324, 'max_bin': 339}. Best is trial 179 with value: 0.7035213683286923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:04:24,818]\u001b[0m Trial 183 finished with value: 0.7000358906858277 and parameters: {'n_estimators': 884, 'eta': 0.05787141199039176, 'max_depth': 9, 'alpha': 0.023100000000000002, 'lambda': 35.671578179814766, 'max_bin': 346}. Best is trial 179 with value: 0.7035213683286923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:04:48,366]\u001b[0m Trial 184 finished with value: 0.7037693073445855 and parameters: {'n_estimators': 895, 'eta': 0.06642109429014469, 'max_depth': 9, 'alpha': 0.0307, 'lambda': 36.726715163896934, 'max_bin': 340}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:05:13,175]\u001b[0m Trial 185 finished with value: 0.7015988924257799 and parameters: {'n_estimators': 900, 'eta': 0.0671886497632383, 'max_depth': 9, 'alpha': 0.027800000000000002, 'lambda': 36.768412098985344, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:05:35,665]\u001b[0m Trial 186 finished with value: 0.7011280476228305 and parameters: {'n_estimators': 893, 'eta': 0.06679121437603004, 'max_depth': 9, 'alpha': 0.0303, 'lambda': 36.60145899100877, 'max_bin': 339}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:06:05,678]\u001b[0m Trial 187 finished with value: 0.6984491289182729 and parameters: {'n_estimators': 900, 'eta': 0.04716834841444644, 'max_depth': 9, 'alpha': 0.0432, 'lambda': 35.202349003687, 'max_bin': 346}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:06:29,699]\u001b[0m Trial 188 finished with value: 0.7011141648472348 and parameters: {'n_estimators': 868, 'eta': 0.06996753427517494, 'max_depth': 9, 'alpha': 0.0269, 'lambda': 36.55083383181823, 'max_bin': 340}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:06:52,364]\u001b[0m Trial 189 finished with value: 0.701878104209434 and parameters: {'n_estimators': 876, 'eta': 0.07303030700156833, 'max_depth': 9, 'alpha': 0.0177, 'lambda': 35.88611895145837, 'max_bin': 339}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:07:13,535]\u001b[0m Trial 190 finished with value: 0.702257811894359 and parameters: {'n_estimators': 899, 'eta': 0.07247423752607389, 'max_depth': 9, 'alpha': 0.0022, 'lambda': 35.08851240276756, 'max_bin': 347}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:07:38,357]\u001b[0m Trial 191 finished with value: 0.6997187040390314 and parameters: {'n_estimators': 884, 'eta': 0.07128633161507907, 'max_depth': 9, 'alpha': 0.0009000000000000001, 'lambda': 35.21732080818108, 'max_bin': 348}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:08:03,553]\u001b[0m Trial 192 finished with value: 0.7001028774449221 and parameters: {'n_estimators': 898, 'eta': 0.06549480987920667, 'max_depth': 9, 'alpha': 0.0274, 'lambda': 36.052997391265336, 'max_bin': 339}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:08:26,404]\u001b[0m Trial 193 finished with value: 0.6987281617976311 and parameters: {'n_estimators': 878, 'eta': 0.07298707724642214, 'max_depth': 9, 'alpha': 0.0183, 'lambda': 34.13637574491911, 'max_bin': 343}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:08:48,112]\u001b[0m Trial 194 finished with value: 0.6979044079636706 and parameters: {'n_estimators': 869, 'eta': 0.07509055146320986, 'max_depth': 9, 'alpha': 0.0489, 'lambda': 36.91723271729515, 'max_bin': 336}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:09:11,062]\u001b[0m Trial 195 finished with value: 0.6980912001812194 and parameters: {'n_estimators': 900, 'eta': 0.07302991970139591, 'max_depth': 9, 'alpha': 0.0487, 'lambda': 34.98026187531006, 'max_bin': 340}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:09:33,115]\u001b[0m Trial 196 finished with value: 0.7009294717430483 and parameters: {'n_estimators': 882, 'eta': 0.06830612087763958, 'max_depth': 9, 'alpha': 0.0165, 'lambda': 36.49859425479716, 'max_bin': 347}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:10:00,654]\u001b[0m Trial 197 finished with value: 0.6995614590731818 and parameters: {'n_estimators': 853, 'eta': 0.04322071590857099, 'max_depth': 9, 'alpha': 0.10300000000000001, 'lambda': 35.84128451389324, 'max_bin': 320}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:10:20,676]\u001b[0m Trial 198 finished with value: 0.7011592802403753 and parameters: {'n_estimators': 871, 'eta': 0.07748254247354437, 'max_depth': 9, 'alpha': 0.0434, 'lambda': 34.04562245040293, 'max_bin': 324}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:10:46,489]\u001b[0m Trial 199 finished with value: 0.7012856999625543 and parameters: {'n_estimators': 849, 'eta': 0.06153554827231671, 'max_depth': 9, 'alpha': 0.0025, 'lambda': 34.68189988399848, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7038\n",
      "\tBest params:\n",
      "\t\tn_estimators: 895\n",
      "\t\teta: 0.06642109429014469\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.0307\n",
      "\t\tlambda: 36.726715163896934\n",
      "\t\tmax_bin: 340\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.683009    0.703984    0.706733    0.688618\n",
      "1                    TP  385.000000  407.000000  407.000000  390.000000\n",
      "2                    TN  373.000000  343.000000  332.000000  337.000000\n",
      "3                    FP   71.000000   73.000000   91.000000   89.000000\n",
      "4                    FN   70.000000   76.000000   69.000000   83.000000\n",
      "5              Accuracy    0.843159    0.834260    0.822024    0.808676\n",
      "6             Precision    0.844298    0.847917    0.817269    0.814196\n",
      "7           Sensitivity    0.846154    0.842650    0.855042    0.824524\n",
      "8           Specificity    0.840100    0.824500    0.784900    0.791100\n",
      "9              F1 score    0.845225    0.845275    0.835729    0.819328\n",
      "10  F1 score (weighted)    0.843157    0.834300    0.821659    0.808601\n",
      "11     F1 score (macro)    0.843131    0.833416    0.820777    0.808009\n",
      "12    Balanced Accuracy    0.843122    0.833585    0.819956    0.807802\n",
      "13                  MCC    0.686264    0.666851    0.642550    0.616090\n",
      "14                  NPV    0.842000    0.818600    0.827900    0.802400\n",
      "15              ROC_AUC    0.843122    0.833585    0.819956    0.807802\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_3_cat = np.where((y_pred_xgb_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 08:11:07,708]\u001b[0m Trial 200 finished with value: 0.692015897152783 and parameters: {'n_estimators': 884, 'eta': 0.06447918475648899, 'max_depth': 9, 'alpha': 0.0589, 'lambda': 35.66990361040426, 'max_bin': 347}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:11:28,719]\u001b[0m Trial 201 finished with value: 0.6931592625406789 and parameters: {'n_estimators': 820, 'eta': 0.06657400563042579, 'max_depth': 9, 'alpha': 0.0224, 'lambda': 37.06624314850547, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:11:51,100]\u001b[0m Trial 202 finished with value: 0.692480317795175 and parameters: {'n_estimators': 863, 'eta': 0.06245665935191844, 'max_depth': 9, 'alpha': 0.07690000000000001, 'lambda': 35.20845808142944, 'max_bin': 339}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:12:09,971]\u001b[0m Trial 203 finished with value: 0.6913227202164551 and parameters: {'n_estimators': 844, 'eta': 0.06922145134063956, 'max_depth': 9, 'alpha': 0.037700000000000004, 'lambda': 33.79327008611249, 'max_bin': 343}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:12:29,334]\u001b[0m Trial 204 finished with value: 0.6898064259678444 and parameters: {'n_estimators': 899, 'eta': 0.07676279703823896, 'max_depth': 9, 'alpha': 0.098, 'lambda': 36.09742666469021, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:12:52,383]\u001b[0m Trial 205 finished with value: 0.6931682801634995 and parameters: {'n_estimators': 834, 'eta': 0.0605913521004682, 'max_depth': 9, 'alpha': 0.0626, 'lambda': 36.995091612078085, 'max_bin': 352}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:13:14,207]\u001b[0m Trial 206 finished with value: 0.6953750361295719 and parameters: {'n_estimators': 881, 'eta': 0.06410110319065201, 'max_depth': 9, 'alpha': 0.026600000000000002, 'lambda': 38.079086342978805, 'max_bin': 326}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:13:34,453]\u001b[0m Trial 207 finished with value: 0.6931589364728564 and parameters: {'n_estimators': 857, 'eta': 0.06765797913105666, 'max_depth': 9, 'alpha': 0.1283, 'lambda': 34.75427411374514, 'max_bin': 343}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:13:57,373]\u001b[0m Trial 208 finished with value: 0.6934590773235911 and parameters: {'n_estimators': 824, 'eta': 0.05673147458154045, 'max_depth': 9, 'alpha': 0.0008, 'lambda': 33.79219909517536, 'max_bin': 336}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:14:19,529]\u001b[0m Trial 209 finished with value: 0.6927719210457787 and parameters: {'n_estimators': 864, 'eta': 0.05995782706960759, 'max_depth': 9, 'alpha': 0.0781, 'lambda': 33.07854576730787, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:14:36,450]\u001b[0m Trial 210 finished with value: 0.691628581949423 and parameters: {'n_estimators': 507, 'eta': 0.0622422996857759, 'max_depth': 9, 'alpha': 0.0489, 'lambda': 36.1284957091193, 'max_bin': 318}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:14:57,816]\u001b[0m Trial 211 finished with value: 0.6938314627837422 and parameters: {'n_estimators': 800, 'eta': 0.06331148354969404, 'max_depth': 9, 'alpha': 0.0713, 'lambda': 32.76399939849322, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:15:10,976]\u001b[0m Trial 212 finished with value: 0.6885074006816553 and parameters: {'n_estimators': 367, 'eta': 0.0652045449447546, 'max_depth': 9, 'alpha': 0.041, 'lambda': 35.146740657366074, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:15:32,660]\u001b[0m Trial 213 finished with value: 0.6938317897203267 and parameters: {'n_estimators': 817, 'eta': 0.06280400252397575, 'max_depth': 9, 'alpha': 0.10700000000000001, 'lambda': 34.46754463670317, 'max_bin': 347}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:15:56,015]\u001b[0m Trial 214 finished with value: 0.6937559497982836 and parameters: {'n_estimators': 841, 'eta': 0.06033318286665205, 'max_depth': 9, 'alpha': 0.0603, 'lambda': 37.050259791918535, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:16:15,974]\u001b[0m Trial 215 finished with value: 0.6945147467200118 and parameters: {'n_estimators': 882, 'eta': 0.06637986390418185, 'max_depth': 9, 'alpha': 0.0238, 'lambda': 33.37425437793317, 'max_bin': 324}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:16:38,540]\u001b[0m Trial 216 finished with value: 0.6925292338568229 and parameters: {'n_estimators': 761, 'eta': 0.057900062925844646, 'max_depth': 9, 'alpha': 0.07680000000000001, 'lambda': 35.548434572019694, 'max_bin': 330}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:16:57,892]\u001b[0m Trial 217 finished with value: 0.6946979776992885 and parameters: {'n_estimators': 845, 'eta': 0.07148754625743642, 'max_depth': 9, 'alpha': 0.08560000000000001, 'lambda': 34.08157032905787, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:17:18,199]\u001b[0m Trial 218 finished with value: 0.6912020035141255 and parameters: {'n_estimators': 808, 'eta': 0.06370919972770317, 'max_depth': 9, 'alpha': 0.0531, 'lambda': 38.22180910464886, 'max_bin': 353}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:17:35,496]\u001b[0m Trial 219 finished with value: 0.6872724750387571 and parameters: {'n_estimators': 828, 'eta': 0.06146831508483903, 'max_depth': 6, 'alpha': 0.0007, 'lambda': 36.38670098374552, 'max_bin': 343}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:17:55,994]\u001b[0m Trial 220 finished with value: 0.6949719721329483 and parameters: {'n_estimators': 873, 'eta': 0.06896465168613376, 'max_depth': 9, 'alpha': 0.8496, 'lambda': 34.855249417999296, 'max_bin': 332}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:18:16,994]\u001b[0m Trial 221 finished with value: 0.6927976917007419 and parameters: {'n_estimators': 861, 'eta': 0.058681329487341484, 'max_depth': 9, 'alpha': 0.0712, 'lambda': 32.14835632968259, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:18:38,639]\u001b[0m Trial 222 finished with value: 0.6924759471612896 and parameters: {'n_estimators': 855, 'eta': 0.059124597265663166, 'max_depth': 9, 'alpha': 0.033, 'lambda': 31.997058073028693, 'max_bin': 327}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:19:02,971]\u001b[0m Trial 223 finished with value: 0.6934606892950764 and parameters: {'n_estimators': 888, 'eta': 0.055770916808060296, 'max_depth': 9, 'alpha': 0.0994, 'lambda': 32.91697500989803, 'max_bin': 339}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:19:24,114]\u001b[0m Trial 224 finished with value: 0.6921234711956246 and parameters: {'n_estimators': 900, 'eta': 0.060672044079497243, 'max_depth': 9, 'alpha': 0.0645, 'lambda': 31.363001722573205, 'max_bin': 320}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:19:42,065]\u001b[0m Trial 225 finished with value: 0.6919602067242387 and parameters: {'n_estimators': 866, 'eta': 0.06462209041817711, 'max_depth': 9, 'alpha': 0.0223, 'lambda': 33.526905329135445, 'max_bin': 348}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:20:05,121]\u001b[0m Trial 226 finished with value: 0.6954895619443893 and parameters: {'n_estimators': 787, 'eta': 0.06239043821596177, 'max_depth': 9, 'alpha': 0.1207, 'lambda': 37.239699598188786, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:20:28,320]\u001b[0m Trial 227 finished with value: 0.6941121716789727 and parameters: {'n_estimators': 838, 'eta': 0.05789716312192564, 'max_depth': 9, 'alpha': 0.0463, 'lambda': 35.65696119758961, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:20:31,395]\u001b[0m Trial 228 finished with value: 0.5547219896625608 and parameters: {'n_estimators': 85, 'eta': 0.05401710850763747, 'max_depth': 9, 'alpha': 0.0931, 'lambda': 32.62094688415847, 'max_bin': 326}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 08:20:53,646]\u001b[0m Trial 229 finished with value: 0.6936070430034961 and parameters: {'n_estimators': 817, 'eta': 0.05976666500523775, 'max_depth': 9, 'alpha': 0.06280000000000001, 'lambda': 34.19835006454761, 'max_bin': 343}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:21:13,770]\u001b[0m Trial 230 finished with value: 0.6913807081801548 and parameters: {'n_estimators': 877, 'eta': 0.06599795733405668, 'max_depth': 9, 'alpha': 0.14100000000000001, 'lambda': 36.270368210149904, 'max_bin': 314}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:21:35,863]\u001b[0m Trial 231 finished with value: 0.6947107643111154 and parameters: {'n_estimators': 860, 'eta': 0.05864277368685071, 'max_depth': 9, 'alpha': 0.09230000000000001, 'lambda': 34.340911713624735, 'max_bin': 330}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:21:58,227]\u001b[0m Trial 232 finished with value: 0.6956366303805863 and parameters: {'n_estimators': 850, 'eta': 0.05639587707040614, 'max_depth': 9, 'alpha': 0.0184, 'lambda': 34.790974599336764, 'max_bin': 333}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:22:17,936]\u001b[0m Trial 233 finished with value: 0.6927412088031315 and parameters: {'n_estimators': 868, 'eta': 0.06200003834016134, 'max_depth': 9, 'alpha': 0.1072, 'lambda': 33.55880109580435, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:22:40,872]\u001b[0m Trial 234 finished with value: 0.6938938851152261 and parameters: {'n_estimators': 800, 'eta': 0.059279636787546885, 'max_depth': 9, 'alpha': 0.0393, 'lambda': 35.39572754074053, 'max_bin': 340}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:23:01,424]\u001b[0m Trial 235 finished with value: 0.6907629605481014 and parameters: {'n_estimators': 885, 'eta': 0.06335503143905766, 'max_depth': 9, 'alpha': 0.0746, 'lambda': 34.22781500966275, 'max_bin': 345}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:23:22,439]\u001b[0m Trial 236 finished with value: 0.6936721051310628 and parameters: {'n_estimators': 834, 'eta': 0.06101540325779513, 'max_depth': 9, 'alpha': 0.043000000000000003, 'lambda': 31.71116630090198, 'max_bin': 322}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:23:41,650]\u001b[0m Trial 237 finished with value: 0.6926368444121719 and parameters: {'n_estimators': 858, 'eta': 0.05755045435010966, 'max_depth': 9, 'alpha': 0.0175, 'lambda': 32.60915056716188, 'max_bin': 336}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:24:00,724]\u001b[0m Trial 238 finished with value: 0.6916493213296799 and parameters: {'n_estimators': 824, 'eta': 0.0641754747280016, 'max_depth': 9, 'alpha': 0.08220000000000001, 'lambda': 36.71532740847555, 'max_bin': 348}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:24:16,496]\u001b[0m Trial 239 finished with value: 0.6855595464471491 and parameters: {'n_estimators': 778, 'eta': 0.06700054239819711, 'max_depth': 9, 'alpha': 0.050800000000000005, 'lambda': 11.097089790408074, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:24:28,754]\u001b[0m Trial 240 finished with value: 0.679906504693181 and parameters: {'n_estimators': 848, 'eta': 0.06051838983150913, 'max_depth': 9, 'alpha': 0.1252, 'lambda': 1.204569975144576, 'max_bin': 340}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:24:45,372]\u001b[0m Trial 241 finished with value: 0.690425592078471 and parameters: {'n_estimators': 460, 'eta': 0.05869453808973675, 'max_depth': 9, 'alpha': 0.0014, 'lambda': 35.10742256201672, 'max_bin': 344}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:25:06,085]\u001b[0m Trial 242 finished with value: 0.6952501121812377 and parameters: {'n_estimators': 828, 'eta': 0.06259505262765193, 'max_depth': 9, 'alpha': 0.0171, 'lambda': 34.53823972061402, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:25:27,353]\u001b[0m Trial 243 finished with value: 0.6952027761362485 and parameters: {'n_estimators': 807, 'eta': 0.059840716939955074, 'max_depth': 9, 'alpha': 0.0001, 'lambda': 33.68194279010242, 'max_bin': 351}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:25:51,392]\u001b[0m Trial 244 finished with value: 0.694685904255792 and parameters: {'n_estimators': 884, 'eta': 0.055565496812591014, 'max_depth': 9, 'alpha': 0.033, 'lambda': 35.76436809369382, 'max_bin': 342}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:26:13,715]\u001b[0m Trial 245 finished with value: 0.6932744598908489 and parameters: {'n_estimators': 840, 'eta': 0.061327570571436736, 'max_depth': 9, 'alpha': 0.0666, 'lambda': 34.81284701253961, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:26:33,671]\u001b[0m Trial 246 finished with value: 0.6933421257181035 and parameters: {'n_estimators': 868, 'eta': 0.05262767026446404, 'max_depth': 9, 'alpha': 0.032, 'lambda': 32.91083800332157, 'max_bin': 375}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:26:53,458]\u001b[0m Trial 247 finished with value: 0.6911994640940349 and parameters: {'n_estimators': 762, 'eta': 0.06451090177419135, 'max_depth': 9, 'alpha': 0.097, 'lambda': 36.13239359854704, 'max_bin': 325}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:27:15,546]\u001b[0m Trial 248 finished with value: 0.6928416450355596 and parameters: {'n_estimators': 811, 'eta': 0.05928243789860363, 'max_depth': 9, 'alpha': 0.056, 'lambda': 37.2947780641262, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:27:37,526]\u001b[0m Trial 249 finished with value: 0.6943007514717632 and parameters: {'n_estimators': 845, 'eta': 0.06238110213953034, 'max_depth': 9, 'alpha': 0.0172, 'lambda': 38.18107777101523, 'max_bin': 345}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7038\n",
      "\tBest params:\n",
      "\t\tn_estimators: 895\n",
      "\t\teta: 0.06642109429014469\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.0307\n",
      "\t\tlambda: 36.726715163896934\n",
      "\t\tmax_bin: 340\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683009    0.703984    0.706733    0.688618   \n",
      "1                    TP  385.000000  407.000000  407.000000  390.000000   \n",
      "2                    TN  373.000000  343.000000  332.000000  337.000000   \n",
      "3                    FP   71.000000   73.000000   91.000000   89.000000   \n",
      "4                    FN   70.000000   76.000000   69.000000   83.000000   \n",
      "5              Accuracy    0.843159    0.834260    0.822024    0.808676   \n",
      "6             Precision    0.844298    0.847917    0.817269    0.814196   \n",
      "7           Sensitivity    0.846154    0.842650    0.855042    0.824524   \n",
      "8           Specificity    0.840100    0.824500    0.784900    0.791100   \n",
      "9              F1 score    0.845225    0.845275    0.835729    0.819328   \n",
      "10  F1 score (weighted)    0.843157    0.834300    0.821659    0.808601   \n",
      "11     F1 score (macro)    0.843131    0.833416    0.820777    0.808009   \n",
      "12    Balanced Accuracy    0.843122    0.833585    0.819956    0.807802   \n",
      "13                  MCC    0.686264    0.666851    0.642550    0.616090   \n",
      "14                  NPV    0.842000    0.818600    0.827900    0.802400   \n",
      "15              ROC_AUC    0.843122    0.833585    0.819956    0.807802   \n",
      "\n",
      "          Set4  \n",
      "0     0.660944  \n",
      "1   395.000000  \n",
      "2   348.000000  \n",
      "3    82.000000  \n",
      "4    74.000000  \n",
      "5     0.826474  \n",
      "6     0.828092  \n",
      "7     0.842217  \n",
      "8     0.809300  \n",
      "9     0.835095  \n",
      "10    0.826393  \n",
      "11    0.825998  \n",
      "12    0.825760  \n",
      "13    0.652128  \n",
      "14    0.824600  \n",
      "15    0.825760  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_4_cat = np.where((y_pred_xgb_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c6c1fb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_xgb_4_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 08:28:06,807]\u001b[0m Trial 250 finished with value: 0.6969059372115195 and parameters: {'n_estimators': 886, 'eta': 0.05649417025724927, 'max_depth': 9, 'alpha': 0.0869, 'lambda': 33.88479776007345, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:28:26,503]\u001b[0m Trial 251 finished with value: 0.6907822642635619 and parameters: {'n_estimators': 794, 'eta': 0.06519319286467479, 'max_depth': 9, 'alpha': 0.0516, 'lambda': 35.328100029157504, 'max_bin': 332}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:28:49,115]\u001b[0m Trial 252 finished with value: 0.6903232950485658 and parameters: {'n_estimators': 897, 'eta': 0.0738162398937059, 'max_depth': 9, 'alpha': 0.0728, 'lambda': 33.240228770069216, 'max_bin': 353}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:29:15,474]\u001b[0m Trial 253 finished with value: 0.6931110503286299 and parameters: {'n_estimators': 864, 'eta': 0.06012269566766197, 'max_depth': 9, 'alpha': 0.0313, 'lambda': 34.50742145633948, 'max_bin': 318}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:29:40,261]\u001b[0m Trial 254 finished with value: 0.689453470469669 and parameters: {'n_estimators': 825, 'eta': 0.06296864595182898, 'max_depth': 9, 'alpha': 0.11510000000000001, 'lambda': 36.63642213292298, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:30:06,664]\u001b[0m Trial 255 finished with value: 0.694147243550882 and parameters: {'n_estimators': 900, 'eta': 0.057797018568924524, 'max_depth': 9, 'alpha': 0.0019, 'lambda': 35.2743025659353, 'max_bin': 348}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:30:26,280]\u001b[0m Trial 256 finished with value: 0.69304384501792 and parameters: {'n_estimators': 854, 'eta': 0.07090549092494931, 'max_depth': 9, 'alpha': 0.056, 'lambda': 30.876400355949528, 'max_bin': 305}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:30:55,352]\u001b[0m Trial 257 finished with value: 0.6909437699152317 and parameters: {'n_estimators': 873, 'eta': 0.03881598618819458, 'max_depth': 9, 'alpha': 0.0367, 'lambda': 32.17083391490889, 'max_bin': 328}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:31:18,059]\u001b[0m Trial 258 finished with value: 0.6921526018793306 and parameters: {'n_estimators': 839, 'eta': 0.06708157280941024, 'max_depth': 9, 'alpha': 0.0159, 'lambda': 37.585459656046844, 'max_bin': 336}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:31:42,322]\u001b[0m Trial 259 finished with value: 0.693345908749442 and parameters: {'n_estimators': 778, 'eta': 0.06214657812650407, 'max_depth': 9, 'alpha': 0.0921, 'lambda': 35.88713066063878, 'max_bin': 322}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:32:04,599]\u001b[0m Trial 260 finished with value: 0.68835431090264 and parameters: {'n_estimators': 817, 'eta': 0.06492332291794613, 'max_depth': 9, 'alpha': 0.06960000000000001, 'lambda': 33.21046890116531, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:32:31,365]\u001b[0m Trial 261 finished with value: 0.6933187110278005 and parameters: {'n_estimators': 879, 'eta': 0.06036210560622735, 'max_depth': 9, 'alpha': 0.0388, 'lambda': 34.135160244454276, 'max_bin': 333}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:32:53,928]\u001b[0m Trial 262 finished with value: 0.6923545103892469 and parameters: {'n_estimators': 795, 'eta': 0.06804837629189693, 'max_depth': 8, 'alpha': 0.1474, 'lambda': 36.36255331790541, 'max_bin': 359}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:33:15,953]\u001b[0m Trial 263 finished with value: 0.6900142439557565 and parameters: {'n_estimators': 852, 'eta': 0.05827130831586636, 'max_depth': 9, 'alpha': 0.11080000000000001, 'lambda': 34.86379212850562, 'max_bin': 326}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:33:39,216]\u001b[0m Trial 264 finished with value: 0.693931986315327 and parameters: {'n_estimators': 831, 'eta': 0.05486303292852593, 'max_depth': 9, 'alpha': 0.0594, 'lambda': 33.71330534792638, 'max_bin': 390}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:34:01,625]\u001b[0m Trial 265 finished with value: 0.6925945983975972 and parameters: {'n_estimators': 874, 'eta': 0.06338684548792323, 'max_depth': 9, 'alpha': 0.0, 'lambda': 35.64922542767705, 'max_bin': 345}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:34:26,145]\u001b[0m Trial 266 finished with value: 0.6942480514298521 and parameters: {'n_estimators': 742, 'eta': 0.06132452295379464, 'max_depth': 9, 'alpha': 0.027, 'lambda': 38.23312366081953, 'max_bin': 336}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:34:49,789]\u001b[0m Trial 267 finished with value: 0.6901772382466176 and parameters: {'n_estimators': 804, 'eta': 0.06968652922411625, 'max_depth': 9, 'alpha': 0.08360000000000001, 'lambda': 37.219675338214515, 'max_bin': 350}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:35:13,064]\u001b[0m Trial 268 finished with value: 0.6898443202606284 and parameters: {'n_estimators': 857, 'eta': 0.06587677872315001, 'max_depth': 9, 'alpha': 0.0443, 'lambda': 32.34740226800698, 'max_bin': 330}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:35:36,088]\u001b[0m Trial 269 finished with value: 0.6936509585960563 and parameters: {'n_estimators': 764, 'eta': 0.05671925673023122, 'max_depth': 9, 'alpha': 0.021500000000000002, 'lambda': 34.68513774037322, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:36:00,988]\u001b[0m Trial 270 finished with value: 0.6938074358839782 and parameters: {'n_estimators': 831, 'eta': 0.059091069308015436, 'max_depth': 9, 'alpha': 0.0752, 'lambda': 36.68996530316472, 'max_bin': 316}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:36:24,198]\u001b[0m Trial 271 finished with value: 0.6920851886785748 and parameters: {'n_estimators': 899, 'eta': 0.0612847722347238, 'max_depth': 9, 'alpha': 0.1023, 'lambda': 34.07972459090183, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:36:45,734]\u001b[0m Trial 272 finished with value: 0.6929107394043501 and parameters: {'n_estimators': 865, 'eta': 0.06395014381016195, 'max_depth': 9, 'alpha': 0.053000000000000005, 'lambda': 35.50646310681392, 'max_bin': 345}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:37:08,247]\u001b[0m Trial 273 finished with value: 0.692168240227548 and parameters: {'n_estimators': 846, 'eta': 0.059593002688110075, 'max_depth': 9, 'alpha': 0.1333, 'lambda': 32.83611576469522, 'max_bin': 280}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:37:23,207]\u001b[0m Trial 274 finished with value: 0.686526259810922 and parameters: {'n_estimators': 817, 'eta': 0.05174762822034535, 'max_depth': 9, 'alpha': 0.015000000000000001, 'lambda': 3.1277475417187155, 'max_bin': 324}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:37:44,576]\u001b[0m Trial 275 finished with value: 0.6897385822789949 and parameters: {'n_estimators': 782, 'eta': 0.06228321266243211, 'max_depth': 9, 'alpha': 0.041600000000000005, 'lambda': 31.65893931225635, 'max_bin': 332}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:38:08,092]\u001b[0m Trial 276 finished with value: 0.6928704489826141 and parameters: {'n_estimators': 881, 'eta': 0.06629081013204066, 'max_depth': 9, 'alpha': 0.16440000000000002, 'lambda': 34.87184252393371, 'max_bin': 355}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:38:33,185]\u001b[0m Trial 277 finished with value: 0.6943060356078027 and parameters: {'n_estimators': 835, 'eta': 0.0571519235350525, 'max_depth': 9, 'alpha': 0.07050000000000001, 'lambda': 33.46991119708574, 'max_bin': 339}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:38:57,004]\u001b[0m Trial 278 finished with value: 0.6935767415979409 and parameters: {'n_estimators': 806, 'eta': 0.0643520275036463, 'max_depth': 9, 'alpha': 0.0251, 'lambda': 36.3056610222502, 'max_bin': 328}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 08:39:16,257]\u001b[0m Trial 279 finished with value: 0.6920560479256502 and parameters: {'n_estimators': 863, 'eta': 0.06076762134771962, 'max_depth': 9, 'alpha': 0.095, 'lambda': 15.358370030809956, 'max_bin': 347}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:39:40,960]\u001b[0m Trial 280 finished with value: 0.6923044460414177 and parameters: {'n_estimators': 886, 'eta': 0.05423902580859707, 'max_depth': 8, 'alpha': 0.7185, 'lambda': 34.24291186158626, 'max_bin': 342}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:40:05,875]\u001b[0m Trial 281 finished with value: 0.6936107813444609 and parameters: {'n_estimators': 848, 'eta': 0.05910518593166368, 'max_depth': 9, 'alpha': 0.053500000000000006, 'lambda': 37.53780737370662, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:40:29,958]\u001b[0m Trial 282 finished with value: 0.6908102478059819 and parameters: {'n_estimators': 820, 'eta': 0.07249640735599233, 'max_depth': 9, 'alpha': 0.2503, 'lambda': 35.41738993404361, 'max_bin': 254}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:41:00,897]\u001b[0m Trial 283 finished with value: 0.6885440939710259 and parameters: {'n_estimators': 868, 'eta': 0.02603961702443151, 'max_depth': 9, 'alpha': 0.1208, 'lambda': 29.73514189839655, 'max_bin': 309}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:41:21,836]\u001b[0m Trial 284 finished with value: 0.6920366103679534 and parameters: {'n_estimators': 793, 'eta': 0.06861900918106752, 'max_depth': 9, 'alpha': 0.0809, 'lambda': 32.63496355868436, 'max_bin': 320}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:41:45,631]\u001b[0m Trial 285 finished with value: 0.6927095595568906 and parameters: {'n_estimators': 837, 'eta': 0.063106281428511, 'max_depth': 9, 'alpha': 0.0002, 'lambda': 36.1254425019693, 'max_bin': 350}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:42:14,026]\u001b[0m Trial 286 finished with value: 0.6911583669305477 and parameters: {'n_estimators': 879, 'eta': 0.0481968476186877, 'max_depth': 9, 'alpha': 0.0386, 'lambda': 38.416811041373364, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:42:35,465]\u001b[0m Trial 287 finished with value: 0.6915102586784132 and parameters: {'n_estimators': 858, 'eta': 0.07551612408376934, 'max_depth': 9, 'alpha': 0.0665, 'lambda': 33.57148907794638, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:43:00,327]\u001b[0m Trial 288 finished with value: 0.6929507861635356 and parameters: {'n_estimators': 771, 'eta': 0.057873608365229375, 'max_depth': 9, 'alpha': 0.0256, 'lambda': 30.65655604851601, 'max_bin': 343}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:43:23,781]\u001b[0m Trial 289 finished with value: 0.6930914598023701 and parameters: {'n_estimators': 900, 'eta': 0.06561135771405897, 'max_depth': 9, 'alpha': 0.1023, 'lambda': 34.850649445833774, 'max_bin': 333}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:43:49,791]\u001b[0m Trial 290 finished with value: 0.6930191056289934 and parameters: {'n_estimators': 814, 'eta': 0.06134415205447416, 'max_depth': 9, 'alpha': 0.053500000000000006, 'lambda': 37.06915574674597, 'max_bin': 324}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:44:12,756]\u001b[0m Trial 291 finished with value: 0.6932460959600257 and parameters: {'n_estimators': 849, 'eta': 0.055627407078242826, 'max_depth': 9, 'alpha': 0.019100000000000002, 'lambda': 33.90686234294244, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:44:25,599]\u001b[0m Trial 292 finished with value: 0.6844673623526264 and parameters: {'n_estimators': 420, 'eta': 0.06336537238341695, 'max_depth': 8, 'alpha': 0.0858, 'lambda': 31.942290760214654, 'max_bin': 346}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:44:46,854]\u001b[0m Trial 293 finished with value: 0.692226199159826 and parameters: {'n_estimators': 787, 'eta': 0.06010641220741809, 'max_depth': 9, 'alpha': 0.0356, 'lambda': 35.69332826678186, 'max_bin': 315}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:45:13,061]\u001b[0m Trial 294 finished with value: 0.6697781990212135 and parameters: {'n_estimators': 754, 'eta': 0.01696470538177653, 'max_depth': 9, 'alpha': 0.0699, 'lambda': 34.63350754304633, 'max_bin': 332}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:45:33,405]\u001b[0m Trial 295 finished with value: 0.6927146361454651 and parameters: {'n_estimators': 880, 'eta': 0.07947115687140298, 'max_depth': 9, 'alpha': 0.9796, 'lambda': 20.921036452141017, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:45:45,520]\u001b[0m Trial 296 finished with value: 0.6859343990957429 and parameters: {'n_estimators': 352, 'eta': 0.06701523948072832, 'max_depth': 9, 'alpha': 0.012400000000000001, 'lambda': 36.562647132750364, 'max_bin': 328}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:46:08,364]\u001b[0m Trial 297 finished with value: 0.6918591929432428 and parameters: {'n_estimators': 826, 'eta': 0.06156187971267392, 'max_depth': 9, 'alpha': 0.1153, 'lambda': 33.14380628285282, 'max_bin': 355}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:46:35,244]\u001b[0m Trial 298 finished with value: 0.6922656761650325 and parameters: {'n_estimators': 867, 'eta': 0.04495684905871997, 'max_depth': 9, 'alpha': 0.056400000000000006, 'lambda': 37.55100626237745, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:46:41,300]\u001b[0m Trial 299 finished with value: 0.6508969323110583 and parameters: {'n_estimators': 157, 'eta': 0.05800824684076381, 'max_depth': 10, 'alpha': 0.0385, 'lambda': 35.094868785792386, 'max_bin': 347}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7038\n",
      "\tBest params:\n",
      "\t\tn_estimators: 895\n",
      "\t\teta: 0.06642109429014469\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.0307\n",
      "\t\tlambda: 36.726715163896934\n",
      "\t\tmax_bin: 340\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683009    0.703984    0.706733    0.688618   \n",
      "1                    TP  385.000000  407.000000  407.000000  390.000000   \n",
      "2                    TN  373.000000  343.000000  332.000000  337.000000   \n",
      "3                    FP   71.000000   73.000000   91.000000   89.000000   \n",
      "4                    FN   70.000000   76.000000   69.000000   83.000000   \n",
      "5              Accuracy    0.843159    0.834260    0.822024    0.808676   \n",
      "6             Precision    0.844298    0.847917    0.817269    0.814196   \n",
      "7           Sensitivity    0.846154    0.842650    0.855042    0.824524   \n",
      "8           Specificity    0.840100    0.824500    0.784900    0.791100   \n",
      "9              F1 score    0.845225    0.845275    0.835729    0.819328   \n",
      "10  F1 score (weighted)    0.843157    0.834300    0.821659    0.808601   \n",
      "11     F1 score (macro)    0.843131    0.833416    0.820777    0.808009   \n",
      "12    Balanced Accuracy    0.843122    0.833585    0.819956    0.807802   \n",
      "13                  MCC    0.686264    0.666851    0.642550    0.616090   \n",
      "14                  NPV    0.842000    0.818600    0.827900    0.802400   \n",
      "15              ROC_AUC    0.843122    0.833585    0.819956    0.807802   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.660944    0.688256  \n",
      "1   395.000000  404.000000  \n",
      "2   348.000000  355.000000  \n",
      "3    82.000000   68.000000  \n",
      "4    74.000000   72.000000  \n",
      "5     0.826474    0.844271  \n",
      "6     0.828092    0.855932  \n",
      "7     0.842217    0.848739  \n",
      "8     0.809300    0.839200  \n",
      "9     0.835095    0.852321  \n",
      "10    0.826393    0.844309  \n",
      "11    0.825998    0.843807  \n",
      "12    0.825760    0.843991  \n",
      "13    0.652128    0.687648  \n",
      "14    0.824600    0.831400  \n",
      "15    0.825760    0.843991  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "\n",
    "y_pred_xgb_5_cat = np.where((y_pred_xgb_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 08:47:06,625]\u001b[0m Trial 300 finished with value: 0.6880831613333381 and parameters: {'n_estimators': 802, 'eta': 0.06412832973584537, 'max_depth': 9, 'alpha': 0.001, 'lambda': 31.385714261612733, 'max_bin': 321}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:47:29,798]\u001b[0m Trial 301 finished with value: 0.6906999968411833 and parameters: {'n_estimators': 841, 'eta': 0.07074109032902305, 'max_depth': 9, 'alpha': 0.0847, 'lambda': 34.09534718698298, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:47:36,529]\u001b[0m Trial 302 finished with value: -6.162740500116504 and parameters: {'n_estimators': 886, 'eta': 0.0008964412496685129, 'max_depth': 9, 'alpha': 0.1423, 'lambda': 35.715741157374374, 'max_bin': 342}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:48:01,275]\u001b[0m Trial 303 finished with value: 0.6885120747642601 and parameters: {'n_estimators': 900, 'eta': 0.05946159559217085, 'max_depth': 9, 'alpha': 0.0603, 'lambda': 32.62978870375451, 'max_bin': 352}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:48:24,983]\u001b[0m Trial 304 finished with value: 0.6901886408606286 and parameters: {'n_estimators': 824, 'eta': 0.062346085019470046, 'max_depth': 9, 'alpha': 0.1005, 'lambda': 36.77465711972958, 'max_bin': 326}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:48:51,624]\u001b[0m Trial 305 finished with value: 0.6932775893627523 and parameters: {'n_estimators': 856, 'eta': 0.0656492585594355, 'max_depth': 10, 'alpha': 0.023700000000000002, 'lambda': 38.27844740108192, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:49:13,394]\u001b[0m Trial 306 finished with value: 0.6879526329966383 and parameters: {'n_estimators': 872, 'eta': 0.06777658631925831, 'max_depth': 9, 'alpha': 0.043500000000000004, 'lambda': 34.39441063214425, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:49:39,211]\u001b[0m Trial 307 finished with value: 0.6888862991115727 and parameters: {'n_estimators': 780, 'eta': 0.055431736848456904, 'max_depth': 9, 'alpha': 0.0734, 'lambda': 33.466269877717146, 'max_bin': 343}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:50:05,165]\u001b[0m Trial 308 finished with value: 0.6888659991622005 and parameters: {'n_estimators': 808, 'eta': 0.06065740274105535, 'max_depth': 9, 'alpha': 0.019100000000000002, 'lambda': 36.00184427537701, 'max_bin': 360}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:50:28,537]\u001b[0m Trial 309 finished with value: 0.6882726543282904 and parameters: {'n_estimators': 849, 'eta': 0.05781121712217965, 'max_depth': 9, 'alpha': 0.051300000000000005, 'lambda': 35.22027987794675, 'max_bin': 384}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:50:52,917]\u001b[0m Trial 310 finished with value: 0.6879997259498298 and parameters: {'n_estimators': 884, 'eta': 0.0634431971514065, 'max_depth': 9, 'alpha': 0.1097, 'lambda': 33.08884121099795, 'max_bin': 335}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:51:14,241]\u001b[0m Trial 311 finished with value: 0.6877001146416829 and parameters: {'n_estimators': 834, 'eta': 0.06905721692165276, 'max_depth': 9, 'alpha': 0.0375, 'lambda': 39.49237624498304, 'max_bin': 350}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:51:37,840]\u001b[0m Trial 312 finished with value: 0.6864574561080993 and parameters: {'n_estimators': 866, 'eta': 0.06522025085074148, 'max_depth': 9, 'alpha': 0.0013000000000000002, 'lambda': 32.12170250223816, 'max_bin': 323}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:52:04,710]\u001b[0m Trial 313 finished with value: 0.6901193937375608 and parameters: {'n_estimators': 793, 'eta': 0.05300767373843133, 'max_depth': 9, 'alpha': 0.9418000000000001, 'lambda': 37.763669071669746, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:52:27,749]\u001b[0m Trial 314 finished with value: 0.6907633433672602 and parameters: {'n_estimators': 824, 'eta': 0.06112034453953843, 'max_depth': 9, 'alpha': 0.0867, 'lambda': 34.53473344517236, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:52:51,430]\u001b[0m Trial 315 finished with value: 0.6901447124207595 and parameters: {'n_estimators': 745, 'eta': 0.05932559394312189, 'max_depth': 9, 'alpha': 0.0641, 'lambda': 36.85234496343967, 'max_bin': 347}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:53:13,342]\u001b[0m Trial 316 finished with value: 0.6918899088062366 and parameters: {'n_estimators': 851, 'eta': 0.056617621513624886, 'max_depth': 9, 'alpha': 0.1336, 'lambda': 35.734583010373306, 'max_bin': 327}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:53:35,852]\u001b[0m Trial 317 finished with value: 0.6889544759183794 and parameters: {'n_estimators': 883, 'eta': 0.06286518522413276, 'max_depth': 9, 'alpha': 0.0334, 'lambda': 34.17676259050121, 'max_bin': 317}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:53:55,862]\u001b[0m Trial 318 finished with value: 0.6878649728304551 and parameters: {'n_estimators': 771, 'eta': 0.0767503857198415, 'max_depth': 8, 'alpha': 0.0893, 'lambda': 35.00267510910612, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:54:16,797]\u001b[0m Trial 319 finished with value: 0.6838856821817715 and parameters: {'n_estimators': 810, 'eta': 0.06467720120205232, 'max_depth': 9, 'alpha': 0.0165, 'lambda': 33.58813822974172, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:54:43,810]\u001b[0m Trial 320 finished with value: 0.689796024230681 and parameters: {'n_estimators': 900, 'eta': 0.06177547783635925, 'max_depth': 9, 'alpha': 0.0594, 'lambda': 36.37736327172928, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:55:07,066]\u001b[0m Trial 321 finished with value: 0.6874769590637333 and parameters: {'n_estimators': 838, 'eta': 0.05905624538238489, 'max_depth': 9, 'alpha': 0.0392, 'lambda': 31.218355732410384, 'max_bin': 491}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:55:29,554]\u001b[0m Trial 322 finished with value: 0.6837735142932921 and parameters: {'n_estimators': 870, 'eta': 0.0732896010854029, 'max_depth': 9, 'alpha': 0.0, 'lambda': 37.49616820401985, 'max_bin': 295}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:55:55,569]\u001b[0m Trial 323 finished with value: 0.6884265822525217 and parameters: {'n_estimators': 858, 'eta': 0.0504374716972936, 'max_depth': 9, 'alpha': 0.1188, 'lambda': 32.46345829006367, 'max_bin': 346}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:56:16,989]\u001b[0m Trial 324 finished with value: 0.6858690965698722 and parameters: {'n_estimators': 795, 'eta': 0.06646304496725795, 'max_depth': 9, 'alpha': 0.0792, 'lambda': 35.26524552190364, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:56:42,289]\u001b[0m Trial 325 finished with value: 0.6896145335971541 and parameters: {'n_estimators': 824, 'eta': 0.0572930946434327, 'max_depth': 9, 'alpha': 0.0563, 'lambda': 36.18272634595934, 'max_bin': 354}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:57:06,140]\u001b[0m Trial 326 finished with value: 0.6901650989940183 and parameters: {'n_estimators': 841, 'eta': 0.06301813819679901, 'max_depth': 9, 'alpha': 0.162, 'lambda': 33.183689055289236, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:57:31,636]\u001b[0m Trial 327 finished with value: 0.689085005580132 and parameters: {'n_estimators': 888, 'eta': 0.06065076746233251, 'max_depth': 9, 'alpha': 0.0335, 'lambda': 34.140694423198106, 'max_bin': 323}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:57:54,869]\u001b[0m Trial 328 finished with value: 0.6873662817492786 and parameters: {'n_estimators': 862, 'eta': 0.067969960031373, 'max_depth': 9, 'alpha': 0.097, 'lambda': 38.544441210312186, 'max_bin': 345}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 08:58:24,031]\u001b[0m Trial 329 finished with value: 0.6893934930907515 and parameters: {'n_estimators': 762, 'eta': 0.05470465557749264, 'max_depth': 10, 'alpha': 0.022600000000000002, 'lambda': 36.935384061965394, 'max_bin': 313}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:58:47,521]\u001b[0m Trial 330 finished with value: 0.6898220630725181 and parameters: {'n_estimators': 811, 'eta': 0.06435048872263414, 'max_depth': 9, 'alpha': 0.0718, 'lambda': 35.186610439752705, 'max_bin': 368}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:59:06,815]\u001b[0m Trial 331 finished with value: 0.6877548734023556 and parameters: {'n_estimators': 870, 'eta': 0.0699223596551981, 'max_depth': 9, 'alpha': 0.0487, 'lambda': 30.358217020607864, 'max_bin': 333}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:59:33,591]\u001b[0m Trial 332 finished with value: 0.6868521649583719 and parameters: {'n_estimators': 840, 'eta': 0.041869759886218584, 'max_depth': 9, 'alpha': 0.018600000000000002, 'lambda': 34.485411354529575, 'max_bin': 327}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:59:56,190]\u001b[0m Trial 333 finished with value: 0.6878827546056969 and parameters: {'n_estimators': 787, 'eta': 0.05888569065048657, 'max_depth': 8, 'alpha': 0.06760000000000001, 'lambda': 35.97051924732402, 'max_bin': 340}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:00:18,755]\u001b[0m Trial 334 finished with value: 0.6879331036062354 and parameters: {'n_estimators': 881, 'eta': 0.06123361119302374, 'max_depth': 9, 'alpha': 0.10930000000000001, 'lambda': 32.14113818881033, 'max_bin': 335}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:00:36,092]\u001b[0m Trial 335 finished with value: 0.6860042677941266 and parameters: {'n_estimators': 823, 'eta': 0.0626158932868554, 'max_depth': 9, 'alpha': 0.0, 'lambda': 8.566422093388645, 'max_bin': 378}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:01:02,515]\u001b[0m Trial 336 finished with value: 0.691724425982529 and parameters: {'n_estimators': 855, 'eta': 0.05701694017046474, 'max_depth': 9, 'alpha': 0.045000000000000005, 'lambda': 33.68226383309976, 'max_bin': 395}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:01:24,628]\u001b[0m Trial 337 finished with value: 0.6880872512703485 and parameters: {'n_estimators': 804, 'eta': 0.06608353768299052, 'max_depth': 9, 'alpha': 0.0868, 'lambda': 32.97977288174618, 'max_bin': 319}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:01:48,134]\u001b[0m Trial 338 finished with value: 0.6899812372143096 and parameters: {'n_estimators': 727, 'eta': 0.059717549816020454, 'max_depth': 9, 'alpha': 0.1305, 'lambda': 37.84938263096621, 'max_bin': 350}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:02:09,776]\u001b[0m Trial 339 finished with value: 0.6866630275277986 and parameters: {'n_estimators': 885, 'eta': 0.07182208101853005, 'max_depth': 9, 'alpha': 0.0221, 'lambda': 34.95747740386508, 'max_bin': 344}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:02:32,689]\u001b[0m Trial 340 finished with value: 0.6891359163110274 and parameters: {'n_estimators': 841, 'eta': 0.06455551129530279, 'max_depth': 9, 'alpha': 0.057, 'lambda': 35.74646548986948, 'max_bin': 330}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:02:58,150]\u001b[0m Trial 341 finished with value: 0.6880574401260446 and parameters: {'n_estimators': 860, 'eta': 0.06189560823810562, 'max_depth': 9, 'alpha': 0.0413, 'lambda': 36.76426460065457, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:03:22,925]\u001b[0m Trial 342 finished with value: 0.69076101741891 and parameters: {'n_estimators': 899, 'eta': 0.05854188600606748, 'max_depth': 9, 'alpha': 0.3027, 'lambda': 34.00943529804126, 'max_bin': 325}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:03:49,268]\u001b[0m Trial 343 finished with value: 0.6886445186462444 and parameters: {'n_estimators': 774, 'eta': 0.05510888906582652, 'max_depth': 9, 'alpha': 0.0762, 'lambda': 34.70739172415504, 'max_bin': 349}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:04:13,627]\u001b[0m Trial 344 finished with value: 0.689385755771868 and parameters: {'n_estimators': 821, 'eta': 0.06031945782260075, 'max_depth': 10, 'alpha': 0.0205, 'lambda': 31.805820478086215, 'max_bin': 333}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:04:39,781]\u001b[0m Trial 345 finished with value: 0.6841900969360464 and parameters: {'n_estimators': 875, 'eta': 0.03380141541745081, 'max_depth': 8, 'alpha': 0.1053, 'lambda': 33.098194399748195, 'max_bin': 342}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:04:46,729]\u001b[0m Trial 346 finished with value: 0.6639228628727971 and parameters: {'n_estimators': 207, 'eta': 0.06743500125044287, 'max_depth': 9, 'alpha': 0.0328, 'lambda': 36.37605135317601, 'max_bin': 357}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:05:10,825]\u001b[0m Trial 347 finished with value: 0.6889763989340343 and parameters: {'n_estimators': 847, 'eta': 0.06433002876212635, 'max_depth': 9, 'alpha': 0.0673, 'lambda': 37.355445785759706, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:05:30,442]\u001b[0m Trial 348 finished with value: 0.6897472569011122 and parameters: {'n_estimators': 568, 'eta': 0.06285320018588111, 'max_depth': 9, 'alpha': 0.09140000000000001, 'lambda': 35.35466592348053, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:05:53,099]\u001b[0m Trial 349 finished with value: 0.6890310082067843 and parameters: {'n_estimators': 792, 'eta': 0.05820601120103676, 'max_depth': 9, 'alpha': 0.0149, 'lambda': 38.95925740488662, 'max_bin': 344}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7038\n",
      "\tBest params:\n",
      "\t\tn_estimators: 895\n",
      "\t\teta: 0.06642109429014469\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.0307\n",
      "\t\tlambda: 36.726715163896934\n",
      "\t\tmax_bin: 340\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683009    0.703984    0.706733    0.688618   \n",
      "1                    TP  385.000000  407.000000  407.000000  390.000000   \n",
      "2                    TN  373.000000  343.000000  332.000000  337.000000   \n",
      "3                    FP   71.000000   73.000000   91.000000   89.000000   \n",
      "4                    FN   70.000000   76.000000   69.000000   83.000000   \n",
      "5              Accuracy    0.843159    0.834260    0.822024    0.808676   \n",
      "6             Precision    0.844298    0.847917    0.817269    0.814196   \n",
      "7           Sensitivity    0.846154    0.842650    0.855042    0.824524   \n",
      "8           Specificity    0.840100    0.824500    0.784900    0.791100   \n",
      "9              F1 score    0.845225    0.845275    0.835729    0.819328   \n",
      "10  F1 score (weighted)    0.843157    0.834300    0.821659    0.808601   \n",
      "11     F1 score (macro)    0.843131    0.833416    0.820777    0.808009   \n",
      "12    Balanced Accuracy    0.843122    0.833585    0.819956    0.807802   \n",
      "13                  MCC    0.686264    0.666851    0.642550    0.616090   \n",
      "14                  NPV    0.842000    0.818600    0.827900    0.802400   \n",
      "15              ROC_AUC    0.843122    0.833585    0.819956    0.807802   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.660944    0.688256    0.705296  \n",
      "1   395.000000  404.000000  403.000000  \n",
      "2   348.000000  355.000000  349.000000  \n",
      "3    82.000000   68.000000   78.000000  \n",
      "4    74.000000   72.000000   69.000000  \n",
      "5     0.826474    0.844271    0.836485  \n",
      "6     0.828092    0.855932    0.837838  \n",
      "7     0.842217    0.848739    0.853814  \n",
      "8     0.809300    0.839200    0.817300  \n",
      "9     0.835095    0.852321    0.845750  \n",
      "10    0.826393    0.844309    0.836386  \n",
      "11    0.825998    0.843807    0.835893  \n",
      "12    0.825760    0.843991    0.835572  \n",
      "13    0.652128    0.687648    0.671954  \n",
      "14    0.824600    0.831400    0.834900  \n",
      "15    0.825760    0.843991    0.835572  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_6_cat = np.where((y_pred_xgb_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 09:06:18,153]\u001b[0m Trial 350 finished with value: 0.6931878641397273 and parameters: {'n_estimators': 828, 'eta': 0.060657039555694736, 'max_depth': 9, 'alpha': 0.049600000000000005, 'lambda': 33.79051944143304, 'max_bin': 326}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:06:40,418]\u001b[0m Trial 351 finished with value: 0.6950878978313225 and parameters: {'n_estimators': 867, 'eta': 0.05634735808339813, 'max_depth': 9, 'alpha': 0.0729, 'lambda': 32.44558691544356, 'max_bin': 321}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:07:01,971]\u001b[0m Trial 352 finished with value: 0.69562590837563 and parameters: {'n_estimators': 803, 'eta': 0.05245054729470531, 'max_depth': 9, 'alpha': 0.1295, 'lambda': 34.69062044583382, 'max_bin': 335}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:07:21,884]\u001b[0m Trial 353 finished with value: 0.6960013266018132 and parameters: {'n_estimators': 888, 'eta': 0.06973614120290184, 'max_depth': 9, 'alpha': 0.0354, 'lambda': 35.90311944745632, 'max_bin': 308}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:07:41,437]\u001b[0m Trial 354 finished with value: 0.6930861948539897 and parameters: {'n_estimators': 900, 'eta': 0.07455878527170218, 'max_depth': 9, 'alpha': 0.015600000000000001, 'lambda': 31.387825969020277, 'max_bin': 353}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:08:02,000]\u001b[0m Trial 355 finished with value: 0.696622523155143 and parameters: {'n_estimators': 846, 'eta': 0.06620764613530371, 'max_depth': 9, 'alpha': 0.0523, 'lambda': 38.18594728915136, 'max_bin': 340}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:08:25,961]\u001b[0m Trial 356 finished with value: 0.6962327409433057 and parameters: {'n_estimators': 871, 'eta': 0.06293040427518735, 'max_depth': 9, 'alpha': 0.0969, 'lambda': 36.96707723614472, 'max_bin': 348}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:08:46,586]\u001b[0m Trial 357 finished with value: 0.6943037707395715 and parameters: {'n_estimators': 749, 'eta': 0.05980185233942646, 'max_depth': 9, 'alpha': 0.0017000000000000001, 'lambda': 33.951481788241544, 'max_bin': 330}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:09:07,493]\u001b[0m Trial 358 finished with value: 0.694976424480846 and parameters: {'n_estimators': 832, 'eta': 0.06461647942291814, 'max_depth': 10, 'alpha': 0.061000000000000006, 'lambda': 35.4628627556075, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:09:29,139]\u001b[0m Trial 359 finished with value: 0.6965773322534954 and parameters: {'n_estimators': 813, 'eta': 0.061907043767118, 'max_depth': 9, 'alpha': 0.155, 'lambda': 33.03677776757884, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:09:46,371]\u001b[0m Trial 360 finished with value: 0.6952969829370653 and parameters: {'n_estimators': 525, 'eta': 0.05785029555015536, 'max_depth': 9, 'alpha': 0.6333000000000001, 'lambda': 34.54135083448758, 'max_bin': 317}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:10:09,728]\u001b[0m Trial 361 finished with value: 0.6976573822196301 and parameters: {'n_estimators': 861, 'eta': 0.054089261398783894, 'max_depth': 8, 'alpha': 0.1145, 'lambda': 36.34359347245214, 'max_bin': 362}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:10:30,599]\u001b[0m Trial 362 finished with value: 0.6964229175282712 and parameters: {'n_estimators': 775, 'eta': 0.06837630672013201, 'max_depth': 9, 'alpha': 0.0333, 'lambda': 37.56016993573169, 'max_bin': 333}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:10:51,884]\u001b[0m Trial 363 finished with value: 0.6927959387976903 and parameters: {'n_estimators': 848, 'eta': 0.06065178416030461, 'max_depth': 9, 'alpha': 0.082, 'lambda': 33.47538806233751, 'max_bin': 346}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:11:12,086]\u001b[0m Trial 364 finished with value: 0.6951110199553833 and parameters: {'n_estimators': 883, 'eta': 0.0656447097161401, 'max_depth': 9, 'alpha': 0.0476, 'lambda': 35.23868787494869, 'max_bin': 324}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:11:32,531]\u001b[0m Trial 365 finished with value: 0.6958798550527009 and parameters: {'n_estimators': 833, 'eta': 0.06260541089258345, 'max_depth': 9, 'alpha': 0.0172, 'lambda': 32.51846947890822, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:11:52,761]\u001b[0m Trial 366 finished with value: 0.6976505861698873 and parameters: {'n_estimators': 810, 'eta': 0.0561897424902734, 'max_depth': 9, 'alpha': 0.06470000000000001, 'lambda': 34.639576031679994, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:12:12,775]\u001b[0m Trial 367 finished with value: 0.698674774875594 and parameters: {'n_estimators': 857, 'eta': 0.05883256077028363, 'max_depth': 9, 'alpha': 0.0946, 'lambda': 24.944851931301727, 'max_bin': 344}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:12:32,155]\u001b[0m Trial 368 finished with value: 0.6926855346501316 and parameters: {'n_estimators': 793, 'eta': 0.07177262399147467, 'max_depth': 9, 'alpha': 0.0308, 'lambda': 35.90388946104686, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:12:53,008]\u001b[0m Trial 369 finished with value: 0.6989435867476025 and parameters: {'n_estimators': 900, 'eta': 0.06406286962692526, 'max_depth': 9, 'alpha': 0.0712, 'lambda': 36.50012551468085, 'max_bin': 349}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:13:16,072]\u001b[0m Trial 370 finished with value: 0.6947682364535275 and parameters: {'n_estimators': 875, 'eta': 0.06104547919936903, 'max_depth': 9, 'alpha': 0.0424, 'lambda': 29.05298340735191, 'max_bin': 342}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:13:36,793]\u001b[0m Trial 371 finished with value: 0.6942568971655019 and parameters: {'n_estimators': 829, 'eta': 0.0672471314025694, 'max_depth': 9, 'alpha': 0.0006000000000000001, 'lambda': 34.147835704711795, 'max_bin': 326}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:13:57,651]\u001b[0m Trial 372 finished with value: 0.6970230941306461 and parameters: {'n_estimators': 758, 'eta': 0.059462362053101486, 'max_depth': 9, 'alpha': 0.1192, 'lambda': 30.874994114092353, 'max_bin': 352}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:14:19,962]\u001b[0m Trial 373 finished with value: 0.6968811274818949 and parameters: {'n_estimators': 852, 'eta': 0.06323540639178511, 'max_depth': 9, 'alpha': 0.08270000000000001, 'lambda': 38.525145965401315, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:14:40,967]\u001b[0m Trial 374 finished with value: 0.6971773918457986 and parameters: {'n_estimators': 814, 'eta': 0.05774826983542727, 'max_depth': 9, 'alpha': 0.0267, 'lambda': 33.3865285892089, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:15:04,561]\u001b[0m Trial 375 finished with value: 0.6959416320607511 and parameters: {'n_estimators': 784, 'eta': 0.0660144369159049, 'max_depth': 12, 'alpha': 0.0579, 'lambda': 37.10247128862721, 'max_bin': 320}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:15:26,354]\u001b[0m Trial 376 finished with value: 0.6977322034646793 and parameters: {'n_estimators': 873, 'eta': 0.06165376911470805, 'max_depth': 9, 'alpha': 0.0007, 'lambda': 35.341635450669074, 'max_bin': 313}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:15:47,974]\u001b[0m Trial 377 finished with value: 0.6944377184314352 and parameters: {'n_estimators': 841, 'eta': 0.06422383832767707, 'max_depth': 9, 'alpha': 0.11080000000000001, 'lambda': 31.84875288396993, 'max_bin': 344}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:16:06,444]\u001b[0m Trial 378 finished with value: 0.6946654234819432 and parameters: {'n_estimators': 883, 'eta': 0.056854946569102, 'max_depth': 9, 'alpha': 0.0449, 'lambda': 17.710472708742085, 'max_bin': 387}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 09:16:23,139]\u001b[0m Trial 379 finished with value: 0.689719064481825 and parameters: {'n_estimators': 860, 'eta': 0.0703929697037929, 'max_depth': 8, 'alpha': 0.020900000000000002, 'lambda': 39.53938235694279, 'max_bin': 336}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:16:44,423]\u001b[0m Trial 380 finished with value: 0.6942928861173613 and parameters: {'n_estimators': 800, 'eta': 0.06027236974162061, 'max_depth': 9, 'alpha': 0.1396, 'lambda': 34.24964546496005, 'max_bin': 327}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:16:59,085]\u001b[0m Trial 381 finished with value: 0.6807574013509401 and parameters: {'n_estimators': 820, 'eta': 0.07603286286576785, 'max_depth': 5, 'alpha': 0.08800000000000001, 'lambda': 36.02767712076866, 'max_bin': 356}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:17:24,690]\u001b[0m Trial 382 finished with value: 0.6952308293832111 and parameters: {'n_estimators': 886, 'eta': 0.046692596798437166, 'max_depth': 9, 'alpha': 0.0625, 'lambda': 32.43982337337742, 'max_bin': 298}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:17:48,963]\u001b[0m Trial 383 finished with value: 0.6969118072846203 and parameters: {'n_estimators': 837, 'eta': 0.06283155090434796, 'max_depth': 10, 'alpha': 0.0359, 'lambda': 35.046601481739444, 'max_bin': 340}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:18:10,600]\u001b[0m Trial 384 finished with value: 0.6951984537148161 and parameters: {'n_estimators': 766, 'eta': 0.05385882073072537, 'max_depth': 9, 'alpha': 0.0981, 'lambda': 38.01341118103395, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:18:29,627]\u001b[0m Trial 385 finished with value: 0.6914586271996492 and parameters: {'n_estimators': 860, 'eta': 0.06849694282919948, 'max_depth': 9, 'alpha': 0.0165, 'lambda': 33.65392787550973, 'max_bin': 374}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:18:50,878]\u001b[0m Trial 386 finished with value: 0.6965897490788152 and parameters: {'n_estimators': 870, 'eta': 0.0595594927933613, 'max_depth': 9, 'alpha': 0.26880000000000004, 'lambda': 37.097158880122755, 'max_bin': 347}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:19:13,838]\u001b[0m Trial 387 finished with value: 0.6957086025919984 and parameters: {'n_estimators': 849, 'eta': 0.04956285140851135, 'max_depth': 9, 'alpha': 0.0743, 'lambda': 32.9331583141621, 'max_bin': 335}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:19:33,228]\u001b[0m Trial 388 finished with value: 0.6957281769547448 and parameters: {'n_estimators': 900, 'eta': 0.0662914742037257, 'max_depth': 9, 'alpha': 0.0443, 'lambda': 34.99333831430103, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:19:56,037]\u001b[0m Trial 389 finished with value: 0.6955873083788806 and parameters: {'n_estimators': 828, 'eta': 0.06136781950399222, 'max_depth': 9, 'alpha': 0.059500000000000004, 'lambda': 36.325193317580634, 'max_bin': 322}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:19:57,501]\u001b[0m Trial 390 finished with value: 0.26909919371642965 and parameters: {'n_estimators': 51, 'eta': 0.05622564872117909, 'max_depth': 9, 'alpha': 0.017, 'lambda': 34.336475233949834, 'max_bin': 351}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:20:17,094]\u001b[0m Trial 391 finished with value: 0.6962889361867356 and parameters: {'n_estimators': 790, 'eta': 0.06455610947938611, 'max_depth': 9, 'alpha': 0.10490000000000001, 'lambda': 35.77040144417016, 'max_bin': 288}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:20:39,171]\u001b[0m Trial 392 finished with value: 0.42248662176718244 and parameters: {'n_estimators': 806, 'eta': 0.004323134901343145, 'max_depth': 9, 'alpha': 0.0784, 'lambda': 33.61857704219622, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:20:58,596]\u001b[0m Trial 393 finished with value: 0.6978895053049639 and parameters: {'n_estimators': 884, 'eta': 0.057972978648701365, 'max_depth': 9, 'alpha': 0.0292, 'lambda': 19.66150806764815, 'max_bin': 336}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:21:21,579]\u001b[0m Trial 394 finished with value: 0.6961829248123844 and parameters: {'n_estimators': 838, 'eta': 0.06250526901695333, 'max_depth': 9, 'alpha': 0.0526, 'lambda': 37.13815345329736, 'max_bin': 345}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:21:43,590]\u001b[0m Trial 395 finished with value: 0.6953326056705768 and parameters: {'n_estimators': 868, 'eta': 0.05941631532875959, 'max_depth': 9, 'alpha': 0.0, 'lambda': 31.64320023271181, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:22:00,595]\u001b[0m Trial 396 finished with value: 0.6869459167525613 and parameters: {'n_estimators': 819, 'eta': 0.061331532247857076, 'max_depth': 6, 'alpha': 0.8312, 'lambda': 35.32945555727198, 'max_bin': 332}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:22:18,270]\u001b[0m Trial 397 finished with value: 0.6926151406429755 and parameters: {'n_estimators': 775, 'eta': 0.07382535562214983, 'max_depth': 9, 'alpha': 0.12140000000000001, 'lambda': 32.87491568338106, 'max_bin': 318}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:22:41,353]\u001b[0m Trial 398 finished with value: 0.6988176762436273 and parameters: {'n_estimators': 853, 'eta': 0.06510998124670282, 'max_depth': 10, 'alpha': 0.0298, 'lambda': 34.40855710109144, 'max_bin': 325}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:23:01,722]\u001b[0m Trial 399 finished with value: 0.6980652224971535 and parameters: {'n_estimators': 883, 'eta': 0.06740138465844968, 'max_depth': 8, 'alpha': 0.079, 'lambda': 37.80417530942249, 'max_bin': 339}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7038\n",
      "\tBest params:\n",
      "\t\tn_estimators: 895\n",
      "\t\teta: 0.06642109429014469\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.0307\n",
      "\t\tlambda: 36.726715163896934\n",
      "\t\tmax_bin: 340\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683009    0.703984    0.706733    0.688618   \n",
      "1                    TP  385.000000  407.000000  407.000000  390.000000   \n",
      "2                    TN  373.000000  343.000000  332.000000  337.000000   \n",
      "3                    FP   71.000000   73.000000   91.000000   89.000000   \n",
      "4                    FN   70.000000   76.000000   69.000000   83.000000   \n",
      "5              Accuracy    0.843159    0.834260    0.822024    0.808676   \n",
      "6             Precision    0.844298    0.847917    0.817269    0.814196   \n",
      "7           Sensitivity    0.846154    0.842650    0.855042    0.824524   \n",
      "8           Specificity    0.840100    0.824500    0.784900    0.791100   \n",
      "9              F1 score    0.845225    0.845275    0.835729    0.819328   \n",
      "10  F1 score (weighted)    0.843157    0.834300    0.821659    0.808601   \n",
      "11     F1 score (macro)    0.843131    0.833416    0.820777    0.808009   \n",
      "12    Balanced Accuracy    0.843122    0.833585    0.819956    0.807802   \n",
      "13                  MCC    0.686264    0.666851    0.642550    0.616090   \n",
      "14                  NPV    0.842000    0.818600    0.827900    0.802400   \n",
      "15              ROC_AUC    0.843122    0.833585    0.819956    0.807802   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.660944    0.688256    0.705296    0.684340  \n",
      "1   395.000000  404.000000  403.000000  400.000000  \n",
      "2   348.000000  355.000000  349.000000  339.000000  \n",
      "3    82.000000   68.000000   78.000000   77.000000  \n",
      "4    74.000000   72.000000   69.000000   83.000000  \n",
      "5     0.826474    0.844271    0.836485    0.822024  \n",
      "6     0.828092    0.855932    0.837838    0.838574  \n",
      "7     0.842217    0.848739    0.853814    0.828157  \n",
      "8     0.809300    0.839200    0.817300    0.814900  \n",
      "9     0.835095    0.852321    0.845750    0.833333  \n",
      "10    0.826393    0.844309    0.836386    0.822105  \n",
      "11    0.825998    0.843807    0.835893    0.821201  \n",
      "12    0.825760    0.843991    0.835572    0.821531  \n",
      "13    0.652128    0.687648    0.671954    0.642476  \n",
      "14    0.824600    0.831400    0.834900    0.803300  \n",
      "15    0.825760    0.843991    0.835572    0.821531  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "y_pred_xgb_7_cat = np.where((y_pred_xgb_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 09:23:21,980]\u001b[0m Trial 400 finished with value: 0.6814429727047122 and parameters: {'n_estimators': 739, 'eta': 0.07823980437741387, 'max_depth': 9, 'alpha': 0.051500000000000004, 'lambda': 36.24695130382364, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:23:44,479]\u001b[0m Trial 401 finished with value: 0.6834614501699832 and parameters: {'n_estimators': 900, 'eta': 0.05518280830855603, 'max_depth': 9, 'alpha': 0.10010000000000001, 'lambda': 34.678887103846655, 'max_bin': 347}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:24:05,680]\u001b[0m Trial 402 finished with value: 0.6835447649187394 and parameters: {'n_estimators': 805, 'eta': 0.06329423695381728, 'max_depth': 9, 'alpha': 0.5593, 'lambda': 33.69894788218343, 'max_bin': 309}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:24:26,492]\u001b[0m Trial 403 finished with value: 0.6805244310001218 and parameters: {'n_estimators': 839, 'eta': 0.058240781043411426, 'max_depth': 9, 'alpha': 0.0358, 'lambda': 35.61772719316606, 'max_bin': 328}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:24:47,658]\u001b[0m Trial 404 finished with value: 0.6836115427201415 and parameters: {'n_estimators': 865, 'eta': 0.06130772066766125, 'max_depth': 9, 'alpha': 0.1368, 'lambda': 30.476968512551334, 'max_bin': 354}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:25:06,262]\u001b[0m Trial 405 finished with value: 0.6837264318752957 and parameters: {'n_estimators': 820, 'eta': 0.0705476519185674, 'max_depth': 9, 'alpha': 0.169, 'lambda': 32.33152244032397, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:25:27,612]\u001b[0m Trial 406 finished with value: 0.6856244825081754 and parameters: {'n_estimators': 781, 'eta': 0.06500450436227083, 'max_depth': 9, 'alpha': 0.0165, 'lambda': 36.834666026443166, 'max_bin': 333}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:25:35,089]\u001b[0m Trial 407 finished with value: 0.39849334613928555 and parameters: {'n_estimators': 263, 'eta': 0.012811754981409434, 'max_depth': 9, 'alpha': 0.0, 'lambda': 34.987282540190925, 'max_bin': 345}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:25:58,591]\u001b[0m Trial 408 finished with value: 0.6841169076833562 and parameters: {'n_estimators': 875, 'eta': 0.05940067935120464, 'max_depth': 9, 'alpha': 0.0644, 'lambda': 38.43549908394637, 'max_bin': 323}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:26:20,042]\u001b[0m Trial 409 finished with value: 0.6814094778981561 and parameters: {'n_estimators': 849, 'eta': 0.06208816099163104, 'max_depth': 9, 'alpha': 0.0791, 'lambda': 33.14771875309414, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:26:42,184]\u001b[0m Trial 410 finished with value: 0.6820132600678119 and parameters: {'n_estimators': 829, 'eta': 0.05208301955716944, 'max_depth': 9, 'alpha': 0.0475, 'lambda': 34.100137419098324, 'max_bin': 315}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:26:53,654]\u001b[0m Trial 411 finished with value: 0.6585678977123722 and parameters: {'n_estimators': 321, 'eta': 0.037835859133270304, 'max_depth': 9, 'alpha': 0.0926, 'lambda': 36.053423648462235, 'max_bin': 351}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:27:16,258]\u001b[0m Trial 412 finished with value: 0.6843201931342251 and parameters: {'n_estimators': 799, 'eta': 0.05670284370828602, 'max_depth': 9, 'alpha': 0.033100000000000004, 'lambda': 37.38510182300994, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:27:37,050]\u001b[0m Trial 413 finished with value: 0.6835266044691648 and parameters: {'n_estimators': 887, 'eta': 0.06855699472367385, 'max_depth': 9, 'alpha': 0.0668, 'lambda': 31.300147739945306, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:27:58,360]\u001b[0m Trial 414 finished with value: 0.6821205286218196 and parameters: {'n_estimators': 858, 'eta': 0.06374306790789515, 'max_depth': 9, 'alpha': 0.015700000000000002, 'lambda': 35.4794644983332, 'max_bin': 344}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:28:20,051]\u001b[0m Trial 415 finished with value: 0.6840150339562583 and parameters: {'n_estimators': 868, 'eta': 0.06018758400954769, 'max_depth': 9, 'alpha': 0.11370000000000001, 'lambda': 39.15299080056526, 'max_bin': 382}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:28:35,167]\u001b[0m Trial 416 finished with value: 0.6798230433481887 and parameters: {'n_estimators': 432, 'eta': 0.06645639008929734, 'max_depth': 9, 'alpha': 0.0497, 'lambda': 36.5851251646407, 'max_bin': 333}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:28:56,782]\u001b[0m Trial 417 finished with value: 0.682840630139095 and parameters: {'n_estimators': 840, 'eta': 0.05848641380994983, 'max_depth': 9, 'alpha': 0.031, 'lambda': 34.59155213693221, 'max_bin': 324}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:29:15,982]\u001b[0m Trial 418 finished with value: 0.6842589620059607 and parameters: {'n_estimators': 760, 'eta': 0.06290327435272332, 'max_depth': 9, 'alpha': 0.0886, 'lambda': 32.649493404061474, 'max_bin': 363}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:29:40,424]\u001b[0m Trial 419 finished with value: 0.6834596340936706 and parameters: {'n_estimators': 816, 'eta': 0.06027825040644315, 'max_depth': 10, 'alpha': 0.0644, 'lambda': 33.95918323573325, 'max_bin': 348}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:30:01,039]\u001b[0m Trial 420 finished with value: 0.6814360757485576 and parameters: {'n_estimators': 886, 'eta': 0.05540866227906287, 'max_depth': 9, 'alpha': 0.0181, 'lambda': 33.31220663113565, 'max_bin': 340}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:30:20,105]\u001b[0m Trial 421 finished with value: 0.6818017892101359 and parameters: {'n_estimators': 851, 'eta': 0.07282799941435192, 'max_depth': 9, 'alpha': 0.0434, 'lambda': 35.526337287249035, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:30:41,097]\u001b[0m Trial 422 finished with value: 0.6825562085560714 and parameters: {'n_estimators': 791, 'eta': 0.06483589847734557, 'max_depth': 8, 'alpha': 0.7451, 'lambda': 37.6699362145155, 'max_bin': 359}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:31:02,318]\u001b[0m Trial 423 finished with value: 0.68445456758584 and parameters: {'n_estimators': 871, 'eta': 0.062031003863219504, 'max_depth': 9, 'alpha': 0.0, 'lambda': 32.03631807422974, 'max_bin': 335}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:31:23,588]\u001b[0m Trial 424 finished with value: 0.6831599074631705 and parameters: {'n_estimators': 835, 'eta': 0.05785727800181825, 'max_depth': 9, 'alpha': 0.14800000000000002, 'lambda': 34.898146291234994, 'max_bin': 266}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:31:40,039]\u001b[0m Trial 425 finished with value: 0.6827002382518119 and parameters: {'n_estimators': 485, 'eta': 0.06736058616231957, 'max_depth': 9, 'alpha': 0.33290000000000003, 'lambda': 29.877439967222923, 'max_bin': 398}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:32:00,488]\u001b[0m Trial 426 finished with value: 0.680646832448296 and parameters: {'n_estimators': 815, 'eta': 0.060589086481163304, 'max_depth': 9, 'alpha': 0.0734, 'lambda': 36.49420855226844, 'max_bin': 344}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:32:20,514]\u001b[0m Trial 427 finished with value: 0.682832194755169 and parameters: {'n_estimators': 900, 'eta': 0.06372739408236297, 'max_depth': 9, 'alpha': 0.1057, 'lambda': 33.85076946557629, 'max_bin': 390}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:32:41,607]\u001b[0m Trial 428 finished with value: 0.6847288161471223 and parameters: {'n_estimators': 856, 'eta': 0.05658730725933126, 'max_depth': 9, 'alpha': 0.0555, 'lambda': 35.79362053998008, 'max_bin': 321}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 09:32:59,303]\u001b[0m Trial 429 finished with value: 0.6836996429420336 and parameters: {'n_estimators': 879, 'eta': 0.06584706438515878, 'max_depth': 9, 'alpha': 0.1264, 'lambda': 34.50586952575644, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:33:18,611]\u001b[0m Trial 430 finished with value: 0.6833729062953902 and parameters: {'n_estimators': 776, 'eta': 0.059228081147057354, 'max_depth': 9, 'alpha': 0.029300000000000003, 'lambda': 33.19292671003223, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:33:40,619]\u001b[0m Trial 431 finished with value: 0.6808897444896751 and parameters: {'n_estimators': 831, 'eta': 0.06182820707590314, 'max_depth': 9, 'alpha': 0.0857, 'lambda': 36.80986869343629, 'max_bin': 350}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:34:00,595]\u001b[0m Trial 432 finished with value: 0.6832587572809414 and parameters: {'n_estimators': 803, 'eta': 0.0713955317455658, 'max_depth': 9, 'alpha': 0.0183, 'lambda': 38.271076454419905, 'max_bin': 342}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:34:20,152]\u001b[0m Trial 433 finished with value: 0.6851231201199655 and parameters: {'n_estimators': 856, 'eta': 0.0692125510183632, 'max_depth': 9, 'alpha': 0.0471, 'lambda': 35.23511912063132, 'max_bin': 326}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:34:38,899]\u001b[0m Trial 434 finished with value: 0.6822789262474229 and parameters: {'n_estimators': 876, 'eta': 0.0637229667054709, 'max_depth': 9, 'alpha': 0.0621, 'lambda': 15.985419766725364, 'max_bin': 335}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:34:59,672]\u001b[0m Trial 435 finished with value: 0.682385499196042 and parameters: {'n_estimators': 727, 'eta': 0.05456345052620592, 'max_depth': 9, 'alpha': 0.1022, 'lambda': 32.14981487992974, 'max_bin': 317}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:35:17,815]\u001b[0m Trial 436 finished with value: 0.6826569379121443 and parameters: {'n_estimators': 751, 'eta': 0.06055455886366892, 'max_depth': 9, 'alpha': 0.0332, 'lambda': 11.963188728708879, 'max_bin': 346}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:35:37,566]\u001b[0m Trial 437 finished with value: 0.6824814785706341 and parameters: {'n_estimators': 843, 'eta': 0.05781865933967175, 'max_depth': 8, 'alpha': 0.07880000000000001, 'lambda': 33.930460368007594, 'max_bin': 355}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:35:55,074]\u001b[0m Trial 438 finished with value: 0.6820382112764485 and parameters: {'n_estimators': 825, 'eta': 0.08151138515975706, 'max_depth': 9, 'alpha': 0.0204, 'lambda': 36.08928745597145, 'max_bin': 339}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:36:15,247]\u001b[0m Trial 439 finished with value: 0.6840449615321686 and parameters: {'n_estimators': 793, 'eta': 0.06214532157299675, 'max_depth': 9, 'alpha': 0.0436, 'lambda': 31.21772383649685, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:36:37,879]\u001b[0m Trial 440 finished with value: 0.6818695509056188 and parameters: {'n_estimators': 882, 'eta': 0.06645354819485264, 'max_depth': 10, 'alpha': 0.07060000000000001, 'lambda': 32.713695352416316, 'max_bin': 333}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:36:58,171]\u001b[0m Trial 441 finished with value: 0.6822482046682674 and parameters: {'n_estimators': 868, 'eta': 0.07499164117113302, 'max_depth': 9, 'alpha': 0.0952, 'lambda': 37.38437664631753, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:37:19,007]\u001b[0m Trial 442 finished with value: 0.6815357321432349 and parameters: {'n_estimators': 813, 'eta': 0.05932608863315697, 'max_depth': 9, 'alpha': 0.0005, 'lambda': 34.79416268843661, 'max_bin': 304}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:37:39,703]\u001b[0m Trial 443 finished with value: 0.6817614413468173 and parameters: {'n_estimators': 849, 'eta': 0.06443099596099643, 'max_depth': 9, 'alpha': 0.12040000000000001, 'lambda': 33.61591255919915, 'max_bin': 349}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:37:59,148]\u001b[0m Trial 444 finished with value: 0.6807523736857064 and parameters: {'n_estimators': 898, 'eta': 0.05272293296460535, 'max_depth': 9, 'alpha': 0.049100000000000005, 'lambda': 23.942467072252633, 'max_bin': 323}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:38:18,024]\u001b[0m Trial 445 finished with value: 0.6808332100651322 and parameters: {'n_estimators': 768, 'eta': 0.06231221420704359, 'max_depth': 9, 'alpha': 0.0244, 'lambda': 35.47350821602599, 'max_bin': 336}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:38:39,481]\u001b[0m Trial 446 finished with value: 0.6832081892772931 and parameters: {'n_estimators': 833, 'eta': 0.05685805526968814, 'max_depth': 9, 'alpha': 0.0627, 'lambda': 34.42060713533699, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:39:01,902]\u001b[0m Trial 447 finished with value: 0.6856845120817447 and parameters: {'n_estimators': 862, 'eta': 0.05936392428060801, 'max_depth': 9, 'alpha': 0.0145, 'lambda': 39.71840160752724, 'max_bin': 313}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:39:22,327]\u001b[0m Trial 448 finished with value: 0.6835404984060076 and parameters: {'n_estimators': 885, 'eta': 0.06805313550051265, 'max_depth': 9, 'alpha': 0.08610000000000001, 'lambda': 36.63122687565058, 'max_bin': 344}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:39:42,043]\u001b[0m Trial 449 finished with value: 0.68360533091088 and parameters: {'n_estimators': 797, 'eta': 0.06530454311392997, 'max_depth': 9, 'alpha': 0.039900000000000005, 'lambda': 33.09016312567275, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7038\n",
      "\tBest params:\n",
      "\t\tn_estimators: 895\n",
      "\t\teta: 0.06642109429014469\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.0307\n",
      "\t\tlambda: 36.726715163896934\n",
      "\t\tmax_bin: 340\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683009    0.703984    0.706733    0.688618   \n",
      "1                    TP  385.000000  407.000000  407.000000  390.000000   \n",
      "2                    TN  373.000000  343.000000  332.000000  337.000000   \n",
      "3                    FP   71.000000   73.000000   91.000000   89.000000   \n",
      "4                    FN   70.000000   76.000000   69.000000   83.000000   \n",
      "5              Accuracy    0.843159    0.834260    0.822024    0.808676   \n",
      "6             Precision    0.844298    0.847917    0.817269    0.814196   \n",
      "7           Sensitivity    0.846154    0.842650    0.855042    0.824524   \n",
      "8           Specificity    0.840100    0.824500    0.784900    0.791100   \n",
      "9              F1 score    0.845225    0.845275    0.835729    0.819328   \n",
      "10  F1 score (weighted)    0.843157    0.834300    0.821659    0.808601   \n",
      "11     F1 score (macro)    0.843131    0.833416    0.820777    0.808009   \n",
      "12    Balanced Accuracy    0.843122    0.833585    0.819956    0.807802   \n",
      "13                  MCC    0.686264    0.666851    0.642550    0.616090   \n",
      "14                  NPV    0.842000    0.818600    0.827900    0.802400   \n",
      "15              ROC_AUC    0.843122    0.833585    0.819956    0.807802   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.660944    0.688256    0.705296    0.684340    0.692816  \n",
      "1   395.000000  404.000000  403.000000  400.000000  406.000000  \n",
      "2   348.000000  355.000000  349.000000  339.000000  340.000000  \n",
      "3    82.000000   68.000000   78.000000   77.000000   76.000000  \n",
      "4    74.000000   72.000000   69.000000   83.000000   77.000000  \n",
      "5     0.826474    0.844271    0.836485    0.822024    0.829811  \n",
      "6     0.828092    0.855932    0.837838    0.838574    0.842324  \n",
      "7     0.842217    0.848739    0.853814    0.828157    0.840580  \n",
      "8     0.809300    0.839200    0.817300    0.814900    0.817300  \n",
      "9     0.835095    0.852321    0.845750    0.833333    0.841451  \n",
      "10    0.826393    0.844309    0.836386    0.822105    0.829825  \n",
      "11    0.825998    0.843807    0.835893    0.821201    0.828889  \n",
      "12    0.825760    0.843991    0.835572    0.821531    0.828944  \n",
      "13    0.652128    0.687648    0.671954    0.642476    0.657779  \n",
      "14    0.824600    0.831400    0.834900    0.803300    0.815300  \n",
      "15    0.825760    0.843991    0.835572    0.821531    0.828944  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "y_pred_xgb_8_cat = np.where((y_pred_xgb_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 09:40:12,163]\u001b[0m Trial 450 finished with value: 0.6804493255651594 and parameters: {'n_estimators': 824, 'eta': 0.06097436636316132, 'max_depth': 9, 'alpha': 0.065, 'lambda': 35.75826080488813, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:40:34,701]\u001b[0m Trial 451 finished with value: 0.6853891257335288 and parameters: {'n_estimators': 785, 'eta': 0.06354039222721532, 'max_depth': 9, 'alpha': 0.129, 'lambda': 34.82827604003632, 'max_bin': 326}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:40:58,656]\u001b[0m Trial 452 finished with value: 0.681628600055835 and parameters: {'n_estimators': 847, 'eta': 0.05853978032530675, 'max_depth': 9, 'alpha': 0.0344, 'lambda': 38.488878424891425, 'max_bin': 405}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:41:26,666]\u001b[0m Trial 453 finished with value: 0.6838304410274121 and parameters: {'n_estimators': 869, 'eta': 0.044875425350775586, 'max_depth': 9, 'alpha': 0.6854, 'lambda': 37.60577847040351, 'max_bin': 350}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:41:46,022]\u001b[0m Trial 454 finished with value: 0.6818880734473298 and parameters: {'n_estimators': 806, 'eta': 0.0702643295869132, 'max_depth': 8, 'alpha': 0.1024, 'lambda': 31.812834150959276, 'max_bin': 319}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:41:59,698]\u001b[0m Trial 455 finished with value: 0.6785313068228228 and parameters: {'n_estimators': 887, 'eta': 0.06103817328071973, 'max_depth': 9, 'alpha': 0.0152, 'lambda': 4.537723712801434, 'max_bin': 336}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:42:25,083]\u001b[0m Trial 456 finished with value: 0.6805780580450509 and parameters: {'n_estimators': 858, 'eta': 0.05465560808189246, 'max_depth': 9, 'alpha': 0.2064, 'lambda': 36.26274928203076, 'max_bin': 368}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:42:49,529]\u001b[0m Trial 457 finished with value: 0.6830625805162772 and parameters: {'n_estimators': 899, 'eta': 0.06326600755152274, 'max_depth': 9, 'alpha': 0.0526, 'lambda': 34.46484965593502, 'max_bin': 345}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:43:14,632]\u001b[0m Trial 458 finished with value: 0.6837233267466392 and parameters: {'n_estimators': 830, 'eta': 0.06571263320786828, 'max_depth': 9, 'alpha': 0.1567, 'lambda': 33.63570722487281, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:43:28,512]\u001b[0m Trial 459 finished with value: 0.6525203371342296 and parameters: {'n_estimators': 393, 'eta': 0.02877624106249345, 'max_depth': 9, 'alpha': 0.0786, 'lambda': 32.653199230474684, 'max_bin': 339}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:43:56,089]\u001b[0m Trial 460 finished with value: 0.6812531376532579 and parameters: {'n_estimators': 839, 'eta': 0.05713106599792179, 'max_depth': 10, 'alpha': 0.0337, 'lambda': 36.957387031543846, 'max_bin': 355}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:44:20,539]\u001b[0m Trial 461 finished with value: 0.6812222290970068 and parameters: {'n_estimators': 869, 'eta': 0.06048737144242714, 'max_depth': 9, 'alpha': 0.058100000000000006, 'lambda': 35.278020485513245, 'max_bin': 324}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:44:43,041]\u001b[0m Trial 462 finished with value: 0.6814523470683129 and parameters: {'n_estimators': 779, 'eta': 0.06773743173291667, 'max_depth': 9, 'alpha': 0.10880000000000001, 'lambda': 34.108074608160486, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:45:07,801]\u001b[0m Trial 463 finished with value: 0.6808879888922393 and parameters: {'n_estimators': 817, 'eta': 0.06389913234270844, 'max_depth': 9, 'alpha': 0.0011, 'lambda': 35.663239407818864, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:45:29,049]\u001b[0m Trial 464 finished with value: 0.6830687417090207 and parameters: {'n_estimators': 853, 'eta': 0.07236179034197578, 'max_depth': 9, 'alpha': 0.0816, 'lambda': 33.23205418366444, 'max_bin': 344}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:45:48,296]\u001b[0m Trial 465 finished with value: 0.6823115534966233 and parameters: {'n_estimators': 900, 'eta': 0.06214314668441612, 'max_depth': 9, 'alpha': 0.0175, 'lambda': 13.922158154408187, 'max_bin': 350}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:46:11,809]\u001b[0m Trial 466 finished with value: 0.6801147637893967 and parameters: {'n_estimators': 762, 'eta': 0.05851114669933514, 'max_depth': 9, 'alpha': 0.0461, 'lambda': 38.90485682394489, 'max_bin': 337}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:46:37,217]\u001b[0m Trial 467 finished with value: 0.6839790137892231 and parameters: {'n_estimators': 875, 'eta': 0.056137671812506534, 'max_depth': 9, 'alpha': 0.07100000000000001, 'lambda': 27.08317712332802, 'max_bin': 333}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:47:04,237]\u001b[0m Trial 468 finished with value: 0.6782505232897658 and parameters: {'n_estimators': 802, 'eta': 0.05047152439401344, 'max_depth': 9, 'alpha': 0.0014, 'lambda': 36.3678931954914, 'max_bin': 326}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:47:27,837]\u001b[0m Trial 469 finished with value: 0.6790807978983311 and parameters: {'n_estimators': 846, 'eta': 0.06582676892222573, 'max_depth': 9, 'alpha': 0.0412, 'lambda': 37.90789518588786, 'max_bin': 343}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:47:51,321]\u001b[0m Trial 470 finished with value: 0.6836712529686537 and parameters: {'n_estimators': 744, 'eta': 0.060343254962491315, 'max_depth': 9, 'alpha': 0.1029, 'lambda': 34.942401660208105, 'max_bin': 319}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:48:16,759]\u001b[0m Trial 471 finished with value: 0.6870530934831938 and parameters: {'n_estimators': 817, 'eta': 0.06224084415903695, 'max_depth': 10, 'alpha': 0.0228, 'lambda': 34.026554186030836, 'max_bin': 348}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:48:40,657]\u001b[0m Trial 472 finished with value: 0.6810775667059719 and parameters: {'n_estimators': 882, 'eta': 0.06901073191118111, 'max_depth': 9, 'alpha': 0.08940000000000001, 'lambda': 32.09508708314565, 'max_bin': 329}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:49:03,677]\u001b[0m Trial 473 finished with value: 0.6795783958400364 and parameters: {'n_estimators': 860, 'eta': 0.058745999295276116, 'max_depth': 8, 'alpha': 0.0634, 'lambda': 31.007316256054654, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:49:26,591]\u001b[0m Trial 474 finished with value: 0.6824864940207328 and parameters: {'n_estimators': 788, 'eta': 0.0642427341738066, 'max_depth': 9, 'alpha': 0.1188, 'lambda': 37.213623378081515, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:49:52,487]\u001b[0m Trial 475 finished with value: 0.6828234924940039 and parameters: {'n_estimators': 846, 'eta': 0.06109233036951085, 'max_depth': 9, 'alpha': 0.031900000000000005, 'lambda': 35.88448535061811, 'max_bin': 359}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:50:12,790]\u001b[0m Trial 476 finished with value: 0.6804317086183005 and parameters: {'n_estimators': 828, 'eta': 0.06687628461784278, 'max_depth': 9, 'alpha': 0.0555, 'lambda': 34.98323714816295, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:50:39,610]\u001b[0m Trial 477 finished with value: 0.6829292259734416 and parameters: {'n_estimators': 886, 'eta': 0.05406185571732066, 'max_depth': 9, 'alpha': 0.1443, 'lambda': 32.6350161160354, 'max_bin': 312}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:51:06,132]\u001b[0m Trial 478 finished with value: 0.685383413968672 and parameters: {'n_estimators': 867, 'eta': 0.06278963452160402, 'max_depth': 9, 'alpha': 0.2922, 'lambda': 33.48901275457179, 'max_bin': 346}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 09:51:33,251]\u001b[0m Trial 479 finished with value: 0.6821504110854351 and parameters: {'n_estimators': 807, 'eta': 0.059448538149289874, 'max_depth': 9, 'alpha': 0.07680000000000001, 'lambda': 36.31816153255828, 'max_bin': 322}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:51:57,931]\u001b[0m Trial 480 finished with value: 0.6816320415757999 and parameters: {'n_estimators': 833, 'eta': 0.05659080636618061, 'max_depth': 9, 'alpha': 0.0161, 'lambda': 34.19189310103977, 'max_bin': 354}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:52:22,663]\u001b[0m Trial 481 finished with value: 0.6812515972651775 and parameters: {'n_estimators': 869, 'eta': 0.06502156203223272, 'max_depth': 9, 'alpha': 0.0419, 'lambda': 35.311079798933655, 'max_bin': 378}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:52:43,028]\u001b[0m Trial 482 finished with value: 0.6796682737898901 and parameters: {'n_estimators': 900, 'eta': 0.07397035445469961, 'max_depth': 9, 'alpha': 0.059000000000000004, 'lambda': 36.93879194920582, 'max_bin': 328}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:53:09,361]\u001b[0m Trial 483 finished with value: 0.6826792437963033 and parameters: {'n_estimators': 772, 'eta': 0.06151000568362538, 'max_depth': 9, 'alpha': 0.0881, 'lambda': 37.91564469416052, 'max_bin': 334}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:53:32,380]\u001b[0m Trial 484 finished with value: 0.6855966081577853 and parameters: {'n_estimators': 849, 'eta': 0.07776315104726454, 'max_depth': 9, 'alpha': 0.5263, 'lambda': 34.41395722628546, 'max_bin': 341}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:53:51,581]\u001b[0m Trial 485 finished with value: 0.6805314031138134 and parameters: {'n_estimators': 580, 'eta': 0.06350494933066146, 'max_depth': 9, 'alpha': 0.0329, 'lambda': 21.646049570491442, 'max_bin': 331}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:54:15,947]\u001b[0m Trial 486 finished with value: 0.684171541959879 and parameters: {'n_estimators': 817, 'eta': 0.057788603043792235, 'max_depth': 9, 'alpha': 0.921, 'lambda': 33.00101839300855, 'max_bin': 338}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:54:38,538]\u001b[0m Trial 487 finished with value: 0.6801748309793797 and parameters: {'n_estimators': 881, 'eta': 0.05992455593045334, 'max_depth': 9, 'alpha': 0.0025, 'lambda': 30.211165379550682, 'max_bin': 347}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:55:01,724]\u001b[0m Trial 488 finished with value: 0.6843439891358784 and parameters: {'n_estimators': 789, 'eta': 0.06698428287896366, 'max_depth': 10, 'alpha': 0.1003, 'lambda': 31.838777385793197, 'max_bin': 327}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:55:23,078]\u001b[0m Trial 489 finished with value: 0.683455240602507 and parameters: {'n_estimators': 839, 'eta': 0.06973218446626261, 'max_depth': 9, 'alpha': 0.13190000000000002, 'lambda': 35.81941585482911, 'max_bin': 471}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:55:48,287]\u001b[0m Trial 490 finished with value: 0.6842822081058898 and parameters: {'n_estimators': 859, 'eta': 0.06241997227785752, 'max_depth': 9, 'alpha': 0.051500000000000004, 'lambda': 34.803565868910525, 'max_bin': 335}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:56:12,010]\u001b[0m Trial 491 finished with value: 0.6816586180566939 and parameters: {'n_estimators': 825, 'eta': 0.05514093326824016, 'max_depth': 9, 'alpha': 0.0178, 'lambda': 33.747364891618574, 'max_bin': 343}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:56:34,355]\u001b[0m Trial 492 finished with value: 0.6789036745833315 and parameters: {'n_estimators': 880, 'eta': 0.06468078554451755, 'max_depth': 8, 'alpha': 0.07590000000000001, 'lambda': 36.62367517861097, 'max_bin': 321}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:56:59,966]\u001b[0m Trial 493 finished with value: 0.6806802460263526 and parameters: {'n_estimators': 804, 'eta': 0.05828676448177229, 'max_depth': 9, 'alpha': 0.2577, 'lambda': 38.861497932857574, 'max_bin': 352}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:57:23,070]\u001b[0m Trial 494 finished with value: 0.6856299464935253 and parameters: {'n_estimators': 860, 'eta': 0.041495408619528176, 'max_depth': 9, 'alpha': 0.0339, 'lambda': 7.110571649619889, 'max_bin': 339}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:57:43,735]\u001b[0m Trial 495 finished with value: 0.6793716465135896 and parameters: {'n_estimators': 609, 'eta': 0.06049542781440956, 'max_depth': 9, 'alpha': 0.0654, 'lambda': 35.259441972664334, 'max_bin': 332}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:58:10,376]\u001b[0m Trial 496 finished with value: 0.6806455077910701 and parameters: {'n_estimators': 840, 'eta': 0.05246892108609724, 'max_depth': 9, 'alpha': 0.0179, 'lambda': 32.94602054532521, 'max_bin': 344}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:58:35,459]\u001b[0m Trial 497 finished with value: 0.6846505800272039 and parameters: {'n_estimators': 752, 'eta': 0.06283844716599472, 'max_depth': 9, 'alpha': 0.1106, 'lambda': 37.404405678637396, 'max_bin': 315}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:58:55,661]\u001b[0m Trial 498 finished with value: 0.6796857143297496 and parameters: {'n_estimators': 900, 'eta': 0.06693778156563858, 'max_depth': 9, 'alpha': 0.0007, 'lambda': 34.25227206037919, 'max_bin': 308}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:59:11,214]\u001b[0m Trial 499 finished with value: 0.6758038078174037 and parameters: {'n_estimators': 460, 'eta': 0.05680068767646084, 'max_depth': 9, 'alpha': 0.049, 'lambda': 36.34244001544021, 'max_bin': 394}. Best is trial 184 with value: 0.7037693073445855.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7038\n",
      "\tBest params:\n",
      "\t\tn_estimators: 895\n",
      "\t\teta: 0.06642109429014469\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.0307\n",
      "\t\tlambda: 36.726715163896934\n",
      "\t\tmax_bin: 340\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683009    0.703984    0.706733    0.688618   \n",
      "1                    TP  385.000000  407.000000  407.000000  390.000000   \n",
      "2                    TN  373.000000  343.000000  332.000000  337.000000   \n",
      "3                    FP   71.000000   73.000000   91.000000   89.000000   \n",
      "4                    FN   70.000000   76.000000   69.000000   83.000000   \n",
      "5              Accuracy    0.843159    0.834260    0.822024    0.808676   \n",
      "6             Precision    0.844298    0.847917    0.817269    0.814196   \n",
      "7           Sensitivity    0.846154    0.842650    0.855042    0.824524   \n",
      "8           Specificity    0.840100    0.824500    0.784900    0.791100   \n",
      "9              F1 score    0.845225    0.845275    0.835729    0.819328   \n",
      "10  F1 score (weighted)    0.843157    0.834300    0.821659    0.808601   \n",
      "11     F1 score (macro)    0.843131    0.833416    0.820777    0.808009   \n",
      "12    Balanced Accuracy    0.843122    0.833585    0.819956    0.807802   \n",
      "13                  MCC    0.686264    0.666851    0.642550    0.616090   \n",
      "14                  NPV    0.842000    0.818600    0.827900    0.802400   \n",
      "15              ROC_AUC    0.843122    0.833585    0.819956    0.807802   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.660944    0.688256    0.705296    0.684340    0.692816    0.716655  \n",
      "1   395.000000  404.000000  403.000000  400.000000  406.000000  409.000000  \n",
      "2   348.000000  355.000000  349.000000  339.000000  340.000000  348.000000  \n",
      "3    82.000000   68.000000   78.000000   77.000000   76.000000   81.000000  \n",
      "4    74.000000   72.000000   69.000000   83.000000   77.000000   61.000000  \n",
      "5     0.826474    0.844271    0.836485    0.822024    0.829811    0.842047  \n",
      "6     0.828092    0.855932    0.837838    0.838574    0.842324    0.834694  \n",
      "7     0.842217    0.848739    0.853814    0.828157    0.840580    0.870213  \n",
      "8     0.809300    0.839200    0.817300    0.814900    0.817300    0.811200  \n",
      "9     0.835095    0.852321    0.845750    0.833333    0.841451    0.852083  \n",
      "10    0.826393    0.844309    0.836386    0.822105    0.829825    0.841807  \n",
      "11    0.825998    0.843807    0.835893    0.821201    0.828889    0.841316  \n",
      "12    0.825760    0.843991    0.835572    0.821531    0.828944    0.840701  \n",
      "13    0.652128    0.687648    0.671954    0.642476    0.657779    0.683472  \n",
      "14    0.824600    0.831400    0.834900    0.803300    0.815300    0.850900  \n",
      "15    0.825760    0.843991    0.835572    0.821531    0.828944    0.840701  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_9_cat = np.where((y_pred_xgb_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAEaCAYAAACW4MnmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA8xElEQVR4nO3deVhU5fs/8PcMw4CsiiMhsimQuGC5L8iioZWWmi2ahUtgGpqZYUZliguBpd80scyNtM3lg580NZMUARUX3BEFcUVQGHBDxGGY5/eHP+fjCMKgwMj4fl0X1+XZnnOfG+TmOeeZ80iEEAJERERGQGroAIiIiGoKixoRERkNFjUiIjIaLGpERGQ0WNSIiMhosKgREZHRYFEjIiKjwaJGBjFy5EgEBgZWuE0ikeCXX36p44ieTiEhIQgICKjVc0yfPh0eHh61eo6aIJPJEBsba+gw6DGxqBE9RGlpKWrz3QQqlarW2jaE+no99TVuqhiLGj3RRowYgb59+5Zb36tXL4wcORLA/3oCv/32G1q0aAFzc3MEBgbi7NmzOsds27YNPj4+aNCgAZo1a4ZRo0ahoKBAu/1e7/H777+Hm5sbzMzMcOvWLQQEBOC9997DZ599BoVCARsbG4SEhOD27ds6bQcEBMDOzg62trbw9/fHvn37dM4vkUiwYMECDBs2DLa2tnjnnXcAAF988QVatWoFCwsLODs7Y+zYsbh+/br2uNjYWMhkMuzYsQPe3t5o0KAB/P39kZOTg8TERLRv3x6WlpYIDAzEpUuX9L7m6dOnY9myZdi5cyckEgkkEom2p1JUVISPPvoIzZo1g4WFBdq3b4+4uDhtu+fOnYNEIsGvv/6Kfv36wdLSEp9//rle39N73681a9bA09MTFhYWGDRoEG7cuIG4uDi0bNkS1tbWeOONN3TycO/7M2/ePG1cr7/+OpRKpXYfIQS+/fZbtGjRAnK5HO7u7vjuu+90zu/m5oYvv/wSoaGhaNy4MXx8fODm5oaysjKMGjVKmwsAuHr1Kt599124uLigQYMGaNmyJebOnavzx869uH766Se4urrCxsYGAwcORH5+vs554+Pj4evrCwsLC+3PSFZWlnb7H3/8geeffx7m5uZwc3PDpEmTcOvWLe325ORk+Pj4wNraGtbW1njuueewdetWvXL+VBFEBjBixAjxwgsvVLgNgFi1apUQQojdu3cLiUQizpw5o91++vRpIZFIRHJyshBCiGnTpgkLCwvh4+Mj9u3bJ/bt2ye6dOki2rVrJzQajRBCiH///Vc0aNBALFiwQGRkZIh9+/aJgIAA4evrq91nxIgRwtraWgwaNEgcOnRIHD16VJSWlgp/f39hbW0tQkJCxIkTJ8SGDRtEkyZNxIcffqiNKS4uTqxZs0acOnVKHD9+XAQHB4tGjRoJpVKpc112dnZiwYIF4vTp0+LUqVNCCCFmzpwpEhMTxdmzZ0V8fLxo2bKlGD58uPa4FStWCIlEIvz9/UVKSopITU0VHh4eomfPnsLf31/s2bNHHDx4ULRs2VK89dZb2uOquuabN2+KYcOGie7du4vc3FyRm5sriouLhUajEQEBAcLf318kJSWJrKwssXjxYmFqairi4+OFEEKcPXtWABDNmjUTq1atEllZWTrfo/tNmzZNuLu76yxbWFiIfv36iSNHjoiEhAShUChEnz59xMsvvywOHz4sEhMThb29vfj00091fmasra3Fq6++Ko4ePSp27NghPDw8xKuvvqrdZ+HChcLc3FwsXrxYZGRkiB9++EGYmZmJpUuXavdxdXUV1tbWYtq0aeLUqVMiLS1N5OXlCRMTE/Hdd99pcyGEELm5uSIqKkqkpqaKM2fOiFWrVglLS0uxfPlynbhsbGzE0KFDxbFjx8SuXbuEi4uLzvdw27ZtQiqVio8++kgcPnxYpKeni6VLl4r09HTt97hhw4Zi5cqVIisrS+zcuVN4e3uLd999VwghhFqtFo0aNRIff/yxyMjIEBkZGSIuLk4kJiZWmPOnGYsaGcSIESOEiYmJsLS0LPd1f1ETQghvb2/xxRdfaJc/++wz0bp1a+3ytGnTBACRmZmpXXfq1CkBQGzbtk0IIYS/v7+YMmWKTgznz58XAMShQ4e0Mdna2oqbN2/q7Ofv7y9cXV2FWq3Wrlu8eLGQy+WiqKiowusrKysTDRs2FL/88ot2HQDx3nvvVZmbuLg4IZfLRVlZmRDi7i+8++MUQog5c+YIAOLAgQPadfPmzRONGzfWibuqaw4ODhb+/v46++zYsUOYmZmJa9eu6awfNWqUGDhwoBDif0VtxowZVV5PRUXNxMRE5Ofna9eFhoYKqVQq8vLytOsmTJggOnbsqF0eMWKEsLS01Ilr69atAoDIyMgQQgjh5OQkJk+erHP+iRMniubNm2uXXV1dRe/evcvFaWJiIlasWFHl9UyYMEEEBgbqxKVQKERJSYl23ddffy0cHBy0yz179hT9+/d/aJuurq7ihx9+0Fm3c+dOAUAUFhaKwsJCAUDs2LGjyviedrz9SAbTtWtXHD58uNzXg8aMGYMVK1agrKwMarUasbGxGD16tM4+TZo00RmM8Oyzz0KhUODEiRMAgP379+O7776DlZWV9qt169YAgMzMTO1xrVq1gpWVVbkYunTpAhMTE+2yj48PVCqV9vbR2bNnERQUBA8PD9jY2MDGxgbXr1/H+fPny7XzoLi4OPj5+cHR0RFWVlZ45513oFKpcPnyZe0+EokE3t7e2mUHBwcAQLt27XTWFRQUoKysrFrX/KD9+/dDpVKhWbNmOsf+8ssv5Y6r6Hr00axZMygUCp3YHRwc0KRJE511eXl5Ose1bt0atra22mUfHx8AQHp6Om7cuIHs7Gz4+fnpHOPv749z586huLi42nFrNBpERUXh+eefh0KhgJWVFX788cdy39dWrVrBzMxM5/quXLmiXU5NTa3wNjoA5Ofn4/z585g0aZJOvl9++WUAwOnTp9GoUSOEhITgxRdfxMsvv4yoqCicOnVKr2t42sgMHQA9vRo0aKDXqLigoCBMmTIFmzZtgkajwdWrVzF8+PAqjxP3PffQaDSYMmUKgoKCyu13r0AAgKWlpV6xiwcGkLzyyitQKBSIiYmBs7Mz5HI5evbsWW4QwoPt7927F2+++SbCw8PxzTffoFGjRkhJScGIESN0jpVKpTpF9d4zH1NT03Lr7sWm7zU/SKPRwNbWFvv37y+3TS6XV3o9+ro/buBu7BWt02g01W77Xh7uefB7Begf99y5c/H1119j3rx56NChA6ytrfF///d/2LRpk85+D+ZFIpGUO++Dcd1z7xrnz5+PXr16ldvu5OQEAFiyZAk++ugj/PPPP9i2bRumTp2KhQsXYsyYMXpdy9OCRY2eeDY2Nhg6dCiWLFkCjUaD119/HXZ2djr75OfnIysrC+7u7gCAjIwMFBQUoFWrVgCATp06IS0t7ZGHlu/fvx9lZWXawrJnzx7tQISCggKcOHECmzdvxosvvggAyM7OLtfLqEhycjIUCgVmzZqlXbdu3bpHivFB+lyzXC7X9uzuP+7atWsoKSlB27ZtaySWmnKvR2ZjYwMA2L17N4C7PSUbGxs4OTlh586d6N+/v/aYxMRENG/eHBYWFpW2XVEuEhMT8dJLLyE4OFi7rrJe7sN07NgRW7duxYcfflhu2zPPPANnZ2ecOnWq3B2IB7Vt2xZt27bFpEmTMHbsWPz0008sag/g7UeqF8aMGYMtW7Zg69ateP/998ttt7CwwKhRo5CamooDBw5gxIgR8Pb21n4WbsaMGfjzzz/x8ccf4/Dhw8jKysLff/+N4OBgnVGMD1NQUIBx48YhPT0dmzZtwtSpUzF69GhYWlqiUaNGaNKkCZYsWYKMjAzs2bMHb7/9Nho0aFBluy1btkR+fj6WLVuGM2fOYOXKlVi0aFH1E1QBfa65efPmOHnyJNLS0qBUKnHnzh307t0bgYGBGDx4MNavX48zZ84gNTUV33//PZYsWVIjsT0qiUSC4cOH4/jx40hMTMS4cePQv39/eHp6AgDCw8O1cWZmZmLx4sX44Ycf9BqZ2bx5c+zYsQM5OTnaEZUtW7ZEQkICduzYgYyMDHz55ZfYu3dvteOeOnUqtmzZgokTJ+Lo0aM4deoUYmNjtbcQZ8+ejQULFmDWrFk4fvw4Tp06hf/+97/agnX69GlMmTIFycnJOH/+PPbs2YOkpCTt7WT6HxY1qhc6d+4Mb29vuLu7w9/fv9z2pk2b4v3338frr7+uHcK+fv167S2fXr16Yfv27Th27Bh8fX3Rrl07fPzxx7C2ti5326sib7zxBqytrdGzZ08MHToU/fr1w5w5cwDcvTW4du1aZGVloV27dhg5ciQmTpyIpk2bVtnuK6+8gi+++AKff/45vL298ccff+Cbb76pZnYqps81BwcHo3PnzujRoweaNGmC33//HRKJBBs2bMDgwYMxadIkeHl5oX///ti0aZO2J2woXbp0Qc+ePdGnTx+8+OKLaNOmDVasWKHd/sEHH2DGjBmIjIxE69atER0djaioKJ2e1sPMnTsXqampaN68ufbZ3tSpU+Hv74+BAweie/fuuHr1KiZMmFDtuPv27YvNmzdj79696Nq1K7p06YKff/5Z+30ICgrCmjVrsGnTJnTp0gWdO3fG9OnT0axZMwB3b5dmZmZi6NChePbZZ/H666+jR48eWLhwYbVjMXYSUdENZ6InjFqthqurKyZNmoRPPvlEZ9v06dPxyy+/4PTp07Vy7oCAAHh4eGDp0qW10j7pZ+TIkcjOzkZ8fLyhQ6EnGJ+p0RNNo9EgLy8PixcvRlFREUJCQgwdEhE9wVjU6Il24cIFNG/eHE2bNsWKFSt0hnMTET2Itx+JiMhocKAIEREZDRY1IiIyGnym9gTIyckxdAhPLIVCofMWdiqPOaoac1S5+pgfR0fHCtezp0ZEREaDRY2IiIwGixoRERkNFjUiIjIaLGpERGQ0WNSIiMhosKgREZHRYFEjIiKjwQ9fPwFeWXbS0CEQEdWpv4K9aqVd9tSIiMhosKgREZHRYFEjIiKjwaJGRERGg0WNiIiMBosaEREZDRY1IiIyGixqRERkNFjUiIjIaLCoERGR0WBRewxxcXGGDoGIiO7DovYY1q9fb+gQiIjoPnyhsZ4SExOxZcsWqNVqeHp6okGDBlCpVJg8eTKcnZ0xYcIEzJkzBwUFBSgtLUW/fv0QGBho6LCJiJ4qLGp6yM7Oxu7duzFz5kzIZDIsXboULi4ukMvl+Oabb7T7hYaGwsrKCiqVCuHh4ejatSusra3LtRcfH4/4+HgAQFRUVJ1dBxHRk0KhUNRKuyxqejh+/DjOnj2L8PBwAIBKpYKNjU25/TZv3oz9+/cDAJRKJXJzcyssaoGBgezFEdFTTalUPtbxjo6OFa5nUdODEAL+/v4YNmyYzvqNGzdq/52WloZjx45h1qxZMDMzw/Tp01FaWlrXoRIRPdU4UEQP3t7eSElJwfXr1wEARUVFyM/Ph0wmg1qtBgAUFxfD0tISZmZmuHTpEjIzMw0ZMhHRU4k9NT04OTlh6NChmDVrFoQQMDExQXBwMF544QVMnjwZzZs3xwcffIBt27YhLCwMjo6O8PT0NHTYRERPHYkQQhg6iKddh5nbDR0CEVGd+ivY67GOf9gzNd5+JCIio8GiRkRERoNFjYiIjAaLGhERGQ0WNSIiMhosakREZDRY1IiIyGiwqBERkdHgh6+fADk5OYYO4YmlUCge+8Wnxo45qhpzVLn6mB9++JqIiIweixoRERkNFjUiIjIaLGpERGQ0WNSIiMhocD61J8Ary04aOgQdjzslBBGRobCnRkRERoNFjYiIjAaLGhERGQ0WNSIiMhosakREZDRY1IiIyGiwqBERkdFgUSMiIqPBokZEREaDRY2IiIwGixoRERmNel3Uzp07h4MHD2qXDxw4gP/+97810vamTZtw586dGmmLiIjqRr0vaocOHdIud+rUCYMGDaqRtjdv3lztoqbRaGrk3ERE9Gjq5C39eXl5+Prrr9GyZUtkZGTAzs4On376KeRyebl9L1++jGXLluHGjRswMzPDmDFj0KxZM+zZswfr1q2DVCqFhYUFpk6ditWrV0OlUuHkyZN47bXXoFKpkJWVheDgYMTExEAulyMnJwf5+fkIDQ1FQkICMjMz4eHhgXHjxgEAlixZgqysLKhUKnTr1g1vvfUWNm/ejMLCQkRERMDGxgbTpk1DcnIy1q9fDwBo37493n33XQBAUFAQXnnlFRw5cgTDhw9HamoqDhw4ABMTE7Rr1w7Dhw8vd43x8fGIj48HAERFRdVW2h+ZQqEwdAhaMpnsiYrnScQcVY05qpwx5afOpp7Jzc3FRx99hLFjx2LevHlISUmBn59fuf1++uknjB49Gk2bNkVmZiaWLl2KadOmYd26dfjiiy9gZ2eHW7duQSaTYciQIdoiBgAJCQk6bd26dQtfffUVDhw4gOjoaMycORNOTk4IDw/HuXPn4ObmhrfffhtWVlbQaDSYMWMGzp8/j379+mHTpk2YNm0abGxsUFhYiF9//RXR0dGwtLTErFmzsG/fPnTp0gV37tyBs7MzhgwZgqKiIvzwww/47rvvIJFIcOvWrQpzERgYiMDAwBrPcU1RKpWGDkFLoVA8UfE8iZijqjFHlauP+XF0dKxwfZ0VNXt7e7i5uQEAWrRogfz8/HL7lJSU4NSpU5g3b552nVqtBgC0bNkSMTEx6N69O7p27arXOTt27AiJRAIXFxfY2trCxcUFAODs7Iy8vDy4ublh9+7d+Pfff1FWVoarV68iOzsbrq6uOu1kZWWhTZs2sLGxAQD4+voiPT0dXbp0gVQqRbdu3QAADRo0gFwux48//ogOHTqgY8eO1UsSERE9ljoraqamptp/S6VSqFSqcvtoNBpYWlrim2++Kbft/fffR2ZmJg4ePIhPP/0Uc+bM0fucEolE5/wSiQQajQZ5eXnYuHEjvv76a1hZWSEmJgalpaXl2hFCVHoOqfTuo0kTExNERkbi2LFj2L17N/7++29MmzatyjiJiKhm6D1QpC4GQVhYWMDe3h579uwBcLeYnDt3DsDdZ22enp4YMmQIrK2tUVBQAHNzc9y+ffuRz1dcXAxzc3NYWFjg2rVrOHz4sHabubk5SkpKAACenp44ceIEbty4AY1Gg127dqF169bl2ispKUFxcTE6dOiAkSNHamMnIqK6oVdPTaPRICgoCLGxsTo9ntowYcIELFmyBHFxcVCr1fDx8YGbmxt++eUX5ObmAgDatm0LV1dXKBQK/Pnnn5g8eTJee+21ap/Lzc0Nbm5u+OSTT2Bvb4+WLVtqtwUGBiIyMhKNGjXCtGnTMGzYMERERAC4O1Ckc+fO5dq7ffs25syZg9LSUgghMGLEiEfMAhERPQqJqOze2n0mT56M8PBw2NnZ1XZMT50OM7cbOgQdfwV7GToErfr4ALuuMUdVY44qVx/z89gDRXr27Ino6Gi8/PLLaNy4MSQSiXZb27ZtHz9CIiKix6R3Ufvnn38AAGvXrtVZL5FIsHDhwmqfeOnSpTh16pTOun79+qFXr17VbouIiAioRlGLiYmp0ROHhITUaHtERETVek2WWq1Geno6du/eDeDuaL97IwSJiIgMTe+e2oULFxAdHQ1TU1MUFBSgR48eOHHiBHbu3ImPP/64NmMkIiLSi949tSVLlmDIkCH47rvvIJPdrYWtW7fGyZMnay04IiKi6tC7qGVnZ8PX11dnnbm5eYVvBiEiIjIEvW8/NmnSBGfOnIG7u7t23enTp+Hg4FArgT1NnqTPhRER1Wd6F7UhQ4YgKioKffr0gVqtxvr167Ft2zaMGTOmNuMjIiLSm963Hzt27Ijw8HDcuHEDrVu3Rn5+PsLCwvDcc8/VZnxERER607untmfPHnTv3h0tWrTQWZ+SkqKdeoWIiMiQ9O6p/fjjjxWuX7x4cY0FQ0RE9Diq7KlduXIFALTzj93//uMrV65ALpfXXnRERETVUGVRmzBhgvbfH374oc62hg0b4s0336z5qJ4yrywr/1k/jogkIqq+Kova6tWrAQDTpk3TzidGRET0JNL7mdq9gqZUKpGRkVFrARERET0qvUc/KpVKzJ8/H+fOnQMArFq1CikpKTh8+DDGjh1bW/ERERHpTe+e2k8//YT27dvj559/1r77sV27djh69GitBUdERFQdehe106dPY9CgQZBK/3eIhYUFiouLayUwIiKi6tK7qNna2uLy5cs667Kzs6FQKGo8KCIiokeh9zO1V199FdHR0Rg0aBA0Gg2Sk5Oxfv16DBo0qBbDIyIi0p/eRa13796wsrLCv//+i8aNG2Pnzp0YMmQIunTpUpvxERER6U3vogYAXbp0YREjIqInVrWKWnp6Os6ePYuSkhKd9YMHD67RoIiIiB6F3gNFli9fjnnz5iE9PR2XLl3SfuXk5NRmfHVu3LhxuHHjxmPvQ0REdU/vnlpSUhLmzp0LOzu72oyHiIjokeld1BQKBUxNTWszljo3Z84cFBQUoLS0FP369UNgYKB2W15eHiIjI+Hh4YFz586hadOmGD9+PMzMzAAAf//9N1JTU6FWqzFp0iQ0a9YMp0+fRmxsLFQqFeRyOUJDQ+Ho6GioyyMieuroXdTGjh2LxYsXw8fHB7a2tjrbWrduXeOB1YXQ0FBYWVlBpVIhPDwcXbt21dmek5ODsWPHwsvLC4sWLcLWrVsxYMAAAIC1tTWio6OxdetWbNy4EWPHjoWjoyMiIiJgYmKCo0eP4rfffkNYWFi588bHxyM+Ph4AEBUVVWFs/PzfXTKZjLmoAnNUNeaocsaUH72L2pkzZ3Do0CGkp6eXm0Pthx9+qPHA6sLmzZuxf/9+AHffbZmbm6uzvXHjxvDyujsFjJ+fHzZv3qwtavcKYIsWLbBv3z4AQHFxMWJiYrQfUi8rK6vwvIGBgTq9wooolcpHvCrjolAomIsqMEdVY44qVx/z87C7YHoXtd9//x1TpkxBu3btaiwoQ0pLS8OxY8cwa9YsmJmZYfr06SgtLdXZRyKRPHT53vsvpVKptnitXr0abdq0weTJk5GXl8epeoiI6pjeox/NzMzq7W3GihQXF8PS0hJmZma4dOkSMjMzy+1z/zQ7ycnJ2l5bZW3eG0iTkJBQ4zETEVHl9C5qQ4YMQWxsLK5duwaNRqPzVR89//zz0Gg0CAsLw+rVq+Hp6Vlun2bNmiEhIQFhYWEoKipC3759K21z4MCB+P333zF16tR6mxciovpMIoQQ+uw4ZMiQh267Nzu2McnLy0N0dDTmzp1b6+fqMHN7uXV/BVfeK3xa1Md7/XWNOaoac1S5+pifx36mtnDhwhoLhoiIqDboXdSaNGlSm3E8cezt7eukl0ZERDWnWu9+PHDgAE6cOFHuFVHjx4+v0aCIiIgehd4DRdauXYuffvoJGo0GKSkpsLKywpEjR2BhYVGb8REREelN757ajh078OWXX8LFxQUJCQkYOXIkevbsif/85z+1GR8REZHe9O6p3bp1Cy4uLgDufvBYrVbDw8MDJ06cqLXgiIiIqkPvnpqDgwMuXrwIZ2dnODs7459//oGVlRWsrKxqMz4iIiK96V3UhgwZgps3bwIA3nnnHcyfPx8lJSUICQmpteCIiIiqQ6+iptFoIJfL8eyzzwIAPDw88P3339dqYE8TftCaiKhm6PVMTSqVYs6cOdqX+BIRET2J9B4o0qpVK+3LfYmIiJ5E1XqjyNdff41OnTqhcePGOtOwVPZeSCIiorqid1FTqVTo3LkzAKCwsLDWAiIiInpUehe10NDQ2oyDiIjosVV75Mft27dx8+ZN3D9jzTPPPFOjQRERET0KvYtadnY2FixYgPPnz5fbZozzqRERUf2j9+jHpUuXok2bNli+fDksLCywYsUK9OnTB+PGjavN+IiIiPSmd1E7f/483nnnHVhaWkIIAQsLC7z77rvspRER0RND76JmamqKsrIyAIC1tTWUSiWEECgqKqq14IiIiKpD72dqXl5e2LNnDwICAtCtWzdERkbC1NQUbdq0qc34iIiI9KZ3UZs0aZL232+//TacnZ1RUlICPz+/WgmMiIiouqo9pP/eLUdfX1+dt4oQEREZmt5F7datW1i+fDlSUlKgVqshk8nQrVs3jBo1inOqERHRE0HvgSKLFi2CSqVCdHQ0Vq5ciejoaJSWlmLRokW1GR8REZHe9C5qaWlp+PDDD+Hk5AQzMzM4OTlh3LhxOHHiRG3GR0REpDe9i5qjoyPy8vJ01imVSjg6OtZ4UERERI9C72dqbdu2xezZs+Hr6wuFQgGlUomkpCT4+flh+/bt2v169+5dK4Ea0po1a2Bubo4BAwborC8sLMSKFSvwySefGCgyIiK6n95FLTMzEw4ODsjMzERmZiYAwMHBARkZGTqThxpjUXsYOzs7FjQioieIRNz/uv2HEEIgLy8PCoUCJiYmdRGXXvLy8hAZGQkvLy9kZmbC1dUVAQEBWLt2La5fv44JEyYAAGJjY6FSqSCXyxEaGgpHR0f89ddfuHDhAkJDQ3HhwgXMnz8fkZGRMDMzK3eeNWvW4MqVKygsLERBQQEGDBiAwMBA5OXlITo6GnPnzkVCQgIOHDiAO3fu4MqVK+jSpQvefffdCuOOj49HfHw8ACAqKgoqlar2klTPyWQyqNVqQ4fxRGOOqsYcVa4+5kcul1e4Xq+emkQiQVhYGH7++ecaDaomXL58GZMmTYKTkxPCw8ORnJyMGTNm4MCBA4iLi8P48eMREREBExMTHD16FL/99hvCwsLQr18/REREYN++fYiLi8Po0aMrLGj3XLhwAbNnz0ZJSQmmTJmCDh06lNvn3LlzmDNnDmQyGSZOnIiXXnoJCoWi3H6BgYEIDAzULiuVyppJhhG6d6ubHo45qhpzVLn6mJ+HjefQ+/ajm5sbcnNz0axZsxoLqibY29vDxcUFAODs7Axvb29IJBK4uLggPz8fxcXFiImJweXLlwFA+/5KqVSK0NBQhIWFoU+fPvDy8qr0PJ06dYJcLodcLkebNm1w+vRpuLm56ezTtm1bWFhYAACcnJygVCorLGpERFQ79C5qbdq0QWRkJPz9/cv9ojbkczRTU1PtvyUSiXZZIpFAo9Fg9erVaNOmDSZPnoy8vDxERERo98/NzYW5uTkKCwurPM+Db0+p6G0q98cilUq1BZSIiOqG3kP6T506BXt7e6SnpyMpKUnn60lWXFwMOzs7AEBCQoLO+tjYWERERKCoqAgpKSmVtrN//36oVCrcvHkTaWlpcHd3r82wiYjoEejdU5s2bVptxlFrBg4ciJiYGGzatElnRoHY2Fj07dsXjo6OGDt2LCIiItCqVSvY2tpW2I6HhweioqKgVCrx+uuvw87Ortzn9oiIyLD0Gv14z82bN3Ho0CFcu3YNAwYMQGFhIYQQaNy4cW3GaPRycnIMHcITqz4+wK5rzFHVmKPK1cf8PGygiN63H0+cOIGJEyciKSkJ69atA3B35OGSJUtqJkIiIqLHpPftx9jYWEycOBHe3t4YNWoUgLu35LKysmotuLq2Y8cObN68WWddy5YtERISYqCIiIioOvQuavn5+fD29tY9WCYzqhF+vXr1Qq9evQwdBhERPSK9bz86OTnh8OHDOuuOHTum/YwYERGRoendUwsKCkJ0dDTat28PlUqFn376CampqZg8eXJtxkdERKQ3vYvas88+i2+++QZJSUkwNzeHQqFAZGQkRz4SEdETQ++iBtx9K/2AAQNw8+ZNWFtbV/hWDSIiIkPRu6jdunULy5cvR0pKCtRqNWQyGbp164ZRo0bBysqqNmMkIiLSi94DRRYtWgSVSoXo6GisXLkS0dHRKC0txaJFi2ozPiIiIr3pXdTS0tLw4YcfwsnJCWZmZnBycsK4ceNw4sSJ2oyPiIhIb3oXNUdHx3LvOlQqlQ99VQkREVFd0/uZWtu2bTF79mz4+vpq3xOWlJQEPz8/bN++XbufIaehISKip5veRS0zMxMODg7IzMxEZmYmAMDBwQEZGRnIyMjQ7seiRkREhmL0U88QEdHTQ+9naj///DPOnTtXi6EQERE9Hr17amVlZZg9ezZsbGzg6+sLX19fvk2EiIieKNWaJFSj0eDQoUNISkrCwYMH4enpCT8/P3Tt2hXm5ua1GadR4yShD1cfJy+sa8xR1ZijytXH/Dxs5H21itr9Ll68iAULFuDChQuQy+Xw8fHBW2+9BTs7u8cK9GnEovZw9fE/W11jjqrGHFWuPubnYUWtWu9+LC4uRkpKCpKSknD+/Hl07doVwcHBUCgU+OuvvxAZGYlvv/22RgImIiKqLr2L2ty5c3H48GG0bt0affr0QefOnWFqaqrdPnz4cIwcObI2YiQiItKL3kXN09MTwcHBaNiwYYXbpVIplixZUlNxERERVVuVRe2rr77STjGTmppa4T4REREAADMzsxoMjYiIqHqqLGoPviFk2bJlCA4OrrWAiIiIHlWVRS0gIEBn+eeffy63joiI6Emg9xtF6ougoKAaaWfNmjXYsGFDlfvFxMQgJSWlRs5JRESPx+iKGhERPb2qvP14/PhxnWWNRlNuXdu2bWs2qhpQUlKCOXPm4NatW1Cr1Rg6dCg6d+6MvLw8REZGwsvLC5mZmXB1dUVAQADWrl2L69evY8KECfDw8AAAnD9/HhERESgoKMCAAQMQGBgIIQSWL1+O48ePw97eXuec69atQ2pqKlQqFZ599lm8//772kE2RERU+6osaj/88IPOspWVlc46iUSChQsX1nxkj8nU1BRhYWGwsLDAjRs38MUXX6BTp04AgMuXL2PSpElwcnJCeHg4kpOTMWPGDBw4cABxcXH49NNPAQAXLlzA7NmzUVJSgilTpqBDhw7IzMxETk4O5s6di2vXrmHSpEno1asXAOCll17CG2+8AQD4/vvvkZqaqj3n/eLj4xEfHw8AiIqKgkKhqIuU1EsymYz5qQJzVDXmqHLGlJ8qi1pMTExdxFHjhBD4/fffkZ6eDolEgsLCQly/fh0AYG9vDxcXFwCAs7MzvL29IZFI4OLigvz8fG0bnTp1glwuh1wuR5s2bXD69Gmkp6fDx8cHUqkUdnZ2Or3U48ePY8OGDbhz5w6Kiorg7OxcYVELDAxEYGCgdrm+vZ6mLtXH1/fUNeaoasxR5epjfmrkNVn1SXJyMm7cuIGoqCjIZDKMGzcOKpUKAHTehCKRSLTLEokEGo1GZ9v97i1XdEtRpVJh2bJl+Prrr6FQKLBmzRrt+YiIqG4Y7UCR4uJi2NraQiaT4fjx4zo9MH3t378fKpUKN2/eRFpaGtzd3dGqVSvs3r0bGo0GV69eRVpaGgCgtLQUAGBjY4OSkhLs3bu3Rq+HiIiqZrQ9tZ49eyI6OhqfffYZ3Nzc0KxZs2q34eHhgaioKCiVSrz++uuws7NDly5dcPz4cXzyySdo2rQpWrVqBQCwtLTECy+8gE8++QT29vZwd3ev6UsiIqIqPPLUM1RzOPXMw9XHe/11jTmqGnNUufqYn4c9UzPa249ERPT0YVEjIiKjwaJGRERGg0WNiIiMBosaEREZDRY1IiIyGixqRERkNFjUiIjIaLCoERGR0WBRIyIio8GiRkRERoNFjYiIjAaLGhERGQ0WNSIiMhosakREZDRY1IiIyGiwqBERkdFgUSMiIqPBokZEREaDRY2IiIwGixoRERkNFjUiIjIaLGpERGQ0WNSIiMhosKgREZHRYFEjIiKj8dQXtXHjxuHGjRuPdGxCQgIKCwtrpC0iInp8T31RexwJCQm4evWqocMgIqL/T2boAO7Jy8tDZGQkvLy8kJmZCVdXVwQEBGDt2rW4fv06JkyYAACIjY2FSqWCXC5HaGgoHB0d8ddff+HChQsIDQ3FhQsXMH/+fERGRsLMzKzceW7evIn58+fjxo0b8PDwgBBCuy0xMRFbtmyBWq2Gp6cnQkJCIJVKERQUhD59+iAtLQ2WlpaYOHEiTpw4gaysLCxYsAByuRyzZ88GAPz9999ITU2FWq3GpEmT0KxZs3IxxMfHIz4+HgAQFRUFhUJRGyk1CjKZjPmpAnNUNeaocsaUnyemqAHA5cuXMWnSJDg5OSE8PBzJycmYMWMGDhw4gLi4OIwfPx4REREwMTHB0aNH8dtvvyEsLAz9+vVDREQE9u3bh7i4OIwePbrCggYAa9euhZeXF9544w0cPHhQW1yys7Oxe/duzJw5EzKZDEuXLkVSUhL8/f1x584dNG/eHMOHD8e6deuwdu1aBAcH4++//0ZQUBDc3d217VtbWyM6Ohpbt27Fxo0bMXbs2HIxBAYGIjAwULusVCprOJPGQ6FQMD9VYI6qxhxVrj7mx9HRscL1T1RRs7e3h4uLCwDA2dkZ3t7ekEgkcHFxQX5+PoqLixETE4PLly8DAMrKygAAUqkUoaGhCAsLQ58+feDl5fXQc6SnpyMsLAwA0KFDB1haWgIAjh8/jrNnzyI8PBwAoFKpYGNjAwCQSCTo0aMHAMDX1xfffvvtQ9vv2rUrAKBFixbYt2/fI+eCiIiq74kqaqamptp/SyQS7bJEIoFGo8Hq1avRpk0bTJ48GXl5eYiIiNDun5ubC3Nzc52BGw8jkUjKrRNCwN/fH8OGDXuk4++Rye6mVCqVaosuERHVjXo1UKS4uBh2dnYA7g7SuH99bGwsIiIiUFRUhJSUlIe20apVKyQlJQEADh06hFu3bgEAvL29kZKSguvXrwMAioqKkJ+fD+BuwbvXZnJysrYnaG5ujtu3b9fsRRIR0SOrV0Vt4MCB+P333zF16lRoNBrt+tjYWPTt2xeOjo4YO3Ysfv31V21xetCbb76J9PR0TJkyBUeOHNE+HHVycsLQoUMxa9YshIWFYebMmdqRjWZmZrh48SKmTJmC48eP44033gAABAQEYMmSJZg8eTJUKlUtXz0REVVFIu4f/kcVCgoKwqpVq2qt/ZycnFpru76rjw+w6xpzVDXmqHL1MT8PGyhSr3pqRERElXmiBorUpB07dmDz5s0661q2bImQkJBqt1WbvTQiIqo5RlvUevXqhV69ehk6DCIiqkO8/UhEREaDRY2IiIwGixoRERkNFjUiIjIaLGpERGQ0WNSIiMhosKgREZHRYFEjIiKjwaJGRERGg0WNiIiMBosaEREZDRY1IiIyGixqRERkNFjUiIjIaHDmayIiMhrsqRnYZ599ZugQnmjMT9WYo6oxR5UzpvywqBERkdFgUSMiIqPBomZggYGBhg7hicb8VI05qhpzVDljyg8HihARkdFgT42IiIwGixoRERkNmaEDeBocPnwYK1asgEajwQsvvIBBgwbpbBdCYMWKFTh06BDMzMwQGhqKFi1aGCZYA6kqR0lJSfjzzz8BAObm5ggJCYGbm1vdB2pAVeXontOnT+OLL77Axx9/jG7dutVtkAakT37S0tIQGxuLsrIyWFtbIyIiou4DNaCqclRcXIwFCxagoKAAZWVlePXVV9GrVy/DBPuoBNWqsrIyMX78eHH58mVRWloqwsLCxMWLF3X2SU1NFbNnzxYajUacOnVKhIeHGyhaw9AnRydPnhQ3b94UQghx8OBB5qiCHN3bb/r06SIyMlLs2bPHAJEahj75KSoqEhMnThT5+flCCCGuXbtmiFANRp8c/ec//xGrVq0SQghx/fp1MXLkSFFaWmqIcB8Zbz/WstOnT8PBwQHPPPMMZDIZevTogf379+vsc+DAAfj5+UEikeDZZ5/FrVu3cPXqVQNFXPf0yVHLli1hZWUFAPD09ERBQYEhQjUYfXIEAFu2bEHXrl1hY2NjgCgNR5/8JCcno2vXrlAoFAAAW1tbQ4RqMPrkSCKRoKSkBEIIlJSUwMrKClJp/SoT9SvaeqiwsBCNGzfWLjdu3BiFhYXl9rn3H+1h+xgzfXJ0v+3bt6N9+/Z1EdoTQ9+fo3379qFv3751HZ7B6ZOf3NxcFBUVYfr06ZgyZQp27txZ12EalD45eumll3Dp0iWMGTMGn3zyCUaNGlXvihqfqdUyUcEnJiQSSbX3MWbVuf7jx49jx44dmDFjRm2H9UTRJ0exsbF455136t0voZqgT37Kyspw9uxZTJ06FSqVCl9++SU8PT3h6OhYV2EalD45OnLkCFxdXfHVV1/hypUrmDlzJry8vGBhYVFXYT42FrVa1rhxY51bZQUFBWjUqFG5fZRKZaX7GDN9cgQA58+fx+LFixEeHg5ra+u6DNHg9MlRVlYW5s+fDwC4ceMGDh06BKlUii5dutRprIag7/8za2trmJubw9zcHK1atcL58+efmqKmT4527NiBQYMGQSKRwMHBAfb29sjJyYGHh0ddh/vInr4/6eqYu7s7cnNzkZeXB7Vajd27d6NTp046+3Tq1AmJiYkQQiAjIwMWFhZPVVHTJ0dKpRLffvstxo8f/9T8ErqfPjmKiYnRfnXr1g0hISFPRUED9P9/dvLkSZSVleHOnTs4ffo0mjVrZqCI654+OVIoFDh27BgA4Nq1a8jJyYG9vb0hwn1kfKNIHTh48CB+/vlnaDQa9OrVC4MHD8Y///wDAOjbty+EEFi2bBmOHDkCuVyO0NBQuLu7GzjqulVVjn788Ufs3btX++zRxMQEUVFRhgy5zlWVo/vFxMSgY8eOT9WQfn3ys2HDBuzYsQNSqRS9e/dG//79DRlynasqR4WFhVi0aJF2oNrAgQPh5+dnyJCrjUWNiIiMBm8/EhGR0WBRIyIio8GiRkRERoNFjYiIjAaLGhERGQ0WNSIjsm/fPnzwwQcICgrC2bNn6+ScCQkJmDp16kO3R0ZGIiEhocbPW1vtPqq8vDy89dZbKCsrM3QoTzW+UYTqjXHjxmHMmDFo166doUPB9OnT4evrixdeeMHQoehYtWoV3nvvPXTu3LnG2kxNTcW6deuQnZ0NU1NTPP/883jnnXd03iNYmc8///yxY1izZg0uX76MCRMm1Gi7D5o4cSIGDBiA3r1766zfvHkzEhMTn7rPRtZH7KkRVYMQAhqNxtBhPFR+fj6cnZ0f6diKrislJQULFixAv379sGzZMsybNw8ymQxfffUVioqKHjfcJ46/vz8SExPLrU9MTIS/v78BIqLqYk+N6qWEhAT8+++/cHd3R0JCAqysrPDhhx8iNzcXq1evRmlpKd59910EBAQAuPuGDVNTU1y5cgWZmZlo3rw5xo8fjyZNmgAATp06hdjYWOTk5MDR0REjR45Ey5YtAdztlbVs2RInTpzAmTNn0LVrV6SnpyMzMxOxsbEICAhAcHAwVqxYgX379qG4uBgODg4YOXIkWrVqBeBuTyM7OxtyuRz79u2DQqHAuHHjtG+OUSqViI2NRXp6OoQQ8PHxQXBwMIC7sxJs3LgR165dg4eHB95//31t3PeUlpbivffeg0ajweTJk9GwYUN8//33yM7OxtKlS3Hu3DnY2dlh2LBh2lcjxcTEQC6XQ6lU4sSJE5g8ebJOL1gIgZUrV2Lw4MHw9fUFAMjlcowdOxaTJ0/Gpk2bMGTIEO3+y5cvx86dO9GoUSMEBwfD29tbm7/7e7WVXc/FixcRGxuLM2fOQCaT4eWXX0aLFi2wfv16AMD+/fvh4OCAb775Rtuun58fRo8ejRkzZsDFxQXA3XdffvDBB1i0aBFsbW2RmpqKP/74A/n5+XBycsLo0aPh6upa7ufKz88Pq1evRn5+vjam7OxsnD9/Hj4+Pjh48CD++OMPXLlyBRYWFujVqxfeeuutCn9GH7yz8GBvMyMjAytXrkR2djaaNGmCkSNHok2bNhX/wJP+6n4KN6JHExoaKo4cOSKEEGLHjh1iyJAhYvv27aKsrEz8/vvvYuzYsWLJkiVCpVKJw4cPi6CgIHH79m0hhBALFy4UQUFBIi0tTahUKrF8+XLx5ZdfCiGEuHnzphg5cqTYuXOnUKvVIikpSYwcOVLcuHFDCCHEtGnTxNixY8WFCxeEWq0WpaWlYtq0aSI+Pl4nvp07d4obN24ItVotNmzYIEJCQsSdO3eEEEKsXr1aDBs2TKSmpoqysjLx66+/is8//1wIcXfyxrCwMLFixQpx+/ZtcefOHZGeni6EEGLv3r1i/Pjx4uLFi0KtVot169aJL7744qE5evPNN0Vubq4QQojS0lIxfvx48Z///EeUlpaKY8eOiaCgIHHp0iVtToYPHy7S09NFWVmZNtZ7srOzxZtvvimuXLlS7jyrV6/Wxn/ve7Fx40ZRWloqdu3aJYYPH66d1PX+XFV2PcXFxWL06NFiw4YN4s6dO6K4uFhkZGRozzd//nydGO5vNyYmRvz222/abVu2bBGzZs0SQgiRlZUlgoODRUZGhigrKxM7duwQoaGhQqVSVZjDGTNmiHXr1mmXf/31VxEdHS2EEOL48ePi/PnzoqysTJw7d06EhISIvXv3CiGEuHLlinjzzTeFWq0WQuj+vD54DQUFBWLUqFHan4cjR46IUaNGievXr1cYE+mPtx+p3rK3t0evXr0glUrRo0cPFBQU4I033oCpqSmee+45yGQyXL58Wbt/hw4d0Lp1a5iamuLtt99GRkYGlEolDh48CAcHB/j5+cHExAQ9e/aEo6MjUlNTtccGBATA2dkZJiYmkMkqvsHh5+cHa2trmJiY4NVXX4VarUZOTo52u5eXFzp06ACpVAo/Pz+cO3cOwN3JGwsLCxEUFARzc3PI5XJ4eXkBAOLj4/Haa6/ByckJJiYmeO2113Du3Dnk5+dXmZ/MzEyUlJRg0KBBkMlkaNu2LTp06IDk5GTtPp07d4aXlxekUinkcrnO8Tdv3gQANGzYsFzbDRs21G4H7k642b9/f+3kk46Ojjh48GC54yq7ntTUVDRs2BCvvvoq5HI5GjRoAE9PzyqvEwB69uyJXbt2aZd37dqFnj17AgD+/fdfBAYGwtPTE1KpFAEBAZDJZMjMzKywrftvQWo0GiQlJWl7/G3atIGLiwukUilcXV3h4+ODEydO6BXj/RITE9G+fXvtz0O7du3g7u5eYc6oenj7keqt+2cuvvcL+f5fwHK5HCUlJdrl+wc2mJubw8rKClevXkVhYWG523lNmjTRmUBRn0ERGzduxPbt21FYWAiJRILbt2+X+8V/f2ylpaUoKyuDUqlEkyZNYGJiUq7N/Px8rFixAitXrtSuE0JUGPODrl69CoVCoTO/WnWu6970PteuXSv3pvZr167pTP9jZ2enMzfXg+fR53oKCgrwzDPPVHpND9O2bVuoVCpkZmaiYcOGOHfunHaGAqVSiZ07d+Lvv//W7q9Wqx86EW3Xrl2xbNkyZGRkQKVSQaVSoUOHDgDu/qHw22+/4cKFC1Cr1VCr1Y/00milUomUlBSdP5zKysp4+7EGsKjRU+P+uaRKSkpQVFSERo0awc7ODnv37tXZV6lU4vnnn9cuPziZ4oPL6enp+PPPP/HVV1/ByckJUqkUo0aNqnBixgcpFAoolUqUlZWVK2wKhULnmVZ1NGrUCEqlEhqNRlvYlEolmjZt+tDruJ+joyMaN26MPXv2YODAgdr1Go0Ge/fu1RlhWVhYCCGEtj2lUlluWpOqric/P1+nt3W/qibNlUql6N69O3bt2gVbW1t06NABDRo0AHC3cA8ePBiDBw+utI17zMzM0LVrVyQmJkKlUqFHjx7a3vmCBQvw4osvIjw8HHK5HLGxsbhx48ZD21GpVNrla9euaf/duHFj+Pr6YuzYsXrFRPrj7Ud6ahw6dAgnT56EWq3GH3/8AU9PTygUCrRv3x65ublITk5GWVkZdu/ejezsbO1f5xWxtbXFlStXtMu3b9+GiYkJbGxsoNFosG7dOhQXF+sVl4eHBxo1aoRff/0VJSUlUKlUOHnyJACgT58++O9//4uLFy8CAIqLi7Fnzx692vX09IS5uTk2bNgAtVqNtLQ0pKamwsfHR6/jJRIJgoKCEBcXh+TkZKhUKly7dg0//vgjiouLdaZtuX79OrZs2QK1Wo09e/bg0qVLaN++fbk2K7uejh074tq1a9i0aRNKS0tx+/Zt7S1CW1tb5OfnVzrytGfPnti9ezeSk5O1tx4B4IUXXsC2bduQmZkJIQRKSkpw8OBB3L59+6FtBQQEYPfu3di7d6/OqMfbt2/DysoKcrkcp0+f1rmV+yA3Nzfs2rULarUaWVlZOn84+fr6IjU1FYcPH4ZGo4FKpUJaWprOH170aNhTo6eGj48P1q5di4yMDLRo0UI7Cs3a2hqfffYZVqxYgSVLlsDBwQGfffYZbGxsHtpWv379EBMTg23btsHX1xcjR47E888/j48++ghmZmbo37+/du63qkilUkyZMgXLly9HaGgoJBIJfHx84OXlhS5duqCkpATfffcdlEolLCws4O3tje7du1fZrkwmw6effoqlS5di/fr1sLOzw/jx46s1MWaPHj1gamqKuLg4LF68GDKZDM899xxmzpypc/vR09MTubm5CA4ORsOGDTFp0qQKZyev7HoaNGiAL7/8ErGxsVi3bh1kMhn69+8PT09PdO/eHUlJSQgODoa9vT2io6PLte3p6QkzMzMUFhbqFFR3d3eMGTMGy5cvR25urvaZ5b2RqRVp1aoVLCwsYGpqqjPrc0hICFauXInly5ejdevW6N69O27dulVhG0OGDMH8+fMxatQotG7dGj4+PtqPQSgUCnz66af45ZdfMH/+fEilUnh4eGD06NFVf1OoUpxPjZ4KMTExaNy4MYYOHWroUJ4606ZNQ+/evfk5L6oTvP1IRLXmzp07uHLlSrmBJkS1hUWNiGrF9evX8f7776N169bajygQ1TbefiQiIqPBnhoRERkNFjUiIjIaLGpERGQ0WNSIiMhosKgREZHR+H/ZqmZXEhFgXwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.706836</td>\n",
       "      <td>0.036340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>197.800000</td>\n",
       "      <td>10.850499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>9.786612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>7.659417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>38.400000</td>\n",
       "      <td>7.121173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.829925</td>\n",
       "      <td>0.021386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.839276</td>\n",
       "      <td>0.028663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.837244</td>\n",
       "      <td>0.030658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.821640</td>\n",
       "      <td>0.035132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.837824</td>\n",
       "      <td>0.021768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.829858</td>\n",
       "      <td>0.021405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.829231</td>\n",
       "      <td>0.021475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.829441</td>\n",
       "      <td>0.021516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.659403</td>\n",
       "      <td>0.043094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.820650</td>\n",
       "      <td>0.028444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.829441</td>\n",
       "      <td>0.021516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.706836     0.036340\n",
       "1                    TP       197.800000    10.850499\n",
       "2                    TN       175.000000     9.786612\n",
       "3                    FP        38.000000     7.659417\n",
       "4                    FN        38.400000     7.121173\n",
       "5              Accuracy         0.829925     0.021386\n",
       "6             Precision         0.839276     0.028663\n",
       "7           Sensitivity         0.837244     0.030658\n",
       "8           Specificity         0.821640     0.035132\n",
       "9              F1 score         0.837824     0.021768\n",
       "10  F1 score (weighted)         0.829858     0.021405\n",
       "11     F1 score (macro)         0.829231     0.021475\n",
       "12    Balanced Accuracy         0.829441     0.021516\n",
       "13                  MCC         0.659403     0.043094\n",
       "14                  NPV         0.820650     0.028444\n",
       "15              ROC_AUC         0.829441     0.021516"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.683009</td>\n",
       "      <td>0.703984</td>\n",
       "      <td>0.706733</td>\n",
       "      <td>0.688618</td>\n",
       "      <td>0.660944</td>\n",
       "      <td>0.688256</td>\n",
       "      <td>0.705296</td>\n",
       "      <td>0.684340</td>\n",
       "      <td>0.692816</td>\n",
       "      <td>0.716655</td>\n",
       "      <td>0.693065</td>\n",
       "      <td>0.015861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>400.600000</td>\n",
       "      <td>8.071899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>346.400000</td>\n",
       "      <td>11.529672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>7.381659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>73.400000</td>\n",
       "      <td>6.752777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.843159</td>\n",
       "      <td>0.834260</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.808676</td>\n",
       "      <td>0.826474</td>\n",
       "      <td>0.844271</td>\n",
       "      <td>0.836485</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.829811</td>\n",
       "      <td>0.842047</td>\n",
       "      <td>0.830923</td>\n",
       "      <td>0.011392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.844298</td>\n",
       "      <td>0.847917</td>\n",
       "      <td>0.817269</td>\n",
       "      <td>0.814196</td>\n",
       "      <td>0.828092</td>\n",
       "      <td>0.855932</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.838574</td>\n",
       "      <td>0.842324</td>\n",
       "      <td>0.834694</td>\n",
       "      <td>0.836113</td>\n",
       "      <td>0.013117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.842650</td>\n",
       "      <td>0.855042</td>\n",
       "      <td>0.824524</td>\n",
       "      <td>0.842217</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.853814</td>\n",
       "      <td>0.828157</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.870213</td>\n",
       "      <td>0.845209</td>\n",
       "      <td>0.013186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.840100</td>\n",
       "      <td>0.824500</td>\n",
       "      <td>0.784900</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.839200</td>\n",
       "      <td>0.817300</td>\n",
       "      <td>0.814900</td>\n",
       "      <td>0.817300</td>\n",
       "      <td>0.811200</td>\n",
       "      <td>0.814980</td>\n",
       "      <td>0.017766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.845225</td>\n",
       "      <td>0.845275</td>\n",
       "      <td>0.835729</td>\n",
       "      <td>0.819328</td>\n",
       "      <td>0.835095</td>\n",
       "      <td>0.852321</td>\n",
       "      <td>0.845750</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.841451</td>\n",
       "      <td>0.852083</td>\n",
       "      <td>0.840559</td>\n",
       "      <td>0.009995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.843157</td>\n",
       "      <td>0.834300</td>\n",
       "      <td>0.821659</td>\n",
       "      <td>0.808601</td>\n",
       "      <td>0.826393</td>\n",
       "      <td>0.844309</td>\n",
       "      <td>0.836386</td>\n",
       "      <td>0.822105</td>\n",
       "      <td>0.829825</td>\n",
       "      <td>0.841807</td>\n",
       "      <td>0.830854</td>\n",
       "      <td>0.011412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.843131</td>\n",
       "      <td>0.833416</td>\n",
       "      <td>0.820777</td>\n",
       "      <td>0.808009</td>\n",
       "      <td>0.825998</td>\n",
       "      <td>0.843807</td>\n",
       "      <td>0.835893</td>\n",
       "      <td>0.821201</td>\n",
       "      <td>0.828889</td>\n",
       "      <td>0.841316</td>\n",
       "      <td>0.830244</td>\n",
       "      <td>0.011548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.843122</td>\n",
       "      <td>0.833585</td>\n",
       "      <td>0.819956</td>\n",
       "      <td>0.807802</td>\n",
       "      <td>0.825760</td>\n",
       "      <td>0.843991</td>\n",
       "      <td>0.835572</td>\n",
       "      <td>0.821531</td>\n",
       "      <td>0.828944</td>\n",
       "      <td>0.840701</td>\n",
       "      <td>0.830096</td>\n",
       "      <td>0.011598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.686264</td>\n",
       "      <td>0.666851</td>\n",
       "      <td>0.642550</td>\n",
       "      <td>0.616090</td>\n",
       "      <td>0.652128</td>\n",
       "      <td>0.687648</td>\n",
       "      <td>0.671954</td>\n",
       "      <td>0.642476</td>\n",
       "      <td>0.657779</td>\n",
       "      <td>0.683472</td>\n",
       "      <td>0.660721</td>\n",
       "      <td>0.023085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.827900</td>\n",
       "      <td>0.802400</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.831400</td>\n",
       "      <td>0.834900</td>\n",
       "      <td>0.803300</td>\n",
       "      <td>0.815300</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>0.825130</td>\n",
       "      <td>0.015719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.843122</td>\n",
       "      <td>0.833585</td>\n",
       "      <td>0.819956</td>\n",
       "      <td>0.807802</td>\n",
       "      <td>0.825760</td>\n",
       "      <td>0.843991</td>\n",
       "      <td>0.835572</td>\n",
       "      <td>0.821531</td>\n",
       "      <td>0.828944</td>\n",
       "      <td>0.840701</td>\n",
       "      <td>0.830096</td>\n",
       "      <td>0.011598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.683009    0.703984    0.706733    0.688618   \n",
       "1                    TP  385.000000  407.000000  407.000000  390.000000   \n",
       "2                    TN  373.000000  343.000000  332.000000  337.000000   \n",
       "3                    FP   71.000000   73.000000   91.000000   89.000000   \n",
       "4                    FN   70.000000   76.000000   69.000000   83.000000   \n",
       "5              Accuracy    0.843159    0.834260    0.822024    0.808676   \n",
       "6             Precision    0.844298    0.847917    0.817269    0.814196   \n",
       "7           Sensitivity    0.846154    0.842650    0.855042    0.824524   \n",
       "8           Specificity    0.840100    0.824500    0.784900    0.791100   \n",
       "9              F1 score    0.845225    0.845275    0.835729    0.819328   \n",
       "10  F1 score (weighted)    0.843157    0.834300    0.821659    0.808601   \n",
       "11     F1 score (macro)    0.843131    0.833416    0.820777    0.808009   \n",
       "12    Balanced Accuracy    0.843122    0.833585    0.819956    0.807802   \n",
       "13                  MCC    0.686264    0.666851    0.642550    0.616090   \n",
       "14                  NPV    0.842000    0.818600    0.827900    0.802400   \n",
       "15              ROC_AUC    0.843122    0.833585    0.819956    0.807802   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.660944    0.688256    0.705296    0.684340    0.692816    0.716655   \n",
       "1   395.000000  404.000000  403.000000  400.000000  406.000000  409.000000   \n",
       "2   348.000000  355.000000  349.000000  339.000000  340.000000  348.000000   \n",
       "3    82.000000   68.000000   78.000000   77.000000   76.000000   81.000000   \n",
       "4    74.000000   72.000000   69.000000   83.000000   77.000000   61.000000   \n",
       "5     0.826474    0.844271    0.836485    0.822024    0.829811    0.842047   \n",
       "6     0.828092    0.855932    0.837838    0.838574    0.842324    0.834694   \n",
       "7     0.842217    0.848739    0.853814    0.828157    0.840580    0.870213   \n",
       "8     0.809300    0.839200    0.817300    0.814900    0.817300    0.811200   \n",
       "9     0.835095    0.852321    0.845750    0.833333    0.841451    0.852083   \n",
       "10    0.826393    0.844309    0.836386    0.822105    0.829825    0.841807   \n",
       "11    0.825998    0.843807    0.835893    0.821201    0.828889    0.841316   \n",
       "12    0.825760    0.843991    0.835572    0.821531    0.828944    0.840701   \n",
       "13    0.652128    0.687648    0.671954    0.642476    0.657779    0.683472   \n",
       "14    0.824600    0.831400    0.834900    0.803300    0.815300    0.850900   \n",
       "15    0.825760    0.843991    0.835572    0.821531    0.828944    0.840701   \n",
       "\n",
       "           ave        std  \n",
       "0     0.693065   0.015861  \n",
       "1   400.600000   8.071899  \n",
       "2   346.400000  11.529672  \n",
       "3    78.600000   7.381659  \n",
       "4    73.400000   6.752777  \n",
       "5     0.830923   0.011392  \n",
       "6     0.836113   0.013117  \n",
       "7     0.845209   0.013186  \n",
       "8     0.814980   0.017766  \n",
       "9     0.840559   0.009995  \n",
       "10    0.830854   0.011412  \n",
       "11    0.830244   0.011548  \n",
       "12    0.830096   0.011598  \n",
       "13    0.660721   0.023085  \n",
       "14    0.825130   0.015719  \n",
       "15    0.830096   0.011598  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_xgb0</th>\n",
       "      <th>y_pred_xgb1</th>\n",
       "      <th>y_pred_xgb2</th>\n",
       "      <th>y_pred_xgb3</th>\n",
       "      <th>y_pred_xgb4</th>\n",
       "      <th>y_pred_xgb_ave</th>\n",
       "      <th>y_pred_xgb_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4286867</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>6.965282</td>\n",
       "      <td>7.050974</td>\n",
       "      <td>6.739079</td>\n",
       "      <td>7.017837</td>\n",
       "      <td>6.849146</td>\n",
       "      <td>6.978720</td>\n",
       "      <td>0.160593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3689853</td>\n",
       "      <td>1</td>\n",
       "      <td>6.43</td>\n",
       "      <td>6.434065</td>\n",
       "      <td>6.476816</td>\n",
       "      <td>6.510091</td>\n",
       "      <td>6.577776</td>\n",
       "      <td>6.688014</td>\n",
       "      <td>6.519460</td>\n",
       "      <td>0.090381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3827056</td>\n",
       "      <td>2</td>\n",
       "      <td>7.52</td>\n",
       "      <td>8.618092</td>\n",
       "      <td>8.603618</td>\n",
       "      <td>8.515635</td>\n",
       "      <td>8.471696</td>\n",
       "      <td>8.670535</td>\n",
       "      <td>8.399929</td>\n",
       "      <td>0.398983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3689883</td>\n",
       "      <td>3</td>\n",
       "      <td>7.70</td>\n",
       "      <td>7.431558</td>\n",
       "      <td>7.452407</td>\n",
       "      <td>7.480423</td>\n",
       "      <td>7.313440</td>\n",
       "      <td>7.454924</td>\n",
       "      <td>7.472125</td>\n",
       "      <td>0.115114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2023528</td>\n",
       "      <td>4</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.408311</td>\n",
       "      <td>7.459652</td>\n",
       "      <td>7.614225</td>\n",
       "      <td>7.310424</td>\n",
       "      <td>7.139741</td>\n",
       "      <td>7.367059</td>\n",
       "      <td>0.150370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4464975</td>\n",
       "      <td>4487</td>\n",
       "      <td>4.72</td>\n",
       "      <td>5.115430</td>\n",
       "      <td>5.127100</td>\n",
       "      <td>5.109037</td>\n",
       "      <td>5.086132</td>\n",
       "      <td>5.153950</td>\n",
       "      <td>5.051942</td>\n",
       "      <td>0.149833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL95747</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.654265</td>\n",
       "      <td>6.944799</td>\n",
       "      <td>6.658501</td>\n",
       "      <td>6.720073</td>\n",
       "      <td>6.823662</td>\n",
       "      <td>6.900217</td>\n",
       "      <td>0.328866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL4072618</td>\n",
       "      <td>4489</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.192938</td>\n",
       "      <td>5.047009</td>\n",
       "      <td>5.023721</td>\n",
       "      <td>5.078069</td>\n",
       "      <td>5.008158</td>\n",
       "      <td>5.089983</td>\n",
       "      <td>0.074920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL2408692</td>\n",
       "      <td>4490</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.704311</td>\n",
       "      <td>6.439009</td>\n",
       "      <td>6.437019</td>\n",
       "      <td>6.646519</td>\n",
       "      <td>6.302081</td>\n",
       "      <td>6.481490</td>\n",
       "      <td>0.145839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4226829</td>\n",
       "      <td>4491</td>\n",
       "      <td>5.55</td>\n",
       "      <td>5.687522</td>\n",
       "      <td>5.752780</td>\n",
       "      <td>5.632416</td>\n",
       "      <td>5.713874</td>\n",
       "      <td>5.747449</td>\n",
       "      <td>5.680674</td>\n",
       "      <td>0.070960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_xgb0  y_pred_xgb1  \\\n",
       "0         CHEMBL4286867            0     7.25     6.965282     7.050974   \n",
       "1         CHEMBL3689853            1     6.43     6.434065     6.476816   \n",
       "2         CHEMBL3827056            2     7.52     8.618092     8.603618   \n",
       "3         CHEMBL3689883            3     7.70     7.431558     7.452407   \n",
       "4         CHEMBL2023528            4     7.27     7.408311     7.459652   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "4487      CHEMBL4464975         4487     4.72     5.115430     5.127100   \n",
       "4488        CHEMBL95747         4488     7.60     6.654265     6.944799   \n",
       "4489      CHEMBL4072618         4489     5.19     5.192938     5.047009   \n",
       "4490      CHEMBL2408692         4490     6.36     6.704311     6.439009   \n",
       "4491      CHEMBL4226829         4491     5.55     5.687522     5.752780   \n",
       "\n",
       "      y_pred_xgb2  y_pred_xgb3  y_pred_xgb4  y_pred_xgb_ave  y_pred_xgb_std  \n",
       "0        6.739079     7.017837     6.849146        6.978720        0.160593  \n",
       "1        6.510091     6.577776     6.688014        6.519460        0.090381  \n",
       "2        8.515635     8.471696     8.670535        8.399929        0.398983  \n",
       "3        7.480423     7.313440     7.454924        7.472125        0.115114  \n",
       "4        7.614225     7.310424     7.139741        7.367059        0.150370  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "4487     5.109037     5.086132     5.153950        5.051942        0.149833  \n",
       "4488     6.658501     6.720073     6.823662        6.900217        0.328866  \n",
       "4489     5.023721     5.078069     5.008158        5.089983        0.074920  \n",
       "4490     6.437019     6.646519     6.302081        6.481490        0.145839  \n",
       "4491     5.632416     5.713874     5.747449        5.680674        0.070960  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where((y_pred_optimized_xgb >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id, xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "xgb_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2883ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_xgb.to_csv('mat_met_xgb_opt.csv')\n",
    "xgb_5preds.to_csv('xgb_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6581 with a standard deviation of 0.0312\n",
      "XGBoost optimized model r2_score 0.7100 with a standard deviation of 0.0288\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_xgb.joblib']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"./xgb_reg.joblib\")\n",
    "#joblib.dump(optimized_xgb, \"./optimized_xgb.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"./optimizedCV_xgb.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.653358     0.045847\n",
      "1                    TP       192.900000    11.357328\n",
      "2                    TN       169.900000     7.370361\n",
      "3                    FP        43.100000     7.125073\n",
      "4                    FN        43.300000     6.219146\n",
      "5              Accuracy         0.807660     0.019840\n",
      "6             Precision         0.817391     0.028669\n",
      "7           Sensitivity         0.816297     0.028188\n",
      "8           Specificity         0.798010     0.030224\n",
      "9              F1 score         0.816515     0.022641\n",
      "10  F1 score (weighted)         0.807609     0.019891\n",
      "11     F1 score (macro)         0.806939     0.019572\n",
      "12    Balanced Accuracy         0.807153     0.019665\n",
      "13                  MCC         0.614594     0.039061\n",
      "14                  NPV         0.797500     0.021675\n",
      "15              ROC_AUC         0.807153     0.019665\n",
      "CPU times: user 21.9 s, sys: 0 ns, total: 21.9 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=16)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=16)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:09:30,163]\u001b[0m A new study created in memory with name: KNNregressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:09:32,960]\u001b[0m Trial 0 finished with value: 0.6060364174308477 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 0 with value: 0.6060364174308477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:09:36,447]\u001b[0m Trial 1 finished with value: 0.5286910589146528 and parameters: {'n_neighbors': 23, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 0 with value: 0.6060364174308477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:09:39,237]\u001b[0m Trial 2 finished with value: 0.5650940394849202 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 0 with value: 0.6060364174308477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:09:42,089]\u001b[0m Trial 3 finished with value: 0.5495079088415179 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 0 with value: 0.6060364174308477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:09:44,958]\u001b[0m Trial 4 finished with value: 0.6396342886926973 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:09:47,895]\u001b[0m Trial 5 finished with value: 0.5330272365006085 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 58}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:09:51,431]\u001b[0m Trial 6 finished with value: 0.5841899515562936 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:09:54,251]\u001b[0m Trial 7 finished with value: 0.6009612896804131 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:09:57,170]\u001b[0m Trial 8 finished with value: 0.605694157866137 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:00,038]\u001b[0m Trial 9 finished with value: 0.5744617997998669 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 62}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:02,889]\u001b[0m Trial 10 finished with value: 0.6381540332490288 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:05,767]\u001b[0m Trial 11 finished with value: 0.6307976739967204 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:08,647]\u001b[0m Trial 12 finished with value: 0.6234726934364465 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:11,560]\u001b[0m Trial 13 finished with value: 0.6083228588304382 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:15,087]\u001b[0m Trial 14 finished with value: 0.6188948601637406 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:17,885]\u001b[0m Trial 15 finished with value: 0.6381540332490288 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:22,322]\u001b[0m Trial 16 finished with value: 0.6154998362458338 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:25,210]\u001b[0m Trial 17 finished with value: 0.6168346656914638 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:28,056]\u001b[0m Trial 18 finished with value: 0.6268963574348436 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:30,942]\u001b[0m Trial 19 finished with value: 0.6168346656914638 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 53}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:33,828]\u001b[0m Trial 20 finished with value: 0.6190735289902232 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:36,680]\u001b[0m Trial 21 finished with value: 0.6381540332490288 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:39,608]\u001b[0m Trial 22 finished with value: 0.6381540332490288 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:42,543]\u001b[0m Trial 23 finished with value: 0.6211423065378037 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:45,424]\u001b[0m Trial 24 finished with value: 0.6162209220844153 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:48,307]\u001b[0m Trial 25 finished with value: 0.6234726934364465 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:51,219]\u001b[0m Trial 26 finished with value: 0.6381540332490288 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:54,101]\u001b[0m Trial 27 finished with value: 0.6083228588304382 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 4 with value: 0.6396342886926973.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:56,994]\u001b[0m Trial 28 finished with value: 0.6421543709238988 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:59,861]\u001b[0m Trial 29 finished with value: 0.6021327304865128 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:02,743]\u001b[0m Trial 30 finished with value: 0.6060364174308477 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:05,488]\u001b[0m Trial 31 finished with value: 0.6415702211230899 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:08,324]\u001b[0m Trial 32 finished with value: 0.6421543709238988 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:11,218]\u001b[0m Trial 33 finished with value: 0.6421543709238988 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:14,116]\u001b[0m Trial 34 finished with value: 0.6421543709238988 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:11:16,921]\u001b[0m Trial 35 finished with value: 0.6356223137743446 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:19,862]\u001b[0m Trial 36 finished with value: 0.5852760847866473 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:22,775]\u001b[0m Trial 37 finished with value: 0.6360963379558026 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:25,673]\u001b[0m Trial 38 finished with value: 0.6421543709238988 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:28,616]\u001b[0m Trial 39 finished with value: 0.5415387147615924 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:31,504]\u001b[0m Trial 40 finished with value: 0.6136175349556992 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:34,383]\u001b[0m Trial 41 finished with value: 0.6421543709238988 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:37,232]\u001b[0m Trial 42 finished with value: 0.6415702211230899 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 28 with value: 0.6421543709238988.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:40,114]\u001b[0m Trial 43 finished with value: 0.6452309710436269 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 43 with value: 0.6452309710436269.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:43,050]\u001b[0m Trial 44 finished with value: 0.6404348017834206 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 43 with value: 0.6452309710436269.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:46,560]\u001b[0m Trial 45 finished with value: 0.6408774224112961 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 46}. Best is trial 43 with value: 0.6452309710436269.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:49,427]\u001b[0m Trial 46 finished with value: 0.6452309710436269 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 43 with value: 0.6452309710436269.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:52,305]\u001b[0m Trial 47 finished with value: 0.6452309710436269 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 43 with value: 0.6452309710436269.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:55,134]\u001b[0m Trial 48 finished with value: 0.5934213671886563 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 43 with value: 0.6452309710436269.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:11:57,996]\u001b[0m Trial 49 finished with value: 0.6452309710436269 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 43 with value: 0.6452309710436269.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6452\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 67\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.633858\n",
      "1                    TP  384.000000\n",
      "2                    TN  353.000000\n",
      "3                    FP   91.000000\n",
      "4                    FN   71.000000\n",
      "5              Accuracy    0.819800\n",
      "6             Precision    0.808421\n",
      "7           Sensitivity    0.843956\n",
      "8           Specificity    0.795000\n",
      "9              F1 score    0.825806\n",
      "10  F1 score (weighted)    0.819661\n",
      "11     F1 score (macro)    0.819585\n",
      "12    Balanced Accuracy    0.819501\n",
      "13                  MCC    0.639984\n",
      "14                  NPV    0.832500\n",
      "15              ROC_AUC    0.819501\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_0_cat = np.where((y_pred_knn_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:12:01,384]\u001b[0m Trial 50 finished with value: 0.6505982556079861 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:04,199]\u001b[0m Trial 51 finished with value: 0.6505982556079861 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:07,013]\u001b[0m Trial 52 finished with value: 0.6505982556079861 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:09,918]\u001b[0m Trial 53 finished with value: 0.6458022448633686 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:12,763]\u001b[0m Trial 54 finished with value: 0.6458022448633686 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:15,638]\u001b[0m Trial 55 finished with value: 0.6313411780149205 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 61}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:19,125]\u001b[0m Trial 56 finished with value: 0.6367476836642043 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:22,007]\u001b[0m Trial 57 finished with value: 0.6458022448633686 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:24,895]\u001b[0m Trial 58 finished with value: 0.6352528920961487 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 51}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:28,411]\u001b[0m Trial 59 finished with value: 0.6367476836642043 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:31,301]\u001b[0m Trial 60 finished with value: 0.6458022448633686 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:34,181]\u001b[0m Trial 61 finished with value: 0.6458022448633686 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:37,057]\u001b[0m Trial 62 finished with value: 0.6457315165430308 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:39,860]\u001b[0m Trial 63 finished with value: 0.6431950144893637 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:43,397]\u001b[0m Trial 64 finished with value: 0.6482697233041589 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:46,278]\u001b[0m Trial 65 finished with value: 0.6487350354809625 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:49,315]\u001b[0m Trial 66 finished with value: 0.6482697233041589 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:52,226]\u001b[0m Trial 67 finished with value: 0.6437852199516508 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:55,192]\u001b[0m Trial 68 finished with value: 0.6482697233041589 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:58,148]\u001b[0m Trial 69 finished with value: 0.6482697233041589 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:01,010]\u001b[0m Trial 70 finished with value: 0.6482697233041589 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:04,574]\u001b[0m Trial 71 finished with value: 0.6478097533381414 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:07,472]\u001b[0m Trial 72 finished with value: 0.6482697233041589 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 50 with value: 0.6505982556079861.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:10,348]\u001b[0m Trial 73 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:13,232]\u001b[0m Trial 74 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:16,188]\u001b[0m Trial 75 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:19,681]\u001b[0m Trial 76 finished with value: 0.5322282269359148 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:22,652]\u001b[0m Trial 77 finished with value: 0.6506103286762348 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:26,109]\u001b[0m Trial 78 finished with value: 0.6467863587773227 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:29,012]\u001b[0m Trial 79 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:31,936]\u001b[0m Trial 80 finished with value: 0.6462770226914081 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:34,841]\u001b[0m Trial 81 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:37,745]\u001b[0m Trial 82 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:40,671]\u001b[0m Trial 83 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:43,595]\u001b[0m Trial 84 finished with value: 0.6506103286762348 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:13:46,523]\u001b[0m Trial 85 finished with value: 0.6506103286762348 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:49,422]\u001b[0m Trial 86 finished with value: 0.6368847775514146 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:52,338]\u001b[0m Trial 87 finished with value: 0.6257140006438953 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:55,253]\u001b[0m Trial 88 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:58,211]\u001b[0m Trial 89 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:01,142]\u001b[0m Trial 90 finished with value: 0.6447648470129056 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:04,095]\u001b[0m Trial 91 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:06,999]\u001b[0m Trial 92 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:09,906]\u001b[0m Trial 93 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:12,852]\u001b[0m Trial 94 finished with value: 0.6462770226914081 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:15,783]\u001b[0m Trial 95 finished with value: 0.6507709416512197 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:18,738]\u001b[0m Trial 96 finished with value: 0.6506103286762348 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:21,632]\u001b[0m Trial 97 finished with value: 0.6100919846970517 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:24,559]\u001b[0m Trial 98 finished with value: 0.5811267374938132 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:27,475]\u001b[0m Trial 99 finished with value: 0.6296438601799705 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 73 with value: 0.6507709416512197.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6508\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 30\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.633858    0.655985\n",
      "1                    TP  384.000000  404.000000\n",
      "2                    TN  353.000000  330.000000\n",
      "3                    FP   91.000000   86.000000\n",
      "4                    FN   71.000000   79.000000\n",
      "5              Accuracy    0.819800    0.816463\n",
      "6             Precision    0.808421    0.824490\n",
      "7           Sensitivity    0.843956    0.836439\n",
      "8           Specificity    0.795000    0.793300\n",
      "9              F1 score    0.825806    0.830421\n",
      "10  F1 score (weighted)    0.819661    0.816344\n",
      "11     F1 score (macro)    0.819585    0.815211\n",
      "12    Balanced Accuracy    0.819501    0.814854\n",
      "13                  MCC    0.639984    0.630521\n",
      "14                  NPV    0.832500    0.806800\n",
      "15              ROC_AUC    0.819501    0.814854\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_1_cat = np.where((y_pred_knn_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:14:31,949]\u001b[0m Trial 100 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:35,611]\u001b[0m Trial 101 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:39,206]\u001b[0m Trial 102 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:43,089]\u001b[0m Trial 103 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:47,146]\u001b[0m Trial 104 finished with value: 0.6476642373990927 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:51,567]\u001b[0m Trial 105 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:14:56,026]\u001b[0m Trial 106 finished with value: 0.6276186921919875 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:00,490]\u001b[0m Trial 107 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:04,926]\u001b[0m Trial 108 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:09,373]\u001b[0m Trial 109 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:13,960]\u001b[0m Trial 110 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:18,449]\u001b[0m Trial 111 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:22,869]\u001b[0m Trial 112 finished with value: 0.6476642373990927 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:27,359]\u001b[0m Trial 113 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:31,822]\u001b[0m Trial 114 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:36,218]\u001b[0m Trial 115 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:40,694]\u001b[0m Trial 116 finished with value: 0.6398546654599908 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:45,180]\u001b[0m Trial 117 finished with value: 0.6514136136146497 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:49,566]\u001b[0m Trial 118 finished with value: 0.6004237990744257 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:54,100]\u001b[0m Trial 119 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:57,651]\u001b[0m Trial 120 finished with value: 0.6472828366744732 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:02,116]\u001b[0m Trial 121 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:06,625]\u001b[0m Trial 122 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:11,018]\u001b[0m Trial 123 finished with value: 0.6514136136146497 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:15,453]\u001b[0m Trial 124 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:19,834]\u001b[0m Trial 125 finished with value: 0.6514136136146497 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 100 with value: 0.6551252619712643.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:23,389]\u001b[0m Trial 126 finished with value: 0.655872812886504 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 126 with value: 0.655872812886504.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:27,915]\u001b[0m Trial 127 finished with value: 0.6476642373990927 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 126 with value: 0.655872812886504.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:31,461]\u001b[0m Trial 128 finished with value: 0.6506755564261394 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 126 with value: 0.655872812886504.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:34,424]\u001b[0m Trial 129 finished with value: 0.655872812886504 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 126 with value: 0.655872812886504.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:37,370]\u001b[0m Trial 130 finished with value: 0.6510733067331561 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 126 with value: 0.655872812886504.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:40,843]\u001b[0m Trial 131 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 126 with value: 0.655872812886504.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:44,371]\u001b[0m Trial 132 finished with value: 0.6551252619712643 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 126 with value: 0.655872812886504.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:47,929]\u001b[0m Trial 133 finished with value: 0.6583774426613738 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:51,410]\u001b[0m Trial 134 finished with value: 0.6583774426613738 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:16:54,898]\u001b[0m Trial 135 finished with value: 0.6583774426613738 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:16:57,806]\u001b[0m Trial 136 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:00,746]\u001b[0m Trial 137 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:03,711]\u001b[0m Trial 138 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:06,668]\u001b[0m Trial 139 finished with value: 0.6505666792051127 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:09,582]\u001b[0m Trial 140 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:12,486]\u001b[0m Trial 141 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:15,370]\u001b[0m Trial 142 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:18,279]\u001b[0m Trial 143 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:21,188]\u001b[0m Trial 144 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:24,191]\u001b[0m Trial 145 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:27,136]\u001b[0m Trial 146 finished with value: 0.6295821166001153 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:30,361]\u001b[0m Trial 147 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:33,932]\u001b[0m Trial 148 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:37,570]\u001b[0m Trial 149 finished with value: 0.6582877869542254 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6584\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 24\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.633858    0.655985    0.641940\n",
      "1                    TP  384.000000  404.000000  401.000000\n",
      "2                    TN  353.000000  330.000000  335.000000\n",
      "3                    FP   91.000000   86.000000   88.000000\n",
      "4                    FN   71.000000   79.000000   75.000000\n",
      "5              Accuracy    0.819800    0.816463    0.818687\n",
      "6             Precision    0.808421    0.824490    0.820041\n",
      "7           Sensitivity    0.843956    0.836439    0.842437\n",
      "8           Specificity    0.795000    0.793300    0.792000\n",
      "9              F1 score    0.825806    0.830421    0.831088\n",
      "10  F1 score (weighted)    0.819661    0.816344    0.818494\n",
      "11     F1 score (macro)    0.819585    0.815211    0.817705\n",
      "12    Balanced Accuracy    0.819501    0.814854    0.817200\n",
      "13                  MCC    0.639984    0.630521    0.635755\n",
      "14                  NPV    0.832500    0.806800    0.817100\n",
      "15              ROC_AUC    0.819501    0.814854    0.817200\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_2_cat = np.where((y_pred_knn_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:17:41,363]\u001b[0m Trial 150 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:44,290]\u001b[0m Trial 151 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:47,248]\u001b[0m Trial 152 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:50,155]\u001b[0m Trial 153 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:53,109]\u001b[0m Trial 154 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:56,030]\u001b[0m Trial 155 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:58,966]\u001b[0m Trial 156 finished with value: 0.6535023024695163 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:01,914]\u001b[0m Trial 157 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:04,884]\u001b[0m Trial 158 finished with value: 0.5735308440836407 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:07,882]\u001b[0m Trial 159 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:11,417]\u001b[0m Trial 160 finished with value: 0.6535463425496093 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:14,352]\u001b[0m Trial 161 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:17,265]\u001b[0m Trial 162 finished with value: 0.6543250442000519 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:20,763]\u001b[0m Trial 163 finished with value: 0.6491495768804383 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:23,720]\u001b[0m Trial 164 finished with value: 0.6543250442000519 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:26,700]\u001b[0m Trial 165 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:29,870]\u001b[0m Trial 166 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:33,060]\u001b[0m Trial 167 finished with value: 0.6187858929761805 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:36,828]\u001b[0m Trial 168 finished with value: 0.6539778897437618 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:39,961]\u001b[0m Trial 169 finished with value: 0.6315673055241259 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:43,779]\u001b[0m Trial 170 finished with value: 0.6172085379142846 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:46,871]\u001b[0m Trial 171 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:50,010]\u001b[0m Trial 172 finished with value: 0.5912145926605907 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:53,824]\u001b[0m Trial 173 finished with value: 0.6491495768804383 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:18:56,935]\u001b[0m Trial 174 finished with value: 0.6543250442000519 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:00,094]\u001b[0m Trial 175 finished with value: 0.6543250442000519 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:03,171]\u001b[0m Trial 176 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:06,900]\u001b[0m Trial 177 finished with value: 0.6462554967826459 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:09,988]\u001b[0m Trial 178 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:13,068]\u001b[0m Trial 179 finished with value: 0.6472192057300022 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:16,131]\u001b[0m Trial 180 finished with value: 0.6543250442000519 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:19,947]\u001b[0m Trial 181 finished with value: 0.6475551736549566 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:23,731]\u001b[0m Trial 182 finished with value: 0.6462554967826459 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:27,487]\u001b[0m Trial 183 finished with value: 0.6491495768804383 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:30,545]\u001b[0m Trial 184 finished with value: 0.6472192057300022 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:19:34,304]\u001b[0m Trial 185 finished with value: 0.6475551736549566 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:37,388]\u001b[0m Trial 186 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:40,476]\u001b[0m Trial 187 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:43,622]\u001b[0m Trial 188 finished with value: 0.6543250442000519 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:47,397]\u001b[0m Trial 189 finished with value: 0.6475551736549566 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:51,212]\u001b[0m Trial 190 finished with value: 0.6491495768804383 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:54,959]\u001b[0m Trial 191 finished with value: 0.6475551736549566 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:58,736]\u001b[0m Trial 192 finished with value: 0.6475551736549566 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:01,838]\u001b[0m Trial 193 finished with value: 0.6496926714603524 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:04,996]\u001b[0m Trial 194 finished with value: 0.6361451493879409 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:08,131]\u001b[0m Trial 195 finished with value: 0.6455378111951252 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:12,171]\u001b[0m Trial 196 finished with value: 0.6539778897437618 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:15,368]\u001b[0m Trial 197 finished with value: 0.6455378111951252 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:18,881]\u001b[0m Trial 198 finished with value: 0.627648433947997 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:21,813]\u001b[0m Trial 199 finished with value: 0.6245655246128244 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 133 with value: 0.6583774426613738.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6584\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 24\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.633858    0.655985    0.641940    0.640618\n",
      "1                    TP  384.000000  404.000000  401.000000  382.000000\n",
      "2                    TN  353.000000  330.000000  335.000000  341.000000\n",
      "3                    FP   91.000000   86.000000   88.000000   85.000000\n",
      "4                    FN   71.000000   79.000000   75.000000   91.000000\n",
      "5              Accuracy    0.819800    0.816463    0.818687    0.804227\n",
      "6             Precision    0.808421    0.824490    0.820041    0.817987\n",
      "7           Sensitivity    0.843956    0.836439    0.842437    0.807611\n",
      "8           Specificity    0.795000    0.793300    0.792000    0.800500\n",
      "9              F1 score    0.825806    0.830421    0.831088    0.812766\n",
      "10  F1 score (weighted)    0.819661    0.816344    0.818494    0.804287\n",
      "11     F1 score (macro)    0.819585    0.815211    0.817705    0.803819\n",
      "12    Balanced Accuracy    0.819501    0.814854    0.817200    0.804040\n",
      "13                  MCC    0.639984    0.630521    0.635755    0.607710\n",
      "14                  NPV    0.832500    0.806800    0.817100    0.789400\n",
      "15              ROC_AUC    0.819501    0.814854    0.817200    0.804040\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_3_cat = np.where((y_pred_knn_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:20:27,036]\u001b[0m Trial 200 finished with value: 0.6625617631816063 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 200 with value: 0.6625617631816063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:31,607]\u001b[0m Trial 201 finished with value: 0.6625617631816063 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 200 with value: 0.6625617631816063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:35,261]\u001b[0m Trial 202 finished with value: 0.6625617631816063 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 200 with value: 0.6625617631816063.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:38,167]\u001b[0m Trial 203 finished with value: 0.6631138739217844 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 203 with value: 0.6631138739217844.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:41,054]\u001b[0m Trial 204 finished with value: 0.6631138739217844 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 203 with value: 0.6631138739217844.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:44,012]\u001b[0m Trial 205 finished with value: 0.6683951559699226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:46,953]\u001b[0m Trial 206 finished with value: 0.6683951559699226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:49,894]\u001b[0m Trial 207 finished with value: 0.6683951559699226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:53,509]\u001b[0m Trial 208 finished with value: 0.6632437253424999 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:56,419]\u001b[0m Trial 209 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:20:59,368]\u001b[0m Trial 210 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:02,392]\u001b[0m Trial 211 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:05,381]\u001b[0m Trial 212 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:08,294]\u001b[0m Trial 213 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:11,282]\u001b[0m Trial 214 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:14,281]\u001b[0m Trial 215 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:17,194]\u001b[0m Trial 216 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:20,123]\u001b[0m Trial 217 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:23,085]\u001b[0m Trial 218 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:26,087]\u001b[0m Trial 219 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:28,999]\u001b[0m Trial 220 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:31,974]\u001b[0m Trial 221 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:34,851]\u001b[0m Trial 222 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:37,773]\u001b[0m Trial 223 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:40,678]\u001b[0m Trial 224 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:43,586]\u001b[0m Trial 225 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:46,478]\u001b[0m Trial 226 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:49,401]\u001b[0m Trial 227 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:52,331]\u001b[0m Trial 228 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:55,245]\u001b[0m Trial 229 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:58,187]\u001b[0m Trial 230 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:01,073]\u001b[0m Trial 231 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:03,971]\u001b[0m Trial 232 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:06,888]\u001b[0m Trial 233 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:09,822]\u001b[0m Trial 234 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:22:12,764]\u001b[0m Trial 235 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:15,707]\u001b[0m Trial 236 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:18,645]\u001b[0m Trial 237 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:21,688]\u001b[0m Trial 238 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:24,598]\u001b[0m Trial 239 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:27,567]\u001b[0m Trial 240 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:30,447]\u001b[0m Trial 241 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:33,373]\u001b[0m Trial 242 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:36,286]\u001b[0m Trial 243 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:39,182]\u001b[0m Trial 244 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:42,094]\u001b[0m Trial 245 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:45,024]\u001b[0m Trial 246 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:47,913]\u001b[0m Trial 247 finished with value: 0.656548987640097 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:50,842]\u001b[0m Trial 248 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:22:53,727]\u001b[0m Trial 249 finished with value: 0.663091049073954 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6684\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.633858    0.655985    0.641940    0.640618   \n",
      "1                    TP  384.000000  404.000000  401.000000  382.000000   \n",
      "2                    TN  353.000000  330.000000  335.000000  341.000000   \n",
      "3                    FP   91.000000   86.000000   88.000000   85.000000   \n",
      "4                    FN   71.000000   79.000000   75.000000   91.000000   \n",
      "5              Accuracy    0.819800    0.816463    0.818687    0.804227   \n",
      "6             Precision    0.808421    0.824490    0.820041    0.817987   \n",
      "7           Sensitivity    0.843956    0.836439    0.842437    0.807611   \n",
      "8           Specificity    0.795000    0.793300    0.792000    0.800500   \n",
      "9              F1 score    0.825806    0.830421    0.831088    0.812766   \n",
      "10  F1 score (weighted)    0.819661    0.816344    0.818494    0.804287   \n",
      "11     F1 score (macro)    0.819585    0.815211    0.817705    0.803819   \n",
      "12    Balanced Accuracy    0.819501    0.814854    0.817200    0.804040   \n",
      "13                  MCC    0.639984    0.630521    0.635755    0.607710   \n",
      "14                  NPV    0.832500    0.806800    0.817100    0.789400   \n",
      "15              ROC_AUC    0.819501    0.814854    0.817200    0.804040   \n",
      "\n",
      "          Set4  \n",
      "0     0.581663  \n",
      "1   394.000000  \n",
      "2   348.000000  \n",
      "3    82.000000  \n",
      "4    75.000000  \n",
      "5     0.825362  \n",
      "6     0.827731  \n",
      "7     0.840085  \n",
      "8     0.809300  \n",
      "9     0.833862  \n",
      "10    0.825292  \n",
      "11    0.824903  \n",
      "12    0.824694  \n",
      "13    0.649907  \n",
      "14    0.822700  \n",
      "15    0.824694  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_4_cat = np.where((y_pred_knn_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:22:57,812]\u001b[0m Trial 250 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:01,338]\u001b[0m Trial 251 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:04,830]\u001b[0m Trial 252 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:08,099]\u001b[0m Trial 253 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:11,369]\u001b[0m Trial 254 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:14,633]\u001b[0m Trial 255 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:17,833]\u001b[0m Trial 256 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:21,067]\u001b[0m Trial 257 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:24,334]\u001b[0m Trial 258 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:27,564]\u001b[0m Trial 259 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:30,789]\u001b[0m Trial 260 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:34,008]\u001b[0m Trial 261 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:37,233]\u001b[0m Trial 262 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:40,465]\u001b[0m Trial 263 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:43,701]\u001b[0m Trial 264 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:46,953]\u001b[0m Trial 265 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:50,180]\u001b[0m Trial 266 finished with value: 0.6271867200745123 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:53,440]\u001b[0m Trial 267 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:56,691]\u001b[0m Trial 268 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:59,913]\u001b[0m Trial 269 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:03,099]\u001b[0m Trial 270 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:06,358]\u001b[0m Trial 271 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:09,615]\u001b[0m Trial 272 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:12,819]\u001b[0m Trial 273 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:16,056]\u001b[0m Trial 274 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:19,282]\u001b[0m Trial 275 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:22,490]\u001b[0m Trial 276 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:25,724]\u001b[0m Trial 277 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:28,973]\u001b[0m Trial 278 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:32,196]\u001b[0m Trial 279 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:35,456]\u001b[0m Trial 280 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:38,693]\u001b[0m Trial 281 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:41,877]\u001b[0m Trial 282 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:45,135]\u001b[0m Trial 283 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:48,384]\u001b[0m Trial 284 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:24:51,727]\u001b[0m Trial 285 finished with value: 0.5238029724829627 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:54,956]\u001b[0m Trial 286 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:58,166]\u001b[0m Trial 287 finished with value: 0.6352064075300753 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:01,298]\u001b[0m Trial 288 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:04,534]\u001b[0m Trial 289 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:07,782]\u001b[0m Trial 290 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:10,951]\u001b[0m Trial 291 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:14,205]\u001b[0m Trial 292 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:17,443]\u001b[0m Trial 293 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:20,694]\u001b[0m Trial 294 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:24,047]\u001b[0m Trial 295 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:27,631]\u001b[0m Trial 296 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:31,179]\u001b[0m Trial 297 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:34,707]\u001b[0m Trial 298 finished with value: 0.637414281599172 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:38,265]\u001b[0m Trial 299 finished with value: 0.6432311885334485 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6684\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.633858    0.655985    0.641940    0.640618   \n",
      "1                    TP  384.000000  404.000000  401.000000  382.000000   \n",
      "2                    TN  353.000000  330.000000  335.000000  341.000000   \n",
      "3                    FP   91.000000   86.000000   88.000000   85.000000   \n",
      "4                    FN   71.000000   79.000000   75.000000   91.000000   \n",
      "5              Accuracy    0.819800    0.816463    0.818687    0.804227   \n",
      "6             Precision    0.808421    0.824490    0.820041    0.817987   \n",
      "7           Sensitivity    0.843956    0.836439    0.842437    0.807611   \n",
      "8           Specificity    0.795000    0.793300    0.792000    0.800500   \n",
      "9              F1 score    0.825806    0.830421    0.831088    0.812766   \n",
      "10  F1 score (weighted)    0.819661    0.816344    0.818494    0.804287   \n",
      "11     F1 score (macro)    0.819585    0.815211    0.817705    0.803819   \n",
      "12    Balanced Accuracy    0.819501    0.814854    0.817200    0.804040   \n",
      "13                  MCC    0.639984    0.630521    0.635755    0.607710   \n",
      "14                  NPV    0.832500    0.806800    0.817100    0.789400   \n",
      "15              ROC_AUC    0.819501    0.814854    0.817200    0.804040   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.581663    0.651632  \n",
      "1   394.000000  396.000000  \n",
      "2   348.000000  353.000000  \n",
      "3    82.000000   70.000000  \n",
      "4    75.000000   80.000000  \n",
      "5     0.825362    0.833148  \n",
      "6     0.827731    0.849785  \n",
      "7     0.840085    0.831933  \n",
      "8     0.809300    0.834500  \n",
      "9     0.833862    0.840764  \n",
      "10    0.825292    0.833237  \n",
      "11    0.824903    0.832765  \n",
      "12    0.824694    0.833224  \n",
      "13    0.649907    0.665738  \n",
      "14    0.822700    0.815200  \n",
      "15    0.824694    0.833224  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_5_cat = np.where((y_pred_knn_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:25:41,842]\u001b[0m Trial 300 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:44,808]\u001b[0m Trial 301 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:47,739]\u001b[0m Trial 302 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:50,629]\u001b[0m Trial 303 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:53,503]\u001b[0m Trial 304 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:56,424]\u001b[0m Trial 305 finished with value: 0.6323272271846163 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:25:59,299]\u001b[0m Trial 306 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:02,154]\u001b[0m Trial 307 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:05,099]\u001b[0m Trial 308 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:08,033]\u001b[0m Trial 309 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:10,961]\u001b[0m Trial 310 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:13,957]\u001b[0m Trial 311 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:16,887]\u001b[0m Trial 312 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:19,796]\u001b[0m Trial 313 finished with value: 0.561084597816194 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:22,757]\u001b[0m Trial 314 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:25,682]\u001b[0m Trial 315 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:28,636]\u001b[0m Trial 316 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:31,640]\u001b[0m Trial 317 finished with value: 0.5458063558927602 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:34,611]\u001b[0m Trial 318 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:37,583]\u001b[0m Trial 319 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:40,522]\u001b[0m Trial 320 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:43,422]\u001b[0m Trial 321 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:46,372]\u001b[0m Trial 322 finished with value: 0.5747992396790403 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:49,338]\u001b[0m Trial 323 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:52,270]\u001b[0m Trial 324 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:55,193]\u001b[0m Trial 325 finished with value: 0.5896216400056976 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:58,197]\u001b[0m Trial 326 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:01,152]\u001b[0m Trial 327 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:04,131]\u001b[0m Trial 328 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:07,493]\u001b[0m Trial 329 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:10,442]\u001b[0m Trial 330 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:13,456]\u001b[0m Trial 331 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:16,432]\u001b[0m Trial 332 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:19,345]\u001b[0m Trial 333 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:22,891]\u001b[0m Trial 334 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:27:26,502]\u001b[0m Trial 335 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:30,075]\u001b[0m Trial 336 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:33,578]\u001b[0m Trial 337 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:36,566]\u001b[0m Trial 338 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:39,585]\u001b[0m Trial 339 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:42,639]\u001b[0m Trial 340 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:45,348]\u001b[0m Trial 341 finished with value: 0.6391517219464832 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:49,034]\u001b[0m Trial 342 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:52,635]\u001b[0m Trial 343 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:56,105]\u001b[0m Trial 344 finished with value: 0.5639648687987338 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:59,209]\u001b[0m Trial 345 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:02,162]\u001b[0m Trial 346 finished with value: 0.6388602069675724 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:05,071]\u001b[0m Trial 347 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:08,009]\u001b[0m Trial 348 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:10,973]\u001b[0m Trial 349 finished with value: 0.6378845145895824 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6684\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.633858    0.655985    0.641940    0.640618   \n",
      "1                    TP  384.000000  404.000000  401.000000  382.000000   \n",
      "2                    TN  353.000000  330.000000  335.000000  341.000000   \n",
      "3                    FP   91.000000   86.000000   88.000000   85.000000   \n",
      "4                    FN   71.000000   79.000000   75.000000   91.000000   \n",
      "5              Accuracy    0.819800    0.816463    0.818687    0.804227   \n",
      "6             Precision    0.808421    0.824490    0.820041    0.817987   \n",
      "7           Sensitivity    0.843956    0.836439    0.842437    0.807611   \n",
      "8           Specificity    0.795000    0.793300    0.792000    0.800500   \n",
      "9              F1 score    0.825806    0.830421    0.831088    0.812766   \n",
      "10  F1 score (weighted)    0.819661    0.816344    0.818494    0.804287   \n",
      "11     F1 score (macro)    0.819585    0.815211    0.817705    0.803819   \n",
      "12    Balanced Accuracy    0.819501    0.814854    0.817200    0.804040   \n",
      "13                  MCC    0.639984    0.630521    0.635755    0.607710   \n",
      "14                  NPV    0.832500    0.806800    0.817100    0.789400   \n",
      "15              ROC_AUC    0.819501    0.814854    0.817200    0.804040   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.581663    0.651632    0.662666  \n",
      "1   394.000000  396.000000  403.000000  \n",
      "2   348.000000  353.000000  352.000000  \n",
      "3    82.000000   70.000000   75.000000  \n",
      "4    75.000000   80.000000   69.000000  \n",
      "5     0.825362    0.833148    0.839822  \n",
      "6     0.827731    0.849785    0.843096  \n",
      "7     0.840085    0.831933    0.853814  \n",
      "8     0.809300    0.834500    0.824400  \n",
      "9     0.833862    0.840764    0.848421  \n",
      "10    0.825292    0.833237    0.839761  \n",
      "11    0.824903    0.832765    0.839305  \n",
      "12    0.824694    0.833224    0.839085  \n",
      "13    0.649907    0.665738    0.678685  \n",
      "14    0.822700    0.815200    0.836100  \n",
      "15    0.824694    0.833224    0.839085  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_6_cat = np.where((y_pred_knn_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:28:14,424]\u001b[0m Trial 350 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:17,372]\u001b[0m Trial 351 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:20,334]\u001b[0m Trial 352 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:22,867]\u001b[0m Trial 353 finished with value: 0.6476144876119785 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:25,786]\u001b[0m Trial 354 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:28,753]\u001b[0m Trial 355 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:31,736]\u001b[0m Trial 356 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:34,676]\u001b[0m Trial 357 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:37,623]\u001b[0m Trial 358 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:40,562]\u001b[0m Trial 359 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:43,504]\u001b[0m Trial 360 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:46,090]\u001b[0m Trial 361 finished with value: 0.6476144876119785 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 77}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:49,035]\u001b[0m Trial 362 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:51,979]\u001b[0m Trial 363 finished with value: 0.6428668401171024 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:54,884]\u001b[0m Trial 364 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:28:57,880]\u001b[0m Trial 365 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:00,872]\u001b[0m Trial 366 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:03,835]\u001b[0m Trial 367 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:06,795]\u001b[0m Trial 368 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:09,773]\u001b[0m Trial 369 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:12,736]\u001b[0m Trial 370 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:15,674]\u001b[0m Trial 371 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:18,679]\u001b[0m Trial 372 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:21,672]\u001b[0m Trial 373 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:24,269]\u001b[0m Trial 374 finished with value: 0.6476144876119785 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:26,884]\u001b[0m Trial 375 finished with value: 0.6457072141067728 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 54}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:29,831]\u001b[0m Trial 376 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:32,771]\u001b[0m Trial 377 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:35,696]\u001b[0m Trial 378 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:38,700]\u001b[0m Trial 379 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:41,677]\u001b[0m Trial 380 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:44,619]\u001b[0m Trial 381 finished with value: 0.6062774215600818 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:47,543]\u001b[0m Trial 382 finished with value: 0.6428668401171024 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:50,491]\u001b[0m Trial 383 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:53,422]\u001b[0m Trial 384 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:29:56,415]\u001b[0m Trial 385 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:59,338]\u001b[0m Trial 386 finished with value: 0.6221505222524076 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:02,310]\u001b[0m Trial 387 finished with value: 0.5806858166990501 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:05,313]\u001b[0m Trial 388 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:08,262]\u001b[0m Trial 389 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:11,216]\u001b[0m Trial 390 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:14,220]\u001b[0m Trial 391 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:17,181]\u001b[0m Trial 392 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:20,139]\u001b[0m Trial 393 finished with value: 0.5946719252157526 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:23,062]\u001b[0m Trial 394 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:25,987]\u001b[0m Trial 395 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:28,911]\u001b[0m Trial 396 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:31,843]\u001b[0m Trial 397 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:34,764]\u001b[0m Trial 398 finished with value: 0.6459842545943817 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:37,686]\u001b[0m Trial 399 finished with value: 0.6479457660400335 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6684\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.633858    0.655985    0.641940    0.640618   \n",
      "1                    TP  384.000000  404.000000  401.000000  382.000000   \n",
      "2                    TN  353.000000  330.000000  335.000000  341.000000   \n",
      "3                    FP   91.000000   86.000000   88.000000   85.000000   \n",
      "4                    FN   71.000000   79.000000   75.000000   91.000000   \n",
      "5              Accuracy    0.819800    0.816463    0.818687    0.804227   \n",
      "6             Precision    0.808421    0.824490    0.820041    0.817987   \n",
      "7           Sensitivity    0.843956    0.836439    0.842437    0.807611   \n",
      "8           Specificity    0.795000    0.793300    0.792000    0.800500   \n",
      "9              F1 score    0.825806    0.830421    0.831088    0.812766   \n",
      "10  F1 score (weighted)    0.819661    0.816344    0.818494    0.804287   \n",
      "11     F1 score (macro)    0.819585    0.815211    0.817705    0.803819   \n",
      "12    Balanced Accuracy    0.819501    0.814854    0.817200    0.804040   \n",
      "13                  MCC    0.639984    0.630521    0.635755    0.607710   \n",
      "14                  NPV    0.832500    0.806800    0.817100    0.789400   \n",
      "15              ROC_AUC    0.819501    0.814854    0.817200    0.804040   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.581663    0.651632    0.662666    0.643630  \n",
      "1   394.000000  396.000000  403.000000  389.000000  \n",
      "2   348.000000  353.000000  352.000000  338.000000  \n",
      "3    82.000000   70.000000   75.000000   78.000000  \n",
      "4    75.000000   80.000000   69.000000   94.000000  \n",
      "5     0.825362    0.833148    0.839822    0.808676  \n",
      "6     0.827731    0.849785    0.843096    0.832976  \n",
      "7     0.840085    0.831933    0.853814    0.805383  \n",
      "8     0.809300    0.834500    0.824400    0.812500  \n",
      "9     0.833862    0.840764    0.848421    0.818947  \n",
      "10    0.825292    0.833237    0.839761    0.808870  \n",
      "11    0.824903    0.832765    0.839305    0.808059  \n",
      "12    0.824694    0.833224    0.839085    0.808942  \n",
      "13    0.649907    0.665738    0.678685    0.616632  \n",
      "14    0.822700    0.815200    0.836100    0.782400  \n",
      "15    0.824694    0.833224    0.839085    0.808942  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_7_cat = np.where((y_pred_knn_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:30:41,743]\u001b[0m Trial 400 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:45,232]\u001b[0m Trial 401 finished with value: 0.6320500998357516 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:48,717]\u001b[0m Trial 402 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:52,219]\u001b[0m Trial 403 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:55,737]\u001b[0m Trial 404 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:30:59,310]\u001b[0m Trial 405 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:02,909]\u001b[0m Trial 406 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:06,478]\u001b[0m Trial 407 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:10,031]\u001b[0m Trial 408 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:14,659]\u001b[0m Trial 409 finished with value: 0.635062707257968 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:18,275]\u001b[0m Trial 410 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:21,857]\u001b[0m Trial 411 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:25,385]\u001b[0m Trial 412 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:28,961]\u001b[0m Trial 413 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:32,423]\u001b[0m Trial 414 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:36,001]\u001b[0m Trial 415 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:39,596]\u001b[0m Trial 416 finished with value: 0.5526796834178493 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:43,074]\u001b[0m Trial 417 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:46,642]\u001b[0m Trial 418 finished with value: 0.527805801375141 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:50,012]\u001b[0m Trial 419 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:52,839]\u001b[0m Trial 420 finished with value: 0.6336347923001184 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 68}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:56,155]\u001b[0m Trial 421 finished with value: 0.6320500998357516 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:59,167]\u001b[0m Trial 422 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:02,362]\u001b[0m Trial 423 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:05,886]\u001b[0m Trial 424 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:09,427]\u001b[0m Trial 425 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:12,993]\u001b[0m Trial 426 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:16,464]\u001b[0m Trial 427 finished with value: 0.6128849646168353 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:20,038]\u001b[0m Trial 428 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:23,623]\u001b[0m Trial 429 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:26,495]\u001b[0m Trial 430 finished with value: 0.6353284749192112 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 59}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:30,088]\u001b[0m Trial 431 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:33,427]\u001b[0m Trial 432 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:36,950]\u001b[0m Trial 433 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:40,606]\u001b[0m Trial 434 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:32:44,215]\u001b[0m Trial 435 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:47,766]\u001b[0m Trial 436 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:51,404]\u001b[0m Trial 437 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:54,969]\u001b[0m Trial 438 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:32:58,472]\u001b[0m Trial 439 finished with value: 0.6300417466090414 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:02,037]\u001b[0m Trial 440 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:05,644]\u001b[0m Trial 441 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:09,218]\u001b[0m Trial 442 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:12,836]\u001b[0m Trial 443 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:15,763]\u001b[0m Trial 444 finished with value: 0.6336347923001184 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:19,281]\u001b[0m Trial 445 finished with value: 0.6406376807556472 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:22,832]\u001b[0m Trial 446 finished with value: 0.6352810578844409 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:26,385]\u001b[0m Trial 447 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:29,846]\u001b[0m Trial 448 finished with value: 0.6356480820038661 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:33,460]\u001b[0m Trial 449 finished with value: 0.6421365859053889 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6684\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.633858    0.655985    0.641940    0.640618   \n",
      "1                    TP  384.000000  404.000000  401.000000  382.000000   \n",
      "2                    TN  353.000000  330.000000  335.000000  341.000000   \n",
      "3                    FP   91.000000   86.000000   88.000000   85.000000   \n",
      "4                    FN   71.000000   79.000000   75.000000   91.000000   \n",
      "5              Accuracy    0.819800    0.816463    0.818687    0.804227   \n",
      "6             Precision    0.808421    0.824490    0.820041    0.817987   \n",
      "7           Sensitivity    0.843956    0.836439    0.842437    0.807611   \n",
      "8           Specificity    0.795000    0.793300    0.792000    0.800500   \n",
      "9              F1 score    0.825806    0.830421    0.831088    0.812766   \n",
      "10  F1 score (weighted)    0.819661    0.816344    0.818494    0.804287   \n",
      "11     F1 score (macro)    0.819585    0.815211    0.817705    0.803819   \n",
      "12    Balanced Accuracy    0.819501    0.814854    0.817200    0.804040   \n",
      "13                  MCC    0.639984    0.630521    0.635755    0.607710   \n",
      "14                  NPV    0.832500    0.806800    0.817100    0.789400   \n",
      "15              ROC_AUC    0.819501    0.814854    0.817200    0.804040   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.581663    0.651632    0.662666    0.643630    0.647108  \n",
      "1   394.000000  396.000000  403.000000  389.000000  384.000000  \n",
      "2   348.000000  353.000000  352.000000  338.000000  336.000000  \n",
      "3    82.000000   70.000000   75.000000   78.000000   80.000000  \n",
      "4    75.000000   80.000000   69.000000   94.000000   99.000000  \n",
      "5     0.825362    0.833148    0.839822    0.808676    0.800890  \n",
      "6     0.827731    0.849785    0.843096    0.832976    0.827586  \n",
      "7     0.840085    0.831933    0.853814    0.805383    0.795031  \n",
      "8     0.809300    0.834500    0.824400    0.812500    0.807700  \n",
      "9     0.833862    0.840764    0.848421    0.818947    0.810982  \n",
      "10    0.825292    0.833237    0.839761    0.808870    0.801115  \n",
      "11    0.824903    0.832765    0.839305    0.808059    0.800321  \n",
      "12    0.824694    0.833224    0.839085    0.808942    0.801362  \n",
      "13    0.649907    0.665738    0.678685    0.616632    0.601360  \n",
      "14    0.822700    0.815200    0.836100    0.782400    0.772400  \n",
      "15    0.824694    0.833224    0.839085    0.808942    0.801362  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_8_cat = np.where((y_pred_knn_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:33:37,723]\u001b[0m Trial 450 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:41,255]\u001b[0m Trial 451 finished with value: 0.6049157327877698 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:44,825]\u001b[0m Trial 452 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:48,412]\u001b[0m Trial 453 finished with value: 0.6416848426225934 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:52,166]\u001b[0m Trial 454 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:55,823]\u001b[0m Trial 455 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:59,352]\u001b[0m Trial 456 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:04,017]\u001b[0m Trial 457 finished with value: 0.6423985327935453 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:07,562]\u001b[0m Trial 458 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:11,129]\u001b[0m Trial 459 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:14,699]\u001b[0m Trial 460 finished with value: 0.6189922055661148 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:18,184]\u001b[0m Trial 461 finished with value: 0.6416848426225934 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:21,767]\u001b[0m Trial 462 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:25,309]\u001b[0m Trial 463 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:28,840]\u001b[0m Trial 464 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:31,766]\u001b[0m Trial 465 finished with value: 0.6373161627353512 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:34,670]\u001b[0m Trial 466 finished with value: 0.6334163569220594 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:38,254]\u001b[0m Trial 467 finished with value: 0.5695539660330479 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:41,887]\u001b[0m Trial 468 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:45,418]\u001b[0m Trial 469 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:48,975]\u001b[0m Trial 470 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:52,587]\u001b[0m Trial 471 finished with value: 0.628752031889298 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:57,200]\u001b[0m Trial 472 finished with value: 0.6377157851180801 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:00,737]\u001b[0m Trial 473 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:04,299]\u001b[0m Trial 474 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:07,923]\u001b[0m Trial 475 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:11,263]\u001b[0m Trial 476 finished with value: 0.6416848426225934 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:14,799]\u001b[0m Trial 477 finished with value: 0.6244753104919317 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:18,367]\u001b[0m Trial 478 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:21,999]\u001b[0m Trial 479 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:25,551]\u001b[0m Trial 480 finished with value: 0.6416848426225934 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:29,003]\u001b[0m Trial 481 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:32,520]\u001b[0m Trial 482 finished with value: 0.5953705063094268 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:37,056]\u001b[0m Trial 483 finished with value: 0.6377157851180801 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:39,940]\u001b[0m Trial 484 finished with value: 0.6398915796457564 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:35:43,477]\u001b[0m Trial 485 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:47,001]\u001b[0m Trial 486 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:50,469]\u001b[0m Trial 487 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:54,016]\u001b[0m Trial 488 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:35:57,526]\u001b[0m Trial 489 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:01,143]\u001b[0m Trial 490 finished with value: 0.6416848426225934 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:04,704]\u001b[0m Trial 491 finished with value: 0.6288934370811138 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:07,566]\u001b[0m Trial 492 finished with value: 0.6373161627353512 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 52}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:11,144]\u001b[0m Trial 493 finished with value: 0.6339484368134342 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:14,749]\u001b[0m Trial 494 finished with value: 0.6416848426225934 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:18,298]\u001b[0m Trial 495 finished with value: 0.6370987468001655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:21,975]\u001b[0m Trial 496 finished with value: 0.6189922055661148 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:24,914]\u001b[0m Trial 497 finished with value: 0.6373161627353512 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:28,474]\u001b[0m Trial 498 finished with value: 0.6403558525143083 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:32,021]\u001b[0m Trial 499 finished with value: 0.6288934370811138 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 205 with value: 0.6683951559699226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6684\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.633858    0.655985    0.641940    0.640618   \n",
      "1                    TP  384.000000  404.000000  401.000000  382.000000   \n",
      "2                    TN  353.000000  330.000000  335.000000  341.000000   \n",
      "3                    FP   91.000000   86.000000   88.000000   85.000000   \n",
      "4                    FN   71.000000   79.000000   75.000000   91.000000   \n",
      "5              Accuracy    0.819800    0.816463    0.818687    0.804227   \n",
      "6             Precision    0.808421    0.824490    0.820041    0.817987   \n",
      "7           Sensitivity    0.843956    0.836439    0.842437    0.807611   \n",
      "8           Specificity    0.795000    0.793300    0.792000    0.800500   \n",
      "9              F1 score    0.825806    0.830421    0.831088    0.812766   \n",
      "10  F1 score (weighted)    0.819661    0.816344    0.818494    0.804287   \n",
      "11     F1 score (macro)    0.819585    0.815211    0.817705    0.803819   \n",
      "12    Balanced Accuracy    0.819501    0.814854    0.817200    0.804040   \n",
      "13                  MCC    0.639984    0.630521    0.635755    0.607710   \n",
      "14                  NPV    0.832500    0.806800    0.817100    0.789400   \n",
      "15              ROC_AUC    0.819501    0.814854    0.817200    0.804040   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.581663    0.651632    0.662666    0.643630    0.647108    0.660323  \n",
      "1   394.000000  396.000000  403.000000  389.000000  384.000000  395.000000  \n",
      "2   348.000000  353.000000  352.000000  338.000000  336.000000  335.000000  \n",
      "3    82.000000   70.000000   75.000000   78.000000   80.000000   94.000000  \n",
      "4    75.000000   80.000000   69.000000   94.000000   99.000000   75.000000  \n",
      "5     0.825362    0.833148    0.839822    0.808676    0.800890    0.812013  \n",
      "6     0.827731    0.849785    0.843096    0.832976    0.827586    0.807771  \n",
      "7     0.840085    0.831933    0.853814    0.805383    0.795031    0.840426  \n",
      "8     0.809300    0.834500    0.824400    0.812500    0.807700    0.780900  \n",
      "9     0.833862    0.840764    0.848421    0.818947    0.810982    0.823775  \n",
      "10    0.825292    0.833237    0.839761    0.808870    0.801115    0.811747  \n",
      "11    0.824903    0.832765    0.839305    0.808059    0.800321    0.811172  \n",
      "12    0.824694    0.833224    0.839085    0.808942    0.801362    0.810656  \n",
      "13    0.649907    0.665738    0.678685    0.616632    0.601360    0.623075  \n",
      "14    0.822700    0.815200    0.836100    0.782400    0.772400    0.817100  \n",
      "15    0.824694    0.833224    0.839085    0.808942    0.801362    0.810656  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_9_cat = np.where((y_pred_knn_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.043749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>195.300000</td>\n",
       "      <td>12.445883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>171.400000</td>\n",
       "      <td>7.618690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>41.600000</td>\n",
       "      <td>5.719363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>40.900000</td>\n",
       "      <td>6.822349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.816341</td>\n",
       "      <td>0.017427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.824287</td>\n",
       "      <td>0.023564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.826323</td>\n",
       "      <td>0.031340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.804850</td>\n",
       "      <td>0.024591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.824979</td>\n",
       "      <td>0.021651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.816252</td>\n",
       "      <td>0.017355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.815563</td>\n",
       "      <td>0.016942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.815578</td>\n",
       "      <td>0.016722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.631845</td>\n",
       "      <td>0.034039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.808240</td>\n",
       "      <td>0.023040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.815578</td>\n",
       "      <td>0.016722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.663500     0.043749\n",
       "1                    TP       195.300000    12.445883\n",
       "2                    TN       171.400000     7.618690\n",
       "3                    FP        41.600000     5.719363\n",
       "4                    FN        40.900000     6.822349\n",
       "5              Accuracy         0.816341     0.017427\n",
       "6             Precision         0.824287     0.023564\n",
       "7           Sensitivity         0.826323     0.031340\n",
       "8           Specificity         0.804850     0.024591\n",
       "9              F1 score         0.824979     0.021651\n",
       "10  F1 score (weighted)         0.816252     0.017355\n",
       "11     F1 score (macro)         0.815563     0.016942\n",
       "12    Balanced Accuracy         0.815578     0.016722\n",
       "13                  MCC         0.631845     0.034039\n",
       "14                  NPV         0.808240     0.023040\n",
       "15              ROC_AUC         0.815578     0.016722"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.633858</td>\n",
       "      <td>0.655985</td>\n",
       "      <td>0.641940</td>\n",
       "      <td>0.640618</td>\n",
       "      <td>0.581663</td>\n",
       "      <td>0.651632</td>\n",
       "      <td>0.662666</td>\n",
       "      <td>0.643630</td>\n",
       "      <td>0.647108</td>\n",
       "      <td>0.660323</td>\n",
       "      <td>0.641942</td>\n",
       "      <td>0.023067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>393.200000</td>\n",
       "      <td>8.148620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>342.100000</td>\n",
       "      <td>8.646772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>82.900000</td>\n",
       "      <td>7.385421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>80.800000</td>\n",
       "      <td>10.271860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.819800</td>\n",
       "      <td>0.816463</td>\n",
       "      <td>0.818687</td>\n",
       "      <td>0.804227</td>\n",
       "      <td>0.825362</td>\n",
       "      <td>0.833148</td>\n",
       "      <td>0.839822</td>\n",
       "      <td>0.808676</td>\n",
       "      <td>0.800890</td>\n",
       "      <td>0.812013</td>\n",
       "      <td>0.817909</td>\n",
       "      <td>0.012343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.808421</td>\n",
       "      <td>0.824490</td>\n",
       "      <td>0.820041</td>\n",
       "      <td>0.817987</td>\n",
       "      <td>0.827731</td>\n",
       "      <td>0.849785</td>\n",
       "      <td>0.843096</td>\n",
       "      <td>0.832976</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.807771</td>\n",
       "      <td>0.825989</td>\n",
       "      <td>0.013563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.843956</td>\n",
       "      <td>0.836439</td>\n",
       "      <td>0.842437</td>\n",
       "      <td>0.807611</td>\n",
       "      <td>0.840085</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.853814</td>\n",
       "      <td>0.805383</td>\n",
       "      <td>0.795031</td>\n",
       "      <td>0.840426</td>\n",
       "      <td>0.829711</td>\n",
       "      <td>0.019724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.793300</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.824400</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.807700</td>\n",
       "      <td>0.780900</td>\n",
       "      <td>0.805010</td>\n",
       "      <td>0.016109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.825806</td>\n",
       "      <td>0.830421</td>\n",
       "      <td>0.831088</td>\n",
       "      <td>0.812766</td>\n",
       "      <td>0.833862</td>\n",
       "      <td>0.840764</td>\n",
       "      <td>0.848421</td>\n",
       "      <td>0.818947</td>\n",
       "      <td>0.810982</td>\n",
       "      <td>0.823775</td>\n",
       "      <td>0.827683</td>\n",
       "      <td>0.011809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.819661</td>\n",
       "      <td>0.816344</td>\n",
       "      <td>0.818494</td>\n",
       "      <td>0.804287</td>\n",
       "      <td>0.825292</td>\n",
       "      <td>0.833237</td>\n",
       "      <td>0.839761</td>\n",
       "      <td>0.808870</td>\n",
       "      <td>0.801115</td>\n",
       "      <td>0.811747</td>\n",
       "      <td>0.817881</td>\n",
       "      <td>0.012293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.819585</td>\n",
       "      <td>0.815211</td>\n",
       "      <td>0.817705</td>\n",
       "      <td>0.803819</td>\n",
       "      <td>0.824903</td>\n",
       "      <td>0.832765</td>\n",
       "      <td>0.839305</td>\n",
       "      <td>0.808059</td>\n",
       "      <td>0.800321</td>\n",
       "      <td>0.811172</td>\n",
       "      <td>0.817284</td>\n",
       "      <td>0.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.819501</td>\n",
       "      <td>0.814854</td>\n",
       "      <td>0.817200</td>\n",
       "      <td>0.804040</td>\n",
       "      <td>0.824694</td>\n",
       "      <td>0.833224</td>\n",
       "      <td>0.839085</td>\n",
       "      <td>0.808942</td>\n",
       "      <td>0.801362</td>\n",
       "      <td>0.810656</td>\n",
       "      <td>0.817356</td>\n",
       "      <td>0.012191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.639984</td>\n",
       "      <td>0.630521</td>\n",
       "      <td>0.635755</td>\n",
       "      <td>0.607710</td>\n",
       "      <td>0.649907</td>\n",
       "      <td>0.665738</td>\n",
       "      <td>0.678685</td>\n",
       "      <td>0.616632</td>\n",
       "      <td>0.601360</td>\n",
       "      <td>0.623075</td>\n",
       "      <td>0.634937</td>\n",
       "      <td>0.024668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.806800</td>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.789400</td>\n",
       "      <td>0.822700</td>\n",
       "      <td>0.815200</td>\n",
       "      <td>0.836100</td>\n",
       "      <td>0.782400</td>\n",
       "      <td>0.772400</td>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.809170</td>\n",
       "      <td>0.021291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.819501</td>\n",
       "      <td>0.814854</td>\n",
       "      <td>0.817200</td>\n",
       "      <td>0.804040</td>\n",
       "      <td>0.824694</td>\n",
       "      <td>0.833224</td>\n",
       "      <td>0.839085</td>\n",
       "      <td>0.808942</td>\n",
       "      <td>0.801362</td>\n",
       "      <td>0.810656</td>\n",
       "      <td>0.817356</td>\n",
       "      <td>0.012191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.633858    0.655985    0.641940    0.640618   \n",
       "1                    TP  384.000000  404.000000  401.000000  382.000000   \n",
       "2                    TN  353.000000  330.000000  335.000000  341.000000   \n",
       "3                    FP   91.000000   86.000000   88.000000   85.000000   \n",
       "4                    FN   71.000000   79.000000   75.000000   91.000000   \n",
       "5              Accuracy    0.819800    0.816463    0.818687    0.804227   \n",
       "6             Precision    0.808421    0.824490    0.820041    0.817987   \n",
       "7           Sensitivity    0.843956    0.836439    0.842437    0.807611   \n",
       "8           Specificity    0.795000    0.793300    0.792000    0.800500   \n",
       "9              F1 score    0.825806    0.830421    0.831088    0.812766   \n",
       "10  F1 score (weighted)    0.819661    0.816344    0.818494    0.804287   \n",
       "11     F1 score (macro)    0.819585    0.815211    0.817705    0.803819   \n",
       "12    Balanced Accuracy    0.819501    0.814854    0.817200    0.804040   \n",
       "13                  MCC    0.639984    0.630521    0.635755    0.607710   \n",
       "14                  NPV    0.832500    0.806800    0.817100    0.789400   \n",
       "15              ROC_AUC    0.819501    0.814854    0.817200    0.804040   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.581663    0.651632    0.662666    0.643630    0.647108    0.660323   \n",
       "1   394.000000  396.000000  403.000000  389.000000  384.000000  395.000000   \n",
       "2   348.000000  353.000000  352.000000  338.000000  336.000000  335.000000   \n",
       "3    82.000000   70.000000   75.000000   78.000000   80.000000   94.000000   \n",
       "4    75.000000   80.000000   69.000000   94.000000   99.000000   75.000000   \n",
       "5     0.825362    0.833148    0.839822    0.808676    0.800890    0.812013   \n",
       "6     0.827731    0.849785    0.843096    0.832976    0.827586    0.807771   \n",
       "7     0.840085    0.831933    0.853814    0.805383    0.795031    0.840426   \n",
       "8     0.809300    0.834500    0.824400    0.812500    0.807700    0.780900   \n",
       "9     0.833862    0.840764    0.848421    0.818947    0.810982    0.823775   \n",
       "10    0.825292    0.833237    0.839761    0.808870    0.801115    0.811747   \n",
       "11    0.824903    0.832765    0.839305    0.808059    0.800321    0.811172   \n",
       "12    0.824694    0.833224    0.839085    0.808942    0.801362    0.810656   \n",
       "13    0.649907    0.665738    0.678685    0.616632    0.601360    0.623075   \n",
       "14    0.822700    0.815200    0.836100    0.782400    0.772400    0.817100   \n",
       "15    0.824694    0.833224    0.839085    0.808942    0.801362    0.810656   \n",
       "\n",
       "           ave        std  \n",
       "0     0.641942   0.023067  \n",
       "1   393.200000   8.148620  \n",
       "2   342.100000   8.646772  \n",
       "3    82.900000   7.385421  \n",
       "4    80.800000  10.271860  \n",
       "5     0.817909   0.012343  \n",
       "6     0.825989   0.013563  \n",
       "7     0.829711   0.019724  \n",
       "8     0.805010   0.016109  \n",
       "9     0.827683   0.011809  \n",
       "10    0.817881   0.012293  \n",
       "11    0.817284   0.012400  \n",
       "12    0.817356   0.012191  \n",
       "13    0.634937   0.024668  \n",
       "14    0.809170   0.021291  \n",
       "15    0.817356   0.012191  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_knn0</th>\n",
       "      <th>y_pred_knn1</th>\n",
       "      <th>y_pred_knn2</th>\n",
       "      <th>y_pred_knn3</th>\n",
       "      <th>y_pred_knn4</th>\n",
       "      <th>y_pred_knn_ave</th>\n",
       "      <th>y_pred_knn_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4286867</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>6.612508</td>\n",
       "      <td>6.612508</td>\n",
       "      <td>6.612508</td>\n",
       "      <td>6.612508</td>\n",
       "      <td>6.352919</td>\n",
       "      <td>6.675492</td>\n",
       "      <td>0.273855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3689853</td>\n",
       "      <td>1</td>\n",
       "      <td>6.43</td>\n",
       "      <td>6.527694</td>\n",
       "      <td>6.565434</td>\n",
       "      <td>6.565434</td>\n",
       "      <td>6.624664</td>\n",
       "      <td>6.565434</td>\n",
       "      <td>6.546443</td>\n",
       "      <td>0.059317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3827056</td>\n",
       "      <td>2</td>\n",
       "      <td>7.52</td>\n",
       "      <td>8.847714</td>\n",
       "      <td>8.676487</td>\n",
       "      <td>8.804887</td>\n",
       "      <td>8.804887</td>\n",
       "      <td>8.667661</td>\n",
       "      <td>8.553606</td>\n",
       "      <td>0.467125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3689883</td>\n",
       "      <td>3</td>\n",
       "      <td>7.70</td>\n",
       "      <td>7.476192</td>\n",
       "      <td>7.475491</td>\n",
       "      <td>7.476192</td>\n",
       "      <td>7.453349</td>\n",
       "      <td>7.476192</td>\n",
       "      <td>7.509569</td>\n",
       "      <td>0.085565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2023528</td>\n",
       "      <td>4</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.565466</td>\n",
       "      <td>7.565466</td>\n",
       "      <td>7.976889</td>\n",
       "      <td>7.377892</td>\n",
       "      <td>7.395386</td>\n",
       "      <td>7.525183</td>\n",
       "      <td>0.227649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4464975</td>\n",
       "      <td>4487</td>\n",
       "      <td>4.72</td>\n",
       "      <td>4.970271</td>\n",
       "      <td>4.968747</td>\n",
       "      <td>4.970271</td>\n",
       "      <td>4.970271</td>\n",
       "      <td>5.018218</td>\n",
       "      <td>4.936296</td>\n",
       "      <td>0.098329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL95747</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.60</td>\n",
       "      <td>7.611925</td>\n",
       "      <td>7.611925</td>\n",
       "      <td>7.611925</td>\n",
       "      <td>7.611925</td>\n",
       "      <td>7.611925</td>\n",
       "      <td>7.609938</td>\n",
       "      <td>0.004444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL4072618</td>\n",
       "      <td>4489</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.225391</td>\n",
       "      <td>5.225391</td>\n",
       "      <td>5.225391</td>\n",
       "      <td>5.447661</td>\n",
       "      <td>5.365936</td>\n",
       "      <td>5.279961</td>\n",
       "      <td>0.093578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL2408692</td>\n",
       "      <td>4490</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.183897</td>\n",
       "      <td>6.056376</td>\n",
       "      <td>6.056376</td>\n",
       "      <td>6.056376</td>\n",
       "      <td>5.786719</td>\n",
       "      <td>6.083291</td>\n",
       "      <td>0.171667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4226829</td>\n",
       "      <td>4491</td>\n",
       "      <td>5.55</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>5.591667</td>\n",
       "      <td>0.018634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_knn0  y_pred_knn1  \\\n",
       "0         CHEMBL4286867            0     7.25     6.612508     6.612508   \n",
       "1         CHEMBL3689853            1     6.43     6.527694     6.565434   \n",
       "2         CHEMBL3827056            2     7.52     8.847714     8.676487   \n",
       "3         CHEMBL3689883            3     7.70     7.476192     7.475491   \n",
       "4         CHEMBL2023528            4     7.27     7.565466     7.565466   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "4487      CHEMBL4464975         4487     4.72     4.970271     4.968747   \n",
       "4488        CHEMBL95747         4488     7.60     7.611925     7.611925   \n",
       "4489      CHEMBL4072618         4489     5.19     5.225391     5.225391   \n",
       "4490      CHEMBL2408692         4490     6.36     6.183897     6.056376   \n",
       "4491      CHEMBL4226829         4491     5.55     5.600000     5.600000   \n",
       "\n",
       "      y_pred_knn2  y_pred_knn3  y_pred_knn4  y_pred_knn_ave  y_pred_knn_std  \n",
       "0        6.612508     6.612508     6.352919        6.675492        0.273855  \n",
       "1        6.565434     6.624664     6.565434        6.546443        0.059317  \n",
       "2        8.804887     8.804887     8.667661        8.553606        0.467125  \n",
       "3        7.476192     7.453349     7.476192        7.509569        0.085565  \n",
       "4        7.976889     7.377892     7.395386        7.525183        0.227649  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "4487     4.970271     4.970271     5.018218        4.936296        0.098329  \n",
       "4488     7.611925     7.611925     7.611925        7.609938        0.004444  \n",
       "4489     5.225391     5.447661     5.365936        5.279961        0.093578  \n",
       "4490     6.056376     6.056376     5.786719        6.083291        0.171667  \n",
       "4491     5.600000     5.600000     5.600000        5.591667        0.018634  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where((y_pred_optimized_knn >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id, knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "knn_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c149767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_knn.to_csv('mat_met_knn_opt.csv')\n",
    "knn_5preds.to_csv('knn_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6461 with a standard deviation of 0.0490\n",
      "KNN optimized model r2_score 0.6563 with a standard deviation of 0.0360\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_knn.joblib']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"./knn_reg.joblib\")\n",
    "#joblib.dump(optimized_knn, \"./optimized_knn.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"./optimizedCV_knn.joblib\")\n",
    "#loaded_rf = joblib.load(\"./optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.682841     0.031607\n",
      "1                    TP       200.100000    11.647126\n",
      "2                    TN       173.300000     7.846443\n",
      "3                    FP        39.700000     6.056218\n",
      "4                    FN        36.100000     5.782156\n",
      "5              Accuracy         0.831257     0.022045\n",
      "6             Precision         0.834193     0.026354\n",
      "7           Sensitivity         0.846704     0.027051\n",
      "8           Specificity         0.813770     0.026699\n",
      "9              F1 score         0.840263     0.024010\n",
      "10  F1 score (weighted)         0.831150     0.022040\n",
      "11     F1 score (macro)         0.830455     0.021908\n",
      "12    Balanced Accuracy         0.830238     0.021904\n",
      "13                  MCC         0.661337     0.044008\n",
      "14                  NPV         0.828020     0.023753\n",
      "15              ROC_AUC         0.830238     0.021904\n",
      "CPU times: user 1min 52s, sys: 20 ms, total: 1min 52s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:39:33,855]\u001b[0m A new study created in memory with name: SVM_regressor_CV\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:40:46,598]\u001b[0m Trial 0 finished with value: 0.6861586526770982 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:42:01,434]\u001b[0m Trial 1 finished with value: 0.5613614315365678 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:43:13,859]\u001b[0m Trial 2 finished with value: 0.6548407940168186 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:44:27,238]\u001b[0m Trial 3 finished with value: 0.6061062996204835 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:45:45,216]\u001b[0m Trial 4 finished with value: 0.005494939194962167 and parameters: {'C': 0.125, 'gamma': 4.0}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:47:04,095]\u001b[0m Trial 5 finished with value: 0.02978428406280259 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:48:17,893]\u001b[0m Trial 6 finished with value: 0.6755386034672826 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:49:36,001]\u001b[0m Trial 7 finished with value: 0.05647114140396432 and parameters: {'C': 0.5, 'gamma': 6.103515625e-05}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:50:53,837]\u001b[0m Trial 8 finished with value: 0.017827961389120885 and parameters: {'C': 0.25, 'gamma': 0.5}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:52:12,326]\u001b[0m Trial 9 finished with value: 0.05048864713897557 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:53:25,012]\u001b[0m Trial 10 finished with value: 0.6861586526770982 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:54:38,049]\u001b[0m Trial 11 finished with value: 0.6861586526770982 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:55:51,054]\u001b[0m Trial 12 finished with value: 0.6861586526770982 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:57:08,152]\u001b[0m Trial 13 finished with value: 0.3258674048709585 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:58:25,864]\u001b[0m Trial 14 finished with value: 0.08934543062449454 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:59:42,348]\u001b[0m Trial 15 finished with value: 0.5492320199191042 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:00:59,423]\u001b[0m Trial 16 finished with value: 0.028943539183224343 and parameters: {'C': 0.03125, 'gamma': 0.00048828125}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:02:17,387]\u001b[0m Trial 17 finished with value: -0.0015338144060247582 and parameters: {'C': 0.015625, 'gamma': 2.0}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:03:33,459]\u001b[0m Trial 18 finished with value: 0.5818711143424531 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:04:54,350]\u001b[0m Trial 19 finished with value: -0.0020461705685119293 and parameters: {'C': 0.0078125, 'gamma': 8.0}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:06:09,690]\u001b[0m Trial 20 finished with value: 0.47720958296868066 and parameters: {'C': 64.0, 'gamma': 3.0517578125e-05}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:07:22,501]\u001b[0m Trial 21 finished with value: 0.6861586526770982 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:08:40,999]\u001b[0m Trial 22 finished with value: 0.0387220654451227 and parameters: {'C': 64.0, 'gamma': 1.0}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:09:54,241]\u001b[0m Trial 23 finished with value: 0.6861586526770982 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:11:16,101]\u001b[0m Trial 24 finished with value: 0.6256573426200696 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:12:32,400]\u001b[0m Trial 25 finished with value: 0.6551149176674796 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:13:49,807]\u001b[0m Trial 26 finished with value: 0.05682414897892325 and parameters: {'C': 0.125, 'gamma': 0.125}. Best is trial 0 with value: 0.6861586526770982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:15:01,863]\u001b[0m Trial 27 finished with value: 0.6869174934677498 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:16:18,445]\u001b[0m Trial 28 finished with value: 0.3347971356874083 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:17:32,676]\u001b[0m Trial 29 finished with value: 0.5613614315365678 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:18:44,853]\u001b[0m Trial 30 finished with value: 0.6869174934677498 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:19:56,698]\u001b[0m Trial 31 finished with value: 0.6869174934677498 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:21:08,650]\u001b[0m Trial 32 finished with value: 0.6869174934677498 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:22:21,118]\u001b[0m Trial 33 finished with value: 0.6869174934677498 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:23:32,694]\u001b[0m Trial 34 finished with value: 0.6869174934677498 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:24:44,739]\u001b[0m Trial 35 finished with value: 0.6869174934677498 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:25:56,778]\u001b[0m Trial 36 finished with value: 0.6711029784829792 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:27:09,105]\u001b[0m Trial 37 finished with value: 0.6209333418991678 and parameters: {'C': 1.0, 'gamma': 0.0078125}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:28:26,077]\u001b[0m Trial 38 finished with value: 0.16714796492984121 and parameters: {'C': 2.0, 'gamma': 6.103515625e-05}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:29:42,847]\u001b[0m Trial 39 finished with value: 0.28293740812871643 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:31:01,226]\u001b[0m Trial 40 finished with value: 0.029655596615776502 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:32:13,080]\u001b[0m Trial 41 finished with value: 0.6869174934677498 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:33:24,884]\u001b[0m Trial 42 finished with value: 0.6869174934677498 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:34:38,587]\u001b[0m Trial 43 finished with value: 0.5317915530276618 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 11:35:53,241]\u001b[0m Trial 44 finished with value: 0.548334106132505 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:37:11,638]\u001b[0m Trial 45 finished with value: 0.10057320417515118 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:38:29,600]\u001b[0m Trial 46 finished with value: 0.050773362055710314 and parameters: {'C': 0.5, 'gamma': 0.25}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:39:47,876]\u001b[0m Trial 47 finished with value: 0.15530000491415946 and parameters: {'C': 0.0625, 'gamma': 0.001953125}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:41:05,767]\u001b[0m Trial 48 finished with value: 0.24944828210324585 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:42:23,645]\u001b[0m Trial 49 finished with value: 0.03212709981391596 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6869\n",
      "\tBest params:\n",
      "\t\tC: 2.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.679806\n",
      "1                    TP  386.000000\n",
      "2                    TN  363.000000\n",
      "3                    FP   81.000000\n",
      "4                    FN   69.000000\n",
      "5              Accuracy    0.833148\n",
      "6             Precision    0.826552\n",
      "7           Sensitivity    0.848352\n",
      "8           Specificity    0.817600\n",
      "9              F1 score    0.837310\n",
      "10  F1 score (weighted)    0.833091\n",
      "11     F1 score (macro)    0.833039\n",
      "12    Balanced Accuracy    0.832960\n",
      "13                  MCC    0.666375\n",
      "14                  NPV    0.840300\n",
      "15              ROC_AUC    0.832960\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_0_cat = np.where((y_pred_svm_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 11:43:51,001]\u001b[0m Trial 50 finished with value: 0.02932827686226521 and parameters: {'C': 0.03125, 'gamma': 0.00048828125}. Best is trial 27 with value: 0.6869174934677498.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:45:02,763]\u001b[0m Trial 51 finished with value: 0.6879162878794457 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:46:14,216]\u001b[0m Trial 52 finished with value: 0.6879162878794457 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:47:32,361]\u001b[0m Trial 53 finished with value: 0.07862763779587414 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:48:49,942]\u001b[0m Trial 54 finished with value: -8.58716432976525e-06 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:50:04,990]\u001b[0m Trial 55 finished with value: 0.4795327109168598 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:51:23,710]\u001b[0m Trial 56 finished with value: 0.053394001544875155 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:52:44,086]\u001b[0m Trial 57 finished with value: 0.0394233323711624 and parameters: {'C': 1.0, 'gamma': 8.0}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:53:55,287]\u001b[0m Trial 58 finished with value: 0.6879162878794457 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:55:07,352]\u001b[0m Trial 59 finished with value: 0.6879162878794457 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:56:24,193]\u001b[0m Trial 60 finished with value: 0.31639426145531396 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:57:36,182]\u001b[0m Trial 61 finished with value: 0.6879162878794457 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:58:48,223]\u001b[0m Trial 62 finished with value: 0.6879162878794457 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:00:00,038]\u001b[0m Trial 63 finished with value: 0.6879162878794457 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:01:16,177]\u001b[0m Trial 64 finished with value: 0.3510558047317956 and parameters: {'C': 0.25, 'gamma': 0.0625}. Best is trial 51 with value: 0.6879162878794457.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:02:29,324]\u001b[0m Trial 65 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:03:41,145]\u001b[0m Trial 66 finished with value: 0.6776422872977018 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:04:54,018]\u001b[0m Trial 67 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:06:11,933]\u001b[0m Trial 68 finished with value: 0.2508729757741432 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:07:25,023]\u001b[0m Trial 69 finished with value: 0.6774697710796704 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:08:37,379]\u001b[0m Trial 70 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:09:50,275]\u001b[0m Trial 71 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:11:03,540]\u001b[0m Trial 72 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:12:16,130]\u001b[0m Trial 73 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:13:35,018]\u001b[0m Trial 74 finished with value: 0.04042290353502796 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:14:48,245]\u001b[0m Trial 75 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:16:06,995]\u001b[0m Trial 76 finished with value: 0.10197997881653867 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:19,208]\u001b[0m Trial 77 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:18:32,820]\u001b[0m Trial 78 finished with value: 0.4766915333802541 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:47,744]\u001b[0m Trial 79 finished with value: 0.3355245843544798 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:21:04,551]\u001b[0m Trial 80 finished with value: 0.04461879608163277 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:16,036]\u001b[0m Trial 81 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:27,481]\u001b[0m Trial 82 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:24:38,726]\u001b[0m Trial 83 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:25:54,907]\u001b[0m Trial 84 finished with value: 0.16786667875412187 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:27:06,035]\u001b[0m Trial 85 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:22,489]\u001b[0m Trial 86 finished with value: 0.2881093054829359 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:39,756]\u001b[0m Trial 87 finished with value: 0.06607126622116596 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:55,145]\u001b[0m Trial 88 finished with value: 0.6157784403777844 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:10,166]\u001b[0m Trial 89 finished with value: 0.41240698336556303 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:21,681]\u001b[0m Trial 90 finished with value: 0.6925039208034811 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:32,752]\u001b[0m Trial 91 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:35:44,180]\u001b[0m Trial 92 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:36:55,430]\u001b[0m Trial 93 finished with value: 0.6949151893739066 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:38:11,971]\u001b[0m Trial 94 finished with value: 0.0360282582536241 and parameters: {'C': 0.5, 'gamma': 1.0}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:39:28,211]\u001b[0m Trial 95 finished with value: 0.09730018233772866 and parameters: {'C': 0.0625, 'gamma': 0.0009765625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:40:47,973]\u001b[0m Trial 96 finished with value: 0.03976717717855559 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:42:04,552]\u001b[0m Trial 97 finished with value: 0.22062189342318583 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:43:16,729]\u001b[0m Trial 98 finished with value: 0.6925464892049689 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:44:33,462]\u001b[0m Trial 99 finished with value: 0.083926300636999 and parameters: {'C': 0.015625, 'gamma': 0.00390625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6949\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.679806    0.705457\n",
      "1                    TP  386.000000  403.000000\n",
      "2                    TN  363.000000  352.000000\n",
      "3                    FP   81.000000   64.000000\n",
      "4                    FN   69.000000   80.000000\n",
      "5              Accuracy    0.833148    0.839822\n",
      "6             Precision    0.826552    0.862955\n",
      "7           Sensitivity    0.848352    0.834369\n",
      "8           Specificity    0.817600    0.846200\n",
      "9              F1 score    0.837310    0.848421\n",
      "10  F1 score (weighted)    0.833091    0.839984\n",
      "11     F1 score (macro)    0.833039    0.839305\n",
      "12    Balanced Accuracy    0.832960    0.840261\n",
      "13                  MCC    0.666375    0.679145\n",
      "14                  NPV    0.840300    0.814800\n",
      "15              ROC_AUC    0.832960    0.840261\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_1_cat = np.where((y_pred_svm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:46:00,220]\u001b[0m Trial 100 finished with value: 0.08004737746695395 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 65 with value: 0.6949151893739066.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:47:11,542]\u001b[0m Trial 101 finished with value: 0.7029789511449824 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:48:22,748]\u001b[0m Trial 102 finished with value: 0.7029789511449824 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:49:33,953]\u001b[0m Trial 103 finished with value: 0.7029789511449824 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:45,207]\u001b[0m Trial 104 finished with value: 0.7029789511449824 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:59,335]\u001b[0m Trial 105 finished with value: 0.5724893215021546 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:53:10,903]\u001b[0m Trial 106 finished with value: 0.7010826241741454 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:54:22,436]\u001b[0m Trial 107 finished with value: 0.6850591956251424 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:55:36,204]\u001b[0m Trial 108 finished with value: 0.4262086586624034 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:48,563]\u001b[0m Trial 109 finished with value: 0.6909494889213389 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:58:04,121]\u001b[0m Trial 110 finished with value: 0.3355642750293944 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:59:15,148]\u001b[0m Trial 111 finished with value: 0.7029789511449824 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:00:25,497]\u001b[0m Trial 112 finished with value: 0.6688563300858517 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:01:36,745]\u001b[0m Trial 113 finished with value: 0.7029789511449824 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:02:54,603]\u001b[0m Trial 114 finished with value: 0.047886358922528585 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:05,788]\u001b[0m Trial 115 finished with value: 0.7029789511449824 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:05:18,048]\u001b[0m Trial 116 finished with value: 0.5275668698496281 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:29,568]\u001b[0m Trial 117 finished with value: 0.699249866399613 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:07:46,717]\u001b[0m Trial 118 finished with value: 0.10603154297382197 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:09:00,741]\u001b[0m Trial 119 finished with value: 0.47607580837349517 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:10:14,515]\u001b[0m Trial 120 finished with value: 0.5560684009761694 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:11:25,108]\u001b[0m Trial 121 finished with value: 0.7029789511449824 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:12:36,765]\u001b[0m Trial 122 finished with value: 0.7029789511449824 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:13:49,517]\u001b[0m Trial 123 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:15:06,071]\u001b[0m Trial 124 finished with value: 0.3235526810872371 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:16:18,651]\u001b[0m Trial 125 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:17:37,350]\u001b[0m Trial 126 finished with value: 0.05135375324159275 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:18:54,030]\u001b[0m Trial 127 finished with value: 0.4135679386448398 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:20:06,194]\u001b[0m Trial 128 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:21:23,732]\u001b[0m Trial 129 finished with value: 0.2938529765144423 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:22:42,524]\u001b[0m Trial 130 finished with value: 0.0705367390908301 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:23:55,307]\u001b[0m Trial 131 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:25:08,365]\u001b[0m Trial 132 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:26:21,523]\u001b[0m Trial 133 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:27:34,443]\u001b[0m Trial 134 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:28:51,906]\u001b[0m Trial 135 finished with value: 0.4111110376710078 and parameters: {'C': 0.5, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:30:06,257]\u001b[0m Trial 136 finished with value: 0.6978706328037896 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:31:24,476]\u001b[0m Trial 137 finished with value: 0.5559357141276255 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:32:47,530]\u001b[0m Trial 138 finished with value: 0.04735674742445038 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:34:06,255]\u001b[0m Trial 139 finished with value: 0.000590447527849347 and parameters: {'C': 0.015625, 'gamma': 1.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:35:26,083]\u001b[0m Trial 140 finished with value: 0.05646508353179438 and parameters: {'C': 0.03125, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:36:40,635]\u001b[0m Trial 141 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:37:55,435]\u001b[0m Trial 142 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:39:10,033]\u001b[0m Trial 143 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:40:25,299]\u001b[0m Trial 144 finished with value: 0.6985504711051671 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:41:46,174]\u001b[0m Trial 145 finished with value: 0.08004737746695395 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:43:04,757]\u001b[0m Trial 146 finished with value: 0.4262086586624034 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:44:19,036]\u001b[0m Trial 147 finished with value: 0.699249866399613 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:45:36,795]\u001b[0m Trial 148 finished with value: 0.571947394947647 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:46:52,226]\u001b[0m Trial 149 finished with value: 0.699249866399613 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7030\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.679806    0.705457    0.705767\n",
      "1                    TP  386.000000  403.000000  408.000000\n",
      "2                    TN  363.000000  352.000000  338.000000\n",
      "3                    FP   81.000000   64.000000   85.000000\n",
      "4                    FN   69.000000   80.000000   68.000000\n",
      "5              Accuracy    0.833148    0.839822    0.829811\n",
      "6             Precision    0.826552    0.862955    0.827586\n",
      "7           Sensitivity    0.848352    0.834369    0.857143\n",
      "8           Specificity    0.817600    0.846200    0.799100\n",
      "9              F1 score    0.837310    0.848421    0.842105\n",
      "10  F1 score (weighted)    0.833091    0.839984    0.829559\n",
      "11     F1 score (macro)    0.833039    0.839305    0.828773\n",
      "12    Balanced Accuracy    0.832960    0.840261    0.828099\n",
      "13                  MCC    0.666375    0.679145    0.658145\n",
      "14                  NPV    0.840300    0.814800    0.832500\n",
      "15              ROC_AUC    0.832960    0.840261    0.828099\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_2_cat = np.where((y_pred_svm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:48:20,317]\u001b[0m Trial 150 finished with value: 0.6658005092943367 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:49:34,543]\u001b[0m Trial 151 finished with value: 0.6963652779338455 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:50:48,602]\u001b[0m Trial 152 finished with value: 0.6963652779338455 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:52:03,243]\u001b[0m Trial 153 finished with value: 0.6963652779338455 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:53:18,244]\u001b[0m Trial 154 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:54:32,205]\u001b[0m Trial 155 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:55:51,844]\u001b[0m Trial 156 finished with value: 0.2522316563179969 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:57:06,840]\u001b[0m Trial 157 finished with value: 0.6857738168575647 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:58:21,680]\u001b[0m Trial 158 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:59:36,736]\u001b[0m Trial 159 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:00:53,330]\u001b[0m Trial 160 finished with value: 0.6842046148968647 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:02:07,370]\u001b[0m Trial 161 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:03:22,236]\u001b[0m Trial 162 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:04:36,939]\u001b[0m Trial 163 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:05:58,606]\u001b[0m Trial 164 finished with value: 0.038451522079240505 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:07:11,955]\u001b[0m Trial 165 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:08:25,151]\u001b[0m Trial 166 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:09:38,410]\u001b[0m Trial 167 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:10:58,982]\u001b[0m Trial 168 finished with value: 0.09877837083508327 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:12:17,976]\u001b[0m Trial 169 finished with value: 0.336620307051445 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:13:35,050]\u001b[0m Trial 170 finished with value: 0.4828733256842934 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:14:48,632]\u001b[0m Trial 171 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:16:01,893]\u001b[0m Trial 172 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:17:15,850]\u001b[0m Trial 173 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:18:29,964]\u001b[0m Trial 174 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:19:44,412]\u001b[0m Trial 175 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:20:58,619]\u001b[0m Trial 176 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:22:17,126]\u001b[0m Trial 177 finished with value: 0.040902360370798016 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:23:33,964]\u001b[0m Trial 178 finished with value: 0.2922037782734489 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:24:51,550]\u001b[0m Trial 179 finished with value: 0.17104008123919295 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:26:04,203]\u001b[0m Trial 180 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:27:16,641]\u001b[0m Trial 181 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:28:28,671]\u001b[0m Trial 182 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:29:41,121]\u001b[0m Trial 183 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:30:59,764]\u001b[0m Trial 184 finished with value: 0.06030578427343174 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:32:11,909]\u001b[0m Trial 185 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:33:29,134]\u001b[0m Trial 186 finished with value: 0.25123291664193675 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:34:43,117]\u001b[0m Trial 187 finished with value: 0.5843786009418175 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:35:56,019]\u001b[0m Trial 188 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:37:08,951]\u001b[0m Trial 189 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:38:23,986]\u001b[0m Trial 190 finished with value: 0.5356489816934866 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:39:36,611]\u001b[0m Trial 191 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:40:48,873]\u001b[0m Trial 192 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:42:00,898]\u001b[0m Trial 193 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:43:15,538]\u001b[0m Trial 194 finished with value: 0.5321199454996723 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:36,295]\u001b[0m Trial 195 finished with value: 0.03810957774990008 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:47,482]\u001b[0m Trial 196 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:57,729]\u001b[0m Trial 197 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:08,953]\u001b[0m Trial 198 finished with value: 0.7013525414401401 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:27,125]\u001b[0m Trial 199 finished with value: 0.003546801471684502 and parameters: {'C': 0.0625, 'gamma': 1.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7030\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.679806    0.705457    0.705767    0.689205\n",
      "1                    TP  386.000000  403.000000  408.000000  396.000000\n",
      "2                    TN  363.000000  352.000000  338.000000  346.000000\n",
      "3                    FP   81.000000   64.000000   85.000000   80.000000\n",
      "4                    FN   69.000000   80.000000   68.000000   77.000000\n",
      "5              Accuracy    0.833148    0.839822    0.829811    0.825362\n",
      "6             Precision    0.826552    0.862955    0.827586    0.831933\n",
      "7           Sensitivity    0.848352    0.834369    0.857143    0.837209\n",
      "8           Specificity    0.817600    0.846200    0.799100    0.812200\n",
      "9              F1 score    0.837310    0.848421    0.842105    0.834563\n",
      "10  F1 score (weighted)    0.833091    0.839984    0.829559    0.825329\n",
      "11     F1 score (macro)    0.833039    0.839305    0.828773    0.824820\n",
      "12    Balanced Accuracy    0.832960    0.840261    0.828099    0.824708\n",
      "13                  MCC    0.666375    0.679145    0.658145    0.649658\n",
      "14                  NPV    0.840300    0.814800    0.832500    0.818000\n",
      "15              ROC_AUC    0.832960    0.840261    0.828099    0.824708\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_3_cat = np.where((y_pred_svm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:50:50,207]\u001b[0m Trial 200 finished with value: 0.6224760934146432 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:03,810]\u001b[0m Trial 201 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:17,513]\u001b[0m Trial 202 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:31,084]\u001b[0m Trial 203 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:44,861]\u001b[0m Trial 204 finished with value: 0.6451326005628639 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:56,097]\u001b[0m Trial 205 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:08,727]\u001b[0m Trial 206 finished with value: 0.6856028054628626 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:21,282]\u001b[0m Trial 207 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:33,031]\u001b[0m Trial 208 finished with value: 0.6857093703022181 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:50,837]\u001b[0m Trial 209 finished with value: 0.12010570991119236 and parameters: {'C': 0.015625, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:02,303]\u001b[0m Trial 210 finished with value: 0.6786244060556543 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:14,449]\u001b[0m Trial 211 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:28,226]\u001b[0m Trial 212 finished with value: 0.5700401528542094 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:45,320]\u001b[0m Trial 213 finished with value: 0.2221585297971699 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:56,944]\u001b[0m Trial 214 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:08,581]\u001b[0m Trial 215 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:19,986]\u001b[0m Trial 216 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:11:31,927]\u001b[0m Trial 217 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:12:47,345]\u001b[0m Trial 218 finished with value: 0.01493935492128049 and parameters: {'C': 0.125, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:14:02,035]\u001b[0m Trial 219 finished with value: 0.07865109964738971 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:15:11,640]\u001b[0m Trial 220 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:16:21,369]\u001b[0m Trial 221 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:17:31,204]\u001b[0m Trial 222 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:18:46,518]\u001b[0m Trial 223 finished with value: 0.03717748080703309 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:19:56,202]\u001b[0m Trial 224 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:21:06,274]\u001b[0m Trial 225 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:22:20,922]\u001b[0m Trial 226 finished with value: 0.10285457019921904 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:23:30,426]\u001b[0m Trial 227 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:24:38,679]\u001b[0m Trial 228 finished with value: 0.6770772639020832 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:25:51,794]\u001b[0m Trial 229 finished with value: 0.33776928946145557 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:27:03,367]\u001b[0m Trial 230 finished with value: 0.48318533005986986 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:28:12,501]\u001b[0m Trial 231 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:29:21,665]\u001b[0m Trial 232 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:30:30,756]\u001b[0m Trial 233 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:31:39,729]\u001b[0m Trial 234 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:32:53,793]\u001b[0m Trial 235 finished with value: 0.04062984592191422 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:34:04,360]\u001b[0m Trial 236 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:35:19,649]\u001b[0m Trial 237 finished with value: 0.11107023147020394 and parameters: {'C': 0.25, 'gamma': 0.125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:36:35,409]\u001b[0m Trial 238 finished with value: 0.17014793817500345 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:37:46,332]\u001b[0m Trial 239 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:38:55,423]\u001b[0m Trial 240 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:40:04,471]\u001b[0m Trial 241 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:41:13,490]\u001b[0m Trial 242 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:42:22,517]\u001b[0m Trial 243 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:43:31,552]\u001b[0m Trial 244 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:44:42,234]\u001b[0m Trial 245 finished with value: 0.5915340267710907 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:45:54,765]\u001b[0m Trial 246 finished with value: 0.3262716573735622 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:47:03,703]\u001b[0m Trial 247 finished with value: 0.6999351494536421 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:48:18,314]\u001b[0m Trial 248 finished with value: 0.3345033069557709 and parameters: {'C': 0.5, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:49:30,632]\u001b[0m Trial 249 finished with value: 0.4179094714458106 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7030\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.679806    0.705457    0.705767    0.689205   \n",
      "1                    TP  386.000000  403.000000  408.000000  396.000000   \n",
      "2                    TN  363.000000  352.000000  338.000000  346.000000   \n",
      "3                    FP   81.000000   64.000000   85.000000   80.000000   \n",
      "4                    FN   69.000000   80.000000   68.000000   77.000000   \n",
      "5              Accuracy    0.833148    0.839822    0.829811    0.825362   \n",
      "6             Precision    0.826552    0.862955    0.827586    0.831933   \n",
      "7           Sensitivity    0.848352    0.834369    0.857143    0.837209   \n",
      "8           Specificity    0.817600    0.846200    0.799100    0.812200   \n",
      "9              F1 score    0.837310    0.848421    0.842105    0.834563   \n",
      "10  F1 score (weighted)    0.833091    0.839984    0.829559    0.825329   \n",
      "11     F1 score (macro)    0.833039    0.839305    0.828773    0.824820   \n",
      "12    Balanced Accuracy    0.832960    0.840261    0.828099    0.824708   \n",
      "13                  MCC    0.666375    0.679145    0.658145    0.649658   \n",
      "14                  NPV    0.840300    0.814800    0.832500    0.818000   \n",
      "15              ROC_AUC    0.832960    0.840261    0.828099    0.824708   \n",
      "\n",
      "          Set4  \n",
      "0     0.661992  \n",
      "1   393.000000  \n",
      "2   357.000000  \n",
      "3    73.000000  \n",
      "4    76.000000  \n",
      "5     0.834260  \n",
      "6     0.843348  \n",
      "7     0.837953  \n",
      "8     0.830200  \n",
      "9     0.840642  \n",
      "10    0.834282  \n",
      "11    0.833994  \n",
      "12    0.834093  \n",
      "13    0.668007  \n",
      "14    0.824500  \n",
      "15    0.834093  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_4_cat = np.where((y_pred_svm_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:50:48,619]\u001b[0m Trial 250 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:52:04,369]\u001b[0m Trial 251 finished with value: 0.05290149858627372 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:53:20,991]\u001b[0m Trial 252 finished with value: 0.04074616516171871 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:54:31,033]\u001b[0m Trial 253 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:55:41,857]\u001b[0m Trial 254 finished with value: 0.6845129636011018 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:57:00,691]\u001b[0m Trial 255 finished with value: -0.0011037055710534816 and parameters: {'C': 0.015625, 'gamma': 8.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:58:10,731]\u001b[0m Trial 256 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:59:26,035]\u001b[0m Trial 257 finished with value: 0.22014400946962934 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:00:35,882]\u001b[0m Trial 258 finished with value: 0.6269837472637496 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:01:44,334]\u001b[0m Trial 259 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:02:52,891]\u001b[0m Trial 260 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:04:01,373]\u001b[0m Trial 261 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:05:16,266]\u001b[0m Trial 262 finished with value: 0.018628814305632603 and parameters: {'C': 0.0078125, 'gamma': 0.0625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:06:26,405]\u001b[0m Trial 263 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:07:36,381]\u001b[0m Trial 264 finished with value: 0.6746061422237943 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:08:44,546]\u001b[0m Trial 265 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:09:53,325]\u001b[0m Trial 266 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:11:07,457]\u001b[0m Trial 267 finished with value: 0.249713193142366 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:12:16,343]\u001b[0m Trial 268 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:13:26,121]\u001b[0m Trial 269 finished with value: 0.6757713065458079 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:14:37,819]\u001b[0m Trial 270 finished with value: 0.4266542325760356 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:15:53,241]\u001b[0m Trial 271 finished with value: 0.02979721063130597 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:17:02,668]\u001b[0m Trial 272 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:18:11,883]\u001b[0m Trial 273 finished with value: 0.6630869330015571 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:19:21,337]\u001b[0m Trial 274 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:20:32,222]\u001b[0m Trial 275 finished with value: 0.5251191678210045 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:21:46,045]\u001b[0m Trial 276 finished with value: 0.3319568974521926 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:22:57,424]\u001b[0m Trial 277 finished with value: 0.5174904463975863 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:24:13,207]\u001b[0m Trial 278 finished with value: 0.08997051948792116 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:25:22,190]\u001b[0m Trial 279 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:26:30,921]\u001b[0m Trial 280 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:27:39,685]\u001b[0m Trial 281 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:28:54,731]\u001b[0m Trial 282 finished with value: 0.03318089670658229 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:30:07,880]\u001b[0m Trial 283 finished with value: 0.32181477676638615 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:31:21,832]\u001b[0m Trial 284 finished with value: 0.28139832979324614 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:32:36,326]\u001b[0m Trial 285 finished with value: 0.02955884262975006 and parameters: {'C': 0.5, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:33:45,152]\u001b[0m Trial 286 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:34:54,864]\u001b[0m Trial 287 finished with value: 0.6843402738718578 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:36:12,932]\u001b[0m Trial 288 finished with value: 0.6320719339408889 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:37:28,540]\u001b[0m Trial 289 finished with value: 0.05290181383308582 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:38:43,569]\u001b[0m Trial 290 finished with value: 0.13620607385824296 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:39:56,443]\u001b[0m Trial 291 finished with value: 0.4086394810605424 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:41:10,467]\u001b[0m Trial 292 finished with value: 0.22014400946962934 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:42:19,514]\u001b[0m Trial 293 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:43:37,820]\u001b[0m Trial 294 finished with value: 0.02927460156152497 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:44:46,732]\u001b[0m Trial 295 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:45:55,723]\u001b[0m Trial 296 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:47:04,460]\u001b[0m Trial 297 finished with value: 0.6944542251677415 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:48:19,063]\u001b[0m Trial 298 finished with value: 0.07800436330366303 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:49:30,167]\u001b[0m Trial 299 finished with value: 0.5241956781365242 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7030\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.679806    0.705457    0.705767    0.689205   \n",
      "1                    TP  386.000000  403.000000  408.000000  396.000000   \n",
      "2                    TN  363.000000  352.000000  338.000000  346.000000   \n",
      "3                    FP   81.000000   64.000000   85.000000   80.000000   \n",
      "4                    FN   69.000000   80.000000   68.000000   77.000000   \n",
      "5              Accuracy    0.833148    0.839822    0.829811    0.825362   \n",
      "6             Precision    0.826552    0.862955    0.827586    0.831933   \n",
      "7           Sensitivity    0.848352    0.834369    0.857143    0.837209   \n",
      "8           Specificity    0.817600    0.846200    0.799100    0.812200   \n",
      "9              F1 score    0.837310    0.848421    0.842105    0.834563   \n",
      "10  F1 score (weighted)    0.833091    0.839984    0.829559    0.825329   \n",
      "11     F1 score (macro)    0.833039    0.839305    0.828773    0.824820   \n",
      "12    Balanced Accuracy    0.832960    0.840261    0.828099    0.824708   \n",
      "13                  MCC    0.666375    0.679145    0.658145    0.649658   \n",
      "14                  NPV    0.840300    0.814800    0.832500    0.818000   \n",
      "15              ROC_AUC    0.832960    0.840261    0.828099    0.824708   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.661992    0.692112  \n",
      "1   393.000000  409.000000  \n",
      "2   357.000000  356.000000  \n",
      "3    73.000000   67.000000  \n",
      "4    76.000000   67.000000  \n",
      "5     0.834260    0.850945  \n",
      "6     0.843348    0.859244  \n",
      "7     0.837953    0.859244  \n",
      "8     0.830200    0.841600  \n",
      "9     0.840642    0.859244  \n",
      "10    0.834282    0.850945  \n",
      "11    0.833994    0.850426  \n",
      "12    0.834093    0.850426  \n",
      "13    0.668007    0.700851  \n",
      "14    0.824500    0.841600  \n",
      "15    0.834093    0.850426  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_5_cat = np.where((y_pred_svm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:50:54,533]\u001b[0m Trial 300 finished with value: 0.011101421796047128 and parameters: {'C': 0.125, 'gamma': 1.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:52:04,402]\u001b[0m Trial 301 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:53:15,041]\u001b[0m Trial 302 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:54:28,434]\u001b[0m Trial 303 finished with value: 0.5612531937021741 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:55:39,556]\u001b[0m Trial 304 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:56:51,737]\u001b[0m Trial 305 finished with value: 0.55701782485736 and parameters: {'C': 1.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:58:00,702]\u001b[0m Trial 306 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:59:09,722]\u001b[0m Trial 307 finished with value: 0.6778791885946118 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:00:18,672]\u001b[0m Trial 308 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:01:30,324]\u001b[0m Trial 309 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:02:47,427]\u001b[0m Trial 310 finished with value: 0.02991316586780409 and parameters: {'C': 0.25, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:03:59,040]\u001b[0m Trial 311 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:05:11,959]\u001b[0m Trial 312 finished with value: 0.679012137670207 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:06:23,704]\u001b[0m Trial 313 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:07:35,444]\u001b[0m Trial 314 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:08:46,709]\u001b[0m Trial 315 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:10:01,590]\u001b[0m Trial 316 finished with value: 0.32127812757844076 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:11:16,312]\u001b[0m Trial 317 finished with value: 0.09356443943938841 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:12:25,485]\u001b[0m Trial 318 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:13:40,163]\u001b[0m Trial 319 finished with value: 0.030621742318443203 and parameters: {'C': 0.5, 'gamma': 4.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:14:49,483]\u001b[0m Trial 320 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:16:01,238]\u001b[0m Trial 321 finished with value: 0.48308156481467923 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:17:10,581]\u001b[0m Trial 322 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:18:20,164]\u001b[0m Trial 323 finished with value: 0.6877236688630417 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:19:33,723]\u001b[0m Trial 324 finished with value: 0.16860332657906238 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:20:47,401]\u001b[0m Trial 325 finished with value: 0.2188262717488813 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:22:00,890]\u001b[0m Trial 326 finished with value: -0.001095331554835799 and parameters: {'C': 0.015625, 'gamma': 2.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:23:13,552]\u001b[0m Trial 327 finished with value: 0.33516648878511124 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:24:22,369]\u001b[0m Trial 328 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:25:31,175]\u001b[0m Trial 329 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:26:44,415]\u001b[0m Trial 330 finished with value: 0.27818493859296506 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:27:54,916]\u001b[0m Trial 331 finished with value: 0.5824484489252255 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:29:08,649]\u001b[0m Trial 332 finished with value: 0.07813913579154323 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:30:17,110]\u001b[0m Trial 333 finished with value: 0.6919639137070653 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:31:26,170]\u001b[0m Trial 334 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:32:40,450]\u001b[0m Trial 335 finished with value: 0.06039155758414163 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:33:51,245]\u001b[0m Trial 336 finished with value: 0.5368877571004261 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:35:05,113]\u001b[0m Trial 337 finished with value: 0.05664499360020716 and parameters: {'C': 0.125, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:36:14,214]\u001b[0m Trial 338 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:37:28,689]\u001b[0m Trial 339 finished with value: 0.04996708921457851 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:38:37,783]\u001b[0m Trial 340 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:39:46,904]\u001b[0m Trial 341 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:40:55,388]\u001b[0m Trial 342 finished with value: 0.6642822335362533 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:42:12,368]\u001b[0m Trial 343 finished with value: 0.040696061253069314 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 17:43:22,151]\u001b[0m Trial 344 finished with value: 0.628315967176029 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:44:30,964]\u001b[0m Trial 345 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:45:39,842]\u001b[0m Trial 346 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:46:51,687]\u001b[0m Trial 347 finished with value: 0.3451610009460665 and parameters: {'C': 0.25, 'gamma': 0.0625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:48:00,636]\u001b[0m Trial 348 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:49:09,520]\u001b[0m Trial 349 finished with value: 0.6992537545690346 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7029789511449824.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7030\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.679806    0.705457    0.705767    0.689205   \n",
      "1                    TP  386.000000  403.000000  408.000000  396.000000   \n",
      "2                    TN  363.000000  352.000000  338.000000  346.000000   \n",
      "3                    FP   81.000000   64.000000   85.000000   80.000000   \n",
      "4                    FN   69.000000   80.000000   68.000000   77.000000   \n",
      "5              Accuracy    0.833148    0.839822    0.829811    0.825362   \n",
      "6             Precision    0.826552    0.862955    0.827586    0.831933   \n",
      "7           Sensitivity    0.848352    0.834369    0.857143    0.837209   \n",
      "8           Specificity    0.817600    0.846200    0.799100    0.812200   \n",
      "9              F1 score    0.837310    0.848421    0.842105    0.834563   \n",
      "10  F1 score (weighted)    0.833091    0.839984    0.829559    0.825329   \n",
      "11     F1 score (macro)    0.833039    0.839305    0.828773    0.824820   \n",
      "12    Balanced Accuracy    0.832960    0.840261    0.828099    0.824708   \n",
      "13                  MCC    0.666375    0.679145    0.658145    0.649658   \n",
      "14                  NPV    0.840300    0.814800    0.832500    0.818000   \n",
      "15              ROC_AUC    0.832960    0.840261    0.828099    0.824708   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.661992    0.692112    0.701017  \n",
      "1   393.000000  409.000000  399.000000  \n",
      "2   357.000000  356.000000  360.000000  \n",
      "3    73.000000   67.000000   67.000000  \n",
      "4    76.000000   67.000000   73.000000  \n",
      "5     0.834260    0.850945    0.844271  \n",
      "6     0.843348    0.859244    0.856223  \n",
      "7     0.837953    0.859244    0.845339  \n",
      "8     0.830200    0.841600    0.843100  \n",
      "9     0.840642    0.859244    0.850746  \n",
      "10    0.834282    0.850945    0.844317  \n",
      "11    0.833994    0.850426    0.843978  \n",
      "12    0.834093    0.850426    0.844215  \n",
      "13    0.668007    0.700851    0.688031  \n",
      "14    0.824500    0.841600    0.831400  \n",
      "15    0.834093    0.850426    0.844215  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_6_cat = np.where((y_pred_svm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 17:50:28,177]\u001b[0m Trial 350 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:51:37,582]\u001b[0m Trial 351 finished with value: 0.697400772262081 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:52:46,686]\u001b[0m Trial 352 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:53:55,723]\u001b[0m Trial 353 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:55:04,204]\u001b[0m Trial 354 finished with value: 0.6841101862459678 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:56:16,771]\u001b[0m Trial 355 finished with value: 0.3207164019991003 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:57:30,314]\u001b[0m Trial 356 finished with value: 0.24822231500434797 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:58:39,526]\u001b[0m Trial 357 finished with value: 0.6208809515735229 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:59:48,530]\u001b[0m Trial 358 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:01:03,152]\u001b[0m Trial 359 finished with value: 0.03585126132490231 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:02:13,282]\u001b[0m Trial 360 finished with value: 0.687716971718182 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:03:22,963]\u001b[0m Trial 361 finished with value: 0.6922861809234678 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:04:32,545]\u001b[0m Trial 362 finished with value: 0.6920979073822359 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:05:41,249]\u001b[0m Trial 363 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:06:54,856]\u001b[0m Trial 364 finished with value: 0.09756830896244824 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:08:08,908]\u001b[0m Trial 365 finished with value: 0.13394949397847253 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:09:22,436]\u001b[0m Trial 366 finished with value: 0.21830724438447593 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:10:30,838]\u001b[0m Trial 367 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:11:44,510]\u001b[0m Trial 368 finished with value: 0.03904191551575685 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:12:56,672]\u001b[0m Trial 369 finished with value: 0.33262964662006844 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:14:09,761]\u001b[0m Trial 370 finished with value: 0.0035384658586207094 and parameters: {'C': 0.0078125, 'gamma': 0.00048828125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:15:17,976]\u001b[0m Trial 371 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:16:31,053]\u001b[0m Trial 372 finished with value: 0.09926137550437772 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:17:39,380]\u001b[0m Trial 373 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:18:47,684]\u001b[0m Trial 374 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:19:56,065]\u001b[0m Trial 375 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:21:08,757]\u001b[0m Trial 376 finished with value: 0.28997986169669937 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:22:17,021]\u001b[0m Trial 377 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:23:25,231]\u001b[0m Trial 378 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:24:35,676]\u001b[0m Trial 379 finished with value: 0.428677237207331 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:25:45,573]\u001b[0m Trial 380 finished with value: 0.5870471861099265 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:26:53,887]\u001b[0m Trial 381 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:28:02,134]\u001b[0m Trial 382 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:29:10,428]\u001b[0m Trial 383 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:30:18,780]\u001b[0m Trial 384 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:31:30,353]\u001b[0m Trial 385 finished with value: 0.4153177471653608 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:32:43,859]\u001b[0m Trial 386 finished with value: 0.05902278043288559 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:33:55,563]\u001b[0m Trial 387 finished with value: 0.41455271065298777 and parameters: {'C': 1.0, 'gamma': 0.0009765625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:35:03,958]\u001b[0m Trial 388 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:36:12,199]\u001b[0m Trial 389 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:37:20,513]\u001b[0m Trial 390 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:38:28,839]\u001b[0m Trial 391 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:39:45,293]\u001b[0m Trial 392 finished with value: 0.035368936991730285 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:40:53,674]\u001b[0m Trial 393 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 18:42:03,051]\u001b[0m Trial 394 finished with value: 0.5341605667786775 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:43:16,550]\u001b[0m Trial 395 finished with value: 0.046611089985133594 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:44:27,219]\u001b[0m Trial 396 finished with value: 0.697400772262081 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:45:37,730]\u001b[0m Trial 397 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:46:45,975]\u001b[0m Trial 398 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:47:54,303]\u001b[0m Trial 399 finished with value: 0.7041860882767477 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7042\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.679806    0.705457    0.705767    0.689205   \n",
      "1                    TP  386.000000  403.000000  408.000000  396.000000   \n",
      "2                    TN  363.000000  352.000000  338.000000  346.000000   \n",
      "3                    FP   81.000000   64.000000   85.000000   80.000000   \n",
      "4                    FN   69.000000   80.000000   68.000000   77.000000   \n",
      "5              Accuracy    0.833148    0.839822    0.829811    0.825362   \n",
      "6             Precision    0.826552    0.862955    0.827586    0.831933   \n",
      "7           Sensitivity    0.848352    0.834369    0.857143    0.837209   \n",
      "8           Specificity    0.817600    0.846200    0.799100    0.812200   \n",
      "9              F1 score    0.837310    0.848421    0.842105    0.834563   \n",
      "10  F1 score (weighted)    0.833091    0.839984    0.829559    0.825329   \n",
      "11     F1 score (macro)    0.833039    0.839305    0.828773    0.824820   \n",
      "12    Balanced Accuracy    0.832960    0.840261    0.828099    0.824708   \n",
      "13                  MCC    0.666375    0.679145    0.658145    0.649658   \n",
      "14                  NPV    0.840300    0.814800    0.832500    0.818000   \n",
      "15              ROC_AUC    0.832960    0.840261    0.828099    0.824708   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.661992    0.692112    0.701017    0.687767  \n",
      "1   393.000000  409.000000  399.000000  398.000000  \n",
      "2   357.000000  356.000000  360.000000  347.000000  \n",
      "3    73.000000   67.000000   67.000000   69.000000  \n",
      "4    76.000000   67.000000   73.000000   85.000000  \n",
      "5     0.834260    0.850945    0.844271    0.828699  \n",
      "6     0.843348    0.859244    0.856223    0.852248  \n",
      "7     0.837953    0.859244    0.845339    0.824017  \n",
      "8     0.830200    0.841600    0.843100    0.834100  \n",
      "9     0.840642    0.859244    0.850746    0.837895  \n",
      "10    0.834282    0.850945    0.844317    0.828872  \n",
      "11    0.833994    0.850426    0.843978    0.828145  \n",
      "12    0.834093    0.850426    0.844215    0.829076  \n",
      "13    0.668007    0.700851    0.688031    0.656819  \n",
      "14    0.824500    0.841600    0.831400    0.803200  \n",
      "15    0.834093    0.850426    0.844215    0.829076  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_7_cat = np.where((y_pred_svm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 18:49:12,063]\u001b[0m Trial 400 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:50:20,307]\u001b[0m Trial 401 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:51:30,397]\u001b[0m Trial 402 finished with value: 0.5590190038088061 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:52:41,606]\u001b[0m Trial 403 finished with value: 0.6529473303405207 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:53:49,750]\u001b[0m Trial 404 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:54:58,002]\u001b[0m Trial 405 finished with value: 0.6163690282150046 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:56:05,950]\u001b[0m Trial 406 finished with value: 0.6729676169665659 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:57:17,492]\u001b[0m Trial 407 finished with value: 0.32782040840773774 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:58:25,662]\u001b[0m Trial 408 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:59:33,797]\u001b[0m Trial 409 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:00:42,009]\u001b[0m Trial 410 finished with value: 0.6783919725439036 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:01:54,909]\u001b[0m Trial 411 finished with value: 0.1706305295747153 and parameters: {'C': 2.0, 'gamma': 6.103515625e-05}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:03:03,330]\u001b[0m Trial 412 finished with value: 0.678386980615445 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:04:11,506]\u001b[0m Trial 413 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:05:24,742]\u001b[0m Trial 414 finished with value: 0.1397414278866169 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:06:38,200]\u001b[0m Trial 415 finished with value: 0.09070240345839747 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:07:50,866]\u001b[0m Trial 416 finished with value: 0.1973761319983575 and parameters: {'C': 0.03125, 'gamma': 0.0078125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:08:59,037]\u001b[0m Trial 417 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:10:13,055]\u001b[0m Trial 418 finished with value: 0.03478456233416777 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:11:26,503]\u001b[0m Trial 419 finished with value: 0.00142396570587503 and parameters: {'C': 0.0078125, 'gamma': 0.0001220703125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:12:34,934]\u001b[0m Trial 420 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:13:43,113]\u001b[0m Trial 421 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:14:57,604]\u001b[0m Trial 422 finished with value: 0.03771560100457705 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:16:10,618]\u001b[0m Trial 423 finished with value: 0.47980974457623554 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:17:21,357]\u001b[0m Trial 424 finished with value: 0.4318744852381373 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:18:29,900]\u001b[0m Trial 425 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:19:38,488]\u001b[0m Trial 426 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:20:46,698]\u001b[0m Trial 427 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:21:58,794]\u001b[0m Trial 428 finished with value: 0.170828299131271 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:23:05,439]\u001b[0m Trial 429 finished with value: 0.6652546358275598 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:24:16,915]\u001b[0m Trial 430 finished with value: 0.28185904339810147 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:25:29,327]\u001b[0m Trial 431 finished with value: 0.055147294770899045 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:26:38,094]\u001b[0m Trial 432 finished with value: 0.5809172554686977 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:27:48,468]\u001b[0m Trial 433 finished with value: 0.4156322580928492 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:28:56,661]\u001b[0m Trial 434 finished with value: 0.5323626855609266 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:30:03,633]\u001b[0m Trial 435 finished with value: 0.6845982957894142 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:31:10,510]\u001b[0m Trial 436 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:32:25,620]\u001b[0m Trial 437 finished with value: 0.034348810662071516 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:33:38,186]\u001b[0m Trial 438 finished with value: 0.04451962596173573 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:34:46,866]\u001b[0m Trial 439 finished with value: 0.5353789964542591 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:35:53,723]\u001b[0m Trial 440 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:37:03,905]\u001b[0m Trial 441 finished with value: 0.32782040840773774 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:38:10,897]\u001b[0m Trial 442 finished with value: 0.6798035292894373 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:39:17,712]\u001b[0m Trial 443 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 19:40:26,834]\u001b[0m Trial 444 finished with value: 0.4890511817719322 and parameters: {'C': 0.5, 'gamma': 0.00390625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:41:33,640]\u001b[0m Trial 445 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:42:40,495]\u001b[0m Trial 446 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:43:46,687]\u001b[0m Trial 447 finished with value: 0.687149317215001 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:44:55,399]\u001b[0m Trial 448 finished with value: 0.5590190038088061 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:46:02,220]\u001b[0m Trial 449 finished with value: 0.6896695894266649 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7042\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.679806    0.705457    0.705767    0.689205   \n",
      "1                    TP  386.000000  403.000000  408.000000  396.000000   \n",
      "2                    TN  363.000000  352.000000  338.000000  346.000000   \n",
      "3                    FP   81.000000   64.000000   85.000000   80.000000   \n",
      "4                    FN   69.000000   80.000000   68.000000   77.000000   \n",
      "5              Accuracy    0.833148    0.839822    0.829811    0.825362   \n",
      "6             Precision    0.826552    0.862955    0.827586    0.831933   \n",
      "7           Sensitivity    0.848352    0.834369    0.857143    0.837209   \n",
      "8           Specificity    0.817600    0.846200    0.799100    0.812200   \n",
      "9              F1 score    0.837310    0.848421    0.842105    0.834563   \n",
      "10  F1 score (weighted)    0.833091    0.839984    0.829559    0.825329   \n",
      "11     F1 score (macro)    0.833039    0.839305    0.828773    0.824820   \n",
      "12    Balanced Accuracy    0.832960    0.840261    0.828099    0.824708   \n",
      "13                  MCC    0.666375    0.679145    0.658145    0.649658   \n",
      "14                  NPV    0.840300    0.814800    0.832500    0.818000   \n",
      "15              ROC_AUC    0.832960    0.840261    0.828099    0.824708   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.661992    0.692112    0.701017    0.687767    0.694730  \n",
      "1   393.000000  409.000000  399.000000  398.000000  407.000000  \n",
      "2   357.000000  356.000000  360.000000  347.000000  339.000000  \n",
      "3    73.000000   67.000000   67.000000   69.000000   77.000000  \n",
      "4    76.000000   67.000000   73.000000   85.000000   76.000000  \n",
      "5     0.834260    0.850945    0.844271    0.828699    0.829811  \n",
      "6     0.843348    0.859244    0.856223    0.852248    0.840909  \n",
      "7     0.837953    0.859244    0.845339    0.824017    0.842650  \n",
      "8     0.830200    0.841600    0.843100    0.834100    0.814900  \n",
      "9     0.840642    0.859244    0.850746    0.837895    0.841779  \n",
      "10    0.834282    0.850945    0.844317    0.828872    0.829796  \n",
      "11    0.833994    0.850426    0.843978    0.828145    0.828832  \n",
      "12    0.834093    0.850426    0.844215    0.829076    0.828777  \n",
      "13    0.668007    0.700851    0.688031    0.656819    0.657665  \n",
      "14    0.824500    0.841600    0.831400    0.803200    0.816900  \n",
      "15    0.834093    0.850426    0.844215    0.829076    0.828777  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_8_cat = np.where((y_pred_svm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 19:47:18,772]\u001b[0m Trial 450 finished with value: 0.684733214416044 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:48:28,202]\u001b[0m Trial 451 finished with value: 0.5377987922646418 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:49:39,285]\u001b[0m Trial 452 finished with value: 0.1028342039312079 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:50:46,347]\u001b[0m Trial 453 finished with value: 0.6735433795157324 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:51:57,321]\u001b[0m Trial 454 finished with value: 0.21766074393080181 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:53:04,671]\u001b[0m Trial 455 finished with value: 0.6839619907728756 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:54:11,327]\u001b[0m Trial 456 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:55:23,556]\u001b[0m Trial 457 finished with value: 0.02888311783155576 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:56:34,927]\u001b[0m Trial 458 finished with value: 0.07539355953505508 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:57:41,582]\u001b[0m Trial 459 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:58:48,262]\u001b[0m Trial 460 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:00:00,129]\u001b[0m Trial 461 finished with value: 0.08888765531011747 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:01:10,667]\u001b[0m Trial 462 finished with value: 0.33449188169226485 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:02:22,020]\u001b[0m Trial 463 finished with value: 0.09728002162886983 and parameters: {'C': 0.125, 'gamma': 0.00048828125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:03:28,772]\u001b[0m Trial 464 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:04:35,466]\u001b[0m Trial 465 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:05:47,552]\u001b[0m Trial 466 finished with value: 0.03161756714550955 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:06:54,229]\u001b[0m Trial 467 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:08:00,578]\u001b[0m Trial 468 finished with value: 0.6576690298423842 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:09:11,453]\u001b[0m Trial 469 finished with value: 0.280690258652604 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:10:18,102]\u001b[0m Trial 470 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:11:29,305]\u001b[0m Trial 471 finished with value: 0.16657419184056094 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:12:35,964]\u001b[0m Trial 472 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:13:46,630]\u001b[0m Trial 473 finished with value: 0.3265348227058913 and parameters: {'C': 0.25, 'gamma': 0.001953125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:14:58,479]\u001b[0m Trial 474 finished with value: 0.05010304049641329 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:16:05,139]\u001b[0m Trial 475 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:17:12,213]\u001b[0m Trial 476 finished with value: 0.6880633628669257 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:18:22,360]\u001b[0m Trial 477 finished with value: 0.4125735545224706 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:19:29,589]\u001b[0m Trial 478 finished with value: 0.6860556645879318 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:20:38,280]\u001b[0m Trial 479 finished with value: 0.5288013528272978 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:21:44,920]\u001b[0m Trial 480 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:22:59,658]\u001b[0m Trial 481 finished with value: 0.028467251660240146 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:24:09,853]\u001b[0m Trial 482 finished with value: 0.32016101178265444 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:25:21,925]\u001b[0m Trial 483 finished with value: 0.03825283366525123 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:26:28,572]\u001b[0m Trial 484 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:27:34,622]\u001b[0m Trial 485 finished with value: 0.6838295854645582 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:28:43,506]\u001b[0m Trial 486 finished with value: 0.48492760982675287 and parameters: {'C': 0.5, 'gamma': 0.00390625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:29:50,148]\u001b[0m Trial 487 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:30:57,496]\u001b[0m Trial 488 finished with value: 0.683864958908256 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:32:05,722]\u001b[0m Trial 489 finished with value: 0.5603756058734211 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:33:13,065]\u001b[0m Trial 490 finished with value: 0.684733214416044 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:34:19,725]\u001b[0m Trial 491 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:35:26,399]\u001b[0m Trial 492 finished with value: 0.6733851168181559 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:36:33,056]\u001b[0m Trial 493 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 20:37:44,529]\u001b[0m Trial 494 finished with value: 0.1336570943869106 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:38:51,179]\u001b[0m Trial 495 finished with value: 0.6879215576825776 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:40:02,358]\u001b[0m Trial 496 finished with value: 0.25114000443746143 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:41:13,723]\u001b[0m Trial 497 finished with value: 0.07539355953505508 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:42:24,706]\u001b[0m Trial 498 finished with value: 0.21766074393080181 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:43:36,968]\u001b[0m Trial 499 finished with value: 0.02888311783155576 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 350 with value: 0.7041860882767477.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7042\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.679806    0.705457    0.705767    0.689205   \n",
      "1                    TP  386.000000  403.000000  408.000000  396.000000   \n",
      "2                    TN  363.000000  352.000000  338.000000  346.000000   \n",
      "3                    FP   81.000000   64.000000   85.000000   80.000000   \n",
      "4                    FN   69.000000   80.000000   68.000000   77.000000   \n",
      "5              Accuracy    0.833148    0.839822    0.829811    0.825362   \n",
      "6             Precision    0.826552    0.862955    0.827586    0.831933   \n",
      "7           Sensitivity    0.848352    0.834369    0.857143    0.837209   \n",
      "8           Specificity    0.817600    0.846200    0.799100    0.812200   \n",
      "9              F1 score    0.837310    0.848421    0.842105    0.834563   \n",
      "10  F1 score (weighted)    0.833091    0.839984    0.829559    0.825329   \n",
      "11     F1 score (macro)    0.833039    0.839305    0.828773    0.824820   \n",
      "12    Balanced Accuracy    0.832960    0.840261    0.828099    0.824708   \n",
      "13                  MCC    0.666375    0.679145    0.658145    0.649658   \n",
      "14                  NPV    0.840300    0.814800    0.832500    0.818000   \n",
      "15              ROC_AUC    0.832960    0.840261    0.828099    0.824708   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.661992    0.692112    0.701017    0.687767    0.694730    0.728578  \n",
      "1   393.000000  409.000000  399.000000  398.000000  407.000000  410.000000  \n",
      "2   357.000000  356.000000  360.000000  347.000000  339.000000  346.000000  \n",
      "3    73.000000   67.000000   67.000000   69.000000   77.000000   83.000000  \n",
      "4    76.000000   67.000000   73.000000   85.000000   76.000000   60.000000  \n",
      "5     0.834260    0.850945    0.844271    0.828699    0.829811    0.840934  \n",
      "6     0.843348    0.859244    0.856223    0.852248    0.840909    0.831643  \n",
      "7     0.837953    0.859244    0.845339    0.824017    0.842650    0.872340  \n",
      "8     0.830200    0.841600    0.843100    0.834100    0.814900    0.806500  \n",
      "9     0.840642    0.859244    0.850746    0.837895    0.841779    0.851506  \n",
      "10    0.834282    0.850945    0.844317    0.828872    0.829796    0.840643  \n",
      "11    0.833994    0.850426    0.843978    0.828145    0.828832    0.840124  \n",
      "12    0.834093    0.850426    0.844215    0.829076    0.828777    0.839434  \n",
      "13    0.668007    0.700851    0.688031    0.656819    0.657665    0.681359  \n",
      "14    0.824500    0.841600    0.831400    0.803200    0.816900    0.852200  \n",
      "15    0.834093    0.850426    0.844215    0.829076    0.828777    0.839434  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_9_cat = np.where((y_pred_svm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7042\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.712493</td>\n",
       "      <td>0.030623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>200.700000</td>\n",
       "      <td>12.147702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>176.200000</td>\n",
       "      <td>8.256984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>7.083627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>7.261007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.026312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.844872</td>\n",
       "      <td>0.030123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.849293</td>\n",
       "      <td>0.033075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.827480</td>\n",
       "      <td>0.031275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.846846</td>\n",
       "      <td>0.028041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.838987</td>\n",
       "      <td>0.026357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.838372</td>\n",
       "      <td>0.026317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.838389</td>\n",
       "      <td>0.026386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.677255</td>\n",
       "      <td>0.052645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.832860</td>\n",
       "      <td>0.029729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.838389</td>\n",
       "      <td>0.026386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.712493     0.030623\n",
       "1                    TP       200.700000    12.147702\n",
       "2                    TN       176.200000     8.256984\n",
       "3                    FP        36.800000     7.083627\n",
       "4                    FN        35.500000     7.261007\n",
       "5              Accuracy         0.839049     0.026312\n",
       "6             Precision         0.844872     0.030123\n",
       "7           Sensitivity         0.849293     0.033075\n",
       "8           Specificity         0.827480     0.031275\n",
       "9              F1 score         0.846846     0.028041\n",
       "10  F1 score (weighted)         0.838987     0.026357\n",
       "11     F1 score (macro)         0.838372     0.026317\n",
       "12    Balanced Accuracy         0.838389     0.026386\n",
       "13                  MCC         0.677255     0.052645\n",
       "14                  NPV         0.832860     0.029729\n",
       "15              ROC_AUC         0.838389     0.026386"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.679806</td>\n",
       "      <td>0.705457</td>\n",
       "      <td>0.705767</td>\n",
       "      <td>0.689205</td>\n",
       "      <td>0.661992</td>\n",
       "      <td>0.692112</td>\n",
       "      <td>0.701017</td>\n",
       "      <td>0.687767</td>\n",
       "      <td>0.694730</td>\n",
       "      <td>0.728578</td>\n",
       "      <td>0.694643</td>\n",
       "      <td>0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>386.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>400.900000</td>\n",
       "      <td>7.894442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>350.400000</td>\n",
       "      <td>8.579044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>74.600000</td>\n",
       "      <td>7.574812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>73.100000</td>\n",
       "      <td>7.248755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.833148</td>\n",
       "      <td>0.839822</td>\n",
       "      <td>0.829811</td>\n",
       "      <td>0.825362</td>\n",
       "      <td>0.834260</td>\n",
       "      <td>0.850945</td>\n",
       "      <td>0.844271</td>\n",
       "      <td>0.828699</td>\n",
       "      <td>0.829811</td>\n",
       "      <td>0.840934</td>\n",
       "      <td>0.835706</td>\n",
       "      <td>0.008056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.826552</td>\n",
       "      <td>0.862955</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.843348</td>\n",
       "      <td>0.859244</td>\n",
       "      <td>0.856223</td>\n",
       "      <td>0.852248</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.831643</td>\n",
       "      <td>0.843264</td>\n",
       "      <td>0.013686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.848352</td>\n",
       "      <td>0.834369</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.837953</td>\n",
       "      <td>0.859244</td>\n",
       "      <td>0.845339</td>\n",
       "      <td>0.824017</td>\n",
       "      <td>0.842650</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.845862</td>\n",
       "      <td>0.014028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.846200</td>\n",
       "      <td>0.799100</td>\n",
       "      <td>0.812200</td>\n",
       "      <td>0.830200</td>\n",
       "      <td>0.841600</td>\n",
       "      <td>0.843100</td>\n",
       "      <td>0.834100</td>\n",
       "      <td>0.814900</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.824550</td>\n",
       "      <td>0.016647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.837310</td>\n",
       "      <td>0.848421</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.834563</td>\n",
       "      <td>0.840642</td>\n",
       "      <td>0.859244</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.837895</td>\n",
       "      <td>0.841779</td>\n",
       "      <td>0.851506</td>\n",
       "      <td>0.844421</td>\n",
       "      <td>0.007767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.833091</td>\n",
       "      <td>0.839984</td>\n",
       "      <td>0.829559</td>\n",
       "      <td>0.825329</td>\n",
       "      <td>0.834282</td>\n",
       "      <td>0.850945</td>\n",
       "      <td>0.844317</td>\n",
       "      <td>0.828872</td>\n",
       "      <td>0.829796</td>\n",
       "      <td>0.840643</td>\n",
       "      <td>0.835682</td>\n",
       "      <td>0.008062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.833039</td>\n",
       "      <td>0.839305</td>\n",
       "      <td>0.828773</td>\n",
       "      <td>0.824820</td>\n",
       "      <td>0.833994</td>\n",
       "      <td>0.850426</td>\n",
       "      <td>0.843978</td>\n",
       "      <td>0.828145</td>\n",
       "      <td>0.828832</td>\n",
       "      <td>0.840124</td>\n",
       "      <td>0.835143</td>\n",
       "      <td>0.008134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.832960</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.828099</td>\n",
       "      <td>0.824708</td>\n",
       "      <td>0.834093</td>\n",
       "      <td>0.850426</td>\n",
       "      <td>0.844215</td>\n",
       "      <td>0.829076</td>\n",
       "      <td>0.828777</td>\n",
       "      <td>0.839434</td>\n",
       "      <td>0.835205</td>\n",
       "      <td>0.008180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.666375</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.658145</td>\n",
       "      <td>0.649658</td>\n",
       "      <td>0.668007</td>\n",
       "      <td>0.700851</td>\n",
       "      <td>0.688031</td>\n",
       "      <td>0.656819</td>\n",
       "      <td>0.657665</td>\n",
       "      <td>0.681359</td>\n",
       "      <td>0.670605</td>\n",
       "      <td>0.016273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.840300</td>\n",
       "      <td>0.814800</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.824500</td>\n",
       "      <td>0.841600</td>\n",
       "      <td>0.831400</td>\n",
       "      <td>0.803200</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>0.852200</td>\n",
       "      <td>0.827540</td>\n",
       "      <td>0.014819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.832960</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.828099</td>\n",
       "      <td>0.824708</td>\n",
       "      <td>0.834093</td>\n",
       "      <td>0.850426</td>\n",
       "      <td>0.844215</td>\n",
       "      <td>0.829076</td>\n",
       "      <td>0.828777</td>\n",
       "      <td>0.839434</td>\n",
       "      <td>0.835205</td>\n",
       "      <td>0.008180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.679806    0.705457    0.705767    0.689205   \n",
       "1                    TP  386.000000  403.000000  408.000000  396.000000   \n",
       "2                    TN  363.000000  352.000000  338.000000  346.000000   \n",
       "3                    FP   81.000000   64.000000   85.000000   80.000000   \n",
       "4                    FN   69.000000   80.000000   68.000000   77.000000   \n",
       "5              Accuracy    0.833148    0.839822    0.829811    0.825362   \n",
       "6             Precision    0.826552    0.862955    0.827586    0.831933   \n",
       "7           Sensitivity    0.848352    0.834369    0.857143    0.837209   \n",
       "8           Specificity    0.817600    0.846200    0.799100    0.812200   \n",
       "9              F1 score    0.837310    0.848421    0.842105    0.834563   \n",
       "10  F1 score (weighted)    0.833091    0.839984    0.829559    0.825329   \n",
       "11     F1 score (macro)    0.833039    0.839305    0.828773    0.824820   \n",
       "12    Balanced Accuracy    0.832960    0.840261    0.828099    0.824708   \n",
       "13                  MCC    0.666375    0.679145    0.658145    0.649658   \n",
       "14                  NPV    0.840300    0.814800    0.832500    0.818000   \n",
       "15              ROC_AUC    0.832960    0.840261    0.828099    0.824708   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.661992    0.692112    0.701017    0.687767    0.694730    0.728578   \n",
       "1   393.000000  409.000000  399.000000  398.000000  407.000000  410.000000   \n",
       "2   357.000000  356.000000  360.000000  347.000000  339.000000  346.000000   \n",
       "3    73.000000   67.000000   67.000000   69.000000   77.000000   83.000000   \n",
       "4    76.000000   67.000000   73.000000   85.000000   76.000000   60.000000   \n",
       "5     0.834260    0.850945    0.844271    0.828699    0.829811    0.840934   \n",
       "6     0.843348    0.859244    0.856223    0.852248    0.840909    0.831643   \n",
       "7     0.837953    0.859244    0.845339    0.824017    0.842650    0.872340   \n",
       "8     0.830200    0.841600    0.843100    0.834100    0.814900    0.806500   \n",
       "9     0.840642    0.859244    0.850746    0.837895    0.841779    0.851506   \n",
       "10    0.834282    0.850945    0.844317    0.828872    0.829796    0.840643   \n",
       "11    0.833994    0.850426    0.843978    0.828145    0.828832    0.840124   \n",
       "12    0.834093    0.850426    0.844215    0.829076    0.828777    0.839434   \n",
       "13    0.668007    0.700851    0.688031    0.656819    0.657665    0.681359   \n",
       "14    0.824500    0.841600    0.831400    0.803200    0.816900    0.852200   \n",
       "15    0.834093    0.850426    0.844215    0.829076    0.828777    0.839434   \n",
       "\n",
       "           ave       std  \n",
       "0     0.694643  0.017646  \n",
       "1   400.900000  7.894442  \n",
       "2   350.400000  8.579044  \n",
       "3    74.600000  7.574812  \n",
       "4    73.100000  7.248755  \n",
       "5     0.835706  0.008056  \n",
       "6     0.843264  0.013686  \n",
       "7     0.845862  0.014028  \n",
       "8     0.824550  0.016647  \n",
       "9     0.844421  0.007767  \n",
       "10    0.835682  0.008062  \n",
       "11    0.835143  0.008134  \n",
       "12    0.835205  0.008180  \n",
       "13    0.670605  0.016273  \n",
       "14    0.827540  0.014819  \n",
       "15    0.835205  0.008180  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_svm0</th>\n",
       "      <th>y_pred_svm1</th>\n",
       "      <th>y_pred_svm2</th>\n",
       "      <th>y_pred_svm3</th>\n",
       "      <th>y_pred_svm4</th>\n",
       "      <th>y_pred_svm_ave</th>\n",
       "      <th>y_pred_svm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4286867</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>6.906270</td>\n",
       "      <td>6.880988</td>\n",
       "      <td>6.622951</td>\n",
       "      <td>6.897095</td>\n",
       "      <td>6.782075</td>\n",
       "      <td>6.889896</td>\n",
       "      <td>0.188396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3689853</td>\n",
       "      <td>1</td>\n",
       "      <td>6.43</td>\n",
       "      <td>6.374422</td>\n",
       "      <td>6.422655</td>\n",
       "      <td>6.340521</td>\n",
       "      <td>6.369552</td>\n",
       "      <td>6.381926</td>\n",
       "      <td>6.386513</td>\n",
       "      <td>0.031004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3827056</td>\n",
       "      <td>2</td>\n",
       "      <td>7.52</td>\n",
       "      <td>8.671562</td>\n",
       "      <td>8.636064</td>\n",
       "      <td>8.595810</td>\n",
       "      <td>8.559175</td>\n",
       "      <td>8.710785</td>\n",
       "      <td>8.448899</td>\n",
       "      <td>0.418288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3689883</td>\n",
       "      <td>3</td>\n",
       "      <td>7.70</td>\n",
       "      <td>7.402379</td>\n",
       "      <td>7.380729</td>\n",
       "      <td>7.355947</td>\n",
       "      <td>7.366139</td>\n",
       "      <td>7.367487</td>\n",
       "      <td>7.428780</td>\n",
       "      <td>0.122169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2023528</td>\n",
       "      <td>4</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.691218</td>\n",
       "      <td>7.699364</td>\n",
       "      <td>7.823058</td>\n",
       "      <td>7.101727</td>\n",
       "      <td>7.099170</td>\n",
       "      <td>7.447423</td>\n",
       "      <td>0.298964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>CHEMBL4464975</td>\n",
       "      <td>4487</td>\n",
       "      <td>4.72</td>\n",
       "      <td>5.215120</td>\n",
       "      <td>5.298787</td>\n",
       "      <td>5.276510</td>\n",
       "      <td>5.216893</td>\n",
       "      <td>5.212179</td>\n",
       "      <td>5.156582</td>\n",
       "      <td>0.198059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>CHEMBL95747</td>\n",
       "      <td>4488</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.879247</td>\n",
       "      <td>6.961059</td>\n",
       "      <td>6.791553</td>\n",
       "      <td>6.814314</td>\n",
       "      <td>6.967780</td>\n",
       "      <td>7.002325</td>\n",
       "      <td>0.275388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>CHEMBL4072618</td>\n",
       "      <td>4489</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.382753</td>\n",
       "      <td>5.312837</td>\n",
       "      <td>5.215983</td>\n",
       "      <td>5.359619</td>\n",
       "      <td>5.383309</td>\n",
       "      <td>5.307417</td>\n",
       "      <td>0.077821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>CHEMBL2408692</td>\n",
       "      <td>4490</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.325874</td>\n",
       "      <td>6.423393</td>\n",
       "      <td>6.410205</td>\n",
       "      <td>6.408914</td>\n",
       "      <td>6.090587</td>\n",
       "      <td>6.336496</td>\n",
       "      <td>0.115011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>CHEMBL4226829</td>\n",
       "      <td>4491</td>\n",
       "      <td>5.55</td>\n",
       "      <td>5.700181</td>\n",
       "      <td>5.700237</td>\n",
       "      <td>5.699669</td>\n",
       "      <td>5.699920</td>\n",
       "      <td>5.699841</td>\n",
       "      <td>5.674975</td>\n",
       "      <td>0.055891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_svm0  y_pred_svm1  \\\n",
       "0         CHEMBL4286867            0     7.25     6.906270     6.880988   \n",
       "1         CHEMBL3689853            1     6.43     6.374422     6.422655   \n",
       "2         CHEMBL3827056            2     7.52     8.671562     8.636064   \n",
       "3         CHEMBL3689883            3     7.70     7.402379     7.380729   \n",
       "4         CHEMBL2023528            4     7.27     7.691218     7.699364   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "4487      CHEMBL4464975         4487     4.72     5.215120     5.298787   \n",
       "4488        CHEMBL95747         4488     7.60     6.879247     6.961059   \n",
       "4489      CHEMBL4072618         4489     5.19     5.382753     5.312837   \n",
       "4490      CHEMBL2408692         4490     6.36     6.325874     6.423393   \n",
       "4491      CHEMBL4226829         4491     5.55     5.700181     5.700237   \n",
       "\n",
       "      y_pred_svm2  y_pred_svm3  y_pred_svm4  y_pred_svm_ave  y_pred_svm_std  \n",
       "0        6.622951     6.897095     6.782075        6.889896        0.188396  \n",
       "1        6.340521     6.369552     6.381926        6.386513        0.031004  \n",
       "2        8.595810     8.559175     8.710785        8.448899        0.418288  \n",
       "3        7.355947     7.366139     7.367487        7.428780        0.122169  \n",
       "4        7.823058     7.101727     7.099170        7.447423        0.298964  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "4487     5.276510     5.216893     5.212179        5.156582        0.198059  \n",
       "4488     6.791553     6.814314     6.967780        7.002325        0.275388  \n",
       "4489     5.215983     5.359619     5.383309        5.307417        0.077821  \n",
       "4490     6.410205     6.408914     6.090587        6.336496        0.115011  \n",
       "4491     5.699669     5.699920     5.699841        5.674975        0.055891  \n",
       "\n",
       "[4492 rows x 10 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where((y_pred_optimized_svm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id, svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "svm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6394fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_svm.to_csv('mat_met_svm_opt.csv')\n",
    "svm_5preds.to_csv('svm_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.6853 with a standard deviation of 0.0411\n",
      "SVM optimized model r2_score 0.7155 with a standard deviation of 0.0416\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_svm.joblib']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, \"./svm_reg.joblib\")\n",
    "#joblib.dump(optimized_svm, \"./optimized_svm.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"./optimizedCV_svm.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
