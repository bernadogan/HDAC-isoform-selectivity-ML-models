{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/anaconda3/envs/teachopencadd/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingclfressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "levels_up = 3\n",
    "HDAC1= HERE.parents[levels_up-1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>n_rot</th>\n",
       "      <th>n_heavy</th>\n",
       "      <th>n_hba</th>\n",
       "      <th>n_hbd</th>\n",
       "      <th>logp</th>\n",
       "      <th>num_ar</th>\n",
       "      <th>num_sa</th>\n",
       "      <th>num_alip</th>\n",
       "      <th>pchembl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4516302</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[9413970, 1506859, 7998790, 22094156, 553789, ...</td>\n",
       "      <td>342.157957</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5122</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4067265</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[926304, 2662097, 582748, 1458307, 1038221, 43...</td>\n",
       "      <td>666.410483</td>\n",
       "      <td>16.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.3354</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL481211</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3192076, 3346815, 1578989, 11956864, 15143291...</td>\n",
       "      <td>396.169859</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.9852</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4648704</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2322832, 4026098, 991465, 1396494, 3858537, 9...</td>\n",
       "      <td>546.320606</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.1073</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3758354</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[7275495, 10511, 8033062, 5854825, 11301804, 1...</td>\n",
       "      <td>407.167891</td>\n",
       "      <td>12.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.7740</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4516302  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL4067265  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2       CHEMBL481211  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4648704  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3758354  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  molecular_weight  n_rot  \\\n",
       "0  [9413970, 1506859, 7998790, 22094156, 553789, ...        342.157957    8.0   \n",
       "1  [926304, 2662097, 582748, 1458307, 1038221, 43...        666.410483   16.0   \n",
       "2  [3192076, 3346815, 1578989, 11956864, 15143291...        396.169859    6.0   \n",
       "3  [2322832, 4026098, 991465, 1396494, 3858537, 9...        546.320606   13.0   \n",
       "4  [7275495, 10511, 8033062, 5854825, 11301804, 1...        407.167891   12.0   \n",
       "\n",
       "   n_heavy  n_hba  n_hbd    logp  num_ar  num_sa  num_alip  pchembl  \n",
       "0     25.0    5.0    3.0  2.5122     2.0     0.0       0.0     6.22  \n",
       "1     48.0    6.0    6.0  3.3354     2.0     1.0       1.0     7.38  \n",
       "2     30.0    6.0    3.0  3.9852     4.0     0.0       0.0     6.96  \n",
       "3     40.0    7.0    1.0  6.1073     3.0     2.0       2.0     9.05  \n",
       "4     28.0    5.0    3.0  3.7740     2.0     0.0       0.0     6.89  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1/\"HDAC1_4492compounds_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>pActivity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL327146</td>\n",
       "      <td>O=C(CCCCCC(C(=O)Nc1ccc2ncccc2c1)C(=O)Nc1ccc2nc...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL116620</td>\n",
       "      <td>O=C(/C=C/c1cccc(C(C(=O)Nc2ccccc2)C(=O)Nc2ccccc...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2093007</td>\n",
       "      <td>C/C=C1\\NC(=O)[C@@H](CSC)NC(=O)[C@@H](C(C)C)CC(...</td>\n",
       "      <td>5.20</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL316457</td>\n",
       "      <td>CC(C)c1cc(C(C)C)c(S(=O)(=O)Nc2ccc(/C=C/C(=O)NO...</td>\n",
       "      <td>6.22</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                             smiles  \\\n",
       "0       CHEMBL327146  O=C(CCCCCC(C(=O)Nc1ccc2ncccc2c1)C(=O)Nc1ccc2nc...   \n",
       "1       CHEMBL116620  O=C(/C=C/c1cccc(C(C(=O)Nc2ccccc2)C(=O)Nc2ccccc...   \n",
       "2      CHEMBL2093007  C/C=C1\\NC(=O)[C@@H](CSC)NC(=O)[C@@H](C(C)C)CC(...   \n",
       "3       CHEMBL316457  CC(C)c1cc(C(C)C)c(S(=O)(=O)Nc2ccc(/C=C/C(=O)NO...   \n",
       "\n",
       "   pActivity          label  \n",
       "0       9.00  Single points  \n",
       "1       9.00  Single points  \n",
       "2       5.20  Single points  \n",
       "3       6.22  Single points  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1/\"HDAC1_4492compounds_withTypes-Ki_newThreshold.csv\", index_col=0)\n",
    "df_labeled.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>n_rot</th>\n",
       "      <th>n_heavy</th>\n",
       "      <th>n_hba</th>\n",
       "      <th>n_hbd</th>\n",
       "      <th>logp</th>\n",
       "      <th>num_ar</th>\n",
       "      <th>num_sa</th>\n",
       "      <th>num_alip</th>\n",
       "      <th>pchembl</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4516302</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[9413970, 1506859, 7998790, 22094156, 553789, ...</td>\n",
       "      <td>342.157957</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5122</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.22</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4067265</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[926304, 2662097, 582748, 1458307, 1038221, 43...</td>\n",
       "      <td>666.410483</td>\n",
       "      <td>16.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.3354</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL481211</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3192076, 3346815, 1578989, 11956864, 15143291...</td>\n",
       "      <td>396.169859</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.9852</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.96</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4648704</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2322832, 4026098, 991465, 1396494, 3858537, 9...</td>\n",
       "      <td>546.320606</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.1073</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.05</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4516302  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL4067265  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2       CHEMBL481211  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4648704  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  molecular_weight  n_rot  \\\n",
       "0  [9413970, 1506859, 7998790, 22094156, 553789, ...        342.157957    8.0   \n",
       "1  [926304, 2662097, 582748, 1458307, 1038221, 43...        666.410483   16.0   \n",
       "2  [3192076, 3346815, 1578989, 11956864, 15143291...        396.169859    6.0   \n",
       "3  [2322832, 4026098, 991465, 1396494, 3858537, 9...        546.320606   13.0   \n",
       "\n",
       "   n_heavy  n_hba  n_hbd    logp  num_ar  num_sa  num_alip  pchembl  \\\n",
       "0     25.0    5.0    3.0  2.5122     2.0     0.0       0.0     6.22   \n",
       "1     48.0    6.0    6.0  3.3354     2.0     1.0       1.0     7.38   \n",
       "2     30.0    6.0    3.0  3.9852     4.0     0.0       0.0     6.96   \n",
       "3     40.0    7.0    1.0  6.1073     3.0     2.0       2.0     9.05   \n",
       "\n",
       "           label  Class  \n",
       "0  Single points    0.0  \n",
       "1  Single points    0.0  \n",
       "2  Single points    0.0  \n",
       "3  Single points    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"activity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.pchembl >= 6.6].index, \"activity\"] = 1.0\n",
    "\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y =  df[\"activity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['activity'])).astype(float)\n",
    "    \n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['activity'])).astype(float)\n",
    "    \n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       203.500000     8.996913\n",
      "1                    TN       169.100000     7.908505\n",
      "2                    FP        43.900000     7.880355\n",
      "3                    FN        32.700000     5.888784\n",
      "4              Accuracy         0.829465     0.021203\n",
      "5             Precision         0.822792     0.029642\n",
      "6           Sensitivity         0.861604     0.023819\n",
      "7           Specificity         0.794280     0.033061\n",
      "8              F1 score         0.841413     0.020672\n",
      "9   F1 score (weighted)         0.829129     0.021288\n",
      "10     F1 score (macro)         0.828320     0.021256\n",
      "11    Balanced Accuracy         0.827945     0.020965\n",
      "12                  MCC         0.658442     0.041817\n",
      "13                  NPV         0.838230     0.026797\n",
      "14              ROC_AUC         0.827945     0.020965\n",
      "CPU times: user 4min 10s, sys: 105 ms, total: 4min 10s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_clf =  RandomForestClassifier(random_state=1121218, max_features = None, n_jobs=8,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_clf.fit(x_train, y_train)\n",
    "        y_pred = rf_clf.predict(x_test)  \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggestegorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggestegorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestClassifier(**param_grid, n_jobs=8, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred,  average=\"macro\")\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggestegorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggestegorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestClassifier(**param_grid, n_jobs=8, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 15:39:41,859]\u001b[0m A new study created in memory with name: RFclassifier\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:42:45,153]\u001b[0m Trial 0 finished with value: 0.82400262448396 and parameters: {'n_estimators': 705}. Best is trial 0 with value: 0.82400262448396.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:46:47,257]\u001b[0m Trial 1 finished with value: 0.8236614019505788 and parameters: {'n_estimators': 950}. Best is trial 0 with value: 0.82400262448396.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:50:38,252]\u001b[0m Trial 2 finished with value: 0.8231117785792496 and parameters: {'n_estimators': 844}. Best is trial 0 with value: 0.82400262448396.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:53:03,158]\u001b[0m Trial 3 finished with value: 0.8200781856127908 and parameters: {'n_estimators': 418}. Best is trial 0 with value: 0.82400262448396.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:55:31,651]\u001b[0m Trial 4 finished with value: 0.8219785398759332 and parameters: {'n_estimators': 489}. Best is trial 0 with value: 0.82400262448396.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 15:57:32,629]\u001b[0m Trial 5 finished with value: 0.8217699811110941 and parameters: {'n_estimators': 472}. Best is trial 0 with value: 0.82400262448396.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:01:43,376]\u001b[0m Trial 6 finished with value: 0.8239504941277749 and parameters: {'n_estimators': 977}. Best is trial 0 with value: 0.82400262448396.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:04:48,031]\u001b[0m Trial 7 finished with value: 0.8236887662838631 and parameters: {'n_estimators': 718}. Best is trial 0 with value: 0.82400262448396.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:07:06,224]\u001b[0m Trial 8 finished with value: 0.8219650318385993 and parameters: {'n_estimators': 541}. Best is trial 0 with value: 0.82400262448396.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:07:50,105]\u001b[0m Trial 9 finished with value: 0.8243249934425305 and parameters: {'n_estimators': 162}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:08:18,042]\u001b[0m Trial 10 finished with value: 0.8206662322588288 and parameters: {'n_estimators': 103}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:09:12,199]\u001b[0m Trial 11 finished with value: 0.8214651445609666 and parameters: {'n_estimators': 205}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:12:26,747]\u001b[0m Trial 12 finished with value: 0.8242946599591665 and parameters: {'n_estimators': 702}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:14:12,058]\u001b[0m Trial 13 finished with value: 0.8232063358617421 and parameters: {'n_estimators': 301}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:17:41,750]\u001b[0m Trial 14 finished with value: 0.824025918124635 and parameters: {'n_estimators': 659}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:19:12,764]\u001b[0m Trial 15 finished with value: 0.8206953341664038 and parameters: {'n_estimators': 344}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:22:43,128]\u001b[0m Trial 16 finished with value: 0.8228205160801793 and parameters: {'n_estimators': 823}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:25:19,441]\u001b[0m Trial 17 finished with value: 0.822056463185383 and parameters: {'n_estimators': 605}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:25:52,191]\u001b[0m Trial 18 finished with value: 0.8230058572397351 and parameters: {'n_estimators': 122}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:29:25,171]\u001b[0m Trial 19 finished with value: 0.8228205160801793 and parameters: {'n_estimators': 824}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:30:33,600]\u001b[0m Trial 20 finished with value: 0.820612531869884 and parameters: {'n_estimators': 261}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:33:27,345]\u001b[0m Trial 21 finished with value: 0.8243131306406498 and parameters: {'n_estimators': 655}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:37:42,467]\u001b[0m Trial 22 finished with value: 0.8242720487012871 and parameters: {'n_estimators': 748}. Best is trial 9 with value: 0.8243249934425305.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:40:32,488]\u001b[0m Trial 23 finished with value: 0.824855181078374 and parameters: {'n_estimators': 623}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:43:07,798]\u001b[0m Trial 24 finished with value: 0.8220563984250505 and parameters: {'n_estimators': 594}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:44:44,262]\u001b[0m Trial 25 finished with value: 0.8203994532096018 and parameters: {'n_estimators': 368}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:47:04,620]\u001b[0m Trial 26 finished with value: 0.8216888753465037 and parameters: {'n_estimators': 539}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:49:40,829]\u001b[0m Trial 27 finished with value: 0.8223222561189886 and parameters: {'n_estimators': 599}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:51:38,444]\u001b[0m Trial 28 finished with value: 0.8214631878492451 and parameters: {'n_estimators': 446}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:55:00,962]\u001b[0m Trial 29 finished with value: 0.8233813504235978 and parameters: {'n_estimators': 767}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 16:58:48,578]\u001b[0m Trial 30 finished with value: 0.8237109457705678 and parameters: {'n_estimators': 666}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:01:58,481]\u001b[0m Trial 31 finished with value: 0.8225907546147306 and parameters: {'n_estimators': 676}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:05:44,454]\u001b[0m Trial 32 finished with value: 0.82362251644058 and parameters: {'n_estimators': 872}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:09:42,540]\u001b[0m Trial 33 finished with value: 0.8236260225980182 and parameters: {'n_estimators': 919}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:12:20,619]\u001b[0m Trial 34 finished with value: 0.8234445948784034 and parameters: {'n_estimators': 613}. Best is trial 23 with value: 0.824855181078374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:15:42,213]\u001b[0m Trial 35 finished with value: 0.8250674264961845 and parameters: {'n_estimators': 777}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:19:52,091]\u001b[0m Trial 36 finished with value: 0.8245056970582425 and parameters: {'n_estimators': 774}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:23:37,423]\u001b[0m Trial 37 finished with value: 0.8242354483431298 and parameters: {'n_estimators': 786}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:27:27,636]\u001b[0m Trial 38 finished with value: 0.8244784805113016 and parameters: {'n_estimators': 887}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:31:25,429]\u001b[0m Trial 39 finished with value: 0.8236260225980182 and parameters: {'n_estimators': 918}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:35:42,849]\u001b[0m Trial 40 finished with value: 0.8222042044806761 and parameters: {'n_estimators': 991}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:40:11,692]\u001b[0m Trial 41 finished with value: 0.8236260225980182 and parameters: {'n_estimators': 918}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:44:38,134]\u001b[0m Trial 42 finished with value: 0.82362251644058 and parameters: {'n_estimators': 869}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:47:45,827]\u001b[0m Trial 43 finished with value: 0.8228311030436203 and parameters: {'n_estimators': 724}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:49:58,979]\u001b[0m Trial 44 finished with value: 0.822549205689022 and parameters: {'n_estimators': 509}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 17:53:28,492]\u001b[0m Trial 45 finished with value: 0.8236703813272281 and parameters: {'n_estimators': 809}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 17:57:13,977]\u001b[0m Trial 46 finished with value: 0.8238933724530296 and parameters: {'n_estimators': 867}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:00:39,276]\u001b[0m Trial 47 finished with value: 0.8236715695564649 and parameters: {'n_estimators': 747}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:01:46,947]\u001b[0m Trial 48 finished with value: 0.8232171559085131 and parameters: {'n_estimators': 190}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:06:41,780]\u001b[0m Trial 49 finished with value: 0.8233585988413973 and parameters: {'n_estimators': 963}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8251\n",
      "\tBest params:\n",
      "\t\tn_estimators: 777\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFclassifier\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  396.000000\n",
      "1                    TN  343.000000\n",
      "2                    FP   91.000000\n",
      "3                    FN   69.000000\n",
      "4              Accuracy    0.822024\n",
      "5             Precision    0.813142\n",
      "6           Sensitivity    0.851613\n",
      "7           Specificity    0.790300\n",
      "8              F1 score    0.831933\n",
      "9   F1 score (weighted)    0.821767\n",
      "10     F1 score (macro)    0.821404\n",
      "11    Balanced Accuracy    0.820968\n",
      "12                  MCC    0.643798\n",
      "13                  NPV    0.832500\n",
      "14              ROC_AUC    0.820968\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_rf_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_rf_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_rf_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_rf_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_rf_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_rf_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_rf_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_rf_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_rf_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_rf_0)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP), np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                           np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 18:08:42,886]\u001b[0m Trial 50 finished with value: 0.8161567351633501 and parameters: {'n_estimators': 382}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:11:48,889]\u001b[0m Trial 51 finished with value: 0.8173362597959943 and parameters: {'n_estimators': 714}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:14:14,569]\u001b[0m Trial 52 finished with value: 0.81650717118186 and parameters: {'n_estimators': 567}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:17:01,700]\u001b[0m Trial 53 finished with value: 0.8178819987734098 and parameters: {'n_estimators': 645}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:20:26,747]\u001b[0m Trial 54 finished with value: 0.8167787958047745 and parameters: {'n_estimators': 791}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:24:01,046]\u001b[0m Trial 55 finished with value: 0.8170321224559745 and parameters: {'n_estimators': 685}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:27:20,901]\u001b[0m Trial 56 finished with value: 0.816533728404089 and parameters: {'n_estimators': 629}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:31:02,172]\u001b[0m Trial 57 finished with value: 0.8167942874946167 and parameters: {'n_estimators': 847}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:33:34,421]\u001b[0m Trial 58 finished with value: 0.8162556954680934 and parameters: {'n_estimators': 560}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:35:49,876]\u001b[0m Trial 59 finished with value: 0.8161495952064053 and parameters: {'n_estimators': 505}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:39:09,868]\u001b[0m Trial 60 finished with value: 0.8176273895564282 and parameters: {'n_estimators': 746}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:42:16,369]\u001b[0m Trial 61 finished with value: 0.8176272405764266 and parameters: {'n_estimators': 698}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:46:26,715]\u001b[0m Trial 62 finished with value: 0.8173186081268591 and parameters: {'n_estimators': 770}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:49:47,633]\u001b[0m Trial 63 finished with value: 0.817609537214088 and parameters: {'n_estimators': 647}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:53:02,976]\u001b[0m Trial 64 finished with value: 0.8170671971941126 and parameters: {'n_estimators': 724}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 18:56:47,204]\u001b[0m Trial 65 finished with value: 0.8170654618373525 and parameters: {'n_estimators': 833}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:00:47,269]\u001b[0m Trial 66 finished with value: 0.8178986012579053 and parameters: {'n_estimators': 890}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:02:06,983]\u001b[0m Trial 67 finished with value: 0.8172820330268893 and parameters: {'n_estimators': 288}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:04:49,522]\u001b[0m Trial 68 finished with value: 0.8159442473035963 and parameters: {'n_estimators': 591}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:09:27,933]\u001b[0m Trial 69 finished with value: 0.8167934507226875 and parameters: {'n_estimators': 807}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:12:38,549]\u001b[0m Trial 70 finished with value: 0.8170514435900273 and parameters: {'n_estimators': 675}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:15:57,679]\u001b[0m Trial 71 finished with value: 0.8173362597959943 and parameters: {'n_estimators': 732}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:19:08,068]\u001b[0m Trial 72 finished with value: 0.8176272405764266 and parameters: {'n_estimators': 701}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:22:37,016]\u001b[0m Trial 73 finished with value: 0.8176096241750844 and parameters: {'n_estimators': 772}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:24:46,342]\u001b[0m Trial 74 finished with value: 0.8167335141483711 and parameters: {'n_estimators': 471}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:27:54,640]\u001b[0m Trial 75 finished with value: 0.8165314805213393 and parameters: {'n_estimators': 625}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:28:46,782]\u001b[0m Trial 76 finished with value: 0.8127960034776021 and parameters: {'n_estimators': 142}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:33:26,825]\u001b[0m Trial 77 finished with value: 0.8178984977557009 and parameters: {'n_estimators': 895}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:36:49,483]\u001b[0m Trial 78 finished with value: 0.8176241329785215 and parameters: {'n_estimators': 743}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:40:39,315]\u001b[0m Trial 79 finished with value: 0.8167942874946167 and parameters: {'n_estimators': 849}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:44:14,871]\u001b[0m Trial 80 finished with value: 0.8167934507226875 and parameters: {'n_estimators': 796}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:47:48,780]\u001b[0m Trial 81 finished with value: 0.8167934507226875 and parameters: {'n_estimators': 785}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:50:47,865]\u001b[0m Trial 82 finished with value: 0.8173372180342995 and parameters: {'n_estimators': 661}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:54:59,190]\u001b[0m Trial 83 finished with value: 0.8184427976149257 and parameters: {'n_estimators': 937}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 19:58:22,997]\u001b[0m Trial 84 finished with value: 0.8167739782033812 and parameters: {'n_estimators': 757}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:02:08,141]\u001b[0m Trial 85 finished with value: 0.8173514476687392 and parameters: {'n_estimators': 825}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:05:16,691]\u001b[0m Trial 86 finished with value: 0.8173376342134121 and parameters: {'n_estimators': 694}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:08:27,319]\u001b[0m Trial 87 finished with value: 0.8173362597959943 and parameters: {'n_estimators': 712}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:11:21,745]\u001b[0m Trial 88 finished with value: 0.816486575439834 and parameters: {'n_estimators': 642}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:14:00,650]\u001b[0m Trial 89 finished with value: 0.8159437288421059 and parameters: {'n_estimators': 585}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:17:34,000]\u001b[0m Trial 90 finished with value: 0.8167934507226875 and parameters: {'n_estimators': 785}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:20:31,697]\u001b[0m Trial 91 finished with value: 0.8173372180342995 and parameters: {'n_estimators': 655}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:23:20,667]\u001b[0m Trial 92 finished with value: 0.8168224965695648 and parameters: {'n_estimators': 623}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:26:21,102]\u001b[0m Trial 93 finished with value: 0.8173379070873436 and parameters: {'n_estimators': 669}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:29:39,734]\u001b[0m Trial 94 finished with value: 0.8176241329785215 and parameters: {'n_estimators': 738}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:32:00,943]\u001b[0m Trial 95 finished with value: 0.8161875154773505 and parameters: {'n_estimators': 522}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:33:04,342]\u001b[0m Trial 96 finished with value: 0.8172657656672158 and parameters: {'n_estimators': 227}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 20:36:08,474]\u001b[0m Trial 97 finished with value: 0.8170499131947848 and parameters: {'n_estimators': 684}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:39:35,166]\u001b[0m Trial 98 finished with value: 0.8170695116456048 and parameters: {'n_estimators': 762}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:43:15,276]\u001b[0m Trial 99 finished with value: 0.8165064766249859 and parameters: {'n_estimators': 809}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8251\n",
      "\tBest params:\n",
      "\t\tn_estimators: 777\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  396.000000  403.000000\n",
      "1                    TN  343.000000  352.000000\n",
      "2                    FP   91.000000   91.000000\n",
      "3                    FN   69.000000   53.000000\n",
      "4              Accuracy    0.822024    0.839822\n",
      "5             Precision    0.813142    0.815789\n",
      "6           Sensitivity    0.851613    0.883772\n",
      "7           Specificity    0.790300    0.794600\n",
      "8              F1 score    0.831933    0.848421\n",
      "9   F1 score (weighted)    0.821767    0.839437\n",
      "10     F1 score (macro)    0.821404    0.839305\n",
      "11    Balanced Accuracy    0.820968    0.839177\n",
      "12                  MCC    0.643798    0.681632\n",
      "13                  NPV    0.832500    0.869100\n",
      "14              ROC_AUC    0.820968    0.839177\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_rf_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_rf_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_rf_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_rf_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_rf_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_rf_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_rf_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_rf_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_rf_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_rf_1)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 20:46:16,229]\u001b[0m Trial 100 finished with value: 0.8147372518716034 and parameters: {'n_estimators': 569}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:49:30,086]\u001b[0m Trial 101 finished with value: 0.8142267070593745 and parameters: {'n_estimators': 708}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:52:50,892]\u001b[0m Trial 102 finished with value: 0.8147927612508876 and parameters: {'n_estimators': 725}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:55:46,124]\u001b[0m Trial 103 finished with value: 0.8141989921068783 and parameters: {'n_estimators': 636}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 20:59:42,633]\u001b[0m Trial 104 finished with value: 0.8158764605241853 and parameters: {'n_estimators': 850}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:02:32,029]\u001b[0m Trial 105 finished with value: 0.8139272115703149 and parameters: {'n_estimators': 616}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:05:58,525]\u001b[0m Trial 106 finished with value: 0.8150713490192316 and parameters: {'n_estimators': 754}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:09:08,840]\u001b[0m Trial 107 finished with value: 0.8133761496396655 and parameters: {'n_estimators': 684}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:11:53,838]\u001b[0m Trial 108 finished with value: 0.8144769983595012 and parameters: {'n_estimators': 600}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:15:29,028]\u001b[0m Trial 109 finished with value: 0.8158942237864721 and parameters: {'n_estimators': 781}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:19:34,980]\u001b[0m Trial 110 finished with value: 0.8161575265430834 and parameters: {'n_estimators': 888}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:23:58,757]\u001b[0m Trial 111 finished with value: 0.8161546401633301 and parameters: {'n_estimators': 955}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:28:15,203]\u001b[0m Trial 112 finished with value: 0.8158823299349376 and parameters: {'n_estimators': 936}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:32:51,147]\u001b[0m Trial 113 finished with value: 0.815886831816415 and parameters: {'n_estimators': 992}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:37:20,571]\u001b[0m Trial 114 finished with value: 0.8155781859886038 and parameters: {'n_estimators': 981}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:39:17,734]\u001b[0m Trial 115 finished with value: 0.8155585598644631 and parameters: {'n_estimators': 421}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:43:01,132]\u001b[0m Trial 116 finished with value: 0.8164163920795426 and parameters: {'n_estimators': 811}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:45:31,403]\u001b[0m Trial 117 finished with value: 0.8155615724382553 and parameters: {'n_estimators': 543}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:48:49,997]\u001b[0m Trial 118 finished with value: 0.8156218005091711 and parameters: {'n_estimators': 720}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:52:02,088]\u001b[0m Trial 119 finished with value: 0.8147759276745876 and parameters: {'n_estimators': 702}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:55:02,991]\u001b[0m Trial 120 finished with value: 0.8141991897021474 and parameters: {'n_estimators': 653}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 21:59:01,495]\u001b[0m Trial 121 finished with value: 0.8161346069967699 and parameters: {'n_estimators': 865}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:03:11,710]\u001b[0m Trial 122 finished with value: 0.816446703832645 and parameters: {'n_estimators': 909}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:06:56,265]\u001b[0m Trial 123 finished with value: 0.8169679678967368 and parameters: {'n_estimators': 830}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:08:31,160]\u001b[0m Trial 124 finished with value: 0.8153457697604575 and parameters: {'n_estimators': 335}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:11:52,793]\u001b[0m Trial 125 finished with value: 0.8150645417874511 and parameters: {'n_estimators': 735}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:15:52,971]\u001b[0m Trial 126 finished with value: 0.8158642235188001 and parameters: {'n_estimators': 868}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:19:22,538]\u001b[0m Trial 127 finished with value: 0.8153121728696888 and parameters: {'n_estimators': 767}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:23:02,621]\u001b[0m Trial 128 finished with value: 0.8164377297272754 and parameters: {'n_estimators': 794}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:26:09,705]\u001b[0m Trial 129 finished with value: 0.8133632258333252 and parameters: {'n_estimators': 670}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:29:36,806]\u001b[0m Trial 130 finished with value: 0.8147718831609103 and parameters: {'n_estimators': 755}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:34:03,463]\u001b[0m Trial 131 finished with value: 0.8158822746153274 and parameters: {'n_estimators': 971}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:38:18,863]\u001b[0m Trial 132 finished with value: 0.8155931526453759 and parameters: {'n_estimators': 935}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:41:25,250]\u001b[0m Trial 133 finished with value: 0.8139066884469269 and parameters: {'n_estimators': 671}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:44:37,669]\u001b[0m Trial 134 finished with value: 0.813390750814143 and parameters: {'n_estimators': 693}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:47:31,754]\u001b[0m Trial 135 finished with value: 0.81446472798001 and parameters: {'n_estimators': 632}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:50:56,579]\u001b[0m Trial 136 finished with value: 0.8155734872547722 and parameters: {'n_estimators': 739}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:51:38,347]\u001b[0m Trial 137 finished with value: 0.8165683163952672 and parameters: {'n_estimators': 146}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:54:19,268]\u001b[0m Trial 138 finished with value: 0.8150317182529264 and parameters: {'n_estimators': 581}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 22:57:35,026]\u001b[0m Trial 139 finished with value: 0.8142424252348514 and parameters: {'n_estimators': 706}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:00:22,995]\u001b[0m Trial 140 finished with value: 0.8142163888598765 and parameters: {'n_estimators': 611}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:03:42,878]\u001b[0m Trial 141 finished with value: 0.8150725798939579 and parameters: {'n_estimators': 724}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:06:51,069]\u001b[0m Trial 142 finished with value: 0.8133761496396655 and parameters: {'n_estimators': 684}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:10:23,408]\u001b[0m Trial 143 finished with value: 0.8155783091292543 and parameters: {'n_estimators': 777}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:13:23,200]\u001b[0m Trial 144 finished with value: 0.8144822523271932 and parameters: {'n_estimators': 652}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:17:14,537]\u001b[0m Trial 145 finished with value: 0.816413751192627 and parameters: {'n_estimators': 837}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 23:20:38,964]\u001b[0m Trial 146 finished with value: 0.8147718831609103 and parameters: {'n_estimators': 749}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:24:23,147]\u001b[0m Trial 147 finished with value: 0.8161447100025043 and parameters: {'n_estimators': 814}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:27:25,249]\u001b[0m Trial 148 finished with value: 0.8139313901301491 and parameters: {'n_estimators': 663}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:30:42,975]\u001b[0m Trial 149 finished with value: 0.815348678046338 and parameters: {'n_estimators': 716}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8251\n",
      "\tBest params:\n",
      "\t\tn_estimators: 777\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  396.000000  403.000000  405.000000\n",
      "1                    TN  343.000000  352.000000  344.000000\n",
      "2                    FP   91.000000   91.000000   80.000000\n",
      "3                    FN   69.000000   53.000000   70.000000\n",
      "4              Accuracy    0.822024    0.839822    0.833148\n",
      "5             Precision    0.813142    0.815789    0.835052\n",
      "6           Sensitivity    0.851613    0.883772    0.852632\n",
      "7           Specificity    0.790300    0.794600    0.811300\n",
      "8              F1 score    0.831933    0.848421    0.843750\n",
      "9   F1 score (weighted)    0.821767    0.839437    0.833021\n",
      "10     F1 score (macro)    0.821404    0.839305    0.832376\n",
      "11    Balanced Accuracy    0.820968    0.839177    0.831976\n",
      "12                  MCC    0.643798    0.681632    0.664960\n",
      "13                  NPV    0.832500    0.869100    0.830900\n",
      "14              ROC_AUC    0.820968    0.839177    0.831976\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_rf_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_rf_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_rf_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_rf_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_rf_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_rf_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_rf_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_rf_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_rf_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_rf_2)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 23:35:11,827]\u001b[0m Trial 150 finished with value: 0.8116479420913203 and parameters: {'n_estimators': 903}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:38:33,661]\u001b[0m Trial 151 finished with value: 0.813519797505105 and parameters: {'n_estimators': 766}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:41:48,659]\u001b[0m Trial 152 finished with value: 0.8140745314901178 and parameters: {'n_estimators': 737}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:45:20,896]\u001b[0m Trial 153 finished with value: 0.813248988297544 and parameters: {'n_estimators': 799}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:48:27,525]\u001b[0m Trial 154 finished with value: 0.8129677335487792 and parameters: {'n_estimators': 694}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:51:40,381]\u001b[0m Trial 155 finished with value: 0.8123877305976995 and parameters: {'n_estimators': 712}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:55:38,778]\u001b[0m Trial 156 finished with value: 0.8135830915921402 and parameters: {'n_estimators': 883}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-16 23:59:31,436]\u001b[0m Trial 157 finished with value: 0.8127333115326312 and parameters: {'n_estimators': 854}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:03:02,994]\u001b[0m Trial 158 finished with value: 0.8132660426858738 and parameters: {'n_estimators': 778}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:06:08,451]\u001b[0m Trial 159 finished with value: 0.813218388707224 and parameters: {'n_estimators': 681}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:09:28,758]\u001b[0m Trial 160 finished with value: 0.8140867125689191 and parameters: {'n_estimators': 738}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:13:05,039]\u001b[0m Trial 161 finished with value: 0.8132514119208988 and parameters: {'n_estimators': 805}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:16:33,758]\u001b[0m Trial 162 finished with value: 0.8138026416157336 and parameters: {'n_estimators': 761}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:20:21,404]\u001b[0m Trial 163 finished with value: 0.8124299667573549 and parameters: {'n_estimators': 833}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:23:57,185]\u001b[0m Trial 164 finished with value: 0.8129822077014559 and parameters: {'n_estimators': 785}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:27:41,711]\u001b[0m Trial 165 finished with value: 0.8129682452144336 and parameters: {'n_estimators': 821}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:30:37,607]\u001b[0m Trial 166 finished with value: 0.8120947567061123 and parameters: {'n_estimators': 642}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:33:55,277]\u001b[0m Trial 167 finished with value: 0.8134917363322213 and parameters: {'n_estimators': 723}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:37:06,356]\u001b[0m Trial 168 finished with value: 0.8129341992323715 and parameters: {'n_estimators': 701}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:40:43,020]\u001b[0m Trial 169 finished with value: 0.8135279764306576 and parameters: {'n_estimators': 796}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:44:08,319]\u001b[0m Trial 170 finished with value: 0.8141070773360713 and parameters: {'n_estimators': 753}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:48:29,859]\u001b[0m Trial 171 finished with value: 0.8127372360206475 and parameters: {'n_estimators': 959}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:52:41,420]\u001b[0m Trial 172 finished with value: 0.812726250517423 and parameters: {'n_estimators': 929}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 00:57:14,175]\u001b[0m Trial 173 finished with value: 0.8127494065263244 and parameters: {'n_estimators': 999}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:01:30,441]\u001b[0m Trial 174 finished with value: 0.8127372360206475 and parameters: {'n_estimators': 944}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:05:56,209]\u001b[0m Trial 175 finished with value: 0.8130302256470557 and parameters: {'n_estimators': 975}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:10:03,199]\u001b[0m Trial 176 finished with value: 0.8121763432410247 and parameters: {'n_estimators': 915}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:13:04,678]\u001b[0m Trial 177 finished with value: 0.8137586676407077 and parameters: {'n_estimators': 665}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:16:55,891]\u001b[0m Trial 178 finished with value: 0.8130022732388671 and parameters: {'n_estimators': 855}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:20:26,791]\u001b[0m Trial 179 finished with value: 0.8132270006213019 and parameters: {'n_estimators': 771}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:23:19,255]\u001b[0m Trial 180 finished with value: 0.8132139210850029 and parameters: {'n_estimators': 631}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:27:19,240]\u001b[0m Trial 181 finished with value: 0.8135967753550783 and parameters: {'n_estimators': 878}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:31:21,397]\u001b[0m Trial 182 finished with value: 0.8121977486376843 and parameters: {'n_estimators': 898}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:35:41,038]\u001b[0m Trial 183 finished with value: 0.8130165311170184 and parameters: {'n_estimators': 950}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:39:48,216]\u001b[0m Trial 184 finished with value: 0.8121763432410247 and parameters: {'n_estimators': 915}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:43:59,969]\u001b[0m Trial 185 finished with value: 0.8121755298276868 and parameters: {'n_estimators': 921}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:48:23,821]\u001b[0m Trial 186 finished with value: 0.8133194170308657 and parameters: {'n_estimators': 970}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:51:34,172]\u001b[0m Trial 187 finished with value: 0.8132317222671883 and parameters: {'n_estimators': 690}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:54:54,496]\u001b[0m Trial 188 finished with value: 0.8138474715322637 and parameters: {'n_estimators': 732}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 01:57:07,828]\u001b[0m Trial 189 finished with value: 0.8121404831876748 and parameters: {'n_estimators': 486}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:00:21,186]\u001b[0m Trial 190 finished with value: 0.8126566118331174 and parameters: {'n_estimators': 710}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:04:37,544]\u001b[0m Trial 191 finished with value: 0.8124301488900499 and parameters: {'n_estimators': 939}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:08:37,938]\u001b[0m Trial 192 finished with value: 0.8135935634163601 and parameters: {'n_estimators': 882}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:12:57,654]\u001b[0m Trial 193 finished with value: 0.8132854928232544 and parameters: {'n_estimators': 955}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:17:02,561]\u001b[0m Trial 194 finished with value: 0.8121977486376843 and parameters: {'n_estimators': 897}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:18:15,255]\u001b[0m Trial 195 finished with value: 0.8133397113856402 and parameters: {'n_estimators': 259}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 02:21:40,067]\u001b[0m Trial 196 finished with value: 0.8140842163553188 and parameters: {'n_estimators': 750}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:25:50,163]\u001b[0m Trial 197 finished with value: 0.8124681209129271 and parameters: {'n_estimators': 922}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:28:55,456]\u001b[0m Trial 198 finished with value: 0.8135019922259303 and parameters: {'n_estimators': 674}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:31:51,681]\u001b[0m Trial 199 finished with value: 0.812104435630763 and parameters: {'n_estimators': 651}. Best is trial 35 with value: 0.8250674264961845.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8251\n",
      "\tBest params:\n",
      "\t\tn_estimators: 777\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  396.000000  403.000000  405.000000  419.000000\n",
      "1                    TN  343.000000  352.000000  344.000000  336.000000\n",
      "2                    FP   91.000000   91.000000   80.000000   93.000000\n",
      "3                    FN   69.000000   53.000000   70.000000   51.000000\n",
      "4              Accuracy    0.822024    0.839822    0.833148    0.839822\n",
      "5             Precision    0.813142    0.815789    0.835052    0.818359\n",
      "6           Sensitivity    0.851613    0.883772    0.852632    0.891489\n",
      "7           Specificity    0.790300    0.794600    0.811300    0.783200\n",
      "8              F1 score    0.831933    0.848421    0.843750    0.853360\n",
      "9   F1 score (weighted)    0.821767    0.839437    0.833021    0.839125\n",
      "10     F1 score (macro)    0.821404    0.839305    0.832376    0.838445\n",
      "11    Balanced Accuracy    0.820968    0.839177    0.831976    0.837353\n",
      "12                  MCC    0.643798    0.681632    0.664960    0.680615\n",
      "13                  NPV    0.832500    0.869100    0.830900    0.868200\n",
      "14              ROC_AUC    0.820968    0.839177    0.831976    0.837353\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_rf_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_rf_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_rf_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_rf_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_rf_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_rf_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_rf_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_rf_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_rf_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_rf_3)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 02:34:59,698]\u001b[0m Trial 200 finished with value: 0.825823476857164 and parameters: {'n_estimators': 607}. Best is trial 200 with value: 0.825823476857164.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:37:44,649]\u001b[0m Trial 201 finished with value: 0.8252798280136693 and parameters: {'n_estimators': 614}. Best is trial 200 with value: 0.825823476857164.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:40:25,385]\u001b[0m Trial 202 finished with value: 0.8261103680589285 and parameters: {'n_estimators': 600}. Best is trial 202 with value: 0.8261103680589285.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:43:04,758]\u001b[0m Trial 203 finished with value: 0.8269406649401688 and parameters: {'n_estimators': 596}. Best is trial 203 with value: 0.8269406649401688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:45:43,959]\u001b[0m Trial 204 finished with value: 0.8269406649401688 and parameters: {'n_estimators': 597}. Best is trial 203 with value: 0.8269406649401688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:48:26,946]\u001b[0m Trial 205 finished with value: 0.8263643923556719 and parameters: {'n_estimators': 601}. Best is trial 203 with value: 0.8269406649401688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:51:05,071]\u001b[0m Trial 206 finished with value: 0.8275033737570974 and parameters: {'n_estimators': 591}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:53:45,492]\u001b[0m Trial 207 finished with value: 0.8269406649401688 and parameters: {'n_estimators': 598}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:56:27,224]\u001b[0m Trial 208 finished with value: 0.8261103680589285 and parameters: {'n_estimators': 600}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 02:58:58,891]\u001b[0m Trial 209 finished with value: 0.8272393846671505 and parameters: {'n_estimators': 564}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:01:28,736]\u001b[0m Trial 210 finished with value: 0.8272206075961168 and parameters: {'n_estimators': 557}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:03:57,372]\u001b[0m Trial 211 finished with value: 0.8272507619639488 and parameters: {'n_estimators': 554}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:06:27,281]\u001b[0m Trial 212 finished with value: 0.8272206075961168 and parameters: {'n_estimators': 558}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:08:58,784]\u001b[0m Trial 213 finished with value: 0.8272206075961168 and parameters: {'n_estimators': 558}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:11:31,334]\u001b[0m Trial 214 finished with value: 0.8272206075961168 and parameters: {'n_estimators': 561}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:13:59,435]\u001b[0m Trial 215 finished with value: 0.8269800735226436 and parameters: {'n_estimators': 552}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:16:27,734]\u001b[0m Trial 216 finished with value: 0.8272507619639488 and parameters: {'n_estimators': 554}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:18:55,472]\u001b[0m Trial 217 finished with value: 0.8269800735226436 and parameters: {'n_estimators': 552}. Best is trial 206 with value: 0.8275033737570974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:21:24,115]\u001b[0m Trial 218 finished with value: 0.8275233617173242 and parameters: {'n_estimators': 553}. Best is trial 218 with value: 0.8275233617173242.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:23:55,138]\u001b[0m Trial 219 finished with value: 0.8272393846671505 and parameters: {'n_estimators': 564}. Best is trial 218 with value: 0.8275233617173242.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:26:24,551]\u001b[0m Trial 220 finished with value: 0.8269800735226436 and parameters: {'n_estimators': 552}. Best is trial 218 with value: 0.8275233617173242.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:28:52,998]\u001b[0m Trial 221 finished with value: 0.8272507619639488 and parameters: {'n_estimators': 554}. Best is trial 218 with value: 0.8275233617173242.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:31:21,955]\u001b[0m Trial 222 finished with value: 0.8272507619639488 and parameters: {'n_estimators': 554}. Best is trial 218 with value: 0.8275233617173242.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:33:49,851]\u001b[0m Trial 223 finished with value: 0.8277944953970255 and parameters: {'n_estimators': 556}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:36:18,338]\u001b[0m Trial 224 finished with value: 0.8277944953970255 and parameters: {'n_estimators': 556}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:38:47,260]\u001b[0m Trial 225 finished with value: 0.8272507619639488 and parameters: {'n_estimators': 554}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:41:09,221]\u001b[0m Trial 226 finished with value: 0.8264043107644387 and parameters: {'n_estimators': 526}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:43:29,921]\u001b[0m Trial 227 finished with value: 0.8263998559175656 and parameters: {'n_estimators': 523}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:45:53,043]\u001b[0m Trial 228 finished with value: 0.8272361886427131 and parameters: {'n_estimators': 531}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:48:21,254]\u001b[0m Trial 229 finished with value: 0.8275233617173242 and parameters: {'n_estimators': 553}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:50:51,841]\u001b[0m Trial 230 finished with value: 0.8272206075961168 and parameters: {'n_estimators': 558}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:53:20,498]\u001b[0m Trial 231 finished with value: 0.8272507619639488 and parameters: {'n_estimators': 554}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:55:49,595]\u001b[0m Trial 232 finished with value: 0.8272507619639488 and parameters: {'n_estimators': 554}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 03:58:17,666]\u001b[0m Trial 233 finished with value: 0.8272206075961168 and parameters: {'n_estimators': 557}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:00:46,151]\u001b[0m Trial 234 finished with value: 0.8275233617173242 and parameters: {'n_estimators': 553}. Best is trial 223 with value: 0.8277944953970255.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:03:18,710]\u001b[0m Trial 235 finished with value: 0.827798506040971 and parameters: {'n_estimators': 567}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:05:52,796]\u001b[0m Trial 236 finished with value: 0.8272343783481692 and parameters: {'n_estimators': 572}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:08:26,626]\u001b[0m Trial 237 finished with value: 0.8275071332945803 and parameters: {'n_estimators': 574}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:11:01,147]\u001b[0m Trial 238 finished with value: 0.8272169249419857 and parameters: {'n_estimators': 575}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:13:25,402]\u001b[0m Trial 239 finished with value: 0.8269609585239195 and parameters: {'n_estimators': 534}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:15:57,192]\u001b[0m Trial 240 finished with value: 0.8275088734262276 and parameters: {'n_estimators': 569}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:18:31,061]\u001b[0m Trial 241 finished with value: 0.8275071332945803 and parameters: {'n_estimators': 574}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:20:49,155]\u001b[0m Trial 242 finished with value: 0.8261307776345994 and parameters: {'n_estimators': 510}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:23:20,768]\u001b[0m Trial 243 finished with value: 0.8275071332945803 and parameters: {'n_estimators': 574}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:25:54,954]\u001b[0m Trial 244 finished with value: 0.8272343783481692 and parameters: {'n_estimators': 576}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:28:31,398]\u001b[0m Trial 245 finished with value: 0.8269589631202392 and parameters: {'n_estimators': 580}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 04:30:55,728]\u001b[0m Trial 246 finished with value: 0.8258589355957661 and parameters: {'n_estimators': 540}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:33:28,834]\u001b[0m Trial 247 finished with value: 0.8275088734262276 and parameters: {'n_estimators': 569}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:36:02,612]\u001b[0m Trial 248 finished with value: 0.8269509767105794 and parameters: {'n_estimators': 577}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:38:25,319]\u001b[0m Trial 249 finished with value: 0.8269482346388637 and parameters: {'n_estimators': 537}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8278\n",
      "\tBest params:\n",
      "\t\tn_estimators: 567\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  396.000000  403.000000  405.000000  419.000000   \n",
      "1                    TN  343.000000  352.000000  344.000000  336.000000   \n",
      "2                    FP   91.000000   91.000000   80.000000   93.000000   \n",
      "3                    FN   69.000000   53.000000   70.000000   51.000000   \n",
      "4              Accuracy    0.822024    0.839822    0.833148    0.839822   \n",
      "5             Precision    0.813142    0.815789    0.835052    0.818359   \n",
      "6           Sensitivity    0.851613    0.883772    0.852632    0.891489   \n",
      "7           Specificity    0.790300    0.794600    0.811300    0.783200   \n",
      "8              F1 score    0.831933    0.848421    0.843750    0.853360   \n",
      "9   F1 score (weighted)    0.821767    0.839437    0.833021    0.839125   \n",
      "10     F1 score (macro)    0.821404    0.839305    0.832376    0.838445   \n",
      "11    Balanced Accuracy    0.820968    0.839177    0.831976    0.837353   \n",
      "12                  MCC    0.643798    0.681632    0.664960    0.680615   \n",
      "13                  NPV    0.832500    0.869100    0.830900    0.868200   \n",
      "14              ROC_AUC    0.820968    0.839177    0.831976    0.837353   \n",
      "\n",
      "          Set4  \n",
      "0   395.000000  \n",
      "1   350.000000  \n",
      "2   101.000000  \n",
      "3    53.000000  \n",
      "4     0.828699  \n",
      "5     0.796371  \n",
      "6     0.881696  \n",
      "7     0.776100  \n",
      "8     0.836864  \n",
      "9     0.828240  \n",
      "10    0.828268  \n",
      "11    0.828875  \n",
      "12    0.661294  \n",
      "13    0.868500  \n",
      "14    0.828875  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_rf_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_rf_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_rf_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_rf_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_rf_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_rf_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_rf_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_rf_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_rf_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_rf_4)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 04:41:12,584]\u001b[0m Trial 250 finished with value: 0.827004215310167 and parameters: {'n_estimators': 575}. Best is trial 235 with value: 0.827798506040971.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:43:24,537]\u001b[0m Trial 251 finished with value: 0.8295461804844522 and parameters: {'n_estimators': 507}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:45:35,886]\u001b[0m Trial 252 finished with value: 0.8292587484684478 and parameters: {'n_estimators': 505}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:47:47,211]\u001b[0m Trial 253 finished with value: 0.8286776888770587 and parameters: {'n_estimators': 501}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:50:00,040]\u001b[0m Trial 254 finished with value: 0.8287073526226827 and parameters: {'n_estimators': 496}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:52:11,852]\u001b[0m Trial 255 finished with value: 0.8275462424718031 and parameters: {'n_estimators': 493}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:54:12,267]\u001b[0m Trial 256 finished with value: 0.8292269295219356 and parameters: {'n_estimators': 455}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:56:15,990]\u001b[0m Trial 257 finished with value: 0.8292328492657921 and parameters: {'n_estimators': 459}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 04:58:19,627]\u001b[0m Trial 258 finished with value: 0.828954920141966 and parameters: {'n_estimators': 463}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:00:25,089]\u001b[0m Trial 259 finished with value: 0.8295037119247075 and parameters: {'n_estimators': 466}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:02:27,081]\u001b[0m Trial 260 finished with value: 0.828127427823874 and parameters: {'n_estimators': 449}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:04:29,967]\u001b[0m Trial 261 finished with value: 0.8292269295219356 and parameters: {'n_estimators': 455}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:06:31,985]\u001b[0m Trial 262 finished with value: 0.8295345949640003 and parameters: {'n_estimators': 454}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:08:35,886]\u001b[0m Trial 263 finished with value: 0.8292328492657921 and parameters: {'n_estimators': 459}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:10:35,537]\u001b[0m Trial 264 finished with value: 0.82894157751807 and parameters: {'n_estimators': 443}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:12:37,726]\u001b[0m Trial 265 finished with value: 0.8286605221182917 and parameters: {'n_estimators': 453}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:14:39,922]\u001b[0m Trial 266 finished with value: 0.8286804338929319 and parameters: {'n_estimators': 452}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:16:42,088]\u001b[0m Trial 267 finished with value: 0.8289534736958604 and parameters: {'n_estimators': 451}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:18:43,820]\u001b[0m Trial 268 finished with value: 0.8286605221182917 and parameters: {'n_estimators': 453}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:20:44,456]\u001b[0m Trial 269 finished with value: 0.8281347810999001 and parameters: {'n_estimators': 450}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:22:44,583]\u001b[0m Trial 270 finished with value: 0.8281347810999001 and parameters: {'n_estimators': 450}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:24:45,084]\u001b[0m Trial 271 finished with value: 0.8292621431540168 and parameters: {'n_estimators': 448}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:26:43,949]\u001b[0m Trial 272 finished with value: 0.8292354147509855 and parameters: {'n_estimators': 442}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:28:45,183]\u001b[0m Trial 273 finished with value: 0.828127427823874 and parameters: {'n_estimators': 449}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:30:48,186]\u001b[0m Trial 274 finished with value: 0.828127427823874 and parameters: {'n_estimators': 449}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:32:51,370]\u001b[0m Trial 275 finished with value: 0.8292621431540168 and parameters: {'n_estimators': 448}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:34:54,194]\u001b[0m Trial 276 finished with value: 0.8292621431540168 and parameters: {'n_estimators': 448}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:36:57,066]\u001b[0m Trial 277 finished with value: 0.8295193725412604 and parameters: {'n_estimators': 447}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:39:01,830]\u001b[0m Trial 278 finished with value: 0.8289537978112353 and parameters: {'n_estimators': 457}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:40:55,753]\u001b[0m Trial 279 finished with value: 0.8286729500417227 and parameters: {'n_estimators': 421}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:42:35,635]\u001b[0m Trial 280 finished with value: 0.828687642286608 and parameters: {'n_estimators': 416}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:44:15,222]\u001b[0m Trial 281 finished with value: 0.8289407651666488 and parameters: {'n_estimators': 411}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:45:56,289]\u001b[0m Trial 282 finished with value: 0.828687642286608 and parameters: {'n_estimators': 416}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:47:34,545]\u001b[0m Trial 283 finished with value: 0.828955775897205 and parameters: {'n_estimators': 412}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:49:13,501]\u001b[0m Trial 284 finished with value: 0.8289467649683342 and parameters: {'n_estimators': 408}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:50:47,552]\u001b[0m Trial 285 finished with value: 0.8272972980373702 and parameters: {'n_estimators': 389}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:52:24,119]\u001b[0m Trial 286 finished with value: 0.8286494558256459 and parameters: {'n_estimators': 397}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:54:17,643]\u001b[0m Trial 287 finished with value: 0.8292434027609945 and parameters: {'n_estimators': 470}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:56:09,897]\u001b[0m Trial 288 finished with value: 0.8292434027609945 and parameters: {'n_estimators': 470}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 05:58:04,672]\u001b[0m Trial 289 finished with value: 0.8287176468071678 and parameters: {'n_estimators': 476}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:00:00,629]\u001b[0m Trial 290 finished with value: 0.8292471102076446 and parameters: {'n_estimators': 480}. Best is trial 251 with value: 0.8295461804844522.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:01:54,248]\u001b[0m Trial 291 finished with value: 0.8298075275997423 and parameters: {'n_estimators': 474}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:03:47,610]\u001b[0m Trial 292 finished with value: 0.8292311908423761 and parameters: {'n_estimators': 472}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:05:40,829]\u001b[0m Trial 293 finished with value: 0.8292311908423761 and parameters: {'n_estimators': 472}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:07:35,048]\u001b[0m Trial 294 finished with value: 0.8295011260743044 and parameters: {'n_estimators': 471}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:09:28,456]\u001b[0m Trial 295 finished with value: 0.8292311908423761 and parameters: {'n_estimators': 472}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:11:21,958]\u001b[0m Trial 296 finished with value: 0.8286519796097774 and parameters: {'n_estimators': 473}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:13:13,730]\u001b[0m Trial 297 finished with value: 0.8294971252169608 and parameters: {'n_estimators': 467}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:15:05,569]\u001b[0m Trial 298 finished with value: 0.8283826715251748 and parameters: {'n_estimators': 469}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:16:48,009]\u001b[0m Trial 299 finished with value: 0.8295452439005381 and parameters: {'n_estimators': 432}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8298\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  396.000000  403.000000  405.000000  419.000000   \n",
      "1                    TN  343.000000  352.000000  344.000000  336.000000   \n",
      "2                    FP   91.000000   91.000000   80.000000   93.000000   \n",
      "3                    FN   69.000000   53.000000   70.000000   51.000000   \n",
      "4              Accuracy    0.822024    0.839822    0.833148    0.839822   \n",
      "5             Precision    0.813142    0.815789    0.835052    0.818359   \n",
      "6           Sensitivity    0.851613    0.883772    0.852632    0.891489   \n",
      "7           Specificity    0.790300    0.794600    0.811300    0.783200   \n",
      "8              F1 score    0.831933    0.848421    0.843750    0.853360   \n",
      "9   F1 score (weighted)    0.821767    0.839437    0.833021    0.839125   \n",
      "10     F1 score (macro)    0.821404    0.839305    0.832376    0.838445   \n",
      "11    Balanced Accuracy    0.820968    0.839177    0.831976    0.837353   \n",
      "12                  MCC    0.643798    0.681632    0.664960    0.680615   \n",
      "13                  NPV    0.832500    0.869100    0.830900    0.868200   \n",
      "14              ROC_AUC    0.820968    0.839177    0.831976    0.837353   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   395.000000  389.000000  \n",
      "1   350.000000  341.000000  \n",
      "2   101.000000   92.000000  \n",
      "3    53.000000   77.000000  \n",
      "4     0.828699    0.812013  \n",
      "5     0.796371    0.808732  \n",
      "6     0.881696    0.834764  \n",
      "7     0.776100    0.787500  \n",
      "8     0.836864    0.821542  \n",
      "9     0.828240    0.811845  \n",
      "10    0.828268    0.811476  \n",
      "11    0.828875    0.811146  \n",
      "12    0.661294    0.623406  \n",
      "13    0.868500    0.815800  \n",
      "14    0.828875    0.811146  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_rf_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_rf_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_rf_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_rf_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_rf_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_rf_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_rf_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_rf_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_rf_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_rf_5)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 06:18:45,183]\u001b[0m Trial 300 finished with value: 0.8207191831243856 and parameters: {'n_estimators': 433}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:20:38,595]\u001b[0m Trial 301 finished with value: 0.8220324245600901 and parameters: {'n_estimators': 476}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:22:34,642]\u001b[0m Trial 302 finished with value: 0.821779898536315 and parameters: {'n_estimators': 482}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:24:18,144]\u001b[0m Trial 303 finished with value: 0.8204488250544619 and parameters: {'n_estimators': 434}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:26:10,353]\u001b[0m Trial 304 finished with value: 0.8214770407590481 and parameters: {'n_estimators': 469}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:28:06,131]\u001b[0m Trial 305 finished with value: 0.8220498874320652 and parameters: {'n_estimators': 484}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:29:47,267]\u001b[0m Trial 306 finished with value: 0.8207340855210825 and parameters: {'n_estimators': 428}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:31:15,471]\u001b[0m Trial 307 finished with value: 0.8212083650322384 and parameters: {'n_estimators': 367}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:33:06,542]\u001b[0m Trial 308 finished with value: 0.8212117733720256 and parameters: {'n_estimators': 466}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:35:07,993]\u001b[0m Trial 309 finished with value: 0.8217629695021621 and parameters: {'n_estimators': 506}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:37:04,505]\u001b[0m Trial 310 finished with value: 0.8226174018298018 and parameters: {'n_estimators': 492}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:38:47,711]\u001b[0m Trial 311 finished with value: 0.820445188295406 and parameters: {'n_estimators': 432}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:40:40,531]\u001b[0m Trial 312 finished with value: 0.8220254054692109 and parameters: {'n_estimators': 471}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:42:37,936]\u001b[0m Trial 313 finished with value: 0.821768826287698 and parameters: {'n_estimators': 488}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:44:29,166]\u001b[0m Trial 314 finished with value: 0.8212117733720256 and parameters: {'n_estimators': 466}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:46:28,052]\u001b[0m Trial 315 finished with value: 0.8217629695021621 and parameters: {'n_estimators': 510}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:48:09,747]\u001b[0m Trial 316 finished with value: 0.8207191831243856 and parameters: {'n_estimators': 431}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:49:53,373]\u001b[0m Trial 317 finished with value: 0.8207191831243856 and parameters: {'n_estimators': 435}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:51:43,426]\u001b[0m Trial 318 finished with value: 0.8206460461195231 and parameters: {'n_estimators': 463}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:53:39,684]\u001b[0m Trial 319 finished with value: 0.8223146713320348 and parameters: {'n_estimators': 497}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:55:32,926]\u001b[0m Trial 320 finished with value: 0.8217454841759017 and parameters: {'n_estimators': 474}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:57:16,744]\u001b[0m Trial 321 finished with value: 0.8204253298935645 and parameters: {'n_estimators': 437}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 06:59:10,732]\u001b[0m Trial 322 finished with value: 0.82232319082635 and parameters: {'n_estimators': 486}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:00:45,403]\u001b[0m Trial 323 finished with value: 0.8220595017859914 and parameters: {'n_estimators': 401}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:02:36,089]\u001b[0m Trial 324 finished with value: 0.8217719298723996 and parameters: {'n_estimators': 470}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:04:24,732]\u001b[0m Trial 325 finished with value: 0.8209409352328745 and parameters: {'n_estimators': 460}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:06:24,870]\u001b[0m Trial 326 finished with value: 0.8217629695021621 and parameters: {'n_estimators': 510}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:08:07,244]\u001b[0m Trial 327 finished with value: 0.8207191831243856 and parameters: {'n_estimators': 435}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:10:03,155]\u001b[0m Trial 328 finished with value: 0.8220429556192854 and parameters: {'n_estimators': 487}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:11:51,866]\u001b[0m Trial 329 finished with value: 0.8209409352328745 and parameters: {'n_estimators': 461}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:13:30,603]\u001b[0m Trial 330 finished with value: 0.8209804384452062 and parameters: {'n_estimators': 420}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:15:14,246]\u001b[0m Trial 331 finished with value: 0.8212541525980359 and parameters: {'n_estimators': 446}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:16:43,372]\u001b[0m Trial 332 finished with value: 0.8214998248261176 and parameters: {'n_estimators': 377}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:18:36,269]\u001b[0m Trial 333 finished with value: 0.8217536883911682 and parameters: {'n_estimators': 481}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:20:35,750]\u001b[0m Trial 334 finished with value: 0.8220571805056138 and parameters: {'n_estimators': 513}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:22:25,112]\u001b[0m Trial 335 finished with value: 0.8206460461195231 and parameters: {'n_estimators': 463}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:24:08,883]\u001b[0m Trial 336 finished with value: 0.820136787858989 and parameters: {'n_estimators': 441}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:26:06,125]\u001b[0m Trial 337 finished with value: 0.8223146713320348 and parameters: {'n_estimators': 497}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:27:45,151]\u001b[0m Trial 338 finished with value: 0.8209804384452062 and parameters: {'n_estimators': 420}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:29:32,644]\u001b[0m Trial 339 finished with value: 0.8209275525504927 and parameters: {'n_estimators': 458}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:31:26,883]\u001b[0m Trial 340 finished with value: 0.8220347500387255 and parameters: {'n_estimators': 483}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:33:10,501]\u001b[0m Trial 341 finished with value: 0.8201647425298786 and parameters: {'n_estimators': 436}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:35:00,030]\u001b[0m Trial 342 finished with value: 0.8212224416638438 and parameters: {'n_estimators': 464}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:36:37,108]\u001b[0m Trial 343 finished with value: 0.8209794644356837 and parameters: {'n_estimators': 411}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:38:34,984]\u001b[0m Trial 344 finished with value: 0.8223161709337885 and parameters: {'n_estimators': 501}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:40:20,596]\u001b[0m Trial 345 finished with value: 0.8212460805032556 and parameters: {'n_estimators': 445}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 07:42:12,745]\u001b[0m Trial 346 finished with value: 0.8220324245600901 and parameters: {'n_estimators': 476}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:43:47,304]\u001b[0m Trial 347 finished with value: 0.8220595017859914 and parameters: {'n_estimators': 401}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:45:35,775]\u001b[0m Trial 348 finished with value: 0.8209409352328745 and parameters: {'n_estimators': 460}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:47:16,249]\u001b[0m Trial 349 finished with value: 0.8201766635895519 and parameters: {'n_estimators': 426}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8298\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  396.000000  403.000000  405.000000  419.000000   \n",
      "1                    TN  343.000000  352.000000  344.000000  336.000000   \n",
      "2                    FP   91.000000   91.000000   80.000000   93.000000   \n",
      "3                    FN   69.000000   53.000000   70.000000   51.000000   \n",
      "4              Accuracy    0.822024    0.839822    0.833148    0.839822   \n",
      "5             Precision    0.813142    0.815789    0.835052    0.818359   \n",
      "6           Sensitivity    0.851613    0.883772    0.852632    0.891489   \n",
      "7           Specificity    0.790300    0.794600    0.811300    0.783200   \n",
      "8              F1 score    0.831933    0.848421    0.843750    0.853360   \n",
      "9   F1 score (weighted)    0.821767    0.839437    0.833021    0.839125   \n",
      "10     F1 score (macro)    0.821404    0.839305    0.832376    0.838445   \n",
      "11    Balanced Accuracy    0.820968    0.839177    0.831976    0.837353   \n",
      "12                  MCC    0.643798    0.681632    0.664960    0.680615   \n",
      "13                  NPV    0.832500    0.869100    0.830900    0.868200   \n",
      "14              ROC_AUC    0.820968    0.839177    0.831976    0.837353   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   395.000000  389.000000  407.000000  \n",
      "1   350.000000  341.000000  329.000000  \n",
      "2   101.000000   92.000000  100.000000  \n",
      "3    53.000000   77.000000   63.000000  \n",
      "4     0.828699    0.812013    0.818687  \n",
      "5     0.796371    0.808732    0.802761  \n",
      "6     0.881696    0.834764    0.865957  \n",
      "7     0.776100    0.787500    0.766900  \n",
      "8     0.836864    0.821542    0.833163  \n",
      "9     0.828240    0.811845    0.818035  \n",
      "10    0.828268    0.811476    0.817312  \n",
      "11    0.828875    0.811146    0.816429  \n",
      "12    0.661294    0.623406    0.637436  \n",
      "13    0.868500    0.815800    0.839300  \n",
      "14    0.828875    0.811146    0.816429  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_rf_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_rf_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_rf_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_rf_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_rf_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_rf_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_rf_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_rf_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_rf_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_rf_6)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 07:49:24,639]\u001b[0m Trial 350 finished with value: 0.8244832869569997 and parameters: {'n_estimators': 486}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:51:09,179]\u001b[0m Trial 351 finished with value: 0.8225575188562043 and parameters: {'n_estimators': 444}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:53:00,905]\u001b[0m Trial 352 finished with value: 0.8242077895782591 and parameters: {'n_estimators': 474}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:55:01,358]\u001b[0m Trial 353 finished with value: 0.8241472056242765 and parameters: {'n_estimators': 511}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:56:48,564]\u001b[0m Trial 354 finished with value: 0.8239608133122214 and parameters: {'n_estimators': 458}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 07:58:44,102]\u001b[0m Trial 355 finished with value: 0.8241970197298143 and parameters: {'n_estimators': 495}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:00:24,552]\u001b[0m Trial 356 finished with value: 0.8233932954356069 and parameters: {'n_estimators': 427}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:02:08,406]\u001b[0m Trial 357 finished with value: 0.8233891899094996 and parameters: {'n_estimators': 442}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:04:01,158]\u001b[0m Trial 358 finished with value: 0.8244842730344338 and parameters: {'n_estimators': 476}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:06:03,489]\u001b[0m Trial 359 finished with value: 0.8231359152576095 and parameters: {'n_estimators': 522}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:07:51,854]\u001b[0m Trial 360 finished with value: 0.8239608133122214 and parameters: {'n_estimators': 458}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:09:17,931]\u001b[0m Trial 361 finished with value: 0.820596552619666 and parameters: {'n_estimators': 362}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:10:54,890]\u001b[0m Trial 362 finished with value: 0.8216971698175868 and parameters: {'n_estimators': 405}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:12:48,631]\u001b[0m Trial 363 finished with value: 0.8244507784372533 and parameters: {'n_estimators': 483}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:14:32,118]\u001b[0m Trial 364 finished with value: 0.8225788582171889 and parameters: {'n_estimators': 440}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:16:28,139]\u001b[0m Trial 365 finished with value: 0.8236673129671017 and parameters: {'n_estimators': 496}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:18:16,482]\u001b[0m Trial 366 finished with value: 0.8239425481152999 and parameters: {'n_estimators': 464}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:19:47,302]\u001b[0m Trial 367 finished with value: 0.8219230138318426 and parameters: {'n_estimators': 389}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:21:28,033]\u001b[0m Trial 368 finished with value: 0.8233932954356069 and parameters: {'n_estimators': 427}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:23:15,164]\u001b[0m Trial 369 finished with value: 0.822806008799855 and parameters: {'n_estimators': 453}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:25:06,911]\u001b[0m Trial 370 finished with value: 0.8244842730344338 and parameters: {'n_estimators': 476}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:26:46,078]\u001b[0m Trial 371 finished with value: 0.8225840743889341 and parameters: {'n_estimators': 418}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:28:47,900]\u001b[0m Trial 372 finished with value: 0.8236304399979064 and parameters: {'n_estimators': 515}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:30:45,334]\u001b[0m Trial 373 finished with value: 0.8244571622728734 and parameters: {'n_estimators': 497}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:32:31,954]\u001b[0m Trial 374 finished with value: 0.8228608867840915 and parameters: {'n_estimators': 448}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:34:22,620]\u001b[0m Trial 375 finished with value: 0.8244828320605919 and parameters: {'n_estimators': 470}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:36:06,561]\u001b[0m Trial 376 finished with value: 0.8233891899094996 and parameters: {'n_estimators': 442}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:38:00,772]\u001b[0m Trial 377 finished with value: 0.8244507784372533 and parameters: {'n_estimators': 485}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:39:49,502]\u001b[0m Trial 378 finished with value: 0.8234059097389232 and parameters: {'n_estimators': 460}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:41:30,405]\u001b[0m Trial 379 finished with value: 0.8225772657650671 and parameters: {'n_estimators': 428}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:43:21,860]\u001b[0m Trial 380 finished with value: 0.825026518702861 and parameters: {'n_estimators': 473}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:45:19,161]\u001b[0m Trial 381 finished with value: 0.8236560796811382 and parameters: {'n_estimators': 498}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:47:06,748]\u001b[0m Trial 382 finished with value: 0.8228524920686183 and parameters: {'n_estimators': 454}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:48:43,078]\u001b[0m Trial 383 finished with value: 0.8216971698175868 and parameters: {'n_estimators': 405}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:50:24,953]\u001b[0m Trial 384 finished with value: 0.8223058214425958 and parameters: {'n_estimators': 434}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:52:18,988]\u001b[0m Trial 385 finished with value: 0.8244507784372533 and parameters: {'n_estimators': 483}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:54:08,106]\u001b[0m Trial 386 finished with value: 0.8236692447210151 and parameters: {'n_estimators': 462}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:56:10,059]\u001b[0m Trial 387 finished with value: 0.8241771392848232 and parameters: {'n_estimators': 512}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:57:49,001]\u001b[0m Trial 388 finished with value: 0.8225840743889341 and parameters: {'n_estimators': 418}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 08:59:33,719]\u001b[0m Trial 389 finished with value: 0.8233731133972368 and parameters: {'n_estimators': 445}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:01:25,145]\u001b[0m Trial 390 finished with value: 0.8242077895782591 and parameters: {'n_estimators': 474}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:03:20,801]\u001b[0m Trial 391 finished with value: 0.823386217925752 and parameters: {'n_estimators': 494}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:05:22,981]\u001b[0m Trial 392 finished with value: 0.8231359152576095 and parameters: {'n_estimators': 522}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:07:10,071]\u001b[0m Trial 393 finished with value: 0.8234059097389232 and parameters: {'n_estimators': 460}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:08:51,943]\u001b[0m Trial 394 finished with value: 0.8222869360616916 and parameters: {'n_estimators': 437}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:10:22,433]\u001b[0m Trial 395 finished with value: 0.8222966147854791 and parameters: {'n_estimators': 386}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 09:12:14,073]\u001b[0m Trial 396 finished with value: 0.8247755124676648 and parameters: {'n_estimators': 480}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:13:50,853]\u001b[0m Trial 397 finished with value: 0.8220058552321007 and parameters: {'n_estimators': 417}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:15:38,038]\u001b[0m Trial 398 finished with value: 0.8230800789074546 and parameters: {'n_estimators': 455}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:17:35,848]\u001b[0m Trial 399 finished with value: 0.8230765745549629 and parameters: {'n_estimators': 504}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8298\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  396.000000  403.000000  405.000000  419.000000   \n",
      "1                    TN  343.000000  352.000000  344.000000  336.000000   \n",
      "2                    FP   91.000000   91.000000   80.000000   93.000000   \n",
      "3                    FN   69.000000   53.000000   70.000000   51.000000   \n",
      "4              Accuracy    0.822024    0.839822    0.833148    0.839822   \n",
      "5             Precision    0.813142    0.815789    0.835052    0.818359   \n",
      "6           Sensitivity    0.851613    0.883772    0.852632    0.891489   \n",
      "7           Specificity    0.790300    0.794600    0.811300    0.783200   \n",
      "8              F1 score    0.831933    0.848421    0.843750    0.853360   \n",
      "9   F1 score (weighted)    0.821767    0.839437    0.833021    0.839125   \n",
      "10     F1 score (macro)    0.821404    0.839305    0.832376    0.838445   \n",
      "11    Balanced Accuracy    0.820968    0.839177    0.831976    0.837353   \n",
      "12                  MCC    0.643798    0.681632    0.664960    0.680615   \n",
      "13                  NPV    0.832500    0.869100    0.830900    0.868200   \n",
      "14              ROC_AUC    0.820968    0.839177    0.831976    0.837353   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   395.000000  389.000000  407.000000  399.000000  \n",
      "1   350.000000  341.000000  329.000000  349.000000  \n",
      "2   101.000000   92.000000  100.000000   89.000000  \n",
      "3    53.000000   77.000000   63.000000   62.000000  \n",
      "4     0.828699    0.812013    0.818687    0.832036  \n",
      "5     0.796371    0.808732    0.802761    0.817623  \n",
      "6     0.881696    0.834764    0.865957    0.865510  \n",
      "7     0.776100    0.787500    0.766900    0.796800  \n",
      "8     0.836864    0.821542    0.833163    0.840885  \n",
      "9     0.828240    0.811845    0.818035    0.831754  \n",
      "10    0.828268    0.811476    0.817312    0.831514  \n",
      "11    0.828875    0.811146    0.816429    0.831157  \n",
      "12    0.661294    0.623406    0.637436    0.664539  \n",
      "13    0.868500    0.815800    0.839300    0.849100  \n",
      "14    0.828875    0.811146    0.816429    0.831157  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_rf_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_rf_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_rf_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_rf_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_rf_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_rf_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_rf_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_rf_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_rf_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_rf_7)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 09:19:34,786]\u001b[0m Trial 400 finished with value: 0.8204926081420293 and parameters: {'n_estimators': 443}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:21:26,605]\u001b[0m Trial 401 finished with value: 0.8196108303899485 and parameters: {'n_estimators': 468}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:23:23,190]\u001b[0m Trial 402 finished with value: 0.8210058121895596 and parameters: {'n_estimators': 488}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:25:05,601]\u001b[0m Trial 403 finished with value: 0.8201831839134532 and parameters: {'n_estimators': 431}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:26:58,366]\u001b[0m Trial 404 finished with value: 0.819881579537738 and parameters: {'n_estimators': 469}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:28:44,861]\u001b[0m Trial 405 finished with value: 0.8202477307211758 and parameters: {'n_estimators': 448}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:30:22,713]\u001b[0m Trial 406 finished with value: 0.8216467548818562 and parameters: {'n_estimators': 410}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:30:47,996]\u001b[0m Trial 407 finished with value: 0.8149482881576873 and parameters: {'n_estimators': 102}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:32:43,802]\u001b[0m Trial 408 finished with value: 0.8204224022723015 and parameters: {'n_estimators': 487}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:34:33,212]\u001b[0m Trial 409 finished with value: 0.8204889101341163 and parameters: {'n_estimators': 462}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:36:16,528]\u001b[0m Trial 410 finished with value: 0.8199568700942887 and parameters: {'n_estimators': 432}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:38:18,281]\u001b[0m Trial 411 finished with value: 0.8220819677465488 and parameters: {'n_estimators': 514}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:40:06,455]\u001b[0m Trial 412 finished with value: 0.8199506594452426 and parameters: {'n_estimators': 452}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:41:58,454]\u001b[0m Trial 413 finished with value: 0.820721150824944 and parameters: {'n_estimators': 478}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:43:55,176]\u001b[0m Trial 414 finished with value: 0.8212640038883837 and parameters: {'n_estimators': 500}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:45:33,604]\u001b[0m Trial 415 finished with value: 0.8198905993456741 and parameters: {'n_estimators': 423}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:47:05,904]\u001b[0m Trial 416 finished with value: 0.821036688335307 and parameters: {'n_estimators': 396}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:48:55,971]\u001b[0m Trial 417 finished with value: 0.819881579537738 and parameters: {'n_estimators': 469}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:50:41,715]\u001b[0m Trial 418 finished with value: 0.8207983979176546 and parameters: {'n_estimators': 442}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:52:36,187]\u001b[0m Trial 419 finished with value: 0.820721150824944 and parameters: {'n_estimators': 483}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:54:24,506]\u001b[0m Trial 420 finished with value: 0.819963647013536 and parameters: {'n_estimators': 454}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:56:28,371]\u001b[0m Trial 421 finished with value: 0.8220467480837801 and parameters: {'n_estimators': 523}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 09:58:26,274]\u001b[0m Trial 422 finished with value: 0.8212488246903572 and parameters: {'n_estimators': 498}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:00:05,366]\u001b[0m Trial 423 finished with value: 0.8204758412029548 and parameters: {'n_estimators': 429}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:01:56,965]\u001b[0m Trial 424 finished with value: 0.819881579537738 and parameters: {'n_estimators': 467}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:03:42,768]\u001b[0m Trial 425 finished with value: 0.8202267698858254 and parameters: {'n_estimators': 447}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:05:21,302]\u001b[0m Trial 426 finished with value: 0.8202134414497142 and parameters: {'n_estimators': 409}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:07:17,275]\u001b[0m Trial 427 finished with value: 0.8204281757481627 and parameters: {'n_estimators': 485}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:09:08,823]\u001b[0m Trial 428 finished with value: 0.8204889101341163 and parameters: {'n_estimators': 461}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:10:25,491]\u001b[0m Trial 429 finished with value: 0.8198849215531497 and parameters: {'n_estimators': 347}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:12:04,214]\u001b[0m Trial 430 finished with value: 0.8207777254599907 and parameters: {'n_estimators': 434}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:13:58,262]\u001b[0m Trial 431 finished with value: 0.8215497527303246 and parameters: {'n_estimators': 502}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:15:57,754]\u001b[0m Trial 432 finished with value: 0.8220871252748834 and parameters: {'n_estimators': 528}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:17:43,806]\u001b[0m Trial 433 finished with value: 0.819881579537738 and parameters: {'n_estimators': 467}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:19:25,075]\u001b[0m Trial 434 finished with value: 0.8204926081420293 and parameters: {'n_estimators': 443}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:21:12,426]\u001b[0m Trial 435 finished with value: 0.8207313212561418 and parameters: {'n_estimators': 474}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:23:03,568]\u001b[0m Trial 436 finished with value: 0.8210058121895596 and parameters: {'n_estimators': 490}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:24:39,249]\u001b[0m Trial 437 finished with value: 0.8210452914128508 and parameters: {'n_estimators': 422}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:26:23,202]\u001b[0m Trial 438 finished with value: 0.8207764355810998 and parameters: {'n_estimators': 458}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:27:54,179]\u001b[0m Trial 439 finished with value: 0.8204486672059165 and parameters: {'n_estimators': 400}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:29:35,472]\u001b[0m Trial 440 finished with value: 0.8202267698858254 and parameters: {'n_estimators': 447}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:31:30,349]\u001b[0m Trial 441 finished with value: 0.82236771658849 and parameters: {'n_estimators': 508}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:33:18,963]\u001b[0m Trial 442 finished with value: 0.8204444000418925 and parameters: {'n_estimators': 481}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:34:55,471]\u001b[0m Trial 443 finished with value: 0.8204758412029548 and parameters: {'n_estimators': 429}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:36:38,938]\u001b[0m Trial 444 finished with value: 0.8202193889508077 and parameters: {'n_estimators': 460}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:38:22,883]\u001b[0m Trial 445 finished with value: 0.820721150824944 and parameters: {'n_estimators': 483}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:39:55,336]\u001b[0m Trial 446 finished with value: 0.8205116464388336 and parameters: {'n_estimators': 444}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:41:21,793]\u001b[0m Trial 447 finished with value: 0.8204833918654355 and parameters: {'n_estimators': 415}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:42:58,429]\u001b[0m Trial 448 finished with value: 0.819881579537738 and parameters: {'n_estimators': 465}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:44:42,555]\u001b[0m Trial 449 finished with value: 0.8209754601716831 and parameters: {'n_estimators': 501}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8298\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  396.000000  403.000000  405.000000  419.000000   \n",
      "1                    TN  343.000000  352.000000  344.000000  336.000000   \n",
      "2                    FP   91.000000   91.000000   80.000000   93.000000   \n",
      "3                    FN   69.000000   53.000000   70.000000   51.000000   \n",
      "4              Accuracy    0.822024    0.839822    0.833148    0.839822   \n",
      "5             Precision    0.813142    0.815789    0.835052    0.818359   \n",
      "6           Sensitivity    0.851613    0.883772    0.852632    0.891489   \n",
      "7           Specificity    0.790300    0.794600    0.811300    0.783200   \n",
      "8              F1 score    0.831933    0.848421    0.843750    0.853360   \n",
      "9   F1 score (weighted)    0.821767    0.839437    0.833021    0.839125   \n",
      "10     F1 score (macro)    0.821404    0.839305    0.832376    0.838445   \n",
      "11    Balanced Accuracy    0.820968    0.839177    0.831976    0.837353   \n",
      "12                  MCC    0.643798    0.681632    0.664960    0.680615   \n",
      "13                  NPV    0.832500    0.869100    0.830900    0.868200   \n",
      "14              ROC_AUC    0.820968    0.839177    0.831976    0.837353   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   395.000000  389.000000  407.000000  399.000000  413.000000  \n",
      "1   350.000000  341.000000  329.000000  349.000000  337.000000  \n",
      "2   101.000000   92.000000  100.000000   89.000000   77.000000  \n",
      "3    53.000000   77.000000   63.000000   62.000000   72.000000  \n",
      "4     0.828699    0.812013    0.818687    0.832036    0.834260  \n",
      "5     0.796371    0.808732    0.802761    0.817623    0.842857  \n",
      "6     0.881696    0.834764    0.865957    0.865510    0.851546  \n",
      "7     0.776100    0.787500    0.766900    0.796800    0.814000  \n",
      "8     0.836864    0.821542    0.833163    0.840885    0.847179  \n",
      "9     0.828240    0.811845    0.818035    0.831754    0.834182  \n",
      "10    0.828268    0.811476    0.817312    0.831514    0.833067  \n",
      "11    0.828875    0.811146    0.816429    0.831157    0.832778  \n",
      "12    0.661294    0.623406    0.637436    0.664539    0.666187  \n",
      "13    0.868500    0.815800    0.839300    0.849100    0.824000  \n",
      "14    0.828875    0.811146    0.816429    0.831157    0.832778  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_rf_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_rf_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_rf_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_rf_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_rf_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_rf_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_rf_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_rf_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_rf_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_rf_8)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 10:46:29,716]\u001b[0m Trial 450 finished with value: 0.8166730215070086 and parameters: {'n_estimators': 475}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:47:46,284]\u001b[0m Trial 451 finished with value: 0.8184007881450561 and parameters: {'n_estimators': 376}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:49:16,552]\u001b[0m Trial 452 finished with value: 0.8169941364035527 and parameters: {'n_estimators': 446}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:50:43,279]\u001b[0m Trial 453 finished with value: 0.8175491537160262 and parameters: {'n_estimators': 430}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:52:29,234]\u001b[0m Trial 454 finished with value: 0.8147549117303182 and parameters: {'n_estimators': 525}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:54:08,591]\u001b[0m Trial 455 finished with value: 0.8161161232065665 and parameters: {'n_estimators': 491}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:55:41,070]\u001b[0m Trial 456 finished with value: 0.8161352473170386 and parameters: {'n_estimators': 457}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:56:20,063]\u001b[0m Trial 457 finished with value: 0.8151053600949776 and parameters: {'n_estimators': 189}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:57:55,705]\u001b[0m Trial 458 finished with value: 0.81695735812805 and parameters: {'n_estimators': 473}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 10:59:17,699]\u001b[0m Trial 459 finished with value: 0.8178061931014537 and parameters: {'n_estimators': 407}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:00:46,055]\u001b[0m Trial 460 finished with value: 0.816676052481934 and parameters: {'n_estimators': 441}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:02:27,811]\u001b[0m Trial 461 finished with value: 0.8152787883521697 and parameters: {'n_estimators': 505}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:04:00,878]\u001b[0m Trial 462 finished with value: 0.8164195839380801 and parameters: {'n_estimators': 460}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:05:26,161]\u001b[0m Trial 463 finished with value: 0.8166928684988287 and parameters: {'n_estimators': 420}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:07:04,544]\u001b[0m Trial 464 finished with value: 0.8164310846925298 and parameters: {'n_estimators': 487}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:08:23,956]\u001b[0m Trial 465 finished with value: 0.8189292097189845 and parameters: {'n_estimators': 393}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:09:59,521]\u001b[0m Trial 466 finished with value: 0.8166730215070086 and parameters: {'n_estimators': 471}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:11:29,403]\u001b[0m Trial 467 finished with value: 0.8164044103274545 and parameters: {'n_estimators': 443}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:13:00,980]\u001b[0m Trial 468 finished with value: 0.8166966579495734 and parameters: {'n_estimators': 455}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:14:45,280]\u001b[0m Trial 469 finished with value: 0.8164264609979677 and parameters: {'n_estimators': 516}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:16:11,781]\u001b[0m Trial 470 finished with value: 0.8175491537160262 and parameters: {'n_estimators': 430}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:17:50,710]\u001b[0m Trial 471 finished with value: 0.8161161232065665 and parameters: {'n_estimators': 491}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:19:25,622]\u001b[0m Trial 472 finished with value: 0.8164117886152514 and parameters: {'n_estimators': 472}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:20:56,879]\u001b[0m Trial 473 finished with value: 0.8166954092175889 and parameters: {'n_estimators': 453}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:22:23,999]\u001b[0m Trial 474 finished with value: 0.816972061457337 and parameters: {'n_estimators': 431}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:23:59,739]\u001b[0m Trial 475 finished with value: 0.816409617689293 and parameters: {'n_estimators': 477}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:25:38,631]\u001b[0m Trial 476 finished with value: 0.8156031773209993 and parameters: {'n_estimators': 493}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:27:01,701]\u001b[0m Trial 477 finished with value: 0.8175178076048002 and parameters: {'n_estimators': 413}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:28:32,898]\u001b[0m Trial 478 finished with value: 0.816719872619441 and parameters: {'n_estimators': 456}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:30:19,628]\u001b[0m Trial 479 finished with value: 0.8147667554745895 and parameters: {'n_estimators': 532}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:32:02,108]\u001b[0m Trial 480 finished with value: 0.8153184068916287 and parameters: {'n_estimators': 508}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:33:30,431]\u001b[0m Trial 481 finished with value: 0.8167123052662413 and parameters: {'n_estimators': 438}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:35:04,969]\u001b[0m Trial 482 finished with value: 0.8166730215070086 and parameters: {'n_estimators': 471}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:36:36,141]\u001b[0m Trial 483 finished with value: 0.8166954092175889 and parameters: {'n_estimators': 453}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:38:13,377]\u001b[0m Trial 484 finished with value: 0.8175005924810208 and parameters: {'n_estimators': 483}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:39:38,645]\u001b[0m Trial 485 finished with value: 0.816415808990486 and parameters: {'n_estimators': 422}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:41:12,344]\u001b[0m Trial 486 finished with value: 0.8164088860074902 and parameters: {'n_estimators': 464}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:42:52,169]\u001b[0m Trial 487 finished with value: 0.8153202263067248 and parameters: {'n_estimators': 498}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:44:20,835]\u001b[0m Trial 488 finished with value: 0.816676052481934 and parameters: {'n_estimators': 440}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:45:56,737]\u001b[0m Trial 489 finished with value: 0.816409617689293 and parameters: {'n_estimators': 478}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:47:18,221]\u001b[0m Trial 490 finished with value: 0.8186766852366747 and parameters: {'n_estimators': 402}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:48:49,563]\u001b[0m Trial 491 finished with value: 0.8166954092175889 and parameters: {'n_estimators': 453}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:50:33,642]\u001b[0m Trial 492 finished with value: 0.8163999177850807 and parameters: {'n_estimators': 515}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:52:02,017]\u001b[0m Trial 493 finished with value: 0.8167123052662413 and parameters: {'n_estimators': 438}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:53:35,808]\u001b[0m Trial 494 finished with value: 0.8166966434464227 and parameters: {'n_estimators': 466}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:55:14,651]\u001b[0m Trial 495 finished with value: 0.8158829532442352 and parameters: {'n_estimators': 490}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 11:56:38,741]\u001b[0m Trial 496 finished with value: 0.8166868973770851 and parameters: {'n_estimators': 416}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:57:57,189]\u001b[0m Trial 497 finished with value: 0.8192319919247092 and parameters: {'n_estimators': 387}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 11:59:27,975]\u001b[0m Trial 498 finished with value: 0.8166961041816929 and parameters: {'n_estimators': 450}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:01:04,246]\u001b[0m Trial 499 finished with value: 0.8166730215070086 and parameters: {'n_estimators': 475}. Best is trial 291 with value: 0.8298075275997423.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8298\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  396.000000  403.000000  405.000000  419.000000   \n",
      "1                    TN  343.000000  352.000000  344.000000  336.000000   \n",
      "2                    FP   91.000000   91.000000   80.000000   93.000000   \n",
      "3                    FN   69.000000   53.000000   70.000000   51.000000   \n",
      "4              Accuracy    0.822024    0.839822    0.833148    0.839822   \n",
      "5             Precision    0.813142    0.815789    0.835052    0.818359   \n",
      "6           Sensitivity    0.851613    0.883772    0.852632    0.891489   \n",
      "7           Specificity    0.790300    0.794600    0.811300    0.783200   \n",
      "8              F1 score    0.831933    0.848421    0.843750    0.853360   \n",
      "9   F1 score (weighted)    0.821767    0.839437    0.833021    0.839125   \n",
      "10     F1 score (macro)    0.821404    0.839305    0.832376    0.838445   \n",
      "11    Balanced Accuracy    0.820968    0.839177    0.831976    0.837353   \n",
      "12                  MCC    0.643798    0.681632    0.664960    0.680615   \n",
      "13                  NPV    0.832500    0.869100    0.830900    0.868200   \n",
      "14              ROC_AUC    0.820968    0.839177    0.831976    0.837353   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   395.000000  389.000000  407.000000  399.000000  413.000000  421.000000  \n",
      "1   350.000000  341.000000  329.000000  349.000000  337.000000  330.000000  \n",
      "2   101.000000   92.000000  100.000000   89.000000   77.000000   75.000000  \n",
      "3    53.000000   77.000000   63.000000   62.000000   72.000000   73.000000  \n",
      "4     0.828699    0.812013    0.818687    0.832036    0.834260    0.835373  \n",
      "5     0.796371    0.808732    0.802761    0.817623    0.842857    0.848790  \n",
      "6     0.881696    0.834764    0.865957    0.865510    0.851546    0.852227  \n",
      "7     0.776100    0.787500    0.766900    0.796800    0.814000    0.814800  \n",
      "8     0.836864    0.821542    0.833163    0.840885    0.847179    0.850505  \n",
      "9     0.828240    0.811845    0.818035    0.831754    0.834182    0.835335  \n",
      "10    0.828268    0.811476    0.817312    0.831514    0.833067    0.833668  \n",
      "11    0.828875    0.811146    0.816429    0.831157    0.832778    0.833521  \n",
      "12    0.661294    0.623406    0.637436    0.664539    0.666187    0.667345  \n",
      "13    0.868500    0.815800    0.839300    0.849100    0.824000    0.818900  \n",
      "14    0.828875    0.811146    0.816429    0.831157    0.832778    0.833521  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_rf_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_rf_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_rf_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_rf_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_rf_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_rf_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_rf_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_rf_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_rf_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_rf_9)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })  \n",
    "\n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8298\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>204.600000</td>\n",
       "      <td>8.643559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>169.800000</td>\n",
       "      <td>7.786027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>43.200000</td>\n",
       "      <td>7.315129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>4.671426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.833475</td>\n",
       "      <td>0.016882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.825907</td>\n",
       "      <td>0.027157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.866218</td>\n",
       "      <td>0.019091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.797510</td>\n",
       "      <td>0.030660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.845272</td>\n",
       "      <td>0.016761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.833124</td>\n",
       "      <td>0.017016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.832318</td>\n",
       "      <td>0.016909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.831867</td>\n",
       "      <td>0.016717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.666448</td>\n",
       "      <td>0.032878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.843270</td>\n",
       "      <td>0.020613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.831867</td>\n",
       "      <td>0.016717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       204.600000     8.643559\n",
       "1                    TN       169.800000     7.786027\n",
       "2                    FP        43.200000     7.315129\n",
       "3                    FN        31.600000     4.671426\n",
       "4              Accuracy         0.833475     0.016882\n",
       "5             Precision         0.825907     0.027157\n",
       "6           Sensitivity         0.866218     0.019091\n",
       "7           Specificity         0.797510     0.030660\n",
       "8              F1 score         0.845272     0.016761\n",
       "9   F1 score (weighted)         0.833124     0.017016\n",
       "10     F1 score (macro)         0.832318     0.016909\n",
       "11    Balanced Accuracy         0.831867     0.016717\n",
       "12                  MCC         0.666448     0.032878\n",
       "13                  NPV         0.843270     0.020613\n",
       "14              ROC_AUC         0.831867     0.016717"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>419.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>404.700000</td>\n",
       "      <td>10.520351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>341.100000</td>\n",
       "      <td>8.061569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>88.900000</td>\n",
       "      <td>8.937437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>64.300000</td>\n",
       "      <td>9.369573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.839822</td>\n",
       "      <td>0.833148</td>\n",
       "      <td>0.839822</td>\n",
       "      <td>0.828699</td>\n",
       "      <td>0.812013</td>\n",
       "      <td>0.818687</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.834260</td>\n",
       "      <td>0.835373</td>\n",
       "      <td>0.829588</td>\n",
       "      <td>0.009244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.813142</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.835052</td>\n",
       "      <td>0.818359</td>\n",
       "      <td>0.796371</td>\n",
       "      <td>0.808732</td>\n",
       "      <td>0.802761</td>\n",
       "      <td>0.817623</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.848790</td>\n",
       "      <td>0.819948</td>\n",
       "      <td>0.017106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.851613</td>\n",
       "      <td>0.883772</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.891489</td>\n",
       "      <td>0.881696</td>\n",
       "      <td>0.834764</td>\n",
       "      <td>0.865957</td>\n",
       "      <td>0.865510</td>\n",
       "      <td>0.851546</td>\n",
       "      <td>0.852227</td>\n",
       "      <td>0.863121</td>\n",
       "      <td>0.017917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.790300</td>\n",
       "      <td>0.794600</td>\n",
       "      <td>0.811300</td>\n",
       "      <td>0.783200</td>\n",
       "      <td>0.776100</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.766900</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.814800</td>\n",
       "      <td>0.793550</td>\n",
       "      <td>0.016212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.848421</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.853360</td>\n",
       "      <td>0.836864</td>\n",
       "      <td>0.821542</td>\n",
       "      <td>0.833163</td>\n",
       "      <td>0.840885</td>\n",
       "      <td>0.847179</td>\n",
       "      <td>0.850505</td>\n",
       "      <td>0.840760</td>\n",
       "      <td>0.009904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.821767</td>\n",
       "      <td>0.839437</td>\n",
       "      <td>0.833021</td>\n",
       "      <td>0.839125</td>\n",
       "      <td>0.828240</td>\n",
       "      <td>0.811845</td>\n",
       "      <td>0.818035</td>\n",
       "      <td>0.831754</td>\n",
       "      <td>0.834182</td>\n",
       "      <td>0.835335</td>\n",
       "      <td>0.829274</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.821404</td>\n",
       "      <td>0.839305</td>\n",
       "      <td>0.832376</td>\n",
       "      <td>0.838445</td>\n",
       "      <td>0.828268</td>\n",
       "      <td>0.811476</td>\n",
       "      <td>0.817312</td>\n",
       "      <td>0.831514</td>\n",
       "      <td>0.833067</td>\n",
       "      <td>0.833668</td>\n",
       "      <td>0.828684</td>\n",
       "      <td>0.009143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.820968</td>\n",
       "      <td>0.839177</td>\n",
       "      <td>0.831976</td>\n",
       "      <td>0.837353</td>\n",
       "      <td>0.828875</td>\n",
       "      <td>0.811146</td>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.831157</td>\n",
       "      <td>0.832778</td>\n",
       "      <td>0.833521</td>\n",
       "      <td>0.828338</td>\n",
       "      <td>0.009180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.643798</td>\n",
       "      <td>0.681632</td>\n",
       "      <td>0.664960</td>\n",
       "      <td>0.680615</td>\n",
       "      <td>0.661294</td>\n",
       "      <td>0.623406</td>\n",
       "      <td>0.637436</td>\n",
       "      <td>0.664539</td>\n",
       "      <td>0.666187</td>\n",
       "      <td>0.667345</td>\n",
       "      <td>0.659121</td>\n",
       "      <td>0.018664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.869100</td>\n",
       "      <td>0.830900</td>\n",
       "      <td>0.868200</td>\n",
       "      <td>0.868500</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.839300</td>\n",
       "      <td>0.849100</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.818900</td>\n",
       "      <td>0.841630</td>\n",
       "      <td>0.020915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.820968</td>\n",
       "      <td>0.839177</td>\n",
       "      <td>0.831976</td>\n",
       "      <td>0.837353</td>\n",
       "      <td>0.828875</td>\n",
       "      <td>0.811146</td>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.831157</td>\n",
       "      <td>0.832778</td>\n",
       "      <td>0.833521</td>\n",
       "      <td>0.828338</td>\n",
       "      <td>0.009180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  396.000000  403.000000  405.000000  419.000000   \n",
       "1                    TN  343.000000  352.000000  344.000000  336.000000   \n",
       "2                    FP   91.000000   91.000000   80.000000   93.000000   \n",
       "3                    FN   69.000000   53.000000   70.000000   51.000000   \n",
       "4              Accuracy    0.822024    0.839822    0.833148    0.839822   \n",
       "5             Precision    0.813142    0.815789    0.835052    0.818359   \n",
       "6           Sensitivity    0.851613    0.883772    0.852632    0.891489   \n",
       "7           Specificity    0.790300    0.794600    0.811300    0.783200   \n",
       "8              F1 score    0.831933    0.848421    0.843750    0.853360   \n",
       "9   F1 score (weighted)    0.821767    0.839437    0.833021    0.839125   \n",
       "10     F1 score (macro)    0.821404    0.839305    0.832376    0.838445   \n",
       "11    Balanced Accuracy    0.820968    0.839177    0.831976    0.837353   \n",
       "12                  MCC    0.643798    0.681632    0.664960    0.680615   \n",
       "13                  NPV    0.832500    0.869100    0.830900    0.868200   \n",
       "14              ROC_AUC    0.820968    0.839177    0.831976    0.837353   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   395.000000  389.000000  407.000000  399.000000  413.000000  421.000000   \n",
       "1   350.000000  341.000000  329.000000  349.000000  337.000000  330.000000   \n",
       "2   101.000000   92.000000  100.000000   89.000000   77.000000   75.000000   \n",
       "3    53.000000   77.000000   63.000000   62.000000   72.000000   73.000000   \n",
       "4     0.828699    0.812013    0.818687    0.832036    0.834260    0.835373   \n",
       "5     0.796371    0.808732    0.802761    0.817623    0.842857    0.848790   \n",
       "6     0.881696    0.834764    0.865957    0.865510    0.851546    0.852227   \n",
       "7     0.776100    0.787500    0.766900    0.796800    0.814000    0.814800   \n",
       "8     0.836864    0.821542    0.833163    0.840885    0.847179    0.850505   \n",
       "9     0.828240    0.811845    0.818035    0.831754    0.834182    0.835335   \n",
       "10    0.828268    0.811476    0.817312    0.831514    0.833067    0.833668   \n",
       "11    0.828875    0.811146    0.816429    0.831157    0.832778    0.833521   \n",
       "12    0.661294    0.623406    0.637436    0.664539    0.666187    0.667345   \n",
       "13    0.868500    0.815800    0.839300    0.849100    0.824000    0.818900   \n",
       "14    0.828875    0.811146    0.816429    0.831157    0.832778    0.833521   \n",
       "\n",
       "           ave        std  \n",
       "0   404.700000  10.520351  \n",
       "1   341.100000   8.061569  \n",
       "2    88.900000   8.937437  \n",
       "3    64.300000   9.369573  \n",
       "4     0.829588   0.009244  \n",
       "5     0.819948   0.017106  \n",
       "6     0.863121   0.017917  \n",
       "7     0.793550   0.016212  \n",
       "8     0.840760   0.009904  \n",
       "9     0.829274   0.009243  \n",
       "10    0.828684   0.009143  \n",
       "11    0.828338   0.009180  \n",
       "12    0.659121   0.018664  \n",
       "13    0.841630   0.020915  \n",
       "14    0.828338   0.009180  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_rf0</th>\n",
       "      <th>y_pred_rf1</th>\n",
       "      <th>y_pred_rf2</th>\n",
       "      <th>y_pred_rf3</th>\n",
       "      <th>y_pred_rf4</th>\n",
       "      <th>y_pred_rf_ave</th>\n",
       "      <th>y_pred_rf_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>4487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>4488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>4489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>4490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>4491</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_test_idx0  y_test0  y_pred_rf0  y_pred_rf1  y_pred_rf2  y_pred_rf3  \\\n",
       "0               0      0.0         0.0         0.0         0.0         0.0   \n",
       "1               1      1.0         1.0         1.0         1.0         1.0   \n",
       "2               2      1.0         1.0         1.0         1.0         1.0   \n",
       "3               3      1.0         1.0         1.0         1.0         1.0   \n",
       "4               4      1.0         0.0         0.0         0.0         0.0   \n",
       "...           ...      ...         ...         ...         ...         ...   \n",
       "4487         4487      1.0         1.0         1.0         1.0         1.0   \n",
       "4488         4488      1.0         1.0         1.0         1.0         1.0   \n",
       "4489         4489      0.0         1.0         1.0         1.0         1.0   \n",
       "4490         4490      0.0         0.0         0.0         0.0         0.0   \n",
       "4491         4491      1.0         1.0         1.0         1.0         1.0   \n",
       "\n",
       "      y_pred_rf4  y_pred_rf_ave  y_pred_rf_std  \n",
       "0            0.0            0.0            0.0  \n",
       "1            1.0            1.0            0.0  \n",
       "2            1.0            1.0            0.0  \n",
       "3            1.0            1.0            0.0  \n",
       "4            0.0            0.0            0.0  \n",
       "...          ...            ...            ...  \n",
       "4487         1.0            1.0            0.0  \n",
       "4488         1.0            1.0            0.0  \n",
       "4489         0.0            0.8            0.4  \n",
       "4490         0.0            0.0            0.0  \n",
       "4491         1.0            1.0            0.0  \n",
       "\n",
       "[4492 rows x 9 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=1121218, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "   \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_rf)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "    \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_rf))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_rf))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_rf))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_rf))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_rf, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_rf, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_rf))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_rf))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_rf))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "rf_5preds = pd.concat([rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "rf_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d030d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_5preds.to_csv('rf_5test_CV_result.csv')\n",
    "mat_met_optimized_rf.to_csv('mat_met_rf_opt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model f1_score 0.8308 with a standard deviation of 0.0278\n",
      "RF optimized model f1_score 0.8319 with a standard deviation of 0.0263\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"RF baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_rf_clf.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_clf, \"./rf_clf.joblib\")\n",
    "#joblib.dump(optimized_rf, \"./optimized_rf.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, \"./optimizedCV_rf_clf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       200.100000     8.900062\n",
      "1                    TN       171.300000     8.819801\n",
      "2                    FP        41.700000     8.353974\n",
      "3                    FN        36.100000     6.100091\n",
      "4              Accuracy         0.826796     0.017186\n",
      "5             Precision         0.828134     0.029879\n",
      "6           Sensitivity         0.847234     0.024678\n",
      "7           Specificity         0.804550     0.036179\n",
      "8              F1 score         0.837065     0.017148\n",
      "9   F1 score (weighted)         0.826599     0.017334\n",
      "10     F1 score (macro)         0.825881     0.017294\n",
      "11    Balanced Accuracy         0.825895     0.017280\n",
      "12                  MCC         0.653159     0.033713\n",
      "13                  NPV         0.826410     0.023653\n",
      "14              ROC_AUC         0.825895     0.017280\n",
      "CPU times: user 18.9 s, sys: 132 ms, total: 19.1 s\n",
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_clf = lgbm.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=8,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_clf.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"logloss\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_clf.predict(X_test) \n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggestegorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggestegorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "  \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:16:51,655]\u001b[0m A new study created in memory with name: LGBMClassifier\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:16:54,549]\u001b[0m Trial 0 finished with value: 0.8216017717901994 and parameters: {'n_estimators': 264, 'learning_rate': 0.08850344354416637, 'max_depth': 7, 'max_bin': 164, 'num_leaves': 651}. Best is trial 0 with value: 0.8216017717901994.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:16:56,611]\u001b[0m Trial 1 finished with value: 0.8126155176367084 and parameters: {'n_estimators': 103, 'learning_rate': 0.16354430242488383, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 632}. Best is trial 0 with value: 0.8216017717901994.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:16:58,399]\u001b[0m Trial 2 finished with value: 0.7998107946277385 and parameters: {'n_estimators': 64, 'learning_rate': 0.05758706851045406, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 618}. Best is trial 0 with value: 0.8216017717901994.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:00,874]\u001b[0m Trial 3 finished with value: 0.8151716971945243 and parameters: {'n_estimators': 710, 'learning_rate': 0.1831564217006419, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 570}. Best is trial 0 with value: 0.8216017717901994.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:02,894]\u001b[0m Trial 4 finished with value: 0.8060748885457247 and parameters: {'n_estimators': 157, 'learning_rate': 0.07831946319796944, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 438}. Best is trial 0 with value: 0.8216017717901994.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:07,451]\u001b[0m Trial 5 finished with value: 0.822986538902206 and parameters: {'n_estimators': 839, 'learning_rate': 0.056010570242836025, 'max_depth': 9, 'max_bin': 286, 'num_leaves': 187}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:10,572]\u001b[0m Trial 6 finished with value: 0.794261070519358 and parameters: {'n_estimators': 450, 'learning_rate': 0.037942709814165906, 'max_depth': 4, 'max_bin': 159, 'num_leaves': 225}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:12,920]\u001b[0m Trial 7 finished with value: 0.8110634084897044 and parameters: {'n_estimators': 90, 'learning_rate': 0.050733712506787515, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 642}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:14,928]\u001b[0m Trial 8 finished with value: 0.8149872983235739 and parameters: {'n_estimators': 188, 'learning_rate': 0.10984888449226984, 'max_depth': 6, 'max_bin': 154, 'num_leaves': 78}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:16,142]\u001b[0m Trial 9 finished with value: 0.7764143679786939 and parameters: {'n_estimators': 131, 'learning_rate': 0.052831613400627545, 'max_depth': 4, 'max_bin': 179, 'num_leaves': 574}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:18,833]\u001b[0m Trial 10 finished with value: 0.8138103270779098 and parameters: {'n_estimators': 899, 'learning_rate': 0.12760052579973113, 'max_depth': 9, 'max_bin': 207, 'num_leaves': 258}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:24,189]\u001b[0m Trial 11 finished with value: 0.8016038837300897 and parameters: {'n_estimators': 353, 'learning_rate': 0.00934106973935224, 'max_depth': 8, 'max_bin': 208, 'num_leaves': 384}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:27,563]\u001b[0m Trial 12 finished with value: 0.8166552219612455 and parameters: {'n_estimators': 658, 'learning_rate': 0.0886986147597765, 'max_depth': 6, 'max_bin': 187, 'num_leaves': 35}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:30,213]\u001b[0m Trial 13 finished with value: 0.8174955547550811 and parameters: {'n_estimators': 303, 'learning_rate': 0.1332703892505523, 'max_depth': 8, 'max_bin': 299, 'num_leaves': 199}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:36,866]\u001b[0m Trial 14 finished with value: 0.7870606731950069 and parameters: {'n_estimators': 629, 'learning_rate': 0.0062731525516862285, 'max_depth': 6, 'max_bin': 294, 'num_leaves': 747}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:40,692]\u001b[0m Trial 15 finished with value: 0.819112518346062 and parameters: {'n_estimators': 809, 'learning_rate': 0.0817233593933028, 'max_depth': 9, 'max_bin': 221, 'num_leaves': 402}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:43,597]\u001b[0m Trial 16 finished with value: 0.7746697121815656 and parameters: {'n_estimators': 512, 'learning_rate': 0.027404412585892837, 'max_depth': 3, 'max_bin': 180, 'num_leaves': 301}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:46,364]\u001b[0m Trial 17 finished with value: 0.8155419043976553 and parameters: {'n_estimators': 325, 'learning_rate': 0.1178696262095385, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 125}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:50,935]\u001b[0m Trial 18 finished with value: 0.822770891737323 and parameters: {'n_estimators': 531, 'learning_rate': 0.06302409047112525, 'max_depth': 9, 'max_bin': 200, 'num_leaves': 509}. Best is trial 5 with value: 0.822986538902206.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:17:55,671]\u001b[0m Trial 19 finished with value: 0.8233452199658636 and parameters: {'n_estimators': 525, 'learning_rate': 0.06508097324842677, 'max_depth': 10, 'max_bin': 228, 'num_leaves': 480}. Best is trial 19 with value: 0.8233452199658636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:18:04,923]\u001b[0m Trial 20 finished with value: 0.8228630377145082 and parameters: {'n_estimators': 764, 'learning_rate': 0.022624487187229703, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 335}. Best is trial 19 with value: 0.8233452199658636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:18:12,545]\u001b[0m Trial 21 finished with value: 0.8250885217589001 and parameters: {'n_estimators': 783, 'learning_rate': 0.02898615468873547, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 367}. Best is trial 21 with value: 0.8250885217589001.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:18:17,278]\u001b[0m Trial 22 finished with value: 0.8226593757989045 and parameters: {'n_estimators': 871, 'learning_rate': 0.07002008427745372, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 472}. Best is trial 21 with value: 0.8250885217589001.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:18:23,571]\u001b[0m Trial 23 finished with value: 0.8295934115345324 and parameters: {'n_estimators': 615, 'learning_rate': 0.0404096120295208, 'max_depth': 10, 'max_bin': 226, 'num_leaves': 157}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:18:30,294]\u001b[0m Trial 24 finished with value: 0.8213455595573972 and parameters: {'n_estimators': 583, 'learning_rate': 0.032192712105492696, 'max_depth': 10, 'max_bin': 228, 'num_leaves': 334}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:18:36,133]\u001b[0m Trial 25 finished with value: 0.8247915469809328 and parameters: {'n_estimators': 426, 'learning_rate': 0.04139616221112029, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 137}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:18:42,119]\u001b[0m Trial 26 finished with value: 0.825819179673746 and parameters: {'n_estimators': 403, 'learning_rate': 0.04010706885150273, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 143}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:18:54,005]\u001b[0m Trial 27 finished with value: 0.8197165025813827 and parameters: {'n_estimators': 732, 'learning_rate': 0.011671421062887846, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 120}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:02,756]\u001b[0m Trial 28 finished with value: 0.821811243851075 and parameters: {'n_estimators': 645, 'learning_rate': 0.023991656035265735, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 266}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:09,946]\u001b[0m Trial 29 finished with value: 0.7757589218524965 and parameters: {'n_estimators': 383, 'learning_rate': 0.00292837628462295, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 35}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:15,324]\u001b[0m Trial 30 finished with value: 0.8218562502296409 and parameters: {'n_estimators': 584, 'learning_rate': 0.04350075520108831, 'max_depth': 10, 'max_bin': 237, 'num_leaves': 172}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:19:19,790]\u001b[0m Trial 31 finished with value: 0.8189919439583381 and parameters: {'n_estimators': 238, 'learning_rate': 0.03805414406390006, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 133}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:23,160]\u001b[0m Trial 32 finished with value: 0.8183918147554584 and parameters: {'n_estimators': 432, 'learning_rate': 0.09418086494176878, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 85}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:30,171]\u001b[0m Trial 33 finished with value: 0.8199249783839202 and parameters: {'n_estimators': 402, 'learning_rate': 0.019343972166720593, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 156}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:32,821]\u001b[0m Trial 34 finished with value: 0.8175131294391763 and parameters: {'n_estimators': 475, 'learning_rate': 0.15400969019068744, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 227}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:36,921]\u001b[0m Trial 35 finished with value: 0.8244614256311236 and parameters: {'n_estimators': 703, 'learning_rate': 0.0741640344525348, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 84}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:40,919]\u001b[0m Trial 36 finished with value: 0.8187999651014193 and parameters: {'n_estimators': 244, 'learning_rate': 0.045393138987906624, 'max_depth': 10, 'max_bin': 234, 'num_leaves': 289}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:47,830]\u001b[0m Trial 37 finished with value: 0.8222306162959203 and parameters: {'n_estimators': 592, 'learning_rate': 0.03456078161578807, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 214}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:19:58,847]\u001b[0m Trial 38 finished with value: 0.8272711598353807 and parameters: {'n_estimators': 792, 'learning_rate': 0.015695140160049206, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 350}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:20:00,873]\u001b[0m Trial 39 finished with value: 0.8114069318890147 and parameters: {'n_estimators': 784, 'learning_rate': 0.18929085464736556, 'max_depth': 8, 'max_bin': 195, 'num_leaves': 371}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:20:11,871]\u001b[0m Trial 40 finished with value: 0.8265298062724282 and parameters: {'n_estimators': 695, 'learning_rate': 0.014447051817052489, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 442}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:20:22,036]\u001b[0m Trial 41 finished with value: 0.821392243772352 and parameters: {'n_estimators': 688, 'learning_rate': 0.016643134197482132, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 411}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:20:26,964]\u001b[0m Trial 42 finished with value: 0.8225047395713417 and parameters: {'n_estimators': 833, 'learning_rate': 0.053225912798255726, 'max_depth': 9, 'max_bin': 273, 'num_leaves': 440}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:20:44,431]\u001b[0m Trial 43 finished with value: 0.7953785222366656 and parameters: {'n_estimators': 731, 'learning_rate': 0.001833718208106518, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 339}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:20:52,222]\u001b[0m Trial 44 finished with value: 0.822545103954956 and parameters: {'n_estimators': 758, 'learning_rate': 0.027561499327481136, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 496}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:21:03,830]\u001b[0m Trial 45 finished with value: 0.8202990011377356 and parameters: {'n_estimators': 847, 'learning_rate': 0.01488429932614319, 'max_depth': 9, 'max_bin': 211, 'num_leaves': 592}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:21:16,699]\u001b[0m Trial 46 finished with value: 0.7015016987797484 and parameters: {'n_estimators': 685, 'learning_rate': 0.0002176516239279242, 'max_depth': 8, 'max_bin': 264, 'num_leaves': 244}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:21:22,090]\u001b[0m Trial 47 finished with value: 0.8199257526604192 and parameters: {'n_estimators': 806, 'learning_rate': 0.04955953979209308, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 522}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:21:28,651]\u001b[0m Trial 48 finished with value: 0.8228699065544101 and parameters: {'n_estimators': 627, 'learning_rate': 0.031660006502584936, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 314}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:21:33,156]\u001b[0m Trial 49 finished with value: 0.8224566894602041 and parameters: {'n_estimators': 669, 'learning_rate': 0.05923055425994954, 'max_depth': 9, 'max_bin': 233, 'num_leaves': 542}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8296\n",
      "\tBest params:\n",
      "\t\tn_estimators: 615\n",
      "\t\tlearning_rate: 0.0404096120295208\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 157\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  385.000000\n",
      "1                    TN  356.000000\n",
      "2                    FP   78.000000\n",
      "3                    FN   80.000000\n",
      "4              Accuracy    0.824249\n",
      "5             Precision    0.831533\n",
      "6           Sensitivity    0.827957\n",
      "7           Specificity    0.820300\n",
      "8              F1 score    0.829741\n",
      "9   F1 score (weighted)    0.824262\n",
      "10     F1 score (macro)    0.824066\n",
      "11    Balanced Accuracy    0.824117\n",
      "12                  MCC    0.648140\n",
      "13                  NPV    0.816500\n",
      "14              ROC_AUC    0.824117\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_lgbm_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_lgbm_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_lgbm_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_lgbm_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_lgbm_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_lgbm_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_lgbm_0)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:21:40,976]\u001b[0m Trial 50 finished with value: 0.7934046637409444 and parameters: {'n_estimators': 893, 'learning_rate': 0.011373365748957972, 'max_depth': 5, 'max_bin': 167, 'num_leaves': 440}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:21:46,386]\u001b[0m Trial 51 finished with value: 0.8277198649289886 and parameters: {'n_estimators': 438, 'learning_rate': 0.04306271350544206, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 361}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:21:54,530]\u001b[0m Trial 52 finished with value: 0.8214645679092044 and parameters: {'n_estimators': 461, 'learning_rate': 0.020281424820927017, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 382}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:00,770]\u001b[0m Trial 53 finished with value: 0.8236564283305796 and parameters: {'n_estimators': 501, 'learning_rate': 0.03819445028112902, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 697}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:05,999]\u001b[0m Trial 54 finished with value: 0.8206156922741776 and parameters: {'n_estimators': 552, 'learning_rate': 0.047438759422632655, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 360}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:12,435]\u001b[0m Trial 55 finished with value: 0.8209123632304532 and parameters: {'n_estimators': 349, 'learning_rate': 0.02924414538141635, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 403}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:17,058]\u001b[0m Trial 56 finished with value: 0.8234395901612407 and parameters: {'n_estimators': 739, 'learning_rate': 0.056647145836350485, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 465}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:20,499]\u001b[0m Trial 57 finished with value: 0.819102849870134 and parameters: {'n_estimators': 789, 'learning_rate': 0.0837036995015654, 'max_depth': 8, 'max_bin': 232, 'num_leaves': 279}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:24,801]\u001b[0m Trial 58 finished with value: 0.8241809388998875 and parameters: {'n_estimators': 622, 'learning_rate': 0.06719914436863614, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 422}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:26,839]\u001b[0m Trial 59 finished with value: 0.8110630265500038 and parameters: {'n_estimators': 830, 'learning_rate': 0.1737266059405858, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 192}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:33,680]\u001b[0m Trial 60 finished with value: 0.8002998452408159 and parameters: {'n_estimators': 307, 'learning_rate': 0.008008118745777178, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 318}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:39,555]\u001b[0m Trial 61 finished with value: 0.8278090444471854 and parameters: {'n_estimators': 425, 'learning_rate': 0.0396040517473513, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 58}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:46,041]\u001b[0m Trial 62 finished with value: 0.8191770391896727 and parameters: {'n_estimators': 363, 'learning_rate': 0.02446011682986802, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 64}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:51,893]\u001b[0m Trial 63 finished with value: 0.8206042768178505 and parameters: {'n_estimators': 441, 'learning_rate': 0.03604480524267342, 'max_depth': 11, 'max_bin': 240, 'num_leaves': 53}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:22:57,308]\u001b[0m Trial 64 finished with value: 0.8246270342874474 and parameters: {'n_estimators': 407, 'learning_rate': 0.04155567512736516, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 100}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:06,097]\u001b[0m Trial 65 finished with value: 0.8197196356106137 and parameters: {'n_estimators': 498, 'learning_rate': 0.018699647518918016, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 166}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:12,550]\u001b[0m Trial 66 finished with value: 0.819783973881076 and parameters: {'n_estimators': 384, 'learning_rate': 0.03022633222239776, 'max_depth': 11, 'max_bin': 236, 'num_leaves': 352}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:15,601]\u001b[0m Trial 67 finished with value: 0.8240312025707276 and parameters: {'n_estimators': 565, 'learning_rate': 0.11019896618296289, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 247}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:19,990]\u001b[0m Trial 68 finished with value: 0.8184251147027103 and parameters: {'n_estimators': 468, 'learning_rate': 0.05963850382146105, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 96}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:24,339]\u001b[0m Trial 69 finished with value: 0.8211479984038483 and parameters: {'n_estimators': 275, 'learning_rate': 0.05049765828555329, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 148}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:26,761]\u001b[0m Trial 70 finished with value: 0.8193309134213387 and parameters: {'n_estimators': 758, 'learning_rate': 0.140734963180145, 'max_depth': 6, 'max_bin': 279, 'num_leaves': 110}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:32,184]\u001b[0m Trial 71 finished with value: 0.8214142910890839 and parameters: {'n_estimators': 423, 'learning_rate': 0.04220796399881371, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 144}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:41,453]\u001b[0m Trial 72 finished with value: 0.8173049484377957 and parameters: {'n_estimators': 484, 'learning_rate': 0.013463885196680988, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 205}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:47,786]\u001b[0m Trial 73 finished with value: 0.8177081891522728 and parameters: {'n_estimators': 347, 'learning_rate': 0.023138477780323435, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 56}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:53,524]\u001b[0m Trial 74 finished with value: 0.8183731525831639 and parameters: {'n_estimators': 403, 'learning_rate': 0.03961835719526029, 'max_depth': 10, 'max_bin': 231, 'num_leaves': 384}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:23:59,873]\u001b[0m Trial 75 finished with value: 0.8183612120840846 and parameters: {'n_estimators': 530, 'learning_rate': 0.0329291800960111, 'max_depth': 9, 'max_bin': 251, 'num_leaves': 186}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:24:03,643]\u001b[0m Trial 76 finished with value: 0.8217582975998445 and parameters: {'n_estimators': 708, 'learning_rate': 0.07305658173379934, 'max_depth': 11, 'max_bin': 243, 'num_leaves': 123}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:24:13,415]\u001b[0m Trial 77 finished with value: 0.8120468065849702 and parameters: {'n_estimators': 449, 'learning_rate': 0.008174792624700696, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 460}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:24:21,370]\u001b[0m Trial 78 finished with value: 0.8225492569990976 and parameters: {'n_estimators': 777, 'learning_rate': 0.025257494734166074, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 299}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:24:22,764]\u001b[0m Trial 79 finished with value: 0.7890589316690784 and parameters: {'n_estimators': 61, 'learning_rate': 0.04558256263333112, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 31}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:24:25,225]\u001b[0m Trial 80 finished with value: 0.7889636192229045 and parameters: {'n_estimators': 423, 'learning_rate': 0.054357819867670965, 'max_depth': 3, 'max_bin': 206, 'num_leaves': 423}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:24:30,833]\u001b[0m Trial 81 finished with value: 0.8220743968390718 and parameters: {'n_estimators': 403, 'learning_rate': 0.039292406646017214, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 103}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:24:36,111]\u001b[0m Trial 82 finished with value: 0.8207455338918417 and parameters: {'n_estimators': 383, 'learning_rate': 0.04325719575602448, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 76}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:24:42,674]\u001b[0m Trial 83 finished with value: 0.8254294179533954 and parameters: {'n_estimators': 860, 'learning_rate': 0.034037876483512246, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 136}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:24:53,759]\u001b[0m Trial 84 finished with value: 0.8206133191034132 and parameters: {'n_estimators': 871, 'learning_rate': 0.01604730326806434, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 175}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:25:00,467]\u001b[0m Trial 85 finished with value: 0.8228274050430457 and parameters: {'n_estimators': 808, 'learning_rate': 0.03320931272993716, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 230}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:25:07,959]\u001b[0m Trial 86 finished with value: 0.8211556049522665 and parameters: {'n_estimators': 877, 'learning_rate': 0.027574948596775103, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 134}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:25:12,101]\u001b[0m Trial 87 finished with value: 0.8223319319512268 and parameters: {'n_estimators': 847, 'learning_rate': 0.06152272621085236, 'max_depth': 9, 'max_bin': 219, 'num_leaves': 269}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:25:21,035]\u001b[0m Trial 88 finished with value: 0.8205730368138038 and parameters: {'n_estimators': 723, 'learning_rate': 0.022336895476490683, 'max_depth': 10, 'max_bin': 268, 'num_leaves': 348}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:25:26,117]\u001b[0m Trial 89 finished with value: 0.8237042474276418 and parameters: {'n_estimators': 601, 'learning_rate': 0.05067280503271125, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 157}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:25:32,438]\u001b[0m Trial 90 finished with value: 0.8256823593593061 and parameters: {'n_estimators': 824, 'learning_rate': 0.036353819345633656, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 322}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:25:38,725]\u001b[0m Trial 91 finished with value: 0.8211958485585106 and parameters: {'n_estimators': 832, 'learning_rate': 0.036639916297283345, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 366}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:25:43,841]\u001b[0m Trial 92 finished with value: 0.8215354835112889 and parameters: {'n_estimators': 798, 'learning_rate': 0.047494291317756565, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 316}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:25:51,756]\u001b[0m Trial 93 finished with value: 0.8209100406058518 and parameters: {'n_estimators': 655, 'learning_rate': 0.028809588528829266, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 403}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:26:07,137]\u001b[0m Trial 94 finished with value: 0.814918069685916 and parameters: {'n_estimators': 768, 'learning_rate': 0.0065677164262107415, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 454}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:26:17,401]\u001b[0m Trial 95 finished with value: 0.8262060029993774 and parameters: {'n_estimators': 824, 'learning_rate': 0.019320709592627053, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 490}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:26:27,554]\u001b[0m Trial 96 finished with value: 0.8210889673841996 and parameters: {'n_estimators': 900, 'learning_rate': 0.018263155876762, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 542}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:26:40,420]\u001b[0m Trial 97 finished with value: 0.819154377735123 and parameters: {'n_estimators': 864, 'learning_rate': 0.01218339399978038, 'max_depth': 10, 'max_bin': 284, 'num_leaves': 485}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:26:57,711]\u001b[0m Trial 98 finished with value: 0.7990480114143195 and parameters: {'n_estimators': 818, 'learning_rate': 0.0029821278680352142, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 391}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:27:04,238]\u001b[0m Trial 99 finished with value: 0.8203649837457412 and parameters: {'n_estimators': 854, 'learning_rate': 0.03475416463271274, 'max_depth': 10, 'max_bin': 296, 'num_leaves': 325}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8296\n",
      "\tBest params:\n",
      "\t\tn_estimators: 615\n",
      "\t\tlearning_rate: 0.0404096120295208\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 157\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  385.000000  395.000000\n",
      "1                    TN  356.000000  364.000000\n",
      "2                    FP   78.000000   79.000000\n",
      "3                    FN   80.000000   61.000000\n",
      "4              Accuracy    0.824249    0.844271\n",
      "5             Precision    0.831533    0.833333\n",
      "6           Sensitivity    0.827957    0.866228\n",
      "7           Specificity    0.820300    0.821700\n",
      "8              F1 score    0.829741    0.849462\n",
      "9   F1 score (weighted)    0.824262    0.844164\n",
      "10     F1 score (macro)    0.824066    0.844086\n",
      "11    Balanced Accuracy    0.824117    0.843949\n",
      "12                  MCC    0.648140    0.688851\n",
      "13                  NPV    0.816500    0.856500\n",
      "14              ROC_AUC    0.824117    0.843949\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_lgbm_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_lgbm_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_lgbm_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_lgbm_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_lgbm_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_lgbm_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_lgbm_1)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:27:14,098]\u001b[0m Trial 100 finished with value: 0.8238697953207883 and parameters: {'n_estimators': 745, 'learning_rate': 0.019691406190368778, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 506}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:27:22,794]\u001b[0m Trial 101 finished with value: 0.8237039010648809 and parameters: {'n_estimators': 818, 'learning_rate': 0.025679654069702802, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 418}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:27:29,436]\u001b[0m Trial 102 finished with value: 0.821806489978723 and parameters: {'n_estimators': 366, 'learning_rate': 0.03138360166925416, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 446}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:27:35,421]\u001b[0m Trial 103 finished with value: 0.8270517496970425 and parameters: {'n_estimators': 686, 'learning_rate': 0.04456297921089894, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 529}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:27:41,971]\u001b[0m Trial 104 finished with value: 0.8252068710083422 and parameters: {'n_estimators': 670, 'learning_rate': 0.037376796570929946, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 566}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:27:48,116]\u001b[0m Trial 105 finished with value: 0.827539588048729 and parameters: {'n_estimators': 678, 'learning_rate': 0.04439194150262854, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 607}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:27:53,435]\u001b[0m Trial 106 finished with value: 0.8271087292112578 and parameters: {'n_estimators': 695, 'learning_rate': 0.05628332595852368, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 661}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:27:58,437]\u001b[0m Trial 107 finished with value: 0.8239288193052838 and parameters: {'n_estimators': 641, 'learning_rate': 0.05651111669442567, 'max_depth': 10, 'max_bin': 254, 'num_leaves': 652}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:02,230]\u001b[0m Trial 108 finished with value: 0.8206920357834686 and parameters: {'n_estimators': 719, 'learning_rate': 0.09552995984117718, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 532}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:06,726]\u001b[0m Trial 109 finished with value: 0.8262200792134348 and parameters: {'n_estimators': 692, 'learning_rate': 0.06762437327088734, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 624}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:12,438]\u001b[0m Trial 110 finished with value: 0.8253866508492426 and parameters: {'n_estimators': 695, 'learning_rate': 0.05316633747368575, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 616}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:16,960]\u001b[0m Trial 111 finished with value: 0.8209256622143256 and parameters: {'n_estimators': 682, 'learning_rate': 0.06444469650498998, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 679}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:23,055]\u001b[0m Trial 112 finished with value: 0.8221323110967319 and parameters: {'n_estimators': 669, 'learning_rate': 0.04614490007696385, 'max_depth': 10, 'max_bin': 254, 'num_leaves': 625}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:29,344]\u001b[0m Trial 113 finished with value: 0.8274200094379983 and parameters: {'n_estimators': 605, 'learning_rate': 0.04325275272236628, 'max_depth': 9, 'max_bin': 248, 'num_leaves': 597}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:34,896]\u001b[0m Trial 114 finished with value: 0.8268446106108007 and parameters: {'n_estimators': 625, 'learning_rate': 0.04977049465347327, 'max_depth': 9, 'max_bin': 270, 'num_leaves': 598}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:38,858]\u001b[0m Trial 115 finished with value: 0.827770155144324 and parameters: {'n_estimators': 606, 'learning_rate': 0.07704862555091418, 'max_depth': 9, 'max_bin': 275, 'num_leaves': 597}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:43,178]\u001b[0m Trial 116 finished with value: 0.8182322814539214 and parameters: {'n_estimators': 622, 'learning_rate': 0.06897878637352868, 'max_depth': 9, 'max_bin': 274, 'num_leaves': 597}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:46,706]\u001b[0m Trial 117 finished with value: 0.823579572533499 and parameters: {'n_estimators': 552, 'learning_rate': 0.08476183684815945, 'max_depth': 8, 'max_bin': 244, 'num_leaves': 652}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:50,917]\u001b[0m Trial 118 finished with value: 0.8222071538834367 and parameters: {'n_estimators': 593, 'learning_rate': 0.07663103331859787, 'max_depth': 9, 'max_bin': 251, 'num_leaves': 563}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:55,298]\u001b[0m Trial 119 finished with value: 0.8240657923089871 and parameters: {'n_estimators': 612, 'learning_rate': 0.06379371253729958, 'max_depth': 9, 'max_bin': 239, 'num_leaves': 587}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:28:59,795]\u001b[0m Trial 120 finished with value: 0.8274945864665242 and parameters: {'n_estimators': 640, 'learning_rate': 0.07139981138088586, 'max_depth': 9, 'max_bin': 248, 'num_leaves': 681}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:05,007]\u001b[0m Trial 121 finished with value: 0.8247768290979034 and parameters: {'n_estimators': 653, 'learning_rate': 0.05622872082244564, 'max_depth': 9, 'max_bin': 249, 'num_leaves': 703}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:09,315]\u001b[0m Trial 122 finished with value: 0.8264822348526355 and parameters: {'n_estimators': 642, 'learning_rate': 0.07267392339835732, 'max_depth': 9, 'max_bin': 247, 'num_leaves': 608}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:13,010]\u001b[0m Trial 123 finished with value: 0.822437934513123 and parameters: {'n_estimators': 572, 'learning_rate': 0.07851506745771612, 'max_depth': 8, 'max_bin': 241, 'num_leaves': 603}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:16,562]\u001b[0m Trial 124 finished with value: 0.8237129601215841 and parameters: {'n_estimators': 639, 'learning_rate': 0.08844892091514475, 'max_depth': 9, 'max_bin': 246, 'num_leaves': 671}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:21,672]\u001b[0m Trial 125 finished with value: 0.8252390252977932 and parameters: {'n_estimators': 609, 'learning_rate': 0.059910808088330866, 'max_depth': 9, 'max_bin': 258, 'num_leaves': 730}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:27,018]\u001b[0m Trial 126 finished with value: 0.8259415353777513 and parameters: {'n_estimators': 626, 'learning_rate': 0.049525518537808816, 'max_depth': 9, 'max_bin': 276, 'num_leaves': 575}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:32,504]\u001b[0m Trial 127 finished with value: 0.8212992941483052 and parameters: {'n_estimators': 674, 'learning_rate': 0.04365224730937467, 'max_depth': 8, 'max_bin': 269, 'num_leaves': 613}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:36,848]\u001b[0m Trial 128 finished with value: 0.8246174877702492 and parameters: {'n_estimators': 709, 'learning_rate': 0.07872456160812388, 'max_depth': 9, 'max_bin': 281, 'num_leaves': 644}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:40,864]\u001b[0m Trial 129 finished with value: 0.8250545697266881 and parameters: {'n_estimators': 544, 'learning_rate': 0.07335613558480343, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 675}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:44,971]\u001b[0m Trial 130 finished with value: 0.8201084680461141 and parameters: {'n_estimators': 574, 'learning_rate': 0.0698233285403499, 'max_depth': 8, 'max_bin': 265, 'num_leaves': 707}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:29:49,727]\u001b[0m Trial 131 finished with value: 0.8265289063916557 and parameters: {'n_estimators': 662, 'learning_rate': 0.06751541860482858, 'max_depth': 10, 'max_bin': 252, 'num_leaves': 632}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:55,037]\u001b[0m Trial 132 finished with value: 0.8244065590507622 and parameters: {'n_estimators': 655, 'learning_rate': 0.05450671094775163, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 662}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:29:59,960]\u001b[0m Trial 133 finished with value: 0.8276243771814681 and parameters: {'n_estimators': 636, 'learning_rate': 0.061556024534686324, 'max_depth': 10, 'max_bin': 255, 'num_leaves': 634}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:04,805]\u001b[0m Trial 134 finished with value: 0.8220894677902676 and parameters: {'n_estimators': 592, 'learning_rate': 0.06253807579131138, 'max_depth': 10, 'max_bin': 258, 'num_leaves': 634}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:10,490]\u001b[0m Trial 135 finished with value: 0.8255227983799791 and parameters: {'n_estimators': 635, 'learning_rate': 0.05155240586840222, 'max_depth': 10, 'max_bin': 244, 'num_leaves': 581}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:15,207]\u001b[0m Trial 136 finished with value: 0.8203375043478964 and parameters: {'n_estimators': 741, 'learning_rate': 0.059162661547715927, 'max_depth': 10, 'max_bin': 255, 'num_leaves': 545}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:21,285]\u001b[0m Trial 137 finished with value: 0.825977091833203 and parameters: {'n_estimators': 664, 'learning_rate': 0.04558634657355542, 'max_depth': 10, 'max_bin': 251, 'num_leaves': 689}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:27,368]\u001b[0m Trial 138 finished with value: 0.8229118699997153 and parameters: {'n_estimators': 609, 'learning_rate': 0.042055399866954536, 'max_depth': 9, 'max_bin': 239, 'num_leaves': 633}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:33,205]\u001b[0m Trial 139 finished with value: 0.8261172687101505 and parameters: {'n_estimators': 516, 'learning_rate': 0.04840744019953459, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 560}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:37,063]\u001b[0m Trial 140 finished with value: 0.8229066494332333 and parameters: {'n_estimators': 701, 'learning_rate': 0.10470535850875497, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 721}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:41,237]\u001b[0m Trial 141 finished with value: 0.827385082126178 and parameters: {'n_estimators': 641, 'learning_rate': 0.07202303616780995, 'max_depth': 9, 'max_bin': 247, 'num_leaves': 610}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:45,625]\u001b[0m Trial 142 finished with value: 0.826234330794518 and parameters: {'n_estimators': 654, 'learning_rate': 0.06533892887342292, 'max_depth': 9, 'max_bin': 251, 'num_leaves': 660}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:51,151]\u001b[0m Trial 143 finished with value: 0.8221737531362864 and parameters: {'n_estimators': 684, 'learning_rate': 0.05562909993392069, 'max_depth': 9, 'max_bin': 247, 'num_leaves': 595}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:55,454]\u001b[0m Trial 144 finished with value: 0.8212665543494033 and parameters: {'n_estimators': 619, 'learning_rate': 0.061124634037300375, 'max_depth': 7, 'max_bin': 255, 'num_leaves': 617}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:30:59,122]\u001b[0m Trial 145 finished with value: 0.8139808212072095 and parameters: {'n_estimators': 724, 'learning_rate': 0.06872408257897261, 'max_depth': 4, 'max_bin': 244, 'num_leaves': 639}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:31:02,355]\u001b[0m Trial 146 finished with value: 0.8191787379295679 and parameters: {'n_estimators': 172, 'learning_rate': 0.05106455035139598, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 519}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:31:05,972]\u001b[0m Trial 147 finished with value: 0.8229137005300803 and parameters: {'n_estimators': 583, 'learning_rate': 0.0881707609802726, 'max_depth': 9, 'max_bin': 279, 'num_leaves': 597}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:31:12,230]\u001b[0m Trial 148 finished with value: 0.8273570532355041 and parameters: {'n_estimators': 633, 'learning_rate': 0.039929330317668356, 'max_depth': 10, 'max_bin': 235, 'num_leaves': 551}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:31:18,455]\u001b[0m Trial 149 finished with value: 0.8282650368308507 and parameters: {'n_estimators': 636, 'learning_rate': 0.04027306384901902, 'max_depth': 9, 'max_bin': 236, 'num_leaves': 547}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8296\n",
      "\tBest params:\n",
      "\t\tn_estimators: 615\n",
      "\t\tlearning_rate: 0.0404096120295208\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 157\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  385.000000  395.000000  403.000000\n",
      "1                    TN  356.000000  364.000000  345.000000\n",
      "2                    FP   78.000000   79.000000   79.000000\n",
      "3                    FN   80.000000   61.000000   72.000000\n",
      "4              Accuracy    0.824249    0.844271    0.832036\n",
      "5             Precision    0.831533    0.833333    0.836100\n",
      "6           Sensitivity    0.827957    0.866228    0.848421\n",
      "7           Specificity    0.820300    0.821700    0.813700\n",
      "8              F1 score    0.829741    0.849462    0.842215\n",
      "9   F1 score (weighted)    0.824262    0.844164    0.831951\n",
      "10     F1 score (macro)    0.824066    0.844086    0.831334\n",
      "11    Balanced Accuracy    0.824117    0.843949    0.831050\n",
      "12                  MCC    0.648140    0.688851    0.662769\n",
      "13                  NPV    0.816500    0.856500    0.827300\n",
      "14              ROC_AUC    0.824117    0.843949    0.831050\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_lgbm_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_lgbm_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_lgbm_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_lgbm_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_lgbm_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_lgbm_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_lgbm_2)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:31:24,979]\u001b[0m Trial 150 finished with value: 0.8141648189078493 and parameters: {'n_estimators': 602, 'learning_rate': 0.04131941522198379, 'max_depth': 9, 'max_bin': 235, 'num_leaves': 557}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:31:30,931]\u001b[0m Trial 151 finished with value: 0.8175450768250269 and parameters: {'n_estimators': 633, 'learning_rate': 0.0394971376234668, 'max_depth': 9, 'max_bin': 232, 'num_leaves': 578}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:31:36,371]\u001b[0m Trial 152 finished with value: 0.8184639360505841 and parameters: {'n_estimators': 644, 'learning_rate': 0.04434453862519874, 'max_depth': 9, 'max_bin': 239, 'num_leaves': 550}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:31:42,277]\u001b[0m Trial 153 finished with value: 0.8108149927227013 and parameters: {'n_estimators': 681, 'learning_rate': 0.03859929055193981, 'max_depth': 9, 'max_bin': 230, 'num_leaves': 521}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:31:47,619]\u001b[0m Trial 154 finished with value: 0.8152947569464072 and parameters: {'n_estimators': 563, 'learning_rate': 0.048003880251815095, 'max_depth': 10, 'max_bin': 221, 'num_leaves': 584}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:31:53,942]\u001b[0m Trial 155 finished with value: 0.8121434289834495 and parameters: {'n_estimators': 625, 'learning_rate': 0.031074434484568154, 'max_depth': 8, 'max_bin': 240, 'num_leaves': 502}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:31:58,237]\u001b[0m Trial 156 finished with value: 0.8101020372003838 and parameters: {'n_estimators': 489, 'learning_rate': 0.05708580051866111, 'max_depth': 8, 'max_bin': 226, 'num_leaves': 570}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:02,286]\u001b[0m Trial 157 finished with value: 0.8182668301363509 and parameters: {'n_estimators': 703, 'learning_rate': 0.08234364912583525, 'max_depth': 10, 'max_bin': 237, 'num_leaves': 607}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:07,523]\u001b[0m Trial 158 finished with value: 0.8151796628298115 and parameters: {'n_estimators': 609, 'learning_rate': 0.04697892258881333, 'max_depth': 9, 'max_bin': 234, 'num_leaves': 537}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:12,464]\u001b[0m Trial 159 finished with value: 0.8178991987203933 and parameters: {'n_estimators': 466, 'learning_rate': 0.05244147515346778, 'max_depth': 9, 'max_bin': 244, 'num_leaves': 617}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:19,119]\u001b[0m Trial 160 finished with value: 0.8187065123447719 and parameters: {'n_estimators': 673, 'learning_rate': 0.0351463063171332, 'max_depth': 10, 'max_bin': 230, 'num_leaves': 650}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:23,512]\u001b[0m Trial 161 finished with value: 0.8164047506362582 and parameters: {'n_estimators': 659, 'learning_rate': 0.07062711556481242, 'max_depth': 10, 'max_bin': 247, 'num_leaves': 644}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:27,503]\u001b[0m Trial 162 finished with value: 0.8130365094085678 and parameters: {'n_estimators': 660, 'learning_rate': 0.07591836666453335, 'max_depth': 10, 'max_bin': 241, 'num_leaves': 625}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:33,203]\u001b[0m Trial 163 finished with value: 0.8216094401239878 and parameters: {'n_estimators': 637, 'learning_rate': 0.042993018180965493, 'max_depth': 10, 'max_bin': 249, 'num_leaves': 590}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:37,409]\u001b[0m Trial 164 finished with value: 0.8144616195987266 and parameters: {'n_estimators': 433, 'learning_rate': 0.059049211698102365, 'max_depth': 9, 'max_bin': 253, 'num_leaves': 688}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:39,800]\u001b[0m Trial 165 finished with value: 0.8084855215097182 and parameters: {'n_estimators': 115, 'learning_rate': 0.06613770943741112, 'max_depth': 10, 'max_bin': 242, 'num_leaves': 667}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:45,739]\u001b[0m Trial 166 finished with value: 0.8196048760191303 and parameters: {'n_estimators': 594, 'learning_rate': 0.04076953929843275, 'max_depth': 10, 'max_bin': 251, 'num_leaves': 632}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:50,910]\u001b[0m Trial 167 finished with value: 0.8143477813164779 and parameters: {'n_estimators': 681, 'learning_rate': 0.04764681542082433, 'max_depth': 9, 'max_bin': 256, 'num_leaves': 608}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:55,778]\u001b[0m Trial 168 finished with value: 0.8165688554930943 and parameters: {'n_estimators': 622, 'learning_rate': 0.05422247332077615, 'max_depth': 10, 'max_bin': 273, 'num_leaves': 573}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:32:59,850]\u001b[0m Trial 169 finished with value: 0.8126290389024501 and parameters: {'n_estimators': 646, 'learning_rate': 0.07975905614997278, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 603}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:04,461]\u001b[0m Trial 170 finished with value: 0.8159134577326765 and parameters: {'n_estimators': 700, 'learning_rate': 0.06395700326179675, 'max_depth': 10, 'max_bin': 156, 'num_leaves': 533}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:08,573]\u001b[0m Trial 171 finished with value: 0.8180814273723183 and parameters: {'n_estimators': 637, 'learning_rate': 0.06966957689061652, 'max_depth': 9, 'max_bin': 243, 'num_leaves': 472}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:12,429]\u001b[0m Trial 172 finished with value: 0.8162271096019242 and parameters: {'n_estimators': 652, 'learning_rate': 0.07468173414986543, 'max_depth': 9, 'max_bin': 247, 'num_leaves': 609}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:17,255]\u001b[0m Trial 173 finished with value: 0.8150626416504887 and parameters: {'n_estimators': 610, 'learning_rate': 0.05104289469142153, 'max_depth': 9, 'max_bin': 246, 'num_leaves': 632}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:21,128]\u001b[0m Trial 174 finished with value: 0.8124624356507051 and parameters: {'n_estimators': 584, 'learning_rate': 0.07391044086858187, 'max_depth': 9, 'max_bin': 249, 'num_leaves': 585}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:26,495]\u001b[0m Trial 175 finished with value: 0.8133271361443143 and parameters: {'n_estimators': 668, 'learning_rate': 0.03658348652633753, 'max_depth': 8, 'max_bin': 210, 'num_leaves': 660}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:29,549]\u001b[0m Trial 176 finished with value: 0.8145354095295939 and parameters: {'n_estimators': 634, 'learning_rate': 0.11806000751777827, 'max_depth': 9, 'max_bin': 253, 'num_leaves': 621}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:35,212]\u001b[0m Trial 177 finished with value: 0.8177598194023797 and parameters: {'n_estimators': 723, 'learning_rate': 0.04525658346343538, 'max_depth': 12, 'max_bin': 168, 'num_leaves': 551}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:37,465]\u001b[0m Trial 178 finished with value: 0.8096537619218145 and parameters: {'n_estimators': 446, 'learning_rate': 0.19902851595978877, 'max_depth': 10, 'max_bin': 247, 'num_leaves': 596}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:41,245]\u001b[0m Trial 179 finished with value: 0.8153518445660906 and parameters: {'n_estimators': 656, 'learning_rate': 0.08394381365252687, 'max_depth': 9, 'max_bin': 257, 'num_leaves': 349}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:45,530]\u001b[0m Trial 180 finished with value: 0.8154785190549699 and parameters: {'n_estimators': 691, 'learning_rate': 0.0718858176150693, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 645}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:33:49,861]\u001b[0m Trial 181 finished with value: 0.8129090040876429 and parameters: {'n_estimators': 621, 'learning_rate': 0.06736324762299081, 'max_depth': 9, 'max_bin': 251, 'num_leaves': 661}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:54,263]\u001b[0m Trial 182 finished with value: 0.8123710342348864 and parameters: {'n_estimators': 659, 'learning_rate': 0.06427982766790369, 'max_depth': 9, 'max_bin': 276, 'num_leaves': 670}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:33:58,921]\u001b[0m Trial 183 finished with value: 0.8154123052848163 and parameters: {'n_estimators': 676, 'learning_rate': 0.06140317207599322, 'max_depth': 9, 'max_bin': 250, 'num_leaves': 684}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:02,968]\u001b[0m Trial 184 finished with value: 0.8159069557058789 and parameters: {'n_estimators': 646, 'learning_rate': 0.05820780179667307, 'max_depth': 8, 'max_bin': 245, 'num_leaves': 616}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:07,095]\u001b[0m Trial 185 finished with value: 0.8134930516995155 and parameters: {'n_estimators': 605, 'learning_rate': 0.06689934004356517, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 382}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:11,954]\u001b[0m Trial 186 finished with value: 0.8124697782400732 and parameters: {'n_estimators': 629, 'learning_rate': 0.04091262503361358, 'max_depth': 6, 'max_bin': 282, 'num_leaves': 47}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:18,816]\u001b[0m Trial 187 finished with value: 0.8190866156815766 and parameters: {'n_estimators': 715, 'learning_rate': 0.03258513667318814, 'max_depth': 10, 'max_bin': 286, 'num_leaves': 656}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:24,059]\u001b[0m Trial 188 finished with value: 0.8152134190947393 and parameters: {'n_estimators': 753, 'learning_rate': 0.050142276256665355, 'max_depth': 9, 'max_bin': 253, 'num_leaves': 637}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:28,120]\u001b[0m Trial 189 finished with value: 0.8137283281146785 and parameters: {'n_estimators': 642, 'learning_rate': 0.07819034842265861, 'max_depth': 11, 'max_bin': 238, 'num_leaves': 570}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:31,774]\u001b[0m Trial 190 finished with value: 0.8134615024759142 and parameters: {'n_estimators': 693, 'learning_rate': 0.09498143305401247, 'max_depth': 9, 'max_bin': 242, 'num_leaves': 597}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:36,124]\u001b[0m Trial 191 finished with value: 0.8132662643308219 and parameters: {'n_estimators': 690, 'learning_rate': 0.06582928277730628, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 624}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:40,720]\u001b[0m Trial 192 finished with value: 0.8155163996165802 and parameters: {'n_estimators': 668, 'learning_rate': 0.06186304857497499, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 626}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:44,923]\u001b[0m Trial 193 finished with value: 0.8125161245930729 and parameters: {'n_estimators': 595, 'learning_rate': 0.07043290511532817, 'max_depth': 11, 'max_bin': 245, 'num_leaves': 650}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:49,189]\u001b[0m Trial 194 finished with value: 0.8198226175253647 and parameters: {'n_estimators': 650, 'learning_rate': 0.07338669107298947, 'max_depth': 10, 'max_bin': 258, 'num_leaves': 707}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:54,011]\u001b[0m Trial 195 finished with value: 0.814972261676352 and parameters: {'n_estimators': 732, 'learning_rate': 0.054917018620223426, 'max_depth': 10, 'max_bin': 255, 'num_leaves': 612}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:34:59,842]\u001b[0m Trial 196 finished with value: 0.8162400920554574 and parameters: {'n_estimators': 623, 'learning_rate': 0.04342275057663584, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 585}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:35:06,045]\u001b[0m Trial 197 finished with value: 0.8177951572479657 and parameters: {'n_estimators': 685, 'learning_rate': 0.038518669415476825, 'max_depth': 10, 'max_bin': 241, 'num_leaves': 558}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:35:10,252]\u001b[0m Trial 198 finished with value: 0.8181473125534146 and parameters: {'n_estimators': 412, 'learning_rate': 0.06608109703140796, 'max_depth': 9, 'max_bin': 249, 'num_leaves': 637}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:35:17,778]\u001b[0m Trial 199 finished with value: 0.8170903145159985 and parameters: {'n_estimators': 706, 'learning_rate': 0.028164124327572638, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 519}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8296\n",
      "\tBest params:\n",
      "\t\tn_estimators: 615\n",
      "\t\tlearning_rate: 0.0404096120295208\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 157\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  385.000000  395.000000  403.000000  397.000000\n",
      "1                    TN  356.000000  364.000000  345.000000  351.000000\n",
      "2                    FP   78.000000   79.000000   79.000000   78.000000\n",
      "3                    FN   80.000000   61.000000   72.000000   73.000000\n",
      "4              Accuracy    0.824249    0.844271    0.832036    0.832036\n",
      "5             Precision    0.831533    0.833333    0.836100    0.835789\n",
      "6           Sensitivity    0.827957    0.866228    0.848421    0.844681\n",
      "7           Specificity    0.820300    0.821700    0.813700    0.818200\n",
      "8              F1 score    0.829741    0.849462    0.842215    0.840212\n",
      "9   F1 score (weighted)    0.824262    0.844164    0.831951    0.831988\n",
      "10     F1 score (macro)    0.824066    0.844086    0.831334    0.831595\n",
      "11    Balanced Accuracy    0.824117    0.843949    0.831050    0.831431\n",
      "12                  MCC    0.648140    0.688851    0.662769    0.663241\n",
      "13                  NPV    0.816500    0.856500    0.827300    0.827800\n",
      "14              ROC_AUC    0.824117    0.843949    0.831050    0.831431\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_lgbm_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_lgbm_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_lgbm_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_lgbm_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_lgbm_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_lgbm_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_lgbm_3)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:35:23,134]\u001b[0m Trial 200 finished with value: 0.826197952327206 and parameters: {'n_estimators': 666, 'learning_rate': 0.058286898149226864, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 72}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:35:34,298]\u001b[0m Trial 201 finished with value: 0.8186348411948211 and parameters: {'n_estimators': 610, 'learning_rate': 0.011618548812107367, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 498}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:35:45,243]\u001b[0m Trial 202 finished with value: 0.8290250875105762 and parameters: {'n_estimators': 644, 'learning_rate': 0.015114100122304956, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 603}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:35:58,727]\u001b[0m Trial 203 finished with value: 0.8116039424246226 and parameters: {'n_estimators': 645, 'learning_rate': 0.004935064297741789, 'max_depth': 11, 'max_bin': 245, 'num_leaves': 604}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:36:10,042]\u001b[0m Trial 204 finished with value: 0.8286869865767066 and parameters: {'n_estimators': 628, 'learning_rate': 0.0159083769440797, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 591}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:36:21,683]\u001b[0m Trial 205 finished with value: 0.8287343546685616 and parameters: {'n_estimators': 625, 'learning_rate': 0.014892882277000166, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 579}. Best is trial 23 with value: 0.8295934115345324.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:36:31,672]\u001b[0m Trial 206 finished with value: 0.8331890017437669 and parameters: {'n_estimators': 574, 'learning_rate': 0.01793932177679684, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 580}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:36:42,812]\u001b[0m Trial 207 finished with value: 0.826192880315125 and parameters: {'n_estimators': 589, 'learning_rate': 0.01403721357235408, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 566}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:36:53,957]\u001b[0m Trial 208 finished with value: 0.8270909875653872 and parameters: {'n_estimators': 626, 'learning_rate': 0.01513818162587392, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 577}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:37:04,671]\u001b[0m Trial 209 finished with value: 0.8272571517161227 and parameters: {'n_estimators': 565, 'learning_rate': 0.01594470910661687, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 578}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:37:14,970]\u001b[0m Trial 210 finished with value: 0.8267427177799288 and parameters: {'n_estimators': 555, 'learning_rate': 0.01620052224638396, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 575}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:37:26,562]\u001b[0m Trial 211 finished with value: 0.8247551698869657 and parameters: {'n_estimators': 563, 'learning_rate': 0.009138792153949426, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 581}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:37:37,061]\u001b[0m Trial 212 finished with value: 0.8264793478584327 and parameters: {'n_estimators': 569, 'learning_rate': 0.017552733426844935, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 574}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:37:45,846]\u001b[0m Trial 213 finished with value: 0.8259951853234542 and parameters: {'n_estimators': 541, 'learning_rate': 0.021896203525347203, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 563}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:37:56,792]\u001b[0m Trial 214 finished with value: 0.8270191301029083 and parameters: {'n_estimators': 602, 'learning_rate': 0.015373154656283401, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 548}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:38:08,849]\u001b[0m Trial 215 finished with value: 0.8250410411437311 and parameters: {'n_estimators': 600, 'learning_rate': 0.010109382450582918, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 545}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:38:19,915]\u001b[0m Trial 216 finished with value: 0.828975342362942 and parameters: {'n_estimators': 580, 'learning_rate': 0.015202973723947294, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 589}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:38:29,437]\u001b[0m Trial 217 finished with value: 0.8298592562972118 and parameters: {'n_estimators': 590, 'learning_rate': 0.020943396524900203, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 536}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:38:38,271]\u001b[0m Trial 218 finished with value: 0.8242956505724456 and parameters: {'n_estimators': 575, 'learning_rate': 0.024150610683858154, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 585}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:38:47,548]\u001b[0m Trial 219 finished with value: 0.8258489188677876 and parameters: {'n_estimators': 582, 'learning_rate': 0.021758172454705967, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 531}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:38:59,156]\u001b[0m Trial 220 finished with value: 0.8290160627178468 and parameters: {'n_estimators': 613, 'learning_rate': 0.013531952393758545, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 557}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:39:11,822]\u001b[0m Trial 221 finished with value: 0.8221987887761504 and parameters: {'n_estimators': 613, 'learning_rate': 0.008082220573857657, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 555}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:39:23,919]\u001b[0m Trial 222 finished with value: 0.8210525870281635 and parameters: {'n_estimators': 626, 'learning_rate': 0.012719550253406748, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 596}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:39:40,836]\u001b[0m Trial 223 finished with value: 0.5188709612249118 and parameters: {'n_estimators': 587, 'learning_rate': 0.00014107498572826609, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 559}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:39:51,067]\u001b[0m Trial 224 finished with value: 0.8278060311510531 and parameters: {'n_estimators': 617, 'learning_rate': 0.01769629440177658, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 588}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:40:01,533]\u001b[0m Trial 225 finished with value: 0.8235838592956393 and parameters: {'n_estimators': 612, 'learning_rate': 0.01665875574942254, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 581}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:40:11,337]\u001b[0m Trial 226 finished with value: 0.8256673954468425 and parameters: {'n_estimators': 579, 'learning_rate': 0.020426869362375196, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 593}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:40:23,446]\u001b[0m Trial 227 finished with value: 0.8304730037552226 and parameters: {'n_estimators': 623, 'learning_rate': 0.012934089481382164, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 569}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:40:36,337]\u001b[0m Trial 228 finished with value: 0.8193358840745939 and parameters: {'n_estimators': 603, 'learning_rate': 0.007258891372197181, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 595}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:40:47,153]\u001b[0m Trial 229 finished with value: 0.8233792023710256 and parameters: {'n_estimators': 531, 'learning_rate': 0.011490163653957938, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 563}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:40:55,771]\u001b[0m Trial 230 finished with value: 0.8277087871437333 and parameters: {'n_estimators': 597, 'learning_rate': 0.02537664604226046, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 610}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:41:05,523]\u001b[0m Trial 231 finished with value: 0.8276860270971451 and parameters: {'n_estimators': 594, 'learning_rate': 0.01923404039998886, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 608}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:41:14,104]\u001b[0m Trial 232 finished with value: 0.8310572234444216 and parameters: {'n_estimators': 597, 'learning_rate': 0.02577726450647322, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 605}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:41:22,419]\u001b[0m Trial 233 finished with value: 0.8187562242040818 and parameters: {'n_estimators': 597, 'learning_rate': 0.024212313870370754, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 608}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:41:31,828]\u001b[0m Trial 234 finished with value: 0.8262112286685401 and parameters: {'n_estimators': 619, 'learning_rate': 0.020204774783479304, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 606}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:41:40,144]\u001b[0m Trial 235 finished with value: 0.8294489534465199 and parameters: {'n_estimators': 584, 'learning_rate': 0.027039226867406722, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 594}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:41:48,229]\u001b[0m Trial 236 finished with value: 0.8199089164616982 and parameters: {'n_estimators': 587, 'learning_rate': 0.026806487407869093, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 591}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:41:56,490]\u001b[0m Trial 237 finished with value: 0.8244509149757443 and parameters: {'n_estimators': 602, 'learning_rate': 0.026509836889506574, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 600}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:42:06,684]\u001b[0m Trial 238 finished with value: 0.8296891664104203 and parameters: {'n_estimators': 633, 'learning_rate': 0.020799081838896453, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 618}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:42:16,752]\u001b[0m Trial 239 finished with value: 0.8259944278485225 and parameters: {'n_estimators': 577, 'learning_rate': 0.01963762531221733, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 616}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:42:26,913]\u001b[0m Trial 240 finished with value: 0.8304121971414775 and parameters: {'n_estimators': 610, 'learning_rate': 0.020238683542863508, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 614}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:42:35,945]\u001b[0m Trial 241 finished with value: 0.8276981727407466 and parameters: {'n_estimators': 610, 'learning_rate': 0.023723132580647746, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 746}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:42:45,041]\u001b[0m Trial 242 finished with value: 0.8257438493318203 and parameters: {'n_estimators': 612, 'learning_rate': 0.023271342727456602, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 587}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:42:52,674]\u001b[0m Trial 243 finished with value: 0.8256850605360933 and parameters: {'n_estimators': 590, 'learning_rate': 0.02989129067595404, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 606}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:43:02,221]\u001b[0m Trial 244 finished with value: 0.821350409288619 and parameters: {'n_estimators': 622, 'learning_rate': 0.019605026882445427, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 626}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:43:10,640]\u001b[0m Trial 245 finished with value: 0.8245970806816132 and parameters: {'n_estimators': 551, 'learning_rate': 0.024930103034985943, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 592}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:43:20,758]\u001b[0m Trial 246 finished with value: 0.8329658159454588 and parameters: {'n_estimators': 601, 'learning_rate': 0.018565393927349065, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 622}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:43:31,401]\u001b[0m Trial 247 finished with value: 0.8264756806393656 and parameters: {'n_estimators': 596, 'learning_rate': 0.017791315100451885, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 738}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:43:42,416]\u001b[0m Trial 248 finished with value: 0.8269590564300902 and parameters: {'n_estimators': 577, 'learning_rate': 0.0138765019561261, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 720}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:43:52,105]\u001b[0m Trial 249 finished with value: 0.8260676282541777 and parameters: {'n_estimators': 614, 'learning_rate': 0.0214033120017661, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 619}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8332\n",
      "\tBest params:\n",
      "\t\tn_estimators: 574\n",
      "\t\tlearning_rate: 0.01793932177679684\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 237\n",
      "\t\tnum_leaves: 580\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  395.000000  403.000000  397.000000   \n",
      "1                    TN  356.000000  364.000000  345.000000  351.000000   \n",
      "2                    FP   78.000000   79.000000   79.000000   78.000000   \n",
      "3                    FN   80.000000   61.000000   72.000000   73.000000   \n",
      "4              Accuracy    0.824249    0.844271    0.832036    0.832036   \n",
      "5             Precision    0.831533    0.833333    0.836100    0.835789   \n",
      "6           Sensitivity    0.827957    0.866228    0.848421    0.844681   \n",
      "7           Specificity    0.820300    0.821700    0.813700    0.818200   \n",
      "8              F1 score    0.829741    0.849462    0.842215    0.840212   \n",
      "9   F1 score (weighted)    0.824262    0.844164    0.831951    0.831988   \n",
      "10     F1 score (macro)    0.824066    0.844086    0.831334    0.831595   \n",
      "11    Balanced Accuracy    0.824117    0.843949    0.831050    0.831431   \n",
      "12                  MCC    0.648140    0.688851    0.662769    0.663241   \n",
      "13                  NPV    0.816500    0.856500    0.827300    0.827800   \n",
      "14              ROC_AUC    0.824117    0.843949    0.831050    0.831431   \n",
      "\n",
      "          Set4  \n",
      "0   394.000000  \n",
      "1   352.000000  \n",
      "2    99.000000  \n",
      "3    54.000000  \n",
      "4     0.829811  \n",
      "5     0.799189  \n",
      "6     0.879464  \n",
      "7     0.780500  \n",
      "8     0.837407  \n",
      "9     0.829412  \n",
      "10    0.829439  \n",
      "11    0.829976  \n",
      "12    0.663061  \n",
      "13    0.867000  \n",
      "14    0.829976  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_lgbm_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_lgbm_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_lgbm_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_lgbm_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_lgbm_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_lgbm_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_lgbm_4)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:44:05,429]\u001b[0m Trial 250 finished with value: 0.8247197855512953 and parameters: {'n_estimators': 634, 'learning_rate': 0.011389650311090734, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 613}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:44:14,257]\u001b[0m Trial 251 finished with value: 0.8305056340227205 and parameters: {'n_estimators': 623, 'learning_rate': 0.02856352309048336, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 570}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:44:22,362]\u001b[0m Trial 252 finished with value: 0.8310093352481216 and parameters: {'n_estimators': 596, 'learning_rate': 0.029611722423206095, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 571}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:44:31,182]\u001b[0m Trial 253 finished with value: 0.8265257956733949 and parameters: {'n_estimators': 593, 'learning_rate': 0.027457488511874878, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 573}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:44:38,928]\u001b[0m Trial 254 finished with value: 0.8277114700811158 and parameters: {'n_estimators': 560, 'learning_rate': 0.030565236235486275, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 562}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:44:46,662]\u001b[0m Trial 255 finished with value: 0.8299445150937853 and parameters: {'n_estimators': 567, 'learning_rate': 0.031068661804957445, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 564}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:44:54,545]\u001b[0m Trial 256 finished with value: 0.8310664831594652 and parameters: {'n_estimators': 544, 'learning_rate': 0.032030193686854905, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 542}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:45:01,641]\u001b[0m Trial 257 finished with value: 0.8274192488758041 and parameters: {'n_estimators': 539, 'learning_rate': 0.03133434468330284, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 537}. Best is trial 206 with value: 0.8331890017437669.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:45:09,414]\u001b[0m Trial 258 finished with value: 0.8332384321934068 and parameters: {'n_estimators': 511, 'learning_rate': 0.0324004363183632, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 561}. Best is trial 258 with value: 0.8332384321934068.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:45:16,426]\u001b[0m Trial 259 finished with value: 0.8346195193450023 and parameters: {'n_estimators': 565, 'learning_rate': 0.03451673275901852, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 544}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:45:23,596]\u001b[0m Trial 260 finished with value: 0.8319426788094539 and parameters: {'n_estimators': 508, 'learning_rate': 0.03410342541674108, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 543}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:45:30,915]\u001b[0m Trial 261 finished with value: 0.8269298001419179 and parameters: {'n_estimators': 482, 'learning_rate': 0.03272638529144049, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 541}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:45:39,024]\u001b[0m Trial 262 finished with value: 0.8305704164445042 and parameters: {'n_estimators': 517, 'learning_rate': 0.028850444686916644, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 517}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:45:46,287]\u001b[0m Trial 263 finished with value: 0.8299618379655263 and parameters: {'n_estimators': 525, 'learning_rate': 0.0346100265920528, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 518}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:45:53,422]\u001b[0m Trial 264 finished with value: 0.8271552294482156 and parameters: {'n_estimators': 510, 'learning_rate': 0.03286067080467574, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 515}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:46:01,260]\u001b[0m Trial 265 finished with value: 0.8280058958517749 and parameters: {'n_estimators': 514, 'learning_rate': 0.0292129606253072, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 536}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:46:08,662]\u001b[0m Trial 266 finished with value: 0.8291111326979766 and parameters: {'n_estimators': 523, 'learning_rate': 0.034886790764075006, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 513}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:46:16,059]\u001b[0m Trial 267 finished with value: 0.8291244376782284 and parameters: {'n_estimators': 518, 'learning_rate': 0.03452671793790835, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 510}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:46:22,942]\u001b[0m Trial 268 finished with value: 0.8335269218474345 and parameters: {'n_estimators': 522, 'learning_rate': 0.0347927488392677, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 503}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:46:30,415]\u001b[0m Trial 269 finished with value: 0.8291851971861327 and parameters: {'n_estimators': 524, 'learning_rate': 0.03255293447556126, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 512}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:46:37,713]\u001b[0m Trial 270 finished with value: 0.8299172051547948 and parameters: {'n_estimators': 498, 'learning_rate': 0.034497668641935285, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 513}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:46:44,753]\u001b[0m Trial 271 finished with value: 0.8343379196825691 and parameters: {'n_estimators': 495, 'learning_rate': 0.035215422635162516, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 507}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:46:51,987]\u001b[0m Trial 272 finished with value: 0.8295618950087846 and parameters: {'n_estimators': 520, 'learning_rate': 0.03521174875477598, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 485}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:46:58,701]\u001b[0m Trial 273 finished with value: 0.8308238944279523 and parameters: {'n_estimators': 498, 'learning_rate': 0.034995442319368256, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 504}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:47:05,947]\u001b[0m Trial 274 finished with value: 0.8269437358191636 and parameters: {'n_estimators': 499, 'learning_rate': 0.03509876113207513, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 476}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:47:12,906]\u001b[0m Trial 275 finished with value: 0.8305884416690018 and parameters: {'n_estimators': 502, 'learning_rate': 0.03633615835068383, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 499}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:47:20,331]\u001b[0m Trial 276 finished with value: 0.8271691681536953 and parameters: {'n_estimators': 505, 'learning_rate': 0.030575076425681406, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 485}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:47:27,269]\u001b[0m Trial 277 finished with value: 0.8315140503643903 and parameters: {'n_estimators': 492, 'learning_rate': 0.036354932447345924, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 495}. Best is trial 259 with value: 0.8346195193450023.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:47:34,274]\u001b[0m Trial 278 finished with value: 0.8358721271615851 and parameters: {'n_estimators': 486, 'learning_rate': 0.036166579107950805, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 502}. Best is trial 278 with value: 0.8358721271615851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:47:41,365]\u001b[0m Trial 279 finished with value: 0.8329521333662571 and parameters: {'n_estimators': 485, 'learning_rate': 0.03676011679886731, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 498}. Best is trial 278 with value: 0.8358721271615851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:47:47,782]\u001b[0m Trial 280 finished with value: 0.8364344323727007 and parameters: {'n_estimators': 474, 'learning_rate': 0.03748850514767109, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 494}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:47:54,570]\u001b[0m Trial 281 finished with value: 0.8308046555042565 and parameters: {'n_estimators': 471, 'learning_rate': 0.03733334759606042, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 500}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:48:01,196]\u001b[0m Trial 282 finished with value: 0.8319134269566847 and parameters: {'n_estimators': 474, 'learning_rate': 0.0373666582972166, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 499}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:48:07,943]\u001b[0m Trial 283 finished with value: 0.8302366638830708 and parameters: {'n_estimators': 464, 'learning_rate': 0.03716080512042523, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 500}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:48:14,702]\u001b[0m Trial 284 finished with value: 0.8313339189762946 and parameters: {'n_estimators': 464, 'learning_rate': 0.037739469254944195, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 499}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:48:21,598]\u001b[0m Trial 285 finished with value: 0.8310876400964645 and parameters: {'n_estimators': 466, 'learning_rate': 0.03643993520390842, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 495}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:48:27,901]\u001b[0m Trial 286 finished with value: 0.8355059282750801 and parameters: {'n_estimators': 461, 'learning_rate': 0.03983687998998839, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 493}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:48:34,969]\u001b[0m Trial 287 finished with value: 0.8298821437779298 and parameters: {'n_estimators': 483, 'learning_rate': 0.03891959896849392, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 463}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:48:41,601]\u001b[0m Trial 288 finished with value: 0.8318935882917871 and parameters: {'n_estimators': 454, 'learning_rate': 0.0378082779428478, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 490}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:48:48,044]\u001b[0m Trial 289 finished with value: 0.8335801796487836 and parameters: {'n_estimators': 451, 'learning_rate': 0.03805615418458867, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 494}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:48:54,887]\u001b[0m Trial 290 finished with value: 0.83158534167959 and parameters: {'n_estimators': 454, 'learning_rate': 0.037896961614542556, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 493}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:49:01,656]\u001b[0m Trial 291 finished with value: 0.8291065039343959 and parameters: {'n_estimators': 454, 'learning_rate': 0.03826967067053604, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 493}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:49:08,482]\u001b[0m Trial 292 finished with value: 0.8327992541235385 and parameters: {'n_estimators': 475, 'learning_rate': 0.03850603248036219, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 475}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:49:15,017]\u001b[0m Trial 293 finished with value: 0.8293182322995601 and parameters: {'n_estimators': 474, 'learning_rate': 0.038482367446552826, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 474}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:49:21,917]\u001b[0m Trial 294 finished with value: 0.8321740322703522 and parameters: {'n_estimators': 459, 'learning_rate': 0.0385175846123821, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 486}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:49:28,377]\u001b[0m Trial 295 finished with value: 0.832765614043679 and parameters: {'n_estimators': 446, 'learning_rate': 0.0413955959085211, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 451}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:49:34,760]\u001b[0m Trial 296 finished with value: 0.8285374298949693 and parameters: {'n_estimators': 439, 'learning_rate': 0.04171759306844595, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 455}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:49:41,158]\u001b[0m Trial 297 finished with value: 0.8359106953351075 and parameters: {'n_estimators': 461, 'learning_rate': 0.04198500372340586, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 478}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:49:47,434]\u001b[0m Trial 298 finished with value: 0.8317256325279201 and parameters: {'n_estimators': 456, 'learning_rate': 0.04273625834224297, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 483}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:49:53,856]\u001b[0m Trial 299 finished with value: 0.8287727040899819 and parameters: {'n_estimators': 455, 'learning_rate': 0.04162897815424441, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 476}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8364\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n",
      "\t\tlearning_rate: 0.03748850514767109\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 220\n",
      "\t\tnum_leaves: 494\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  395.000000  403.000000  397.000000   \n",
      "1                    TN  356.000000  364.000000  345.000000  351.000000   \n",
      "2                    FP   78.000000   79.000000   79.000000   78.000000   \n",
      "3                    FN   80.000000   61.000000   72.000000   73.000000   \n",
      "4              Accuracy    0.824249    0.844271    0.832036    0.832036   \n",
      "5             Precision    0.831533    0.833333    0.836100    0.835789   \n",
      "6           Sensitivity    0.827957    0.866228    0.848421    0.844681   \n",
      "7           Specificity    0.820300    0.821700    0.813700    0.818200   \n",
      "8              F1 score    0.829741    0.849462    0.842215    0.840212   \n",
      "9   F1 score (weighted)    0.824262    0.844164    0.831951    0.831988   \n",
      "10     F1 score (macro)    0.824066    0.844086    0.831334    0.831595   \n",
      "11    Balanced Accuracy    0.824117    0.843949    0.831050    0.831431   \n",
      "12                  MCC    0.648140    0.688851    0.662769    0.663241   \n",
      "13                  NPV    0.816500    0.856500    0.827300    0.827800   \n",
      "14              ROC_AUC    0.824117    0.843949    0.831050    0.831431   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   394.000000  384.000000  \n",
      "1   352.000000  350.000000  \n",
      "2    99.000000   83.000000  \n",
      "3    54.000000   82.000000  \n",
      "4     0.829811    0.816463  \n",
      "5     0.799189    0.822270  \n",
      "6     0.879464    0.824034  \n",
      "7     0.780500    0.808300  \n",
      "8     0.837407    0.823151  \n",
      "9     0.829412    0.816455  \n",
      "10    0.829439    0.816200  \n",
      "11    0.829976    0.816174  \n",
      "12    0.663061    0.632402  \n",
      "13    0.867000    0.810200  \n",
      "14    0.829976    0.816174  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_lgbm_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_lgbm_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_lgbm_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_lgbm_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_lgbm_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_lgbm_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_lgbm_5)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:50:00,499]\u001b[0m Trial 300 finished with value: 0.8294555961915402 and parameters: {'n_estimators': 453, 'learning_rate': 0.04395691519076264, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 486}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:06,709]\u001b[0m Trial 301 finished with value: 0.8315261796154889 and parameters: {'n_estimators': 485, 'learning_rate': 0.03967220540444852, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 468}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:12,811]\u001b[0m Trial 302 finished with value: 0.8280672301109882 and parameters: {'n_estimators': 479, 'learning_rate': 0.03956457523504514, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 458}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:19,134]\u001b[0m Trial 303 finished with value: 0.8269062522482397 and parameters: {'n_estimators': 433, 'learning_rate': 0.04411425588058844, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 441}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:25,512]\u001b[0m Trial 304 finished with value: 0.8253221930371142 and parameters: {'n_estimators': 464, 'learning_rate': 0.03895704692111917, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 470}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:31,596]\u001b[0m Trial 305 finished with value: 0.826106130993072 and parameters: {'n_estimators': 486, 'learning_rate': 0.040904261102837215, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 490}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:37,422]\u001b[0m Trial 306 finished with value: 0.8281437978104969 and parameters: {'n_estimators': 443, 'learning_rate': 0.045541898791265835, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 483}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:43,668]\u001b[0m Trial 307 finished with value: 0.8280586441472341 and parameters: {'n_estimators': 416, 'learning_rate': 0.036974839692914804, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 463}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:49,849]\u001b[0m Trial 308 finished with value: 0.8225269257761447 and parameters: {'n_estimators': 467, 'learning_rate': 0.04142051226095, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 491}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:56,879]\u001b[0m Trial 309 finished with value: 0.8256202624664901 and parameters: {'n_estimators': 476, 'learning_rate': 0.03692452495384262, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 430}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:50:59,165]\u001b[0m Trial 310 finished with value: 0.8179083297973886 and parameters: {'n_estimators': 451, 'learning_rate': 0.14837902383196527, 'max_depth': 5, 'max_bin': 218, 'num_leaves': 476}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:05,775]\u001b[0m Trial 311 finished with value: 0.8289457409301342 and parameters: {'n_estimators': 428, 'learning_rate': 0.032991698957763785, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 450}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:11,776]\u001b[0m Trial 312 finished with value: 0.8273979615539204 and parameters: {'n_estimators': 489, 'learning_rate': 0.04609863205677473, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 501}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:17,789]\u001b[0m Trial 313 finished with value: 0.8240157122757804 and parameters: {'n_estimators': 456, 'learning_rate': 0.04176744280309276, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 492}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:24,407]\u001b[0m Trial 314 finished with value: 0.8255633529552828 and parameters: {'n_estimators': 489, 'learning_rate': 0.03688857170583663, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 466}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:31,437]\u001b[0m Trial 315 finished with value: 0.8284345131060871 and parameters: {'n_estimators': 473, 'learning_rate': 0.03246495919488511, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 527}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:37,087]\u001b[0m Trial 316 finished with value: 0.8332675551015643 and parameters: {'n_estimators': 441, 'learning_rate': 0.04747751293130428, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 482}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:42,500]\u001b[0m Trial 317 finished with value: 0.8273376278547098 and parameters: {'n_estimators': 440, 'learning_rate': 0.047528070722972174, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 485}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:49,018]\u001b[0m Trial 318 finished with value: 0.8272829675077217 and parameters: {'n_estimators': 390, 'learning_rate': 0.04117050385703364, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 471}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:55,471]\u001b[0m Trial 319 finished with value: 0.8269999434579823 and parameters: {'n_estimators': 459, 'learning_rate': 0.03816061278083731, 'max_depth': 12, 'max_bin': 195, 'num_leaves': 503}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:51:58,066]\u001b[0m Trial 320 finished with value: 0.8235195026689341 and parameters: {'n_estimators': 488, 'learning_rate': 0.17210802445370277, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 487}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:52:03,805]\u001b[0m Trial 321 finished with value: 0.8278272755357353 and parameters: {'n_estimators': 416, 'learning_rate': 0.044625792104375034, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 449}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:52:10,698]\u001b[0m Trial 322 finished with value: 0.8319944085058826 and parameters: {'n_estimators': 470, 'learning_rate': 0.03574425962711425, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 477}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:52:16,766]\u001b[0m Trial 323 finished with value: 0.8266841811582198 and parameters: {'n_estimators': 436, 'learning_rate': 0.04773170350589708, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 475}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:52:23,221]\u001b[0m Trial 324 finished with value: 0.8261814013332817 and parameters: {'n_estimators': 495, 'learning_rate': 0.04099035847883267, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 507}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:52:30,286]\u001b[0m Trial 325 finished with value: 0.8290721711053918 and parameters: {'n_estimators': 474, 'learning_rate': 0.03367383561299674, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 464}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:52:36,398]\u001b[0m Trial 326 finished with value: 0.8285124089519051 and parameters: {'n_estimators': 449, 'learning_rate': 0.043127486557406, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 519}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:52:42,918]\u001b[0m Trial 327 finished with value: 0.825106882539435 and parameters: {'n_estimators': 503, 'learning_rate': 0.038452526207658544, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 485}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:52:49,601]\u001b[0m Trial 328 finished with value: 0.8259726435506389 and parameters: {'n_estimators': 477, 'learning_rate': 0.03472472585512577, 'max_depth': 12, 'max_bin': 199, 'num_leaves': 504}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:52:55,965]\u001b[0m Trial 329 finished with value: 0.8264665718869111 and parameters: {'n_estimators': 457, 'learning_rate': 0.04004508777684269, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 436}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:53:01,988]\u001b[0m Trial 330 finished with value: 0.8273154142947903 and parameters: {'n_estimators': 432, 'learning_rate': 0.04564501005945802, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 455}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:53:09,020]\u001b[0m Trial 331 finished with value: 0.828740818230381 and parameters: {'n_estimators': 489, 'learning_rate': 0.03184851802452236, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 482}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:53:15,628]\u001b[0m Trial 332 finished with value: 0.8287555473706988 and parameters: {'n_estimators': 465, 'learning_rate': 0.0366401074059391, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 522}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:53:21,634]\u001b[0m Trial 333 finished with value: 0.8301365640181153 and parameters: {'n_estimators': 446, 'learning_rate': 0.04240892549314043, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 472}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:53:28,793]\u001b[0m Trial 334 finished with value: 0.8326672825311935 and parameters: {'n_estimators': 422, 'learning_rate': 0.030307734659916707, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 499}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:53:35,868]\u001b[0m Trial 335 finished with value: 0.8286272351026946 and parameters: {'n_estimators': 420, 'learning_rate': 0.029546999751485097, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 508}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:53:42,881]\u001b[0m Trial 336 finished with value: 0.8287145522978422 and parameters: {'n_estimators': 510, 'learning_rate': 0.03352538959451947, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 492}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:53:49,889]\u001b[0m Trial 337 finished with value: 0.8284411263934311 and parameters: {'n_estimators': 397, 'learning_rate': 0.028797730209019903, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 467}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:53:56,752]\u001b[0m Trial 338 finished with value: 0.8262231108014934 and parameters: {'n_estimators': 429, 'learning_rate': 0.035123905785116545, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 527}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:54:02,591]\u001b[0m Trial 339 finished with value: 0.8237082270957984 and parameters: {'n_estimators': 478, 'learning_rate': 0.04697226794388749, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 477}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:54:08,957]\u001b[0m Trial 340 finished with value: 0.8257334964858835 and parameters: {'n_estimators': 445, 'learning_rate': 0.04015111905390981, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 493}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:54:15,959]\u001b[0m Trial 341 finished with value: 0.8256457593291628 and parameters: {'n_estimators': 488, 'learning_rate': 0.03215604035188609, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 450}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:54:21,939]\u001b[0m Trial 342 finished with value: 0.824586194390708 and parameters: {'n_estimators': 502, 'learning_rate': 0.042796451522174345, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 508}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:54:28,271]\u001b[0m Trial 343 finished with value: 0.8249612438421412 and parameters: {'n_estimators': 410, 'learning_rate': 0.03779136888661847, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 526}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:54:35,884]\u001b[0m Trial 344 finished with value: 0.8351400389186073 and parameters: {'n_estimators': 455, 'learning_rate': 0.02866673155928114, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 460}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:54:43,597]\u001b[0m Trial 345 finished with value: 0.8301201775651371 and parameters: {'n_estimators': 450, 'learning_rate': 0.027935112360642646, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 459}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:54:51,059]\u001b[0m Trial 346 finished with value: 0.8284186777868584 and parameters: {'n_estimators': 470, 'learning_rate': 0.030557444012334983, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 418}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:54:56,242]\u001b[0m Trial 347 finished with value: 0.8269759897144509 and parameters: {'n_estimators': 423, 'learning_rate': 0.04993638723753136, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 443}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:55:03,181]\u001b[0m Trial 348 finished with value: 0.8273505286339302 and parameters: {'n_estimators': 439, 'learning_rate': 0.03319153589191992, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 478}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:55:11,095]\u001b[0m Trial 349 finished with value: 0.8320625760108218 and parameters: {'n_estimators': 453, 'learning_rate': 0.026026809684200403, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 462}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.836434\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n",
      "\t\tlearning_rate: 0.03748850514767109\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 220\n",
      "\t\tnum_leaves: 494\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  395.000000  403.000000  397.000000   \n",
      "1                    TN  356.000000  364.000000  345.000000  351.000000   \n",
      "2                    FP   78.000000   79.000000   79.000000   78.000000   \n",
      "3                    FN   80.000000   61.000000   72.000000   73.000000   \n",
      "4              Accuracy    0.824249    0.844271    0.832036    0.832036   \n",
      "5             Precision    0.831533    0.833333    0.836100    0.835789   \n",
      "6           Sensitivity    0.827957    0.866228    0.848421    0.844681   \n",
      "7           Specificity    0.820300    0.821700    0.813700    0.818200   \n",
      "8              F1 score    0.829741    0.849462    0.842215    0.840212   \n",
      "9   F1 score (weighted)    0.824262    0.844164    0.831951    0.831988   \n",
      "10     F1 score (macro)    0.824066    0.844086    0.831334    0.831595   \n",
      "11    Balanced Accuracy    0.824117    0.843949    0.831050    0.831431   \n",
      "12                  MCC    0.648140    0.688851    0.662769    0.663241   \n",
      "13                  NPV    0.816500    0.856500    0.827300    0.827800   \n",
      "14              ROC_AUC    0.824117    0.843949    0.831050    0.831431   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   394.000000  384.000000  405.000000  \n",
      "1   352.000000  350.000000  342.000000  \n",
      "2    99.000000   83.000000   87.000000  \n",
      "3    54.000000   82.000000   65.000000  \n",
      "4     0.829811    0.816463    0.830923  \n",
      "5     0.799189    0.822270    0.823171  \n",
      "6     0.879464    0.824034    0.861702  \n",
      "7     0.780500    0.808300    0.797200  \n",
      "8     0.837407    0.823151    0.841996  \n",
      "9     0.829412    0.816455    0.830632  \n",
      "10    0.829439    0.816200    0.830089  \n",
      "11    0.829976    0.816174    0.829452  \n",
      "12    0.663061    0.632402    0.661181  \n",
      "13    0.867000    0.810200    0.840300  \n",
      "14    0.829976    0.816174    0.829452  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_lgbm_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_lgbm_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_lgbm_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_lgbm_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_lgbm_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_lgbm_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_lgbm_6)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:55:19,570]\u001b[0m Trial 350 finished with value: 0.8263173197772401 and parameters: {'n_estimators': 457, 'learning_rate': 0.028729222147240597, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 479}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:55:26,988]\u001b[0m Trial 351 finished with value: 0.8263339275464372 and parameters: {'n_estimators': 439, 'learning_rate': 0.02560307124096945, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 432}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:55:34,535]\u001b[0m Trial 352 finished with value: 0.8250752801091525 and parameters: {'n_estimators': 464, 'learning_rate': 0.02820624276857278, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 507}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:55:41,019]\u001b[0m Trial 353 finished with value: 0.8260671906201835 and parameters: {'n_estimators': 403, 'learning_rate': 0.033102974259251815, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 459}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:55:48,886]\u001b[0m Trial 354 finished with value: 0.8281551841734662 and parameters: {'n_estimators': 454, 'learning_rate': 0.026282109338453638, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 490}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:55:55,647]\u001b[0m Trial 355 finished with value: 0.8237486289081337 and parameters: {'n_estimators': 533, 'learning_rate': 0.035742328732315344, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 522}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:00,548]\u001b[0m Trial 356 finished with value: 0.8232131070434775 and parameters: {'n_estimators': 473, 'learning_rate': 0.044060638274518504, 'max_depth': 7, 'max_bin': 218, 'num_leaves': 486}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:03,102]\u001b[0m Trial 357 finished with value: 0.7770086421541617 and parameters: {'n_estimators': 425, 'learning_rate': 0.03126374506607933, 'max_depth': 3, 'max_bin': 228, 'num_leaves': 509}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:10,375]\u001b[0m Trial 358 finished with value: 0.827277535696451 and parameters: {'n_estimators': 505, 'learning_rate': 0.02493215822723675, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 472}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:16,127]\u001b[0m Trial 359 finished with value: 0.821570774390658 and parameters: {'n_estimators': 446, 'learning_rate': 0.03921808844294294, 'max_depth': 12, 'max_bin': 189, 'num_leaves': 448}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:20,716]\u001b[0m Trial 360 finished with value: 0.8214661384919204 and parameters: {'n_estimators': 214, 'learning_rate': 0.0348984945360731, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 496}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:27,733]\u001b[0m Trial 361 finished with value: 0.8227002501968235 and parameters: {'n_estimators': 473, 'learning_rate': 0.03192703747075449, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 529}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:30,853]\u001b[0m Trial 362 finished with value: 0.8284744467418044 and parameters: {'n_estimators': 425, 'learning_rate': 0.12348331216705485, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 460}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:36,399]\u001b[0m Trial 363 finished with value: 0.8222143299635496 and parameters: {'n_estimators': 456, 'learning_rate': 0.04283572595916858, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 482}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:42,706]\u001b[0m Trial 364 finished with value: 0.8266593834121222 and parameters: {'n_estimators': 486, 'learning_rate': 0.037492414493197984, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 515}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:47,866]\u001b[0m Trial 365 finished with value: 0.8268717519894407 and parameters: {'n_estimators': 502, 'learning_rate': 0.04705766917675329, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 501}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:55,616]\u001b[0m Trial 366 finished with value: 0.8225000540414789 and parameters: {'n_estimators': 436, 'learning_rate': 0.024283680926158423, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 472}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:56:58,975]\u001b[0m Trial 367 finished with value: 0.7999590467241274 and parameters: {'n_estimators': 376, 'learning_rate': 0.03023801630276709, 'max_depth': 5, 'max_bin': 219, 'num_leaves': 491}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:57:04,704]\u001b[0m Trial 368 finished with value: 0.8239173203507122 and parameters: {'n_estimators': 536, 'learning_rate': 0.040067056831741624, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 515}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:57:11,040]\u001b[0m Trial 369 finished with value: 0.822766879507953 and parameters: {'n_estimators': 468, 'learning_rate': 0.03530586590858514, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 481}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:57:18,520]\u001b[0m Trial 370 finished with value: 0.822526961180202 and parameters: {'n_estimators': 515, 'learning_rate': 0.028290108952082664, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 460}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:57:24,027]\u001b[0m Trial 371 finished with value: 0.8278767318632273 and parameters: {'n_estimators': 412, 'learning_rate': 0.04352376434001228, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 502}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:57:30,604]\u001b[0m Trial 372 finished with value: 0.8291594064404512 and parameters: {'n_estimators': 460, 'learning_rate': 0.035414944475999965, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 440}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:57:35,756]\u001b[0m Trial 373 finished with value: 0.8241576802587474 and parameters: {'n_estimators': 480, 'learning_rate': 0.050046206939019316, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 527}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:57:41,354]\u001b[0m Trial 374 finished with value: 0.8242371686979432 and parameters: {'n_estimators': 443, 'learning_rate': 0.03922861187007093, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 497}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:57:44,518]\u001b[0m Trial 375 finished with value: 0.8209044914892522 and parameters: {'n_estimators': 489, 'learning_rate': 0.1062863846776188, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 477}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:57:51,735]\u001b[0m Trial 376 finished with value: 0.8254291000044065 and parameters: {'n_estimators': 455, 'learning_rate': 0.03264108041956968, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 534}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:57:56,819]\u001b[0m Trial 377 finished with value: 0.8093394063549602 and parameters: {'n_estimators': 508, 'learning_rate': 0.025449117298654515, 'max_depth': 6, 'max_bin': 213, 'num_leaves': 459}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:58:02,522]\u001b[0m Trial 378 finished with value: 0.828589632480042 and parameters: {'n_estimators': 477, 'learning_rate': 0.041938334999863595, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 510}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:58:09,216]\u001b[0m Trial 379 finished with value: 0.8250473337709702 and parameters: {'n_estimators': 431, 'learning_rate': 0.030836285902919508, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 489}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:58:15,756]\u001b[0m Trial 380 finished with value: 0.823991142333847 and parameters: {'n_estimators': 533, 'learning_rate': 0.037128491403854484, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 473}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 12:58:21,661]\u001b[0m Trial 381 finished with value: 0.8217572193852387 and parameters: {'n_estimators': 468, 'learning_rate': 0.04511335976895585, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 498}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:58:28,538]\u001b[0m Trial 382 finished with value: 0.8247627805931149 and parameters: {'n_estimators': 443, 'learning_rate': 0.03427426137166921, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 520}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:58:34,315]\u001b[0m Trial 383 finished with value: 0.8256663743931577 and parameters: {'n_estimators': 397, 'learning_rate': 0.04009617249425578, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 429}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:58:41,656]\u001b[0m Trial 384 finished with value: 0.8296673118046669 and parameters: {'n_estimators': 496, 'learning_rate': 0.02900068242450993, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 545}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:58:49,710]\u001b[0m Trial 385 finished with value: 0.8258488538470944 and parameters: {'n_estimators': 458, 'learning_rate': 0.022898337787453444, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 448}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:58:56,002]\u001b[0m Trial 386 finished with value: 0.8237309297222509 and parameters: {'n_estimators': 421, 'learning_rate': 0.03749001483548242, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 484}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:58:59,314]\u001b[0m Trial 387 finished with value: 0.8051303046957161 and parameters: {'n_estimators': 485, 'learning_rate': 0.04765331678710562, 'max_depth': 4, 'max_bin': 233, 'num_leaves': 469}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:59:06,441]\u001b[0m Trial 388 finished with value: 0.8312572158598247 and parameters: {'n_estimators': 522, 'learning_rate': 0.03279566538249656, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 504}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:59:12,268]\u001b[0m Trial 389 finished with value: 0.8231977979161049 and parameters: {'n_estimators': 446, 'learning_rate': 0.04096124329055286, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 490}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:59:20,091]\u001b[0m Trial 390 finished with value: 0.826718558060071 and parameters: {'n_estimators': 551, 'learning_rate': 0.027507668048277055, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 514}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:59:26,752]\u001b[0m Trial 391 finished with value: 0.8232295015766216 and parameters: {'n_estimators': 473, 'learning_rate': 0.03579301451319788, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 406}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:59:32,469]\u001b[0m Trial 392 finished with value: 0.8273985487887243 and parameters: {'n_estimators': 501, 'learning_rate': 0.04345417274069901, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 464}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:59:39,628]\u001b[0m Trial 393 finished with value: 0.8225198926436151 and parameters: {'n_estimators': 459, 'learning_rate': 0.031220576913837235, 'max_depth': 12, 'max_bin': 174, 'num_leaves': 488}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:59:46,023]\u001b[0m Trial 394 finished with value: 0.8235160661403711 and parameters: {'n_estimators': 429, 'learning_rate': 0.037922434277782725, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 502}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:59:49,022]\u001b[0m Trial 395 finished with value: 0.8216269746174557 and parameters: {'n_estimators': 477, 'learning_rate': 0.13425546155111084, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 534}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 12:59:57,232]\u001b[0m Trial 396 finished with value: 0.8284744808272926 and parameters: {'n_estimators': 494, 'learning_rate': 0.023099696514056575, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 475}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:00:03,696]\u001b[0m Trial 397 finished with value: 0.8240290071004406 and parameters: {'n_estimators': 409, 'learning_rate': 0.034110081271468556, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 450}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:00:09,200]\u001b[0m Trial 398 finished with value: 0.825067549927974 and parameters: {'n_estimators': 445, 'learning_rate': 0.04543316145296177, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 524}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:00:14,243]\u001b[0m Trial 399 finished with value: 0.8272892595717621 and parameters: {'n_estimators': 515, 'learning_rate': 0.050813478065925545, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 481}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8364344\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n",
      "\t\tlearning_rate: 0.03748850514767109\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 220\n",
      "\t\tnum_leaves: 494\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  395.000000  403.000000  397.000000   \n",
      "1                    TN  356.000000  364.000000  345.000000  351.000000   \n",
      "2                    FP   78.000000   79.000000   79.000000   78.000000   \n",
      "3                    FN   80.000000   61.000000   72.000000   73.000000   \n",
      "4              Accuracy    0.824249    0.844271    0.832036    0.832036   \n",
      "5             Precision    0.831533    0.833333    0.836100    0.835789   \n",
      "6           Sensitivity    0.827957    0.866228    0.848421    0.844681   \n",
      "7           Specificity    0.820300    0.821700    0.813700    0.818200   \n",
      "8              F1 score    0.829741    0.849462    0.842215    0.840212   \n",
      "9   F1 score (weighted)    0.824262    0.844164    0.831951    0.831988   \n",
      "10     F1 score (macro)    0.824066    0.844086    0.831334    0.831595   \n",
      "11    Balanced Accuracy    0.824117    0.843949    0.831050    0.831431   \n",
      "12                  MCC    0.648140    0.688851    0.662769    0.663241   \n",
      "13                  NPV    0.816500    0.856500    0.827300    0.827800   \n",
      "14              ROC_AUC    0.824117    0.843949    0.831050    0.831431   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   394.000000  384.000000  405.000000  388.000000  \n",
      "1   352.000000  350.000000  342.000000  353.000000  \n",
      "2    99.000000   83.000000   87.000000   85.000000  \n",
      "3    54.000000   82.000000   65.000000   73.000000  \n",
      "4     0.829811    0.816463    0.830923    0.824249  \n",
      "5     0.799189    0.822270    0.823171    0.820296  \n",
      "6     0.879464    0.824034    0.861702    0.841649  \n",
      "7     0.780500    0.808300    0.797200    0.805900  \n",
      "8     0.837407    0.823151    0.841996    0.830835  \n",
      "9     0.829412    0.816455    0.830632    0.824158  \n",
      "10    0.829439    0.816200    0.830089    0.823982  \n",
      "11    0.829976    0.816174    0.829452    0.823792  \n",
      "12    0.663061    0.632402    0.661181    0.648259  \n",
      "13    0.867000    0.810200    0.840300    0.828600  \n",
      "14    0.829976    0.816174    0.829452    0.823792  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_lgbm_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_lgbm_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_lgbm_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_lgbm_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_lgbm_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_lgbm_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_lgbm_7)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:00:22,973]\u001b[0m Trial 400 finished with value: 0.8208291732219761 and parameters: {'n_estimators': 469, 'learning_rate': 0.02666576342235808, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 510}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:00:29,120]\u001b[0m Trial 401 finished with value: 0.821594658028286 and parameters: {'n_estimators': 453, 'learning_rate': 0.038657641672780345, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 545}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:00:35,350]\u001b[0m Trial 402 finished with value: 0.826451188390503 and parameters: {'n_estimators': 491, 'learning_rate': 0.04151870953066344, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 467}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:00:42,376]\u001b[0m Trial 403 finished with value: 0.8219313543063986 and parameters: {'n_estimators': 430, 'learning_rate': 0.03068464533104851, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 497}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:00:48,466]\u001b[0m Trial 404 finished with value: 0.8217722821006566 and parameters: {'n_estimators': 544, 'learning_rate': 0.035640594793361055, 'max_depth': 11, 'max_bin': 202, 'num_leaves': 483}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:00:55,290]\u001b[0m Trial 405 finished with value: 0.8249882245901476 and parameters: {'n_estimators': 463, 'learning_rate': 0.03341047073495467, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 520}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:01:01,546]\u001b[0m Trial 406 finished with value: 0.8204347492931265 and parameters: {'n_estimators': 481, 'learning_rate': 0.040083551637991356, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 458}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:01:07,317]\u001b[0m Trial 407 finished with value: 0.8190558869270387 and parameters: {'n_estimators': 513, 'learning_rate': 0.0468698226380901, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 497}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:01:14,850]\u001b[0m Trial 408 finished with value: 0.8258582085160336 and parameters: {'n_estimators': 441, 'learning_rate': 0.02749828858539021, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 509}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:01:20,720]\u001b[0m Trial 409 finished with value: 0.8202578465287796 and parameters: {'n_estimators': 468, 'learning_rate': 0.042578939125125725, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 473}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:01:27,315]\u001b[0m Trial 410 finished with value: 0.8221256329078974 and parameters: {'n_estimators': 499, 'learning_rate': 0.036434474940780524, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 439}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:01:36,049]\u001b[0m Trial 411 finished with value: 0.8242874324427293 and parameters: {'n_estimators': 530, 'learning_rate': 0.023109340248989207, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 533}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:01:42,853]\u001b[0m Trial 412 finished with value: 0.8193649837875576 and parameters: {'n_estimators': 418, 'learning_rate': 0.03119352519189432, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 489}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:01:49,056]\u001b[0m Trial 413 finished with value: 0.8253109239967437 and parameters: {'n_estimators': 455, 'learning_rate': 0.0396991485542542, 'max_depth': 12, 'max_bin': 198, 'num_leaves': 459}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:01:55,864]\u001b[0m Trial 414 finished with value: 0.8235953170501468 and parameters: {'n_estimators': 481, 'learning_rate': 0.0336686035994969, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 419}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:02:01,940]\u001b[0m Trial 415 finished with value: 0.8240914684906773 and parameters: {'n_estimators': 439, 'learning_rate': 0.04448723227654983, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 514}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:02:09,742]\u001b[0m Trial 416 finished with value: 0.8241372296795682 and parameters: {'n_estimators': 507, 'learning_rate': 0.028076932473877953, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 483}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:02:16,226]\u001b[0m Trial 417 finished with value: 0.8241511862766073 and parameters: {'n_estimators': 389, 'learning_rate': 0.037087922622856616, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 491}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:02:21,234]\u001b[0m Trial 418 finished with value: 0.8214146794490311 and parameters: {'n_estimators': 458, 'learning_rate': 0.05194247890377115, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 548}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:02:28,587]\u001b[0m Trial 419 finished with value: 0.8246946466313968 and parameters: {'n_estimators': 490, 'learning_rate': 0.03128561822412472, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 502}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:02:34,671]\u001b[0m Trial 420 finished with value: 0.8209033446654761 and parameters: {'n_estimators': 416, 'learning_rate': 0.040671355791378386, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 468}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:02:40,303]\u001b[0m Trial 421 finished with value: 0.8252973880882439 and parameters: {'n_estimators': 473, 'learning_rate': 0.047648109709009914, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 520}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:02:46,776]\u001b[0m Trial 422 finished with value: 0.8208002353807686 and parameters: {'n_estimators': 440, 'learning_rate': 0.03531649084003857, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 448}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:02:54,844]\u001b[0m Trial 423 finished with value: 0.821953615947962 and parameters: {'n_estimators': 526, 'learning_rate': 0.02480807386629682, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 477}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:03:01,279]\u001b[0m Trial 424 finished with value: 0.8240804427245969 and parameters: {'n_estimators': 454, 'learning_rate': 0.03775485810570072, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 533}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:03:07,296]\u001b[0m Trial 425 finished with value: 0.8224972426556663 and parameters: {'n_estimators': 484, 'learning_rate': 0.04340556103172229, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 502}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:03:14,422]\u001b[0m Trial 426 finished with value: 0.8247033396380143 and parameters: {'n_estimators': 430, 'learning_rate': 0.029520740116358468, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 494}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:03:21,042]\u001b[0m Trial 427 finished with value: 0.8268425139835337 and parameters: {'n_estimators': 468, 'learning_rate': 0.033764224118240475, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 464}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:03:27,549]\u001b[0m Trial 428 finished with value: 0.8230810312065977 and parameters: {'n_estimators': 504, 'learning_rate': 0.03892443781259106, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 514}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:03:33,309]\u001b[0m Trial 429 finished with value: 0.823937035986922 and parameters: {'n_estimators': 553, 'learning_rate': 0.0442811629098428, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 482}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:03:41,410]\u001b[0m Trial 430 finished with value: 0.817946659349951 and parameters: {'n_estimators': 448, 'learning_rate': 0.022987302524124453, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 503}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:03:48,425]\u001b[0m Trial 431 finished with value: 0.8224109415443156 and parameters: {'n_estimators': 478, 'learning_rate': 0.031587192498685426, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 548}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:03:55,801]\u001b[0m Trial 432 finished with value: 0.8198738801247594 and parameters: {'n_estimators': 406, 'learning_rate': 0.026871006558585403, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 439}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:02,368]\u001b[0m Trial 433 finished with value: 0.8213491325875465 and parameters: {'n_estimators': 463, 'learning_rate': 0.035423714302257554, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 528}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:07,904]\u001b[0m Trial 434 finished with value: 0.8242108291390036 and parameters: {'n_estimators': 281, 'learning_rate': 0.039646916027548836, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 475}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:13,647]\u001b[0m Trial 435 finished with value: 0.8210965406662861 and parameters: {'n_estimators': 494, 'learning_rate': 0.04908705662183303, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 455}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:20,802]\u001b[0m Trial 436 finished with value: 0.821825680374098 and parameters: {'n_estimators': 514, 'learning_rate': 0.02984286555988764, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 491}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:27,216]\u001b[0m Trial 437 finished with value: 0.8244515857024768 and parameters: {'n_estimators': 430, 'learning_rate': 0.0419761976865833, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 514}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:33,685]\u001b[0m Trial 438 finished with value: 0.8241532669789287 and parameters: {'n_estimators': 444, 'learning_rate': 0.03482231283420639, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 469}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:36,402]\u001b[0m Trial 439 finished with value: 0.8156977144245354 and parameters: {'n_estimators': 475, 'learning_rate': 0.15521709880566015, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 483}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:43,093]\u001b[0m Trial 440 finished with value: 0.8235502346133071 and parameters: {'n_estimators': 539, 'learning_rate': 0.036765067612038835, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 500}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:48,929]\u001b[0m Trial 441 finished with value: 0.8254802579375999 and parameters: {'n_estimators': 456, 'learning_rate': 0.04570778856509393, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 523}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:04:55,720]\u001b[0m Trial 442 finished with value: 0.8135125991296969 and parameters: {'n_estimators': 338, 'learning_rate': 0.02027447643740831, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 484}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:05:02,418]\u001b[0m Trial 443 finished with value: 0.8246812144964174 and parameters: {'n_estimators': 497, 'learning_rate': 0.03226792867995796, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 454}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:05:09,027]\u001b[0m Trial 444 finished with value: 0.8221615536147826 and parameters: {'n_estimators': 423, 'learning_rate': 0.037955218608932605, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 538}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:05:16,774]\u001b[0m Trial 445 finished with value: 0.8239182921220124 and parameters: {'n_estimators': 464, 'learning_rate': 0.027312260681487545, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 499}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:05:23,095]\u001b[0m Trial 446 finished with value: 0.8264882191711077 and parameters: {'n_estimators': 493, 'learning_rate': 0.04179184435685796, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 471}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:05:29,799]\u001b[0m Trial 447 finished with value: 0.8210803337778341 and parameters: {'n_estimators': 524, 'learning_rate': 0.03323154088132063, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 427}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:05:38,281]\u001b[0m Trial 448 finished with value: 0.8221105401129989 and parameters: {'n_estimators': 480, 'learning_rate': 0.02419025733628927, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 510}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:05:44,280]\u001b[0m Trial 449 finished with value: 0.8194462002920998 and parameters: {'n_estimators': 447, 'learning_rate': 0.03870764265486865, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 491}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.83643443\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n",
      "\t\tlearning_rate: 0.03748850514767109\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 220\n",
      "\t\tnum_leaves: 494\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  395.000000  403.000000  397.000000   \n",
      "1                    TN  356.000000  364.000000  345.000000  351.000000   \n",
      "2                    FP   78.000000   79.000000   79.000000   78.000000   \n",
      "3                    FN   80.000000   61.000000   72.000000   73.000000   \n",
      "4              Accuracy    0.824249    0.844271    0.832036    0.832036   \n",
      "5             Precision    0.831533    0.833333    0.836100    0.835789   \n",
      "6           Sensitivity    0.827957    0.866228    0.848421    0.844681   \n",
      "7           Specificity    0.820300    0.821700    0.813700    0.818200   \n",
      "8              F1 score    0.829741    0.849462    0.842215    0.840212   \n",
      "9   F1 score (weighted)    0.824262    0.844164    0.831951    0.831988   \n",
      "10     F1 score (macro)    0.824066    0.844086    0.831334    0.831595   \n",
      "11    Balanced Accuracy    0.824117    0.843949    0.831050    0.831431   \n",
      "12                  MCC    0.648140    0.688851    0.662769    0.663241   \n",
      "13                  NPV    0.816500    0.856500    0.827300    0.827800   \n",
      "14              ROC_AUC    0.824117    0.843949    0.831050    0.831431   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   394.000000  384.000000  405.000000  388.000000  411.000000  \n",
      "1   352.000000  350.000000  342.000000  353.000000  356.000000  \n",
      "2    99.000000   83.000000   87.000000   85.000000   58.000000  \n",
      "3    54.000000   82.000000   65.000000   73.000000   74.000000  \n",
      "4     0.829811    0.816463    0.830923    0.824249    0.853170  \n",
      "5     0.799189    0.822270    0.823171    0.820296    0.876333  \n",
      "6     0.879464    0.824034    0.861702    0.841649    0.847423  \n",
      "7     0.780500    0.808300    0.797200    0.805900    0.859900  \n",
      "8     0.837407    0.823151    0.841996    0.830835    0.861635  \n",
      "9     0.829412    0.816455    0.830632    0.824158    0.853331  \n",
      "10    0.829439    0.816200    0.830089    0.823982    0.852619  \n",
      "11    0.829976    0.816174    0.829452    0.823792    0.853663  \n",
      "12    0.663061    0.632402    0.661181    0.648259    0.705781  \n",
      "13    0.867000    0.810200    0.840300    0.828600    0.827900  \n",
      "14    0.829976    0.816174    0.829452    0.823792    0.853663  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_lgbm_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_lgbm_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_lgbm_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_lgbm_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_lgbm_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_lgbm_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_lgbm_8)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:05:51,834]\u001b[0m Trial 450 finished with value: 0.8203408085545011 and parameters: {'n_estimators': 404, 'learning_rate': 0.030033822839003004, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 468}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:05:57,542]\u001b[0m Trial 451 finished with value: 0.8263296132826479 and parameters: {'n_estimators': 510, 'learning_rate': 0.04488845610419008, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 552}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:03,942]\u001b[0m Trial 452 finished with value: 0.8247480244619915 and parameters: {'n_estimators': 557, 'learning_rate': 0.03468940860534335, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 523}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:10,096]\u001b[0m Trial 453 finished with value: 0.8253027620632827 and parameters: {'n_estimators': 438, 'learning_rate': 0.0384971120625381, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 448}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:15,310]\u001b[0m Trial 454 finished with value: 0.8223090117644958 and parameters: {'n_estimators': 463, 'learning_rate': 0.053414800146403874, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 507}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:20,859]\u001b[0m Trial 455 finished with value: 0.824137759416702 and parameters: {'n_estimators': 480, 'learning_rate': 0.042207004477895216, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 479}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:27,860]\u001b[0m Trial 456 finished with value: 0.8241826064557032 and parameters: {'n_estimators': 427, 'learning_rate': 0.027795408160096225, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 491}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:29,866]\u001b[0m Trial 457 finished with value: 0.8107401456264809 and parameters: {'n_estimators': 457, 'learning_rate': 0.18948804469560604, 'max_depth': 7, 'max_bin': 231, 'num_leaves': 459}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:35,115]\u001b[0m Trial 458 finished with value: 0.8236238925953743 and parameters: {'n_estimators': 487, 'learning_rate': 0.0495193822372813, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 536}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:41,734]\u001b[0m Trial 459 finished with value: 0.8244942806587672 and parameters: {'n_estimators': 378, 'learning_rate': 0.03285943621825524, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 507}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:49,858]\u001b[0m Trial 460 finished with value: 0.8191050390820331 and parameters: {'n_estimators': 443, 'learning_rate': 0.02099172878448253, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 481}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:06:56,149]\u001b[0m Trial 461 finished with value: 0.8235749335776441 and parameters: {'n_estimators': 509, 'learning_rate': 0.0373094907313551, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 495}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:07:02,310]\u001b[0m Trial 462 finished with value: 0.8209101057749851 and parameters: {'n_estimators': 467, 'learning_rate': 0.040919949304576944, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 466}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:07:07,895]\u001b[0m Trial 463 finished with value: 0.8247595284314955 and parameters: {'n_estimators': 528, 'learning_rate': 0.046784079563791256, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 520}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:07:14,885]\u001b[0m Trial 464 finished with value: 0.8250447020718198 and parameters: {'n_estimators': 412, 'learning_rate': 0.030251615186824878, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 558}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:07:21,313]\u001b[0m Trial 465 finished with value: 0.8220175763739735 and parameters: {'n_estimators': 476, 'learning_rate': 0.034962817298867584, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 478}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:07:26,926]\u001b[0m Trial 466 finished with value: 0.819671025724302 and parameters: {'n_estimators': 490, 'learning_rate': 0.04089197311110088, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 438}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:07:35,085]\u001b[0m Trial 467 finished with value: 0.8233412547999889 and parameters: {'n_estimators': 453, 'learning_rate': 0.024684525218039544, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 498}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:07:41,990]\u001b[0m Trial 468 finished with value: 0.8200158389137423 and parameters: {'n_estimators': 431, 'learning_rate': 0.030899285451663892, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 530}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:07:48,387]\u001b[0m Trial 469 finished with value: 0.8235975431958267 and parameters: {'n_estimators': 503, 'learning_rate': 0.03540909002331455, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 509}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:07:53,633]\u001b[0m Trial 470 finished with value: 0.8261070600018583 and parameters: {'n_estimators': 565, 'learning_rate': 0.046248174308867046, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 455}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:08:01,899]\u001b[0m Trial 471 finished with value: 0.8232391563901051 and parameters: {'n_estimators': 545, 'learning_rate': 0.026603500190138966, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 489}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:08:10,862]\u001b[0m Trial 472 finished with value: 0.8221766309867802 and parameters: {'n_estimators': 467, 'learning_rate': 0.01766788027530641, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 472}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:08:16,608]\u001b[0m Trial 473 finished with value: 0.8241892582025286 and parameters: {'n_estimators': 445, 'learning_rate': 0.04208394828539423, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 540}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:08:23,168]\u001b[0m Trial 474 finished with value: 0.8261647400425183 and parameters: {'n_estimators': 480, 'learning_rate': 0.03712525066859216, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 516}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:08:25,374]\u001b[0m Trial 475 finished with value: 0.8055190269172066 and parameters: {'n_estimators': 71, 'learning_rate': 0.032917183537874824, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 493}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:08:31,399]\u001b[0m Trial 476 finished with value: 0.8219529628957629 and parameters: {'n_estimators': 421, 'learning_rate': 0.039030642292574934, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 468}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:08:35,302]\u001b[0m Trial 477 finished with value: 0.8095995768764979 and parameters: {'n_estimators': 519, 'learning_rate': 0.04478138582702286, 'max_depth': 5, 'max_bin': 229, 'num_leaves': 481}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:08:42,795]\u001b[0m Trial 478 finished with value: 0.8250551270945186 and parameters: {'n_estimators': 456, 'learning_rate': 0.028814964507797563, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 501}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:08:49,526]\u001b[0m Trial 479 finished with value: 0.8242198207281485 and parameters: {'n_estimators': 495, 'learning_rate': 0.034167699582790555, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 447}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:08:57,801]\u001b[0m Trial 480 finished with value: 0.8166130651402288 and parameters: {'n_estimators': 439, 'learning_rate': 0.02304921208138319, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 555}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:09:03,558]\u001b[0m Trial 481 finished with value: 0.8227566755738188 and parameters: {'n_estimators': 470, 'learning_rate': 0.03941058323181005, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 405}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:09:09,797]\u001b[0m Trial 482 finished with value: 0.8247519331872777 and parameters: {'n_estimators': 400, 'learning_rate': 0.04314498226593491, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 512}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:09:15,276]\u001b[0m Trial 483 finished with value: 0.8242068654519666 and parameters: {'n_estimators': 485, 'learning_rate': 0.04947983826960449, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 487}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:09:22,555]\u001b[0m Trial 484 finished with value: 0.8214677606328215 and parameters: {'n_estimators': 455, 'learning_rate': 0.030948118180082914, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 526}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:09:28,915]\u001b[0m Trial 485 finished with value: 0.825296842258345 and parameters: {'n_estimators': 509, 'learning_rate': 0.03605403643995453, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 463}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:09:32,212]\u001b[0m Trial 486 finished with value: 0.8216673486105759 and parameters: {'n_estimators': 536, 'learning_rate': 0.10208944387743064, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 427}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:09:40,472]\u001b[0m Trial 487 finished with value: 0.8232668562912574 and parameters: {'n_estimators': 435, 'learning_rate': 0.01966379605833331, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 500}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:09:48,023]\u001b[0m Trial 488 finished with value: 0.819221880403344 and parameters: {'n_estimators': 475, 'learning_rate': 0.028173356018016925, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 480}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:09:54,384]\u001b[0m Trial 489 finished with value: 0.8236053419620271 and parameters: {'n_estimators': 499, 'learning_rate': 0.03793365677189271, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 513}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:09:57,467]\u001b[0m Trial 490 finished with value: 0.8222355224020024 and parameters: {'n_estimators': 455, 'learning_rate': 0.11368821935694423, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 454}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:10:04,024]\u001b[0m Trial 491 finished with value: 0.8219569895334102 and parameters: {'n_estimators': 422, 'learning_rate': 0.033122126025303754, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 538}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:10:10,183]\u001b[0m Trial 492 finished with value: 0.8239140861935008 and parameters: {'n_estimators': 469, 'learning_rate': 0.042650705732702396, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 491}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:10:18,156]\u001b[0m Trial 493 finished with value: 0.8230940863599348 and parameters: {'n_estimators': 445, 'learning_rate': 0.02512750156382003, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 471}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:10:24,518]\u001b[0m Trial 494 finished with value: 0.8222461365130549 and parameters: {'n_estimators': 489, 'learning_rate': 0.03614570795980632, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 505}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:10:30,548]\u001b[0m Trial 495 finished with value: 0.8216627636500888 and parameters: {'n_estimators': 463, 'learning_rate': 0.04030540784660047, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 481}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:10:38,246]\u001b[0m Trial 496 finished with value: 0.8219827276717661 and parameters: {'n_estimators': 557, 'learning_rate': 0.03068533313250807, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 440}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:10:43,644]\u001b[0m Trial 497 finished with value: 0.8241601990904522 and parameters: {'n_estimators': 519, 'learning_rate': 0.04815044999732398, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 463}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:10:47,197]\u001b[0m Trial 498 finished with value: 0.818648264183804 and parameters: {'n_estimators': 412, 'learning_rate': 0.09204986249048425, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 521}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:10:53,119]\u001b[0m Trial 499 finished with value: 0.825305759125586 and parameters: {'n_estimators': 483, 'learning_rate': 0.04415166881395089, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 496}. Best is trial 280 with value: 0.8364344323727007.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.836434432\n",
      "\tBest params:\n",
      "\t\tn_estimators: 474\n",
      "\t\tlearning_rate: 0.03748850514767109\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 220\n",
      "\t\tnum_leaves: 494\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  395.000000  403.000000  397.000000   \n",
      "1                    TN  356.000000  364.000000  345.000000  351.000000   \n",
      "2                    FP   78.000000   79.000000   79.000000   78.000000   \n",
      "3                    FN   80.000000   61.000000   72.000000   73.000000   \n",
      "4              Accuracy    0.824249    0.844271    0.832036    0.832036   \n",
      "5             Precision    0.831533    0.833333    0.836100    0.835789   \n",
      "6           Sensitivity    0.827957    0.866228    0.848421    0.844681   \n",
      "7           Specificity    0.820300    0.821700    0.813700    0.818200   \n",
      "8              F1 score    0.829741    0.849462    0.842215    0.840212   \n",
      "9   F1 score (weighted)    0.824262    0.844164    0.831951    0.831988   \n",
      "10     F1 score (macro)    0.824066    0.844086    0.831334    0.831595   \n",
      "11    Balanced Accuracy    0.824117    0.843949    0.831050    0.831431   \n",
      "12                  MCC    0.648140    0.688851    0.662769    0.663241   \n",
      "13                  NPV    0.816500    0.856500    0.827300    0.827800   \n",
      "14              ROC_AUC    0.824117    0.843949    0.831050    0.831431   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   394.000000  384.000000  405.000000  388.000000  411.000000  417.000000  \n",
      "1   352.000000  350.000000  342.000000  353.000000  356.000000  334.000000  \n",
      "2    99.000000   83.000000   87.000000   85.000000   58.000000   71.000000  \n",
      "3    54.000000   82.000000   65.000000   73.000000   74.000000   77.000000  \n",
      "4     0.829811    0.816463    0.830923    0.824249    0.853170    0.835373  \n",
      "5     0.799189    0.822270    0.823171    0.820296    0.876333    0.854508  \n",
      "6     0.879464    0.824034    0.861702    0.841649    0.847423    0.844130  \n",
      "7     0.780500    0.808300    0.797200    0.805900    0.859900    0.824700  \n",
      "8     0.837407    0.823151    0.841996    0.830835    0.861635    0.849287  \n",
      "9     0.829412    0.816455    0.830632    0.824158    0.853331    0.835475  \n",
      "10    0.829439    0.816200    0.830089    0.823982    0.852619    0.833957  \n",
      "11    0.829976    0.816174    0.829452    0.823792    0.853663    0.834410  \n",
      "12    0.663061    0.632402    0.661181    0.648259    0.705781    0.667990  \n",
      "13    0.867000    0.810200    0.840300    0.828600    0.827900    0.812700  \n",
      "14    0.829976    0.816174    0.829452    0.823792    0.853663    0.834410  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_lgbm_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_lgbm_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_lgbm_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_lgbm_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_lgbm_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_lgbm_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_lgbm_9)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       200.500000     7.863417\n",
      "1                    TN       173.700000     7.746684\n",
      "2                    FP        39.300000     7.498889\n",
      "3                    FN        35.700000     6.412661\n",
      "4              Accuracy         0.833029     0.013778\n",
      "5             Precision         0.836643     0.026819\n",
      "6           Sensitivity         0.849101     0.024874\n",
      "7           Specificity         0.815870     0.031662\n",
      "8              F1 score         0.842295     0.013846\n",
      "9   F1 score (weighted)         0.832914     0.013876\n",
      "10     F1 score (macro)         0.832276     0.013886\n",
      "11    Balanced Accuracy         0.832491     0.013743\n",
      "12                  MCC         0.665820     0.026769\n",
      "13                  NPV         0.830020     0.024923\n",
      "14              ROC_AUC         0.832491     0.013743\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>397.900000</td>\n",
       "      <td>11.029758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>350.300000</td>\n",
       "      <td>8.340663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>79.700000</td>\n",
       "      <td>10.635893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>71.100000</td>\n",
       "      <td>8.698020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.824249</td>\n",
       "      <td>0.844271</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.829811</td>\n",
       "      <td>0.816463</td>\n",
       "      <td>0.830923</td>\n",
       "      <td>0.824249</td>\n",
       "      <td>0.853170</td>\n",
       "      <td>0.835373</td>\n",
       "      <td>0.832258</td>\n",
       "      <td>0.010419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.831533</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.836100</td>\n",
       "      <td>0.835789</td>\n",
       "      <td>0.799189</td>\n",
       "      <td>0.822270</td>\n",
       "      <td>0.823171</td>\n",
       "      <td>0.820296</td>\n",
       "      <td>0.876333</td>\n",
       "      <td>0.854508</td>\n",
       "      <td>0.833252</td>\n",
       "      <td>0.020752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.827957</td>\n",
       "      <td>0.866228</td>\n",
       "      <td>0.848421</td>\n",
       "      <td>0.844681</td>\n",
       "      <td>0.879464</td>\n",
       "      <td>0.824034</td>\n",
       "      <td>0.861702</td>\n",
       "      <td>0.841649</td>\n",
       "      <td>0.847423</td>\n",
       "      <td>0.844130</td>\n",
       "      <td>0.848569</td>\n",
       "      <td>0.016832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.820300</td>\n",
       "      <td>0.821700</td>\n",
       "      <td>0.813700</td>\n",
       "      <td>0.818200</td>\n",
       "      <td>0.780500</td>\n",
       "      <td>0.808300</td>\n",
       "      <td>0.797200</td>\n",
       "      <td>0.805900</td>\n",
       "      <td>0.859900</td>\n",
       "      <td>0.824700</td>\n",
       "      <td>0.815040</td>\n",
       "      <td>0.020628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.829741</td>\n",
       "      <td>0.849462</td>\n",
       "      <td>0.842215</td>\n",
       "      <td>0.840212</td>\n",
       "      <td>0.837407</td>\n",
       "      <td>0.823151</td>\n",
       "      <td>0.841996</td>\n",
       "      <td>0.830835</td>\n",
       "      <td>0.861635</td>\n",
       "      <td>0.849287</td>\n",
       "      <td>0.840594</td>\n",
       "      <td>0.011201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.824262</td>\n",
       "      <td>0.844164</td>\n",
       "      <td>0.831951</td>\n",
       "      <td>0.831988</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.816455</td>\n",
       "      <td>0.830632</td>\n",
       "      <td>0.824158</td>\n",
       "      <td>0.853331</td>\n",
       "      <td>0.835475</td>\n",
       "      <td>0.832183</td>\n",
       "      <td>0.010468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.824066</td>\n",
       "      <td>0.844086</td>\n",
       "      <td>0.831334</td>\n",
       "      <td>0.831595</td>\n",
       "      <td>0.829439</td>\n",
       "      <td>0.816200</td>\n",
       "      <td>0.830089</td>\n",
       "      <td>0.823982</td>\n",
       "      <td>0.852619</td>\n",
       "      <td>0.833957</td>\n",
       "      <td>0.831737</td>\n",
       "      <td>0.010339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.824117</td>\n",
       "      <td>0.843949</td>\n",
       "      <td>0.831050</td>\n",
       "      <td>0.831431</td>\n",
       "      <td>0.829976</td>\n",
       "      <td>0.816174</td>\n",
       "      <td>0.829452</td>\n",
       "      <td>0.823792</td>\n",
       "      <td>0.853663</td>\n",
       "      <td>0.834410</td>\n",
       "      <td>0.831802</td>\n",
       "      <td>0.010590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.648140</td>\n",
       "      <td>0.688851</td>\n",
       "      <td>0.662769</td>\n",
       "      <td>0.663241</td>\n",
       "      <td>0.663061</td>\n",
       "      <td>0.632402</td>\n",
       "      <td>0.661181</td>\n",
       "      <td>0.648259</td>\n",
       "      <td>0.705781</td>\n",
       "      <td>0.667990</td>\n",
       "      <td>0.664167</td>\n",
       "      <td>0.020783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>0.827300</td>\n",
       "      <td>0.827800</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>0.810200</td>\n",
       "      <td>0.840300</td>\n",
       "      <td>0.828600</td>\n",
       "      <td>0.827900</td>\n",
       "      <td>0.812700</td>\n",
       "      <td>0.831480</td>\n",
       "      <td>0.018401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.824117</td>\n",
       "      <td>0.843949</td>\n",
       "      <td>0.831050</td>\n",
       "      <td>0.831431</td>\n",
       "      <td>0.829976</td>\n",
       "      <td>0.816174</td>\n",
       "      <td>0.829452</td>\n",
       "      <td>0.823792</td>\n",
       "      <td>0.853663</td>\n",
       "      <td>0.834410</td>\n",
       "      <td>0.831802</td>\n",
       "      <td>0.010590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  385.000000  395.000000  403.000000  397.000000   \n",
       "1                    TN  356.000000  364.000000  345.000000  351.000000   \n",
       "2                    FP   78.000000   79.000000   79.000000   78.000000   \n",
       "3                    FN   80.000000   61.000000   72.000000   73.000000   \n",
       "4              Accuracy    0.824249    0.844271    0.832036    0.832036   \n",
       "5             Precision    0.831533    0.833333    0.836100    0.835789   \n",
       "6           Sensitivity    0.827957    0.866228    0.848421    0.844681   \n",
       "7           Specificity    0.820300    0.821700    0.813700    0.818200   \n",
       "8              F1 score    0.829741    0.849462    0.842215    0.840212   \n",
       "9   F1 score (weighted)    0.824262    0.844164    0.831951    0.831988   \n",
       "10     F1 score (macro)    0.824066    0.844086    0.831334    0.831595   \n",
       "11    Balanced Accuracy    0.824117    0.843949    0.831050    0.831431   \n",
       "12                  MCC    0.648140    0.688851    0.662769    0.663241   \n",
       "13                  NPV    0.816500    0.856500    0.827300    0.827800   \n",
       "14              ROC_AUC    0.824117    0.843949    0.831050    0.831431   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   394.000000  384.000000  405.000000  388.000000  411.000000  417.000000   \n",
       "1   352.000000  350.000000  342.000000  353.000000  356.000000  334.000000   \n",
       "2    99.000000   83.000000   87.000000   85.000000   58.000000   71.000000   \n",
       "3    54.000000   82.000000   65.000000   73.000000   74.000000   77.000000   \n",
       "4     0.829811    0.816463    0.830923    0.824249    0.853170    0.835373   \n",
       "5     0.799189    0.822270    0.823171    0.820296    0.876333    0.854508   \n",
       "6     0.879464    0.824034    0.861702    0.841649    0.847423    0.844130   \n",
       "7     0.780500    0.808300    0.797200    0.805900    0.859900    0.824700   \n",
       "8     0.837407    0.823151    0.841996    0.830835    0.861635    0.849287   \n",
       "9     0.829412    0.816455    0.830632    0.824158    0.853331    0.835475   \n",
       "10    0.829439    0.816200    0.830089    0.823982    0.852619    0.833957   \n",
       "11    0.829976    0.816174    0.829452    0.823792    0.853663    0.834410   \n",
       "12    0.663061    0.632402    0.661181    0.648259    0.705781    0.667990   \n",
       "13    0.867000    0.810200    0.840300    0.828600    0.827900    0.812700   \n",
       "14    0.829976    0.816174    0.829452    0.823792    0.853663    0.834410   \n",
       "\n",
       "           ave        std  \n",
       "0   397.900000  11.029758  \n",
       "1   350.300000   8.340663  \n",
       "2    79.700000  10.635893  \n",
       "3    71.100000   8.698020  \n",
       "4     0.832258   0.010419  \n",
       "5     0.833252   0.020752  \n",
       "6     0.848569   0.016832  \n",
       "7     0.815040   0.020628  \n",
       "8     0.840594   0.011201  \n",
       "9     0.832183   0.010468  \n",
       "10    0.831737   0.010339  \n",
       "11    0.831802   0.010590  \n",
       "12    0.664167   0.020783  \n",
       "13    0.831480   0.018401  \n",
       "14    0.831802   0.010590  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_lgbm0</th>\n",
       "      <th>y_pred_lgbm1</th>\n",
       "      <th>y_pred_lgbm2</th>\n",
       "      <th>y_pred_lgbm3</th>\n",
       "      <th>y_pred_lgbm4</th>\n",
       "      <th>y_pred_lgbm_ave</th>\n",
       "      <th>y_pred_lgbm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.489898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>4487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>4488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>4489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.489898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>4490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>4491</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_test_idx0  y_test0  y_pred_lgbm0  y_pred_lgbm1  y_pred_lgbm2  \\\n",
       "0               0      0.0           1.0           0.0           1.0   \n",
       "1               1      1.0           1.0           1.0           1.0   \n",
       "2               2      1.0           1.0           1.0           1.0   \n",
       "3               3      1.0           1.0           1.0           1.0   \n",
       "4               4      1.0           0.0           0.0           0.0   \n",
       "...           ...      ...           ...           ...           ...   \n",
       "4487         4487      1.0           1.0           1.0           1.0   \n",
       "4488         4488      1.0           1.0           1.0           1.0   \n",
       "4489         4489      0.0           1.0           1.0           1.0   \n",
       "4490         4490      0.0           0.0           0.0           0.0   \n",
       "4491         4491      1.0           0.0           0.0           0.0   \n",
       "\n",
       "      y_pred_lgbm3  y_pred_lgbm4  y_pred_lgbm_ave  y_pred_lgbm_std  \n",
       "0              0.0           1.0              0.6         0.489898  \n",
       "1              1.0           1.0              1.0         0.000000  \n",
       "2              1.0           1.0              1.0         0.000000  \n",
       "3              1.0           1.0              1.0         0.000000  \n",
       "4              0.0           1.0              0.2         0.400000  \n",
       "...            ...           ...              ...              ...  \n",
       "4487           1.0           1.0              1.0         0.000000  \n",
       "4488           1.0           1.0              1.0         0.000000  \n",
       "4489           0.0           0.0              0.6         0.489898  \n",
       "4490           0.0           0.0              0.0         0.000000  \n",
       "4491           1.0           0.0              0.2         0.400000  \n",
       "\n",
       "[4492 rows x 9 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_lgbm)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "       \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_lgbm))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_lgbm))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_lgbm))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_lgbm))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_lgbm, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_lgbm, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_lgbm))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_lgbm))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_lgbm))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "lgbm_5preds = pd.concat([lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "lgbm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c16510fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_lgbm.to_csv('mat_met_lgbm_opt.csv')\n",
    "lgbm_5preds.to_csv('lgbm_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model f1_score 0.8279 with a standard deviation of 0.0209\n",
      "LightGBM optimized model f1_score 0.8382 with a standard deviation of 0.0178\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#f1_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "f1_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"f1_macro\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)f1_score %0.4f with a standard deviation of %0.4f\" % (f1_cv_lgbm_opt_testSet.mean(), f1_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(f1_cv_lgbm_opt), np.std(f1_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_lgbm_clf.joblib']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_clf, \"./lgbm_clf.joblib\")\n",
    "#joblib.dump(optimized_lgbm, \"./optimized_lgbm.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"./optimizedCV_lgbm_clf.joblib\") \n",
    "#loaded_rf = joblib.load(\"./optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       198.600000     7.676805\n",
      "1                    TN       172.700000     8.667308\n",
      "2                    FP        40.300000     7.513691\n",
      "3                    FN        37.600000     6.850791\n",
      "4              Accuracy         0.826580     0.017077\n",
      "5             Precision         0.831862     0.026795\n",
      "6           Sensitivity         0.841121     0.026473\n",
      "7           Specificity         0.811020     0.032874\n",
      "8              F1 score         0.835962     0.015795\n",
      "9   F1 score (weighted)         0.826489     0.017125\n",
      "10     F1 score (macro)         0.825826     0.017339\n",
      "11    Balanced Accuracy         0.826075     0.017323\n",
      "12                  MCC         0.652805     0.034451\n",
      "13                  NPV         0.821600     0.028929\n",
      "14              ROC_AUC         0.826075     0.017323\n",
      "CPU times: user 16.4 s, sys: 28 ms, total: 16.4 s\n",
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    n_jobs=8,\n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_clf.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"logloss\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_clf.predict(X_test) \n",
    "    \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"logloss\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"logloss\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        \n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:12:16,536]\u001b[0m A new study created in memory with name: XGBClassifier\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:12:25,347]\u001b[0m Trial 0 finished with value: 0.8260633344675565 and parameters: {'n_estimators': 592, 'eta': 0.0951435752711342, 'max_depth': 9, 'alpha': 0.6576000000000001, 'lambda': 8.55936011157803, 'max_bin': 326}. Best is trial 0 with value: 0.8260633344675565.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:12:33,162]\u001b[0m Trial 1 finished with value: 0.8282405670010459 and parameters: {'n_estimators': 583, 'eta': 0.09222300921771683, 'max_depth': 12, 'alpha': 0.08080000000000001, 'lambda': 6.575694449891304, 'max_bin': 286}. Best is trial 1 with value: 0.8282405670010459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:12:48,074]\u001b[0m Trial 2 finished with value: 0.8308313028219778 and parameters: {'n_estimators': 819, 'eta': 0.02929463830239589, 'max_depth': 11, 'alpha': 0.3521, 'lambda': 2.0667544170372816, 'max_bin': 357}. Best is trial 2 with value: 0.8308313028219778.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:12:55,217]\u001b[0m Trial 3 finished with value: 0.8125605825437361 and parameters: {'n_estimators': 257, 'eta': 0.03399604188000641, 'max_depth': 9, 'alpha': 0.5750000000000001, 'lambda': 25.9587510589864, 'max_bin': 289}. Best is trial 2 with value: 0.8308313028219778.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:13:09,801]\u001b[0m Trial 4 finished with value: 0.8244540679476628 and parameters: {'n_estimators': 513, 'eta': 0.02789374623335509, 'max_depth': 10, 'alpha': 0.2763, 'lambda': 27.69319030523658, 'max_bin': 398}. Best is trial 2 with value: 0.8308313028219778.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:13:12,935]\u001b[0m Trial 5 finished with value: 0.7911285837087713 and parameters: {'n_estimators': 149, 'eta': 0.06455413089582616, 'max_depth': 6, 'alpha': 0.4037, 'lambda': 26.26208075234641, 'max_bin': 252}. Best is trial 2 with value: 0.8308313028219778.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:13:22,025]\u001b[0m Trial 6 finished with value: 0.8216223940080527 and parameters: {'n_estimators': 283, 'eta': 0.043172464149044285, 'max_depth': 12, 'alpha': 0.302, 'lambda': 24.296813467862727, 'max_bin': 467}. Best is trial 2 with value: 0.8308313028219778.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:13:28,123]\u001b[0m Trial 7 finished with value: 0.8229338192563846 and parameters: {'n_estimators': 266, 'eta': 0.07371170546781898, 'max_depth': 8, 'alpha': 0.1451, 'lambda': 8.548588369321154, 'max_bin': 390}. Best is trial 2 with value: 0.8308313028219778.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:13:38,806]\u001b[0m Trial 8 finished with value: 0.825831148628232 and parameters: {'n_estimators': 386, 'eta': 0.06145070582705049, 'max_depth': 11, 'alpha': 0.0357, 'lambda': 23.80890857147672, 'max_bin': 321}. Best is trial 2 with value: 0.8308313028219778.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:13:48,687]\u001b[0m Trial 9 finished with value: 0.8330909770787589 and parameters: {'n_estimators': 741, 'eta': 0.08307189807265311, 'max_depth': 10, 'alpha': 0.9568000000000001, 'lambda': 8.184756243986026, 'max_bin': 384}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:14:02,511]\u001b[0m Trial 10 finished with value: 0.7709644532551092 and parameters: {'n_estimators': 810, 'eta': 0.008032526789635876, 'max_depth': 5, 'alpha': 0.9994000000000001, 'lambda': 37.18410308660948, 'max_bin': 456}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:14:28,228]\u001b[0m Trial 11 finished with value: 0.8245288789285992 and parameters: {'n_estimators': 892, 'eta': 0.013963250652712733, 'max_depth': 10, 'alpha': 0.8055, 'lambda': 15.117541753107783, 'max_bin': 365}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:14:39,081]\u001b[0m Trial 12 finished with value: 0.8259354796329642 and parameters: {'n_estimators': 705, 'eta': 0.050131817082133996, 'max_depth': 7, 'alpha': 0.7755000000000001, 'lambda': 3.082338769517562, 'max_bin': 428}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:14:50,053]\u001b[0m Trial 13 finished with value: 0.8288269116025562 and parameters: {'n_estimators': 735, 'eta': 0.08020151453094278, 'max_depth': 11, 'alpha': 0.4655, 'lambda': 14.785731312002891, 'max_bin': 349}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:15:07,753]\u001b[0m Trial 14 finished with value: 0.8296994204161274 and parameters: {'n_estimators': 895, 'eta': 0.0234977541527678, 'max_depth': 10, 'alpha': 0.9397000000000001, 'lambda': 1.3982479310308964, 'max_bin': 410}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:15:24,933]\u001b[0m Trial 15 finished with value: 0.829664810787525 and parameters: {'n_estimators': 691, 'eta': 0.04103393486895659, 'max_depth': 11, 'alpha': 0.6515000000000001, 'lambda': 14.203366412965789, 'max_bin': 346}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:15:36,587]\u001b[0m Trial 16 finished with value: 0.8310945766799721 and parameters: {'n_estimators': 804, 'eta': 0.05847727679442499, 'max_depth': 8, 'alpha': 0.2791, 'lambda': 5.910632192896359, 'max_bin': 490}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:15:47,328]\u001b[0m Trial 17 finished with value: 0.8300739635380957 and parameters: {'n_estimators': 619, 'eta': 0.08006335285817481, 'max_depth': 8, 'alpha': 0.19790000000000002, 'lambda': 18.94478525064069, 'max_bin': 485}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:15:57,137]\u001b[0m Trial 18 finished with value: 0.8266684080371194 and parameters: {'n_estimators': 494, 'eta': 0.07060371677320458, 'max_depth': 7, 'alpha': 0.8068000000000001, 'lambda': 12.373631117988458, 'max_bin': 497}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:16:09,618]\u001b[0m Trial 19 finished with value: 0.8275444081987177 and parameters: {'n_estimators': 787, 'eta': 0.05587916594010283, 'max_depth': 7, 'alpha': 0.5112, 'lambda': 5.791886098723944, 'max_bin': 438}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:16:11,571]\u001b[0m Trial 20 finished with value: 0.8002653001322949 and parameters: {'n_estimators': 61, 'eta': 0.08711752298803581, 'max_depth': 9, 'alpha': 0.2393, 'lambda': 31.497552749969977, 'max_bin': 446}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:16:30,115]\u001b[0m Trial 21 finished with value: 0.8304500796484223 and parameters: {'n_estimators': 819, 'eta': 0.01910668062511585, 'max_depth': 11, 'alpha': 0.35500000000000004, 'lambda': 1.2646970198526795, 'max_bin': 378}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:16:46,857]\u001b[0m Trial 22 finished with value: 0.8285195222600057 and parameters: {'n_estimators': 678, 'eta': 0.03613683092024102, 'max_depth': 10, 'alpha': 0.42900000000000005, 'lambda': 10.265783799765853, 'max_bin': 311}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:17:09,117]\u001b[0m Trial 23 finished with value: 0.7742763668441456 and parameters: {'n_estimators': 779, 'eta': 0.0011888026821927286, 'max_depth': 8, 'alpha': 0.3442, 'lambda': 5.462163317408313, 'max_bin': 418}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:17:25,599]\u001b[0m Trial 24 finished with value: 0.8321522876129057 and parameters: {'n_estimators': 880, 'eta': 0.04934226173665146, 'max_depth': 12, 'alpha': 0.543, 'lambda': 18.29446018435426, 'max_bin': 361}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:17:41,627]\u001b[0m Trial 25 finished with value: 0.828753415467874 and parameters: {'n_estimators': 846, 'eta': 0.05100960416200225, 'max_depth': 12, 'alpha': 0.8715, 'lambda': 18.49059920718551, 'max_bin': 475}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:17:55,713]\u001b[0m Trial 26 finished with value: 0.8261988506015017 and parameters: {'n_estimators': 739, 'eta': 0.05979630361466072, 'max_depth': 9, 'alpha': 0.6899000000000001, 'lambda': 21.10448439072376, 'max_bin': 380}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:18:05,831]\u001b[0m Trial 27 finished with value: 0.831885656890513 and parameters: {'n_estimators': 649, 'eta': 0.09965055806331277, 'max_depth': 6, 'alpha': 0.5233, 'lambda': 16.589908093000513, 'max_bin': 338}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:18:15,270]\u001b[0m Trial 28 finished with value: 0.828565969498141 and parameters: {'n_estimators': 651, 'eta': 0.0985798229108838, 'max_depth': 5, 'alpha': 0.5525, 'lambda': 17.353980952641407, 'max_bin': 336}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:18:22,526]\u001b[0m Trial 29 finished with value: 0.8226155137006874 and parameters: {'n_estimators': 426, 'eta': 0.09147629082843353, 'max_depth': 6, 'alpha': 0.6441, 'lambda': 11.386952324621648, 'max_bin': 301}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:18:32,981]\u001b[0m Trial 30 finished with value: 0.823960914843188 and parameters: {'n_estimators': 568, 'eta': 0.08458391530526192, 'max_depth': 6, 'alpha': 0.7142000000000001, 'lambda': 30.85875759064254, 'max_bin': 327}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:18:41,440]\u001b[0m Trial 31 finished with value: 0.8280218915484021 and parameters: {'n_estimators': 749, 'eta': 0.09990915943149106, 'max_depth': 7, 'alpha': 0.5457000000000001, 'lambda': 8.45190388424962, 'max_bin': 368}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:18:53,804]\u001b[0m Trial 32 finished with value: 0.8277612873039513 and parameters: {'n_estimators': 863, 'eta': 0.07033665251069161, 'max_depth': 12, 'alpha': 0.5997, 'lambda': 16.433546143467552, 'max_bin': 404}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:19:04,058]\u001b[0m Trial 33 finished with value: 0.8292855523876954 and parameters: {'n_estimators': 549, 'eta': 0.09236652799006474, 'max_depth': 9, 'alpha': 0.1173, 'lambda': 21.117276349733856, 'max_bin': 254}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:19:13,448]\u001b[0m Trial 34 finished with value: 0.8272463748818011 and parameters: {'n_estimators': 650, 'eta': 0.07655117955644473, 'max_depth': 8, 'alpha': 0.47690000000000005, 'lambda': 4.524943629875132, 'max_bin': 279}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:19:25,959]\u001b[0m Trial 35 finished with value: 0.8283869648755126 and parameters: {'n_estimators': 767, 'eta': 0.06703333240042768, 'max_depth': 10, 'alpha': 0.8819, 'lambda': 12.85016897230891, 'max_bin': 354}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:19:34,560]\u001b[0m Trial 36 finished with value: 0.8308019880657493 and parameters: {'n_estimators': 606, 'eta': 0.08701823568599873, 'max_depth': 6, 'alpha': 0.41090000000000004, 'lambda': 7.336076836148596, 'max_bin': 336}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:19:49,789]\u001b[0m Trial 37 finished with value: 0.8284513976853847 and parameters: {'n_estimators': 842, 'eta': 0.04126318826643158, 'max_depth': 12, 'alpha': 0.1882, 'lambda': 10.175247542251125, 'max_bin': 271}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:20:00,217]\u001b[0m Trial 38 finished with value: 0.8222579235281936 and parameters: {'n_estimators': 715, 'eta': 0.04694783043565607, 'max_depth': 5, 'alpha': 0.6201, 'lambda': 3.952429763050519, 'max_bin': 393}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:20:15,447]\u001b[0m Trial 39 finished with value: 0.8262983935234504 and parameters: {'n_estimators': 810, 'eta': 0.05612536331977211, 'max_depth': 9, 'alpha': 0.7292000000000001, 'lambda': 22.98779299655321, 'max_bin': 383}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:20:33,565]\u001b[0m Trial 40 finished with value: 0.8316082498441972 and parameters: {'n_estimators': 895, 'eta': 0.033509780927267835, 'max_depth': 11, 'alpha': 0.0031000000000000003, 'lambda': 10.01363683776907, 'max_bin': 295}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:20:50,254]\u001b[0m Trial 41 finished with value: 0.8295932148000462 and parameters: {'n_estimators': 868, 'eta': 0.03227699655666766, 'max_depth': 11, 'alpha': 0.026500000000000003, 'lambda': 7.297608136513599, 'max_bin': 296}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:21:07,291]\u001b[0m Trial 42 finished with value: 0.8311224404021317 and parameters: {'n_estimators': 900, 'eta': 0.036574826759584206, 'max_depth': 12, 'alpha': 0.08020000000000001, 'lambda': 8.893422996659835, 'max_bin': 326}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:21:23,548]\u001b[0m Trial 43 finished with value: 0.8318766632215974 and parameters: {'n_estimators': 900, 'eta': 0.03671102181634524, 'max_depth': 12, 'alpha': 0.0748, 'lambda': 9.97662711779186, 'max_bin': 311}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:21:44,206]\u001b[0m Trial 44 finished with value: 0.827615410680132 and parameters: {'n_estimators': 835, 'eta': 0.02854386869835348, 'max_depth': 12, 'alpha': 0.07400000000000001, 'lambda': 13.671160371252999, 'max_bin': 305}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:21:59,913]\u001b[0m Trial 45 finished with value: 0.8305560600059219 and parameters: {'n_estimators': 870, 'eta': 0.045002958400320564, 'max_depth': 11, 'alpha': 0.015300000000000001, 'lambda': 15.783266308694769, 'max_bin': 313}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:22:20,554]\u001b[0m Trial 46 finished with value: 0.8239350514649375 and parameters: {'n_estimators': 786, 'eta': 0.021286082205412372, 'max_depth': 10, 'alpha': 0.13720000000000002, 'lambda': 10.955440209536633, 'max_bin': 289}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:22:32,121]\u001b[0m Trial 47 finished with value: 0.8196385918672379 and parameters: {'n_estimators': 352, 'eta': 0.02497829634014332, 'max_depth': 12, 'alpha': 0.9888, 'lambda': 18.772415679519995, 'max_bin': 362}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:22:49,532]\u001b[0m Trial 48 finished with value: 0.8321517472942682 and parameters: {'n_estimators': 752, 'eta': 0.03879668681634162, 'max_depth': 11, 'alpha': 0.3452, 'lambda': 12.566162612500895, 'max_bin': 338}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:23:04,948]\u001b[0m Trial 49 finished with value: 0.8298959733794407 and parameters: {'n_estimators': 670, 'eta': 0.048436609054300035, 'max_depth': 11, 'alpha': 0.3451, 'lambda': 16.984249512463897, 'max_bin': 339}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8331\n",
      "\tBest params:\n",
      "\t\tn_estimators: 741\n",
      "\t\teta: 0.08307189807265311\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.9568000000000001\n",
      "\t\tlambda: 8.184756243986026\n",
      "\t\tmax_bin: 384\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBClassifier\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  393.000000\n",
      "1                    TN  358.000000\n",
      "2                    FP   76.000000\n",
      "3                    FN   72.000000\n",
      "4              Accuracy    0.835373\n",
      "5             Precision    0.837953\n",
      "6           Sensitivity    0.845161\n",
      "7           Specificity    0.824900\n",
      "8              F1 score    0.841542\n",
      "9   F1 score (weighted)    0.835344\n",
      "10     F1 score (macro)    0.835123\n",
      "11    Balanced Accuracy    0.835023\n",
      "12                  MCC    0.670279\n",
      "13                  NPV    0.832600\n",
      "14              ROC_AUC    0.835023\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "   \n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_xgb_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_xgb_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_xgb_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_xgb_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_xgb_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_xgb_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_xgb_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_xgb_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_xgb_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_xgb_0)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:23:20,792]\u001b[0m Trial 50 finished with value: 0.8239802386707419 and parameters: {'n_estimators': 527, 'eta': 0.03897454138682993, 'max_depth': 12, 'alpha': 0.5109, 'lambda': 13.131752137830855, 'max_bin': 369}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:23:38,156]\u001b[0m Trial 51 finished with value: 0.8248239452956337 and parameters: {'n_estimators': 719, 'eta': 0.03160399522222444, 'max_depth': 11, 'alpha': 0.22390000000000002, 'lambda': 8.8423322214998, 'max_bin': 318}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:23:51,468]\u001b[0m Trial 52 finished with value: 0.8260593698598008 and parameters: {'n_estimators': 891, 'eta': 0.05232706698963962, 'max_depth': 11, 'alpha': 0.1062, 'lambda': 14.975786722230449, 'max_bin': 346}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:24:05,906]\u001b[0m Trial 53 finished with value: 0.8251366632491983 and parameters: {'n_estimators': 824, 'eta': 0.044406390952206, 'max_depth': 10, 'alpha': 0.0591, 'lambda': 11.99083216664527, 'max_bin': 329}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:24:28,420]\u001b[0m Trial 54 finished with value: 0.8203274125306189 and parameters: {'n_estimators': 762, 'eta': 0.015212157891288491, 'max_depth': 11, 'alpha': 0.17700000000000002, 'lambda': 9.860193544477928, 'max_bin': 356}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:24:43,697]\u001b[0m Trial 55 finished with value: 0.822290693132034 and parameters: {'n_estimators': 619, 'eta': 0.035998464227928205, 'max_depth': 10, 'alpha': 0.2852, 'lambda': 13.912587609918395, 'max_bin': 344}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:24:55,211]\u001b[0m Trial 56 finished with value: 0.8271600226250115 and parameters: {'n_estimators': 451, 'eta': 0.06364862361915896, 'max_depth': 12, 'alpha': 0.0016, 'lambda': 20.253208186832563, 'max_bin': 267}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:25:11,415]\u001b[0m Trial 57 finished with value: 0.8237563606194518 and parameters: {'n_estimators': 855, 'eta': 0.025919722636039033, 'max_depth': 11, 'alpha': 0.3801, 'lambda': 2.7815284678479753, 'max_bin': 291}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:25:24,317]\u001b[0m Trial 58 finished with value: 0.823384818712489 and parameters: {'n_estimators': 795, 'eta': 0.03957794059924867, 'max_depth': 12, 'alpha': 0.46290000000000003, 'lambda': 6.831291711549346, 'max_bin': 307}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:25:34,182]\u001b[0m Trial 59 finished with value: 0.8224040168953668 and parameters: {'n_estimators': 742, 'eta': 0.09572996554257253, 'max_depth': 11, 'alpha': 0.321, 'lambda': 25.550561025363347, 'max_bin': 391}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:25:53,408]\u001b[0m Trial 60 finished with value: 0.8240130124456032 and parameters: {'n_estimators': 689, 'eta': 0.031036046220801073, 'max_depth': 12, 'alpha': 0.5630000000000001, 'lambda': 17.458693484710103, 'max_bin': 315}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:26:08,395]\u001b[0m Trial 61 finished with value: 0.8262436413902249 and parameters: {'n_estimators': 900, 'eta': 0.03562150752028552, 'max_depth': 12, 'alpha': 0.0887, 'lambda': 8.549905704538467, 'max_bin': 332}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:26:22,583]\u001b[0m Trial 62 finished with value: 0.8258672836800418 and parameters: {'n_estimators': 897, 'eta': 0.04302889726721561, 'max_depth': 12, 'alpha': 0.0531, 'lambda': 12.304279482440487, 'max_bin': 319}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:26:34,002]\u001b[0m Trial 63 finished with value: 0.8266193077986157 and parameters: {'n_estimators': 824, 'eta': 0.05333681485062082, 'max_depth': 11, 'alpha': 0.2446, 'lambda': 9.57695612314409, 'max_bin': 326}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:26:52,048]\u001b[0m Trial 64 finished with value: 0.8236794797919955 and parameters: {'n_estimators': 873, 'eta': 0.037934577608521515, 'max_depth': 10, 'alpha': 0.1656, 'lambda': 15.013828588206033, 'max_bin': 371}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:27:11,493]\u001b[0m Trial 65 finished with value: 0.8248670804387725 and parameters: {'n_estimators': 799, 'eta': 0.028082416860003616, 'max_depth': 12, 'alpha': 0.1355, 'lambda': 11.396299290136644, 'max_bin': 351}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:27:18,053]\u001b[0m Trial 66 finished with value: 0.817401257480293 and parameters: {'n_estimators': 204, 'eta': 0.03315987585434555, 'max_depth': 11, 'alpha': 0.7558, 'lambda': 7.412369375384186, 'max_bin': 300}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:27:25,828]\u001b[0m Trial 67 finished with value: 0.820095811081492 and parameters: {'n_estimators': 846, 'eta': 0.08216389221802585, 'max_depth': 12, 'alpha': 0.4479, 'lambda': 6.131592891168463, 'max_bin': 280}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:27:43,130]\u001b[0m Trial 68 finished with value: 0.8251526446324513 and parameters: {'n_estimators': 647, 'eta': 0.047999093420045866, 'max_depth': 12, 'alpha': 0.8190000000000001, 'lambda': 38.296943958422844, 'max_bin': 360}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:28:07,102]\u001b[0m Trial 69 finished with value: 0.817707668668332 and parameters: {'n_estimators': 880, 'eta': 0.01747112511467514, 'max_depth': 10, 'alpha': 0.0419, 'lambda': 20.31822214455029, 'max_bin': 340}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:28:19,410]\u001b[0m Trial 70 finished with value: 0.8248959105553311 and parameters: {'n_estimators': 768, 'eta': 0.04211525743043503, 'max_depth': 11, 'alpha': 0.6703, 'lambda': 4.196259310802983, 'max_bin': 323}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:28:31,018]\u001b[0m Trial 71 finished with value: 0.8212486699932249 and parameters: {'n_estimators': 815, 'eta': 0.05970686788179434, 'max_depth': 8, 'alpha': 0.27040000000000003, 'lambda': 9.120068233265012, 'max_bin': 462}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:28:40,446]\u001b[0m Trial 72 finished with value: 0.819805674709458 and parameters: {'n_estimators': 850, 'eta': 0.07608525414183094, 'max_depth': 9, 'alpha': 0.3919, 'lambda': 10.70577444731515, 'max_bin': 432}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:28:50,619]\u001b[0m Trial 73 finished with value: 0.8207920785226512 and parameters: {'n_estimators': 728, 'eta': 0.05667763000168955, 'max_depth': 7, 'alpha': 0.1075, 'lambda': 4.885575115013911, 'max_bin': 483}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:28:58,756]\u001b[0m Trial 74 finished with value: 0.8240565828528265 and parameters: {'n_estimators': 797, 'eta': 0.08833087101368048, 'max_depth': 6, 'alpha': 0.21860000000000002, 'lambda': 5.62697520187597, 'max_bin': 409}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:29:08,968]\u001b[0m Trial 75 finished with value: 0.8236354031868995 and parameters: {'n_estimators': 836, 'eta': 0.06802076759205214, 'max_depth': 9, 'alpha': 0.6064, 'lambda': 8.12780157975104, 'max_bin': 423}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:29:18,129]\u001b[0m Trial 76 finished with value: 0.8202198341813105 and parameters: {'n_estimators': 869, 'eta': 0.09704872887603694, 'max_depth': 6, 'alpha': 0.0765, 'lambda': 16.0664620188427, 'max_bin': 376}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:29:26,845]\u001b[0m Trial 77 finished with value: 0.821836375735819 and parameters: {'n_estimators': 755, 'eta': 0.09219458801033968, 'max_depth': 8, 'alpha': 0.48950000000000005, 'lambda': 12.15103154456017, 'max_bin': 447}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:29:39,357]\u001b[0m Trial 78 finished with value: 0.8041636920099782 and parameters: {'n_estimators': 776, 'eta': 0.0345195707893016, 'max_depth': 5, 'alpha': 0.5336000000000001, 'lambda': 22.239603247088255, 'max_bin': 500}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:29:52,470]\u001b[0m Trial 79 finished with value: 0.8232056703076015 and parameters: {'n_estimators': 900, 'eta': 0.037273450444507104, 'max_depth': 7, 'alpha': 0.42760000000000004, 'lambda': 2.640314799903607, 'max_bin': 308}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:30:01,054]\u001b[0m Trial 80 finished with value: 0.8242133601295709 and parameters: {'n_estimators': 816, 'eta': 0.07396876793675279, 'max_depth': 10, 'alpha': 0.2611, 'lambda': 7.926810686897084, 'max_bin': 332}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:30:15,827]\u001b[0m Trial 81 finished with value: 0.8242290006016558 and parameters: {'n_estimators': 706, 'eta': 0.030177465774739252, 'max_depth': 11, 'alpha': 0.31620000000000004, 'lambda': 3.391032790088847, 'max_bin': 354}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:30:44,755]\u001b[0m Trial 82 finished with value: 0.8122234798119885 and parameters: {'n_estimators': 859, 'eta': 0.007487820964601703, 'max_depth': 12, 'alpha': 0.3713, 'lambda': 17.997623493614217, 'max_bin': 343}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:30:53,736]\u001b[0m Trial 83 finished with value: 0.8304345831687302 and parameters: {'n_estimators': 878, 'eta': 0.046504633032255216, 'max_depth': 11, 'alpha': 0.15760000000000002, 'lambda': 1.3770577976694334, 'max_bin': 362}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:31:14,122]\u001b[0m Trial 84 finished with value: 0.8265615373641308 and parameters: {'n_estimators': 833, 'eta': 0.024117092266561287, 'max_depth': 12, 'alpha': 0.2973, 'lambda': 5.152165157750122, 'max_bin': 299}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:31:28,562]\u001b[0m Trial 85 finished with value: 0.8243100688428058 and parameters: {'n_estimators': 783, 'eta': 0.04046906353428509, 'max_depth': 11, 'alpha': 0.20350000000000001, 'lambda': 6.742805836269497, 'max_bin': 384}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:31:48,914]\u001b[0m Trial 86 finished with value: 0.8164811568475077 and parameters: {'n_estimators': 732, 'eta': 0.020606882450307624, 'max_depth': 10, 'alpha': 0.0304, 'lambda': 34.85989016005631, 'max_bin': 337}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:32:08,649]\u001b[0m Trial 87 finished with value: 0.8231638699786572 and parameters: {'n_estimators': 809, 'eta': 0.029357285335844734, 'max_depth': 12, 'alpha': 0.5837, 'lambda': 13.41522914671081, 'max_bin': 399}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:32:17,956]\u001b[0m Trial 88 finished with value: 0.8237742962245349 and parameters: {'n_estimators': 852, 'eta': 0.050047046770176, 'max_depth': 11, 'alpha': 0.5238, 'lambda': 2.340474727898446, 'max_bin': 350}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:32:37,346]\u001b[0m Trial 89 finished with value: 0.8242704657585446 and parameters: {'n_estimators': 667, 'eta': 0.026744643911224393, 'max_depth': 12, 'alpha': 0.331, 'lambda': 10.015779229125329, 'max_bin': 313}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:32:47,604]\u001b[0m Trial 90 finished with value: 0.8237621407312 and parameters: {'n_estimators': 324, 'eta': 0.04455818263949265, 'max_depth': 11, 'alpha': 0.9164, 'lambda': 11.372517809309446, 'max_bin': 281}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:32:55,554]\u001b[0m Trial 91 finished with value: 0.8157771737474679 and parameters: {'n_estimators': 536, 'eta': 0.084168254126868, 'max_depth': 6, 'alpha': 0.35300000000000004, 'lambda': 6.3083400157028215, 'max_bin': 325}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:33:07,047]\u001b[0m Trial 92 finished with value: 0.8179808456803304 and parameters: {'n_estimators': 700, 'eta': 0.07906329248763921, 'max_depth': 6, 'alpha': 0.4403, 'lambda': 19.43371215414729, 'max_bin': 333}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:33:24,210]\u001b[0m Trial 93 finished with value: 0.8206915659223932 and parameters: {'n_estimators': 881, 'eta': 0.034428016568813856, 'max_depth': 7, 'alpha': 0.3956, 'lambda': 7.5030330374033145, 'max_bin': 321}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:33:33,406]\u001b[0m Trial 94 finished with value: 0.8179840584521079 and parameters: {'n_estimators': 616, 'eta': 0.08953835514719655, 'max_depth': 6, 'alpha': 0.4806, 'lambda': 9.177904850475135, 'max_bin': 294}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:33:45,363]\u001b[0m Trial 95 finished with value: 0.8105398490578712 and parameters: {'n_estimators': 754, 'eta': 0.03267009355084457, 'max_depth': 5, 'alpha': 0.4068, 'lambda': 14.307855247927002, 'max_bin': 347}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:33:58,981]\u001b[0m Trial 96 finished with value: 0.825095812239972 and parameters: {'n_estimators': 492, 'eta': 0.03881230291161341, 'max_depth': 12, 'alpha': 0.0017000000000000001, 'lambda': 10.321292700962957, 'max_bin': 337}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:34:05,484]\u001b[0m Trial 97 finished with value: 0.8223243931492163 and parameters: {'n_estimators': 554, 'eta': 0.09485079327538762, 'max_depth': 10, 'alpha': 0.3649, 'lambda': 3.90298378405625, 'max_bin': 358}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:34:13,772]\u001b[0m Trial 98 finished with value: 0.8223008925783155 and parameters: {'n_estimators': 586, 'eta': 0.09911753890175419, 'max_depth': 11, 'alpha': 0.8371000000000001, 'lambda': 8.736747909756582, 'max_bin': 305}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:34:23,028]\u001b[0m Trial 99 finished with value: 0.8249398873178831 and parameters: {'n_estimators': 634, 'eta': 0.08630444273059971, 'max_depth': 9, 'alpha': 0.4062, 'lambda': 12.8439740366239, 'max_bin': 373}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8331\n",
      "\tBest params:\n",
      "\t\tn_estimators: 741\n",
      "\t\teta: 0.08307189807265311\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.9568000000000001\n",
      "\t\tlambda: 8.184756243986026\n",
      "\t\tmax_bin: 384\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  393.000000  398.000000\n",
      "1                    TN  358.000000  363.000000\n",
      "2                    FP   76.000000   80.000000\n",
      "3                    FN   72.000000   58.000000\n",
      "4              Accuracy    0.835373    0.846496\n",
      "5             Precision    0.837953    0.832636\n",
      "6           Sensitivity    0.845161    0.872807\n",
      "7           Specificity    0.824900    0.819400\n",
      "8              F1 score    0.841542    0.852248\n",
      "9   F1 score (weighted)    0.835344    0.846350\n",
      "10     F1 score (macro)    0.835123    0.846263\n",
      "11    Balanced Accuracy    0.835023    0.846110\n",
      "12                  MCC    0.670279    0.693543\n",
      "13                  NPV    0.832600    0.862200\n",
      "14              ROC_AUC    0.835023    0.846110\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_xgb_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_xgb_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_xgb_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_xgb_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_xgb_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_xgb_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_xgb_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_xgb_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_xgb_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_xgb_1)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:34:27,723]\u001b[0m Trial 100 finished with value: 0.8230806142065152 and parameters: {'n_estimators': 89, 'eta': 0.0535726392698182, 'max_depth': 12, 'alpha': 0.6355000000000001, 'lambda': 1.9366794922650983, 'max_bin': 329}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:34:44,277]\u001b[0m Trial 101 finished with value: 0.8290863013625746 and parameters: {'n_estimators': 872, 'eta': 0.04577988535138554, 'max_depth': 11, 'alpha': 0.0912, 'lambda': 15.263039960718803, 'max_bin': 312}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:35:02,706]\u001b[0m Trial 102 finished with value: 0.8252276843674871 and parameters: {'n_estimators': 830, 'eta': 0.03671321936950886, 'max_depth': 11, 'alpha': 0.016800000000000002, 'lambda': 15.846361196191928, 'max_bin': 317}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:35:18,221]\u001b[0m Trial 103 finished with value: 0.8305274434498487 and parameters: {'n_estimators': 884, 'eta': 0.04249862667064454, 'max_depth': 12, 'alpha': 0.06420000000000001, 'lambda': 11.207284930029871, 'max_bin': 342}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:35:33,969]\u001b[0m Trial 104 finished with value: 0.8240774676083864 and parameters: {'n_estimators': 600, 'eta': 0.04037246151329676, 'max_depth': 11, 'alpha': 0.0167, 'lambda': 19.56919601261213, 'max_bin': 364}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:35:47,315]\u001b[0m Trial 105 finished with value: 0.8282492978356413 and parameters: {'n_estimators': 797, 'eta': 0.04442081490795945, 'max_depth': 10, 'alpha': 0.038900000000000004, 'lambda': 7.0073276054317395, 'max_bin': 323}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:36:02,342]\u001b[0m Trial 106 finished with value: 0.8226956804203136 and parameters: {'n_estimators': 858, 'eta': 0.04945804912762479, 'max_depth': 6, 'alpha': 0.11370000000000001, 'lambda': 16.810234740327687, 'max_bin': 310}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:36:16,268]\u001b[0m Trial 107 finished with value: 0.8271669818276864 and parameters: {'n_estimators': 837, 'eta': 0.06385439832167566, 'max_depth': 12, 'alpha': 0.9738, 'lambda': 17.98982803669725, 'max_bin': 287}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:36:37,257]\u001b[0m Trial 108 finished with value: 0.8257839381813739 and parameters: {'n_estimators': 890, 'eta': 0.02247484310834071, 'max_depth': 8, 'alpha': 0.1381, 'lambda': 14.305529244796581, 'max_bin': 271}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:36:45,949]\u001b[0m Trial 109 finished with value: 0.8266324612624387 and parameters: {'n_estimators': 864, 'eta': 0.09368534563022428, 'max_depth': 11, 'alpha': 0.4223, 'lambda': 8.177077034801021, 'max_bin': 489}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:36:55,870]\u001b[0m Trial 110 finished with value: 0.8252075183266744 and parameters: {'n_estimators': 811, 'eta': 0.08157309158435894, 'max_depth': 7, 'alpha': 0.3401, 'lambda': 9.43246534135097, 'max_bin': 353}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:37:11,224]\u001b[0m Trial 111 finished with value: 0.8291841924508159 and parameters: {'n_estimators': 881, 'eta': 0.04254326410958666, 'max_depth': 12, 'alpha': 0.0649, 'lambda': 11.12506273519703, 'max_bin': 340}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:37:29,444]\u001b[0m Trial 112 finished with value: 0.8286204353384263 and parameters: {'n_estimators': 897, 'eta': 0.03450154646394262, 'max_depth': 12, 'alpha': 0.0551, 'lambda': 12.580886402971032, 'max_bin': 346}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:37:43,545]\u001b[0m Trial 113 finished with value: 0.8278142212519153 and parameters: {'n_estimators': 844, 'eta': 0.03795939071246871, 'max_depth': 12, 'alpha': 0.0799, 'lambda': 6.024097374015673, 'max_bin': 333}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:37:59,585]\u001b[0m Trial 114 finished with value: 0.8290249119543865 and parameters: {'n_estimators': 771, 'eta': 0.041272175829006996, 'max_depth': 12, 'alpha': 0.3073, 'lambda': 11.74087830562016, 'max_bin': 344}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:38:11,720]\u001b[0m Trial 115 finished with value: 0.8267998218301738 and parameters: {'n_estimators': 866, 'eta': 0.05876482517297891, 'max_depth': 11, 'alpha': 0.0007, 'lambda': 10.609110336758478, 'max_bin': 366}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:38:30,442]\u001b[0m Trial 116 finished with value: 0.828529408269923 and parameters: {'n_estimators': 826, 'eta': 0.031383319647707, 'max_depth': 12, 'alpha': 0.4521, 'lambda': 7.630158794528501, 'max_bin': 300}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:38:46,079]\u001b[0m Trial 117 finished with value: 0.8276887039537385 and parameters: {'n_estimators': 885, 'eta': 0.04343270742430347, 'max_depth': 10, 'alpha': 0.12150000000000001, 'lambda': 9.510256128308392, 'max_bin': 473}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:38:53,945]\u001b[0m Trial 118 finished with value: 0.8276607462247749 and parameters: {'n_estimators': 784, 'eta': 0.09055994583026628, 'max_depth': 12, 'alpha': 0.24300000000000002, 'lambda': 4.998452788284674, 'max_bin': 317}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:39:05,901]\u001b[0m Trial 119 finished with value: 0.817384356832609 and parameters: {'n_estimators': 747, 'eta': 0.052226014366106244, 'max_depth': 5, 'alpha': 0.5095000000000001, 'lambda': 16.020642441609372, 'max_bin': 331}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:39:20,673]\u001b[0m Trial 120 finished with value: 0.8276964760030561 and parameters: {'n_estimators': 675, 'eta': 0.04574878476442193, 'max_depth': 11, 'alpha': 0.0375, 'lambda': 13.594685879515625, 'max_bin': 341}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:39:39,642]\u001b[0m Trial 121 finished with value: 0.8314595043825556 and parameters: {'n_estimators': 847, 'eta': 0.019525799862374623, 'max_depth': 11, 'alpha': 0.2861, 'lambda': 1.1449522054677121, 'max_bin': 358}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:40:05,026]\u001b[0m Trial 122 finished with value: 0.8260634346562972 and parameters: {'n_estimators': 851, 'eta': 0.008458122455095224, 'max_depth': 11, 'alpha': 0.2908, 'lambda': 1.1065536457685896, 'max_bin': 359}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:40:28,100]\u001b[0m Trial 123 finished with value: 0.8291563302427651 and parameters: {'n_estimators': 808, 'eta': 0.016372832790792213, 'max_depth': 11, 'alpha': 0.25780000000000003, 'lambda': 3.3160402650418774, 'max_bin': 350}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:40:37,942]\u001b[0m Trial 124 finished with value: 0.8246697041720676 and parameters: {'n_estimators': 899, 'eta': 0.0715944172457843, 'max_depth': 11, 'alpha': 0.0907, 'lambda': 8.559390908293569, 'max_bin': 326}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:41:06,832]\u001b[0m Trial 125 finished with value: 0.8214210199426564 and parameters: {'n_estimators': 871, 'eta': 0.011172229521919314, 'max_depth': 12, 'alpha': 0.3249, 'lambda': 10.268923828764628, 'max_bin': 381}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:41:16,875]\u001b[0m Trial 126 finished with value: 0.8302256804172347 and parameters: {'n_estimators': 843, 'eta': 0.04730146320394186, 'max_depth': 10, 'alpha': 0.3774, 'lambda': 1.8747845196568287, 'max_bin': 336}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:41:30,261]\u001b[0m Trial 127 finished with value: 0.8275016967383596 and parameters: {'n_estimators': 424, 'eta': 0.02899525973853644, 'max_depth': 12, 'alpha': 0.17220000000000002, 'lambda': 6.568602157517343, 'max_bin': 368}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:41:53,671]\u001b[0m Trial 128 finished with value: 0.8243373452728788 and parameters: {'n_estimators': 821, 'eta': 0.027017048223763163, 'max_depth': 11, 'alpha': 0.2079, 'lambda': 21.680090267835823, 'max_bin': 306}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:42:05,843]\u001b[0m Trial 129 finished with value: 0.8269167494165808 and parameters: {'n_estimators': 795, 'eta': 0.03880578532001394, 'max_depth': 12, 'alpha': 0.0443, 'lambda': 3.6865276969444074, 'max_bin': 351}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:42:24,534]\u001b[0m Trial 130 finished with value: 0.8274095402872973 and parameters: {'n_estimators': 858, 'eta': 0.03609373406415138, 'max_depth': 11, 'alpha': 0.6994, 'lambda': 11.85925950221487, 'max_bin': 451}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:42:45,736]\u001b[0m Trial 131 finished with value: 0.8286771533236061 and parameters: {'n_estimators': 881, 'eta': 0.01915942102434368, 'max_depth': 11, 'alpha': 0.34190000000000004, 'lambda': 2.4663074759877293, 'max_bin': 387}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:43:00,470]\u001b[0m Trial 132 finished with value: 0.8187763936294642 and parameters: {'n_estimators': 829, 'eta': 0.01902509034096398, 'max_depth': 6, 'alpha': 0.2775, 'lambda': 1.7894191691414543, 'max_bin': 357}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:43:23,766]\u001b[0m Trial 133 finished with value: 0.8260274077567337 and parameters: {'n_estimators': 771, 'eta': 0.013220430900533875, 'max_depth': 11, 'alpha': 0.2998, 'lambda': 4.366268124922292, 'max_bin': 396}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:43:32,970]\u001b[0m Trial 134 finished with value: 0.8241506066398937 and parameters: {'n_estimators': 841, 'eta': 0.08528525556634658, 'max_depth': 9, 'alpha': 0.5469, 'lambda': 11.02127738899859, 'max_bin': 378}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:43:57,687]\u001b[0m Trial 135 finished with value: 0.8223106315541525 and parameters: {'n_estimators': 872, 'eta': 0.025475768245608967, 'max_depth': 11, 'alpha': 0.023100000000000002, 'lambda': 28.830637347667157, 'max_bin': 361}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:44:08,779]\u001b[0m Trial 136 finished with value: 0.8295659283561978 and parameters: {'n_estimators': 722, 'eta': 0.07779344049010262, 'max_depth': 10, 'alpha': 0.5680000000000001, 'lambda': 17.48956874319146, 'max_bin': 371}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:44:20,879]\u001b[0m Trial 137 finished with value: 0.8310377067207474 and parameters: {'n_estimators': 689, 'eta': 0.03313061532867453, 'max_depth': 11, 'alpha': 0.3559, 'lambda': 1.1379592817576296, 'max_bin': 294}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:44:33,067]\u001b[0m Trial 138 finished with value: 0.8325403600411649 and parameters: {'n_estimators': 693, 'eta': 0.03332354280180472, 'max_depth': 12, 'alpha': 0.3569, 'lambda': 1.0517545607595302, 'max_bin': 314}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:44:48,678]\u001b[0m Trial 139 finished with value: 0.8268502981586421 and parameters: {'n_estimators': 683, 'eta': 0.03312811418125692, 'max_depth': 11, 'alpha': 0.3608, 'lambda': 5.420512592118357, 'max_bin': 295}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:45:03,107]\u001b[0m Trial 140 finished with value: 0.8286166898410949 and parameters: {'n_estimators': 656, 'eta': 0.030534798952692038, 'max_depth': 12, 'alpha': 0.3884, 'lambda': 2.9644266848838443, 'max_bin': 283}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:45:15,268]\u001b[0m Trial 141 finished with value: 0.8318919396185527 and parameters: {'n_estimators': 695, 'eta': 0.034906395517833484, 'max_depth': 12, 'alpha': 0.3128, 'lambda': 2.2586015704388793, 'max_bin': 321}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:45:26,608]\u001b[0m Trial 142 finished with value: 0.8282865701283363 and parameters: {'n_estimators': 712, 'eta': 0.03583001006207791, 'max_depth': 12, 'alpha': 0.3569, 'lambda': 1.1395874884804273, 'max_bin': 314}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:45:38,601]\u001b[0m Trial 143 finished with value: 0.8241474286155714 and parameters: {'n_estimators': 699, 'eta': 0.033153897826430835, 'max_depth': 6, 'alpha': 0.3183, 'lambda': 2.189007250279972, 'max_bin': 321}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:45:51,753]\u001b[0m Trial 144 finished with value: 0.832201491891384 and parameters: {'n_estimators': 658, 'eta': 0.03742413644690562, 'max_depth': 12, 'alpha': 0.4192, 'lambda': 2.602080354126053, 'max_bin': 300}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:46:08,589]\u001b[0m Trial 145 finished with value: 0.8296627107416585 and parameters: {'n_estimators': 629, 'eta': 0.028173369897768093, 'max_depth': 12, 'alpha': 0.4176, 'lambda': 3.2521510286076065, 'max_bin': 300}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:46:23,387]\u001b[0m Trial 146 finished with value: 0.8294275478968253 and parameters: {'n_estimators': 658, 'eta': 0.03470789208022523, 'max_depth': 12, 'alpha': 0.46130000000000004, 'lambda': 4.136822656145922, 'max_bin': 291}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:46:36,199]\u001b[0m Trial 147 finished with value: 0.8308514498536059 and parameters: {'n_estimators': 605, 'eta': 0.031193065716085305, 'max_depth': 12, 'alpha': 0.3748, 'lambda': 1.0496987663143962, 'max_bin': 304}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:46:48,509]\u001b[0m Trial 148 finished with value: 0.8316934131011919 and parameters: {'n_estimators': 687, 'eta': 0.03113682669548863, 'max_depth': 12, 'alpha': 0.3709, 'lambda': 1.0806807600801343, 'max_bin': 304}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:46:59,674]\u001b[0m Trial 149 finished with value: 0.8296344391974293 and parameters: {'n_estimators': 642, 'eta': 0.03795470197321661, 'max_depth': 12, 'alpha': 0.333, 'lambda': 1.100807749065363, 'max_bin': 294}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8331\n",
      "\tBest params:\n",
      "\t\tn_estimators: 741\n",
      "\t\teta: 0.08307189807265311\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.9568000000000001\n",
      "\t\tlambda: 8.184756243986026\n",
      "\t\tmax_bin: 384\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  393.000000  398.000000  417.000000\n",
      "1                    TN  358.000000  363.000000  347.000000\n",
      "2                    FP   76.000000   80.000000   77.000000\n",
      "3                    FN   72.000000   58.000000   58.000000\n",
      "4              Accuracy    0.835373    0.846496    0.849833\n",
      "5             Precision    0.837953    0.832636    0.844130\n",
      "6           Sensitivity    0.845161    0.872807    0.877895\n",
      "7           Specificity    0.824900    0.819400    0.818400\n",
      "8              F1 score    0.841542    0.852248    0.860681\n",
      "9   F1 score (weighted)    0.835344    0.846350    0.849585\n",
      "10     F1 score (macro)    0.835123    0.846263    0.848917\n",
      "11    Balanced Accuracy    0.835023    0.846110    0.848145\n",
      "12                  MCC    0.670279    0.693543    0.698601\n",
      "13                  NPV    0.832600    0.862200    0.856800\n",
      "14              ROC_AUC    0.835023    0.846110    0.848145\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_xgb_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_xgb_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_xgb_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_xgb_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_xgb_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_xgb_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_xgb_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_xgb_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_xgb_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_xgb_2)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:47:14,781]\u001b[0m Trial 150 finished with value: 0.8180382668368391 and parameters: {'n_estimators': 687, 'eta': 0.03023483043374211, 'max_depth': 12, 'alpha': 0.38970000000000005, 'lambda': 2.037244107808428, 'max_bin': 306}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:47:28,180]\u001b[0m Trial 151 finished with value: 0.8175848963794238 and parameters: {'n_estimators': 735, 'eta': 0.03134320811151156, 'max_depth': 12, 'alpha': 0.3719, 'lambda': 2.9427928353048, 'max_bin': 301}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:47:39,276]\u001b[0m Trial 152 finished with value: 0.8191136082252347 and parameters: {'n_estimators': 699, 'eta': 0.03312952519171, 'max_depth': 12, 'alpha': 0.30770000000000003, 'lambda': 1.062494416092752, 'max_bin': 310}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:47:56,587]\u001b[0m Trial 153 finished with value: 0.8191813881216905 and parameters: {'n_estimators': 677, 'eta': 0.023560561010238727, 'max_depth': 12, 'alpha': 0.3483, 'lambda': 2.6631137982163864, 'max_bin': 288}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:48:08,585]\u001b[0m Trial 154 finished with value: 0.8177562084418817 and parameters: {'n_estimators': 668, 'eta': 0.03594500292237625, 'max_depth': 12, 'alpha': 0.4375, 'lambda': 1.9274341200308756, 'max_bin': 303}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:48:24,698]\u001b[0m Trial 155 finished with value: 0.8205056615072623 and parameters: {'n_estimators': 709, 'eta': 0.0289294575349217, 'max_depth': 12, 'alpha': 0.2853, 'lambda': 3.9276437180088877, 'max_bin': 439}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:48:38,458]\u001b[0m Trial 156 finished with value: 0.8187217098512655 and parameters: {'n_estimators': 731, 'eta': 0.03161922289458663, 'max_depth': 12, 'alpha': 0.3995, 'lambda': 1.8247711558215176, 'max_bin': 296}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:48:50,993]\u001b[0m Trial 157 finished with value: 0.8217361289572571 and parameters: {'n_estimators': 618, 'eta': 0.040298135704748037, 'max_depth': 12, 'alpha': 0.37010000000000004, 'lambda': 4.6979119637033255, 'max_bin': 316}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:49:07,910]\u001b[0m Trial 158 finished with value: 0.8170458476074998 and parameters: {'n_estimators': 653, 'eta': 0.026107805343616692, 'max_depth': 12, 'alpha': 0.7823, 'lambda': 2.8963110447561653, 'max_bin': 310}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:49:18,617]\u001b[0m Trial 159 finished with value: 0.8162601671067267 and parameters: {'n_estimators': 717, 'eta': 0.03729560645170412, 'max_depth': 12, 'alpha': 0.26890000000000003, 'lambda': 1.1746597031179944, 'max_bin': 323}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:49:30,601]\u001b[0m Trial 160 finished with value: 0.8211407992230766 and parameters: {'n_estimators': 756, 'eta': 0.03961147375896844, 'max_depth': 12, 'alpha': 0.3163, 'lambda': 2.4515633384533775, 'max_bin': 284}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:49:43,386]\u001b[0m Trial 161 finished with value: 0.8183978063890777 and parameters: {'n_estimators': 599, 'eta': 0.03405976715821598, 'max_depth': 12, 'alpha': 0.4132, 'lambda': 3.337656408399494, 'max_bin': 276}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:49:58,679]\u001b[0m Trial 162 finished with value: 0.8196645241643538 and parameters: {'n_estimators': 598, 'eta': 0.03497339921856929, 'max_depth': 12, 'alpha': 0.4917, 'lambda': 7.325606212080309, 'max_bin': 319}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:50:11,670]\u001b[0m Trial 163 finished with value: 0.8175811028650714 and parameters: {'n_estimators': 573, 'eta': 0.03232649899960033, 'max_depth': 11, 'alpha': 0.33490000000000003, 'lambda': 1.6763879624260722, 'max_bin': 330}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:50:27,127]\u001b[0m Trial 164 finished with value: 0.8157990952848282 and parameters: {'n_estimators': 685, 'eta': 0.027925803772104255, 'max_depth': 8, 'alpha': 0.3536, 'lambda': 5.86250779844089, 'max_bin': 305}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:50:38,447]\u001b[0m Trial 165 finished with value: 0.8099845038283009 and parameters: {'n_estimators': 621, 'eta': 0.036833889963334226, 'max_depth': 6, 'alpha': 0.3829, 'lambda': 8.084007247573021, 'max_bin': 327}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:50:47,814]\u001b[0m Trial 166 finished with value: 0.81365674709696 and parameters: {'n_estimators': 639, 'eta': 0.06147904817610121, 'max_depth': 7, 'alpha': 0.4726, 'lambda': 3.6724164507509127, 'max_bin': 313}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:50:53,663]\u001b[0m Trial 167 finished with value: 0.8221682422584575 and parameters: {'n_estimators': 661, 'eta': 0.09764669416444939, 'max_depth': 12, 'alpha': 0.4335, 'lambda': 1.0778010463886942, 'max_bin': 336}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:51:01,909]\u001b[0m Trial 168 finished with value: 0.8183807669699815 and parameters: {'n_estimators': 700, 'eta': 0.08837037921365146, 'max_depth': 11, 'alpha': 0.3977, 'lambda': 9.567033464140664, 'max_bin': 290}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:51:09,002]\u001b[0m Trial 169 finished with value: 0.8196549758224734 and parameters: {'n_estimators': 560, 'eta': 0.06915692393491392, 'max_depth': 12, 'alpha': 0.2997, 'lambda': 2.272302288732104, 'max_bin': 417}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:51:24,542]\u001b[0m Trial 170 finished with value: 0.8174520652567047 and parameters: {'n_estimators': 747, 'eta': 0.03112701911602097, 'max_depth': 11, 'alpha': 0.2359, 'lambda': 4.907544536384119, 'max_bin': 297}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:51:54,895]\u001b[0m Trial 171 finished with value: 0.7696978195059094 and parameters: {'n_estimators': 890, 'eta': 0.0004938663828568494, 'max_depth': 11, 'alpha': 0.36210000000000003, 'lambda': 15.16415081142135, 'max_bin': 309}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:52:12,071]\u001b[0m Trial 172 finished with value: 0.815968060348637 and parameters: {'n_estimators': 685, 'eta': 0.041196058684096745, 'max_depth': 11, 'alpha': 0.3346, 'lambda': 19.72342051927384, 'max_bin': 319}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:52:26,663]\u001b[0m Trial 173 finished with value: 0.8179129930425004 and parameters: {'n_estimators': 642, 'eta': 0.05079873554552942, 'max_depth': 11, 'alpha': 0.41090000000000004, 'lambda': 16.31827429513628, 'max_bin': 303}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:52:47,789]\u001b[0m Trial 174 finished with value: 0.8164660867207492 and parameters: {'n_estimators': 865, 'eta': 0.0341892810547493, 'max_depth': 11, 'alpha': 0.3725, 'lambda': 18.274418309004233, 'max_bin': 314}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:53:00,174]\u001b[0m Trial 175 finished with value: 0.8222082787665945 and parameters: {'n_estimators': 609, 'eta': 0.054727103321510547, 'max_depth': 9, 'alpha': 0.5229, 'lambda': 8.828723239798686, 'max_bin': 323}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:53:07,760]\u001b[0m Trial 176 finished with value: 0.8189568427229164 and parameters: {'n_estimators': 717, 'eta': 0.07390466757871936, 'max_depth': 12, 'alpha': 0.8754000000000001, 'lambda': 1.7727324066977657, 'max_bin': 347}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:53:26,032]\u001b[0m Trial 177 finished with value: 0.8165702636213018 and parameters: {'n_estimators': 899, 'eta': 0.037841494255802875, 'max_depth': 10, 'alpha': 0.0015, 'lambda': 18.86106042241453, 'max_bin': 327}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:53:36,381]\u001b[0m Trial 178 finished with value: 0.8085859545895184 and parameters: {'n_estimators': 665, 'eta': 0.04832025732325559, 'max_depth': 5, 'alpha': 0.7425, 'lambda': 6.780019391490456, 'max_bin': 334}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:53:44,257]\u001b[0m Trial 179 finished with value: 0.8202535770468249 and parameters: {'n_estimators': 585, 'eta': 0.06649664905976559, 'max_depth': 12, 'alpha': 0.3214, 'lambda': 2.454392601114922, 'max_bin': 300}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:53:50,752]\u001b[0m Trial 180 finished with value: 0.8127890949800067 and parameters: {'n_estimators': 218, 'eta': 0.04501431762914804, 'max_depth': 11, 'alpha': 0.3473, 'lambda': 1.0572972050552443, 'max_bin': 309}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:54:05,669]\u001b[0m Trial 181 finished with value: 0.8164177497131793 and parameters: {'n_estimators': 881, 'eta': 0.03932089519392362, 'max_depth': 12, 'alpha': 0.068, 'lambda': 10.067190566043257, 'max_bin': 341}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:54:20,213]\u001b[0m Trial 182 finished with value: 0.8201469960839469 and parameters: {'n_estimators': 856, 'eta': 0.04225986507463804, 'max_depth': 12, 'alpha': 0.028900000000000002, 'lambda': 10.769620701455251, 'max_bin': 345}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:54:36,083]\u001b[0m Trial 183 finished with value: 0.8168691100211625 and parameters: {'n_estimators': 900, 'eta': 0.0427619830335311, 'max_depth': 12, 'alpha': 0.0507, 'lambda': 17.143612247566267, 'max_bin': 354}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:54:44,417]\u001b[0m Trial 184 finished with value: 0.8183633006343213 and parameters: {'n_estimators': 856, 'eta': 0.08334684912609162, 'max_depth': 12, 'alpha': 0.08990000000000001, 'lambda': 9.084430806574225, 'max_bin': 340}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:55:01,814]\u001b[0m Trial 185 finished with value: 0.8183221965763021 and parameters: {'n_estimators': 881, 'eta': 0.03516797197893616, 'max_depth': 12, 'alpha': 0.9158000000000001, 'lambda': 8.140122657077617, 'max_bin': 335}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:55:18,329]\u001b[0m Trial 186 finished with value: 0.8178844469355144 and parameters: {'n_estimators': 872, 'eta': 0.03713898306852799, 'max_depth': 12, 'alpha': 0.1044, 'lambda': 12.47998357954085, 'max_bin': 255}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:55:32,598]\u001b[0m Trial 187 finished with value: 0.8158013738876763 and parameters: {'n_estimators': 811, 'eta': 0.029980150451182148, 'max_depth': 11, 'alpha': 0.254, 'lambda': 3.1799298217350787, 'max_bin': 330}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:56:02,686]\u001b[0m Trial 188 finished with value: 0.8114468913514985 and parameters: {'n_estimators': 841, 'eta': 0.004159155972240071, 'max_depth': 12, 'alpha': 0.5923, 'lambda': 1.8612632548557604, 'max_bin': 364}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:56:18,695]\u001b[0m Trial 189 finished with value: 0.8209164003214555 and parameters: {'n_estimators': 736, 'eta': 0.03908375894474305, 'max_depth': 11, 'alpha': 0.3839, 'lambda': 11.513163016473557, 'max_bin': 316}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:56:32,223]\u001b[0m Trial 190 finished with value: 0.8188359013822245 and parameters: {'n_estimators': 787, 'eta': 0.04437210502714343, 'max_depth': 11, 'alpha': 0.062200000000000005, 'lambda': 10.20464595923165, 'max_bin': 293}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:56:48,468]\u001b[0m Trial 191 finished with value: 0.8156033956525504 and parameters: {'n_estimators': 832, 'eta': 0.022159326677641814, 'max_depth': 11, 'alpha': 0.3556, 'lambda': 1.10312131374157, 'max_bin': 368}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:57:09,793]\u001b[0m Trial 192 finished with value: 0.8175595796077622 and parameters: {'n_estimators': 884, 'eta': 0.019063771348453357, 'max_depth': 11, 'alpha': 0.32930000000000004, 'lambda': 2.8404112100552563, 'max_bin': 376}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:57:22,103]\u001b[0m Trial 193 finished with value: 0.8187268288876952 and parameters: {'n_estimators': 823, 'eta': 0.03254693765225602, 'max_depth': 12, 'alpha': 0.2844, 'lambda': 2.0964386232189063, 'max_bin': 386}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:57:32,292]\u001b[0m Trial 194 finished with value: 0.8167296495985512 and parameters: {'n_estimators': 697, 'eta': 0.04138851877792274, 'max_depth': 6, 'alpha': 0.0224, 'lambda': 1.7064298327890686, 'max_bin': 356}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:57:54,910]\u001b[0m Trial 195 finished with value: 0.8206637477270725 and parameters: {'n_estimators': 770, 'eta': 0.014821725661707967, 'max_depth': 11, 'alpha': 0.3014, 'lambda': 3.8883790217046124, 'max_bin': 304}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:58:00,111]\u001b[0m Trial 196 finished with value: 0.8227446802430043 and parameters: {'n_estimators': 673, 'eta': 0.09996059175097434, 'max_depth': 12, 'alpha': 0.3821, 'lambda': 1.0655927426683136, 'max_bin': 323}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:58:17,154]\u001b[0m Trial 197 finished with value: 0.8178124639978412 and parameters: {'n_estimators': 858, 'eta': 0.02427564890318943, 'max_depth': 11, 'alpha': 0.4143, 'lambda': 2.6025415564453818, 'max_bin': 350}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:58:33,733]\u001b[0m Trial 198 finished with value: 0.8177368620845435 and parameters: {'n_estimators': 803, 'eta': 0.03524208177881525, 'max_depth': 12, 'alpha': 0.45220000000000005, 'lambda': 9.660477843821653, 'max_bin': 299}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:58:50,355]\u001b[0m Trial 199 finished with value: 0.8173129441538615 and parameters: {'n_estimators': 633, 'eta': 0.03278210350407935, 'max_depth': 10, 'alpha': 0.3457, 'lambda': 8.559599625465719, 'max_bin': 374}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8331\n",
      "\tBest params:\n",
      "\t\tn_estimators: 741\n",
      "\t\teta: 0.08307189807265311\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.9568000000000001\n",
      "\t\tlambda: 8.184756243986026\n",
      "\t\tmax_bin: 384\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  393.000000  398.000000  417.000000  412.000000\n",
      "1                    TN  358.000000  363.000000  347.000000  346.000000\n",
      "2                    FP   76.000000   80.000000   77.000000   83.000000\n",
      "3                    FN   72.000000   58.000000   58.000000   58.000000\n",
      "4              Accuracy    0.835373    0.846496    0.849833    0.843159\n",
      "5             Precision    0.837953    0.832636    0.844130    0.832323\n",
      "6           Sensitivity    0.845161    0.872807    0.877895    0.876596\n",
      "7           Specificity    0.824900    0.819400    0.818400    0.806500\n",
      "8              F1 score    0.841542    0.852248    0.860681    0.853886\n",
      "9   F1 score (weighted)    0.835344    0.846350    0.849585    0.842837\n",
      "10     F1 score (macro)    0.835123    0.846263    0.848917    0.842309\n",
      "11    Balanced Accuracy    0.835023    0.846110    0.848145    0.841561\n",
      "12                  MCC    0.670279    0.693543    0.698601    0.685935\n",
      "13                  NPV    0.832600    0.862200    0.856800    0.856400\n",
      "14              ROC_AUC    0.835023    0.846110    0.848145    0.841561\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_xgb_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_xgb_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_xgb_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_xgb_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_xgb_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_xgb_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_xgb_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_xgb_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_xgb_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_xgb_3)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 13:58:59,583]\u001b[0m Trial 200 finished with value: 0.8307468183316464 and parameters: {'n_estimators': 900, 'eta': 0.08057576992633064, 'max_depth': 12, 'alpha': 0.36610000000000004, 'lambda': 7.551225032280977, 'max_bin': 308}. Best is trial 9 with value: 0.8330909770787589.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:59:07,577]\u001b[0m Trial 201 finished with value: 0.8335373394618 and parameters: {'n_estimators': 898, 'eta': 0.07859686958613045, 'max_depth': 12, 'alpha': 0.37310000000000004, 'lambda': 7.3831978819179325, 'max_bin': 308}. Best is trial 201 with value: 0.8335373394618.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:59:15,539]\u001b[0m Trial 202 finished with value: 0.833348676150208 and parameters: {'n_estimators': 895, 'eta': 0.07961652296169673, 'max_depth': 12, 'alpha': 0.36910000000000004, 'lambda': 6.940725449801789, 'max_bin': 309}. Best is trial 201 with value: 0.8335373394618.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:59:23,569]\u001b[0m Trial 203 finished with value: 0.8318481538045572 and parameters: {'n_estimators': 893, 'eta': 0.08054666801594629, 'max_depth': 12, 'alpha': 0.3951, 'lambda': 7.2801931196367855, 'max_bin': 309}. Best is trial 201 with value: 0.8335373394618.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:59:31,732]\u001b[0m Trial 204 finished with value: 0.8313646404920266 and parameters: {'n_estimators': 900, 'eta': 0.08027025256438787, 'max_depth': 12, 'alpha': 0.39630000000000004, 'lambda': 7.3599726902015306, 'max_bin': 307}. Best is trial 201 with value: 0.8335373394618.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:59:39,669]\u001b[0m Trial 205 finished with value: 0.827091685674503 and parameters: {'n_estimators': 884, 'eta': 0.07729382326745209, 'max_depth': 12, 'alpha': 0.3992, 'lambda': 6.402625768879955, 'max_bin': 313}. Best is trial 201 with value: 0.8335373394618.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:59:47,282]\u001b[0m Trial 206 finished with value: 0.8314331803533767 and parameters: {'n_estimators': 900, 'eta': 0.081900653188333, 'max_depth': 12, 'alpha': 0.3907, 'lambda': 7.372399479122568, 'max_bin': 306}. Best is trial 201 with value: 0.8335373394618.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 13:59:55,379]\u001b[0m Trial 207 finished with value: 0.8301827514458061 and parameters: {'n_estimators': 900, 'eta': 0.07904598721722464, 'max_depth': 12, 'alpha': 0.38830000000000003, 'lambda': 6.895066924695666, 'max_bin': 306}. Best is trial 201 with value: 0.8335373394618.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:00:03,157]\u001b[0m Trial 208 finished with value: 0.8344074394770488 and parameters: {'n_estimators': 878, 'eta': 0.08183685998857713, 'max_depth': 12, 'alpha': 0.4253, 'lambda': 7.611693575958308, 'max_bin': 295}. Best is trial 208 with value: 0.8344074394770488.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:00:10,304]\u001b[0m Trial 209 finished with value: 0.8310867991241462 and parameters: {'n_estimators': 872, 'eta': 0.08266679317534632, 'max_depth': 12, 'alpha': 0.4176, 'lambda': 5.679454335646211, 'max_bin': 293}. Best is trial 208 with value: 0.8344074394770488.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:00:17,509]\u001b[0m Trial 210 finished with value: 0.8347611586500671 and parameters: {'n_estimators': 873, 'eta': 0.08225145923065898, 'max_depth': 12, 'alpha': 0.4278, 'lambda': 5.910431243078647, 'max_bin': 296}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:00:24,354]\u001b[0m Trial 211 finished with value: 0.8274974917710776 and parameters: {'n_estimators': 868, 'eta': 0.08202418978030297, 'max_depth': 12, 'alpha': 0.42010000000000003, 'lambda': 5.892699452134915, 'max_bin': 286}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:00:32,428]\u001b[0m Trial 212 finished with value: 0.8298700646503174 and parameters: {'n_estimators': 880, 'eta': 0.0835771853278622, 'max_depth': 12, 'alpha': 0.4375, 'lambda': 7.586600880549129, 'max_bin': 295}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:00:41,308]\u001b[0m Trial 213 finished with value: 0.828192390948814 and parameters: {'n_estimators': 883, 'eta': 0.07974863740056005, 'max_depth': 12, 'alpha': 0.4642, 'lambda': 5.86936253438303, 'max_bin': 291}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:00:49,175]\u001b[0m Trial 214 finished with value: 0.8283635548032494 and parameters: {'n_estimators': 897, 'eta': 0.08558060242731916, 'max_depth': 12, 'alpha': 0.4247, 'lambda': 6.775551560659146, 'max_bin': 300}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:00:57,933]\u001b[0m Trial 215 finished with value: 0.8301863425810232 and parameters: {'n_estimators': 866, 'eta': 0.07481526119982987, 'max_depth': 12, 'alpha': 0.402, 'lambda': 7.650714562053702, 'max_bin': 297}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:01:05,809]\u001b[0m Trial 216 finished with value: 0.8334949546790431 and parameters: {'n_estimators': 900, 'eta': 0.07825373254594283, 'max_depth': 12, 'alpha': 0.43770000000000003, 'lambda': 5.4593515711344, 'max_bin': 311}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:01:13,150]\u001b[0m Trial 217 finished with value: 0.8322161699446327 and parameters: {'n_estimators': 899, 'eta': 0.0806582029937965, 'max_depth': 12, 'alpha': 0.43160000000000004, 'lambda': 5.49581239117586, 'max_bin': 310}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:01:21,169]\u001b[0m Trial 218 finished with value: 0.8305447441935151 and parameters: {'n_estimators': 898, 'eta': 0.07566355838859838, 'max_depth': 12, 'alpha': 0.4933, 'lambda': 6.653840154968732, 'max_bin': 310}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:01:30,376]\u001b[0m Trial 219 finished with value: 0.8305627888378024 and parameters: {'n_estimators': 884, 'eta': 0.07197604140723385, 'max_depth': 12, 'alpha': 0.4471, 'lambda': 8.311239078538716, 'max_bin': 316}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:01:37,879]\u001b[0m Trial 220 finished with value: 0.8308382114374127 and parameters: {'n_estimators': 855, 'eta': 0.07929475092191217, 'max_depth': 12, 'alpha': 0.4762, 'lambda': 5.0978270179782434, 'max_bin': 309}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:01:45,602]\u001b[0m Trial 221 finished with value: 0.8300625758790263 and parameters: {'n_estimators': 871, 'eta': 0.08200324946207396, 'max_depth': 12, 'alpha': 0.4324, 'lambda': 6.0883004065014985, 'max_bin': 301}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:01:52,837]\u001b[0m Trial 222 finished with value: 0.8248704549409606 and parameters: {'n_estimators': 885, 'eta': 0.07795896607645976, 'max_depth': 12, 'alpha': 0.4092, 'lambda': 5.292824527682943, 'max_bin': 305}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:02:00,790]\u001b[0m Trial 223 finished with value: 0.8306932891567499 and parameters: {'n_estimators': 899, 'eta': 0.0837053990301056, 'max_depth': 12, 'alpha': 0.4456, 'lambda': 7.396035995358698, 'max_bin': 318}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:02:08,487]\u001b[0m Trial 224 finished with value: 0.8324563104281102 and parameters: {'n_estimators': 871, 'eta': 0.08052297031782776, 'max_depth': 12, 'alpha': 0.4181, 'lambda': 5.628062185750264, 'max_bin': 309}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:02:15,988]\u001b[0m Trial 225 finished with value: 0.8324321441359656 and parameters: {'n_estimators': 850, 'eta': 0.07965762728015015, 'max_depth': 12, 'alpha': 0.3947, 'lambda': 6.814652507302576, 'max_bin': 312}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:02:23,915]\u001b[0m Trial 226 finished with value: 0.8282807294545347 and parameters: {'n_estimators': 849, 'eta': 0.08083525071252927, 'max_depth': 12, 'alpha': 0.39580000000000004, 'lambda': 8.235844632550986, 'max_bin': 309}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:02:32,640]\u001b[0m Trial 227 finished with value: 0.8323769893125788 and parameters: {'n_estimators': 869, 'eta': 0.07675569951926112, 'max_depth': 12, 'alpha': 0.43170000000000003, 'lambda': 7.214128375319704, 'max_bin': 312}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:02:40,930]\u001b[0m Trial 228 finished with value: 0.8338370410508811 and parameters: {'n_estimators': 862, 'eta': 0.07676921849860464, 'max_depth': 12, 'alpha': 0.4334, 'lambda': 7.042124134724532, 'max_bin': 312}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:02:49,243]\u001b[0m Trial 229 finished with value: 0.8341694779654982 and parameters: {'n_estimators': 852, 'eta': 0.07608425320951939, 'max_depth': 12, 'alpha': 0.4567, 'lambda': 6.412317900966858, 'max_bin': 313}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:02:57,197]\u001b[0m Trial 230 finished with value: 0.8329783987717576 and parameters: {'n_estimators': 846, 'eta': 0.07620305923917263, 'max_depth': 12, 'alpha': 0.46280000000000004, 'lambda': 6.3935295347259435, 'max_bin': 313}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:03:05,017]\u001b[0m Trial 231 finished with value: 0.8298941687497938 and parameters: {'n_estimators': 845, 'eta': 0.07634015309298123, 'max_depth': 12, 'alpha': 0.45180000000000003, 'lambda': 6.510471211689606, 'max_bin': 314}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:03:13,066]\u001b[0m Trial 232 finished with value: 0.8292754830411433 and parameters: {'n_estimators': 860, 'eta': 0.07295381800761484, 'max_depth': 12, 'alpha': 0.46480000000000005, 'lambda': 5.3563874702487215, 'max_bin': 314}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:03:21,299]\u001b[0m Trial 233 finished with value: 0.8281126242947169 and parameters: {'n_estimators': 845, 'eta': 0.07734195355825062, 'max_depth': 12, 'alpha': 0.4379, 'lambda': 6.5360963943760355, 'max_bin': 312}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:03:29,241]\u001b[0m Trial 234 finished with value: 0.830685503077692 and parameters: {'n_estimators': 871, 'eta': 0.07522058898620383, 'max_depth': 12, 'alpha': 0.5007, 'lambda': 4.686009878042976, 'max_bin': 319}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:03:37,369]\u001b[0m Trial 235 finished with value: 0.83108773853378 and parameters: {'n_estimators': 869, 'eta': 0.07873391383926766, 'max_depth': 12, 'alpha': 0.4295, 'lambda': 5.683195487069371, 'max_bin': 319}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:03:45,613]\u001b[0m Trial 236 finished with value: 0.8290858204674745 and parameters: {'n_estimators': 842, 'eta': 0.07764090486895354, 'max_depth': 12, 'alpha': 0.4811, 'lambda': 6.532430209872562, 'max_bin': 302}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:03:53,417]\u001b[0m Trial 237 finished with value: 0.8333996663760004 and parameters: {'n_estimators': 858, 'eta': 0.07949798946116203, 'max_depth': 12, 'alpha': 0.4572, 'lambda': 7.9942722172240614, 'max_bin': 312}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:04:01,364]\u001b[0m Trial 238 finished with value: 0.8278835684387292 and parameters: {'n_estimators': 862, 'eta': 0.08016175639223112, 'max_depth': 12, 'alpha': 0.47490000000000004, 'lambda': 7.952477268380145, 'max_bin': 312}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:04:09,807]\u001b[0m Trial 239 finished with value: 0.8325360601008851 and parameters: {'n_estimators': 882, 'eta': 0.07690427955922301, 'max_depth': 12, 'alpha': 0.5168, 'lambda': 6.9627709897104735, 'max_bin': 312}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:04:17,891]\u001b[0m Trial 240 finished with value: 0.8293238520615491 and parameters: {'n_estimators': 878, 'eta': 0.07577181484360658, 'max_depth': 12, 'alpha': 0.5397000000000001, 'lambda': 6.962672627693617, 'max_bin': 316}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:04:26,398]\u001b[0m Trial 241 finished with value: 0.8339833297284478 and parameters: {'n_estimators': 880, 'eta': 0.07872911693882835, 'max_depth': 12, 'alpha': 0.5172, 'lambda': 8.941165603722528, 'max_bin': 311}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:04:34,167]\u001b[0m Trial 242 finished with value: 0.8298915249247159 and parameters: {'n_estimators': 880, 'eta': 0.07859191437171296, 'max_depth': 12, 'alpha': 0.523, 'lambda': 6.155663058597818, 'max_bin': 312}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:04:43,042]\u001b[0m Trial 243 finished with value: 0.8327348106564868 and parameters: {'n_estimators': 860, 'eta': 0.07703088599412355, 'max_depth': 12, 'alpha': 0.4968, 'lambda': 8.84715853339791, 'max_bin': 307}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:04:52,267]\u001b[0m Trial 244 finished with value: 0.8293517082763204 and parameters: {'n_estimators': 857, 'eta': 0.07348004847208367, 'max_depth': 12, 'alpha': 0.5535, 'lambda': 8.511416469529337, 'max_bin': 320}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:05:00,966]\u001b[0m Trial 245 finished with value: 0.832780681497295 and parameters: {'n_estimators': 865, 'eta': 0.08110888815831926, 'max_depth': 12, 'alpha': 0.5021, 'lambda': 8.872946689868959, 'max_bin': 309}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:05:09,621]\u001b[0m Trial 246 finished with value: 0.8274061824314295 and parameters: {'n_estimators': 835, 'eta': 0.07640981750276399, 'max_depth': 12, 'alpha': 0.5063, 'lambda': 8.87487041914786, 'max_bin': 313}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:05:17,716]\u001b[0m Trial 247 finished with value: 0.8294724528223758 and parameters: {'n_estimators': 861, 'eta': 0.08500256874444621, 'max_depth': 12, 'alpha': 0.5177, 'lambda': 9.064383196055728, 'max_bin': 305}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:05:25,866]\u001b[0m Trial 248 finished with value: 0.832702680512654 and parameters: {'n_estimators': 830, 'eta': 0.07839488845939566, 'max_depth': 12, 'alpha': 0.4979, 'lambda': 8.25870615443242, 'max_bin': 322}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:05:34,196]\u001b[0m Trial 249 finished with value: 0.8300705139504773 and parameters: {'n_estimators': 826, 'eta': 0.078072600820722, 'max_depth': 12, 'alpha': 0.4949, 'lambda': 7.982786326993704, 'max_bin': 321}. Best is trial 210 with value: 0.8347611586500671.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8348\n",
      "\tBest params:\n",
      "\t\tn_estimators: 873\n",
      "\t\teta: 0.08225145923065898\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.4278\n",
      "\t\tlambda: 5.910431243078647\n",
      "\t\tmax_bin: 296\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  393.000000  398.000000  417.000000  412.000000   \n",
      "1                    TN  358.000000  363.000000  347.000000  346.000000   \n",
      "2                    FP   76.000000   80.000000   77.000000   83.000000   \n",
      "3                    FN   72.000000   58.000000   58.000000   58.000000   \n",
      "4              Accuracy    0.835373    0.846496    0.849833    0.843159   \n",
      "5             Precision    0.837953    0.832636    0.844130    0.832323   \n",
      "6           Sensitivity    0.845161    0.872807    0.877895    0.876596   \n",
      "7           Specificity    0.824900    0.819400    0.818400    0.806500   \n",
      "8              F1 score    0.841542    0.852248    0.860681    0.853886   \n",
      "9   F1 score (weighted)    0.835344    0.846350    0.849585    0.842837   \n",
      "10     F1 score (macro)    0.835123    0.846263    0.848917    0.842309   \n",
      "11    Balanced Accuracy    0.835023    0.846110    0.848145    0.841561   \n",
      "12                  MCC    0.670279    0.693543    0.698601    0.685935   \n",
      "13                  NPV    0.832600    0.862200    0.856800    0.856400   \n",
      "14              ROC_AUC    0.835023    0.846110    0.848145    0.841561   \n",
      "\n",
      "          Set4  \n",
      "0   400.000000  \n",
      "1   348.000000  \n",
      "2   103.000000  \n",
      "3    48.000000  \n",
      "4     0.832036  \n",
      "5     0.795229  \n",
      "6     0.892857  \n",
      "7     0.771600  \n",
      "8     0.841220  \n",
      "9     0.831439  \n",
      "10    0.831472  \n",
      "11    0.832238  \n",
      "12    0.669229  \n",
      "13    0.878800  \n",
      "14    0.832238  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_xgb_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_xgb_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_xgb_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_xgb_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_xgb_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_xgb_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_xgb_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_xgb_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_xgb_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_xgb_4)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:05:43,994]\u001b[0m Trial 250 finished with value: 0.8359025048349933 and parameters: {'n_estimators': 851, 'eta': 0.0811146584963724, 'max_depth': 12, 'alpha': 0.45830000000000004, 'lambda': 7.1117250074614855, 'max_bin': 315}. Best is trial 250 with value: 0.8359025048349933.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:05:52,525]\u001b[0m Trial 251 finished with value: 0.8336126423720136 and parameters: {'n_estimators': 845, 'eta': 0.08147055644672636, 'max_depth': 12, 'alpha': 0.4575, 'lambda': 7.950404953338693, 'max_bin': 318}. Best is trial 250 with value: 0.8359025048349933.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:06:01,318]\u001b[0m Trial 252 finished with value: 0.8346075033587853 and parameters: {'n_estimators': 831, 'eta': 0.0810942780866794, 'max_depth': 12, 'alpha': 0.4792, 'lambda': 7.101325842535345, 'max_bin': 316}. Best is trial 250 with value: 0.8359025048349933.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:06:10,083]\u001b[0m Trial 253 finished with value: 0.8387114991415864 and parameters: {'n_estimators': 827, 'eta': 0.0823490983362348, 'max_depth': 12, 'alpha': 0.4636, 'lambda': 7.27769590431655, 'max_bin': 316}. Best is trial 253 with value: 0.8387114991415864.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:06:18,395]\u001b[0m Trial 254 finished with value: 0.8343727639107168 and parameters: {'n_estimators': 822, 'eta': 0.08743018791481645, 'max_depth': 12, 'alpha': 0.4667, 'lambda': 6.989738242126741, 'max_bin': 316}. Best is trial 253 with value: 0.8387114991415864.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:06:27,328]\u001b[0m Trial 255 finished with value: 0.8348560153373773 and parameters: {'n_estimators': 816, 'eta': 0.0860549770624014, 'max_depth': 12, 'alpha': 0.4635, 'lambda': 7.264347534638146, 'max_bin': 314}. Best is trial 253 with value: 0.8387114991415864.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:06:35,950]\u001b[0m Trial 256 finished with value: 0.8339278478371084 and parameters: {'n_estimators': 834, 'eta': 0.08690097886383721, 'max_depth': 12, 'alpha': 0.4781, 'lambda': 7.89489985611248, 'max_bin': 315}. Best is trial 253 with value: 0.8387114991415864.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:06:44,827]\u001b[0m Trial 257 finished with value: 0.836474573403857 and parameters: {'n_estimators': 818, 'eta': 0.08643320253179808, 'max_depth': 12, 'alpha': 0.4655, 'lambda': 8.112697443069818, 'max_bin': 317}. Best is trial 253 with value: 0.8387114991415864.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:06:53,101]\u001b[0m Trial 258 finished with value: 0.8336326080639024 and parameters: {'n_estimators': 821, 'eta': 0.08707198570354074, 'max_depth': 12, 'alpha': 0.46790000000000004, 'lambda': 8.047066143641182, 'max_bin': 317}. Best is trial 253 with value: 0.8387114991415864.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:07:01,670]\u001b[0m Trial 259 finished with value: 0.8317184516367213 and parameters: {'n_estimators': 811, 'eta': 0.08723294115557725, 'max_depth': 12, 'alpha': 0.48200000000000004, 'lambda': 8.196704732238086, 'max_bin': 317}. Best is trial 253 with value: 0.8387114991415864.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:07:10,341]\u001b[0m Trial 260 finished with value: 0.8348514224101059 and parameters: {'n_estimators': 823, 'eta': 0.08973740327541015, 'max_depth': 12, 'alpha': 0.4647, 'lambda': 9.032573636050087, 'max_bin': 322}. Best is trial 253 with value: 0.8387114991415864.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:07:19,358]\u001b[0m Trial 261 finished with value: 0.8396177830633036 and parameters: {'n_estimators': 822, 'eta': 0.08701851050696605, 'max_depth': 12, 'alpha': 0.4616, 'lambda': 9.108381395969195, 'max_bin': 326}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:07:28,389]\u001b[0m Trial 262 finished with value: 0.8350129285609386 and parameters: {'n_estimators': 819, 'eta': 0.09009302869518031, 'max_depth': 12, 'alpha': 0.4635, 'lambda': 9.359469502399632, 'max_bin': 326}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:07:36,778]\u001b[0m Trial 263 finished with value: 0.8324825360675817 and parameters: {'n_estimators': 822, 'eta': 0.08969219928863667, 'max_depth': 12, 'alpha': 0.4661, 'lambda': 9.206456453609027, 'max_bin': 325}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:07:45,584]\u001b[0m Trial 264 finished with value: 0.8330682509053202 and parameters: {'n_estimators': 800, 'eta': 0.0873945122663206, 'max_depth': 12, 'alpha': 0.4577, 'lambda': 8.997247428032995, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:07:54,150]\u001b[0m Trial 265 finished with value: 0.8349575660778497 and parameters: {'n_estimators': 805, 'eta': 0.08810351284087482, 'max_depth': 12, 'alpha': 0.4585, 'lambda': 9.663758063596072, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:08:02,566]\u001b[0m Trial 266 finished with value: 0.8319944382213255 and parameters: {'n_estimators': 794, 'eta': 0.08765339716856892, 'max_depth': 12, 'alpha': 0.46130000000000004, 'lambda': 9.787113083442666, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:08:10,779]\u001b[0m Trial 267 finished with value: 0.8322881339783373 and parameters: {'n_estimators': 809, 'eta': 0.09086859067094047, 'max_depth': 12, 'alpha': 0.4642, 'lambda': 7.793697721090136, 'max_bin': 324}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:08:19,759]\u001b[0m Trial 268 finished with value: 0.8341659894345961 and parameters: {'n_estimators': 821, 'eta': 0.08593127198212148, 'max_depth': 12, 'alpha': 0.4536, 'lambda': 7.85225690466116, 'max_bin': 328}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:08:28,363]\u001b[0m Trial 269 finished with value: 0.8362376673850964 and parameters: {'n_estimators': 797, 'eta': 0.08634804978656106, 'max_depth': 12, 'alpha': 0.45170000000000005, 'lambda': 9.497884829286994, 'max_bin': 328}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:08:36,331]\u001b[0m Trial 270 finished with value: 0.8339190927519731 and parameters: {'n_estimators': 816, 'eta': 0.0928383408274059, 'max_depth': 12, 'alpha': 0.48210000000000003, 'lambda': 8.042807893650021, 'max_bin': 329}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:08:44,106]\u001b[0m Trial 271 finished with value: 0.8292213428060119 and parameters: {'n_estimators': 822, 'eta': 0.09306881688741403, 'max_depth': 12, 'alpha': 0.4802, 'lambda': 7.838551084931373, 'max_bin': 330}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:08:53,124]\u001b[0m Trial 272 finished with value: 0.8327084109563352 and parameters: {'n_estimators': 821, 'eta': 0.08478617047728855, 'max_depth': 12, 'alpha': 0.44920000000000004, 'lambda': 9.591729101938581, 'max_bin': 324}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:09:02,108]\u001b[0m Trial 273 finished with value: 0.8356592229261087 and parameters: {'n_estimators': 793, 'eta': 0.08875984345502336, 'max_depth': 12, 'alpha': 0.4778, 'lambda': 7.635811246243545, 'max_bin': 319}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:09:10,726]\u001b[0m Trial 274 finished with value: 0.8364693425332863 and parameters: {'n_estimators': 804, 'eta': 0.08940936123466275, 'max_depth': 12, 'alpha': 0.4797, 'lambda': 7.977631050046311, 'max_bin': 320}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:09:19,172]\u001b[0m Trial 275 finished with value: 0.8330974261891765 and parameters: {'n_estimators': 788, 'eta': 0.08919670334941777, 'max_depth': 12, 'alpha': 0.4844, 'lambda': 8.306407100487567, 'max_bin': 320}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:09:27,331]\u001b[0m Trial 276 finished with value: 0.8330927001156303 and parameters: {'n_estimators': 806, 'eta': 0.09150637613646936, 'max_depth': 12, 'alpha': 0.4798, 'lambda': 7.543287276386064, 'max_bin': 329}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:09:36,729]\u001b[0m Trial 277 finished with value: 0.8334777306588279 and parameters: {'n_estimators': 784, 'eta': 0.08546640129710327, 'max_depth': 12, 'alpha': 0.4753, 'lambda': 9.47662450515629, 'max_bin': 324}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:09:45,690]\u001b[0m Trial 278 finished with value: 0.8330910118566134 and parameters: {'n_estimators': 813, 'eta': 0.08869962722628162, 'max_depth': 12, 'alpha': 0.44970000000000004, 'lambda': 8.562078200979494, 'max_bin': 320}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:09:54,277]\u001b[0m Trial 279 finished with value: 0.8319812089412881 and parameters: {'n_estimators': 799, 'eta': 0.08670594596672673, 'max_depth': 12, 'alpha': 0.44630000000000003, 'lambda': 7.730280781739328, 'max_bin': 318}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:10:03,150]\u001b[0m Trial 280 finished with value: 0.8342314184186155 and parameters: {'n_estimators': 823, 'eta': 0.09524551156938785, 'max_depth': 12, 'alpha': 0.4877, 'lambda': 9.787906695394224, 'max_bin': 332}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:10:11,518]\u001b[0m Trial 281 finished with value: 0.8343577698958586 and parameters: {'n_estimators': 831, 'eta': 0.09430608468024358, 'max_depth': 12, 'alpha': 0.48650000000000004, 'lambda': 10.46060952647452, 'max_bin': 330}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:10:20,300]\u001b[0m Trial 282 finished with value: 0.8337706785102356 and parameters: {'n_estimators': 821, 'eta': 0.0943026801478301, 'max_depth': 12, 'alpha': 0.4878, 'lambda': 10.425217390289442, 'max_bin': 332}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:10:30,082]\u001b[0m Trial 283 finished with value: 0.8311441902850396 and parameters: {'n_estimators': 825, 'eta': 0.0952440653890543, 'max_depth': 12, 'alpha': 0.5086, 'lambda': 10.396087314968234, 'max_bin': 333}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:10:39,061]\u001b[0m Trial 284 finished with value: 0.8332944490586701 and parameters: {'n_estimators': 813, 'eta': 0.09360047034551042, 'max_depth': 12, 'alpha': 0.48400000000000004, 'lambda': 10.099641989900089, 'max_bin': 330}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:10:47,805]\u001b[0m Trial 285 finished with value: 0.8362028485478256 and parameters: {'n_estimators': 797, 'eta': 0.09186890337875878, 'max_depth': 12, 'alpha': 0.5363, 'lambda': 10.765866694398696, 'max_bin': 331}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:10:56,303]\u001b[0m Trial 286 finished with value: 0.8364672474154435 and parameters: {'n_estimators': 785, 'eta': 0.09660019491309124, 'max_depth': 12, 'alpha': 0.5272, 'lambda': 10.984374072885315, 'max_bin': 333}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:11:05,590]\u001b[0m Trial 287 finished with value: 0.832288462122748 and parameters: {'n_estimators': 779, 'eta': 0.09154351836933282, 'max_depth': 12, 'alpha': 0.5433, 'lambda': 10.732862996068455, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:11:14,190]\u001b[0m Trial 288 finished with value: 0.8378642846968205 and parameters: {'n_estimators': 789, 'eta': 0.09583145953648856, 'max_depth': 12, 'alpha': 0.5325, 'lambda': 9.48678917251358, 'max_bin': 334}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:11:22,684]\u001b[0m Trial 289 finished with value: 0.8334386687490136 and parameters: {'n_estimators': 792, 'eta': 0.09687083210966722, 'max_depth': 12, 'alpha': 0.5737, 'lambda': 9.604213708216003, 'max_bin': 336}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:11:31,797]\u001b[0m Trial 290 finished with value: 0.8343008447506225 and parameters: {'n_estimators': 779, 'eta': 0.09017877131880543, 'max_depth': 12, 'alpha': 0.5326000000000001, 'lambda': 11.15411212283881, 'max_bin': 332}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:11:41,633]\u001b[0m Trial 291 finished with value: 0.8355955961252792 and parameters: {'n_estimators': 799, 'eta': 0.0891205718978183, 'max_depth': 12, 'alpha': 0.5313, 'lambda': 11.183901956937637, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:11:51,072]\u001b[0m Trial 292 finished with value: 0.8319676892003942 and parameters: {'n_estimators': 770, 'eta': 0.08996275581223122, 'max_depth': 12, 'alpha': 0.523, 'lambda': 11.510876617685327, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:11:59,664]\u001b[0m Trial 293 finished with value: 0.8353093819410647 and parameters: {'n_estimators': 781, 'eta': 0.09576269742846491, 'max_depth': 12, 'alpha': 0.5467000000000001, 'lambda': 11.305025503602472, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:12:08,565]\u001b[0m Trial 294 finished with value: 0.8327580820919744 and parameters: {'n_estimators': 764, 'eta': 0.09626586967785153, 'max_depth': 12, 'alpha': 0.5374, 'lambda': 11.625435315056123, 'max_bin': 337}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:12:17,550]\u001b[0m Trial 295 finished with value: 0.8347501526806255 and parameters: {'n_estimators': 784, 'eta': 0.09019342425707311, 'max_depth': 12, 'alpha': 0.5634, 'lambda': 10.559151764300534, 'max_bin': 332}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:12:26,652]\u001b[0m Trial 296 finished with value: 0.8346976454186494 and parameters: {'n_estimators': 784, 'eta': 0.09169665602635282, 'max_depth': 12, 'alpha': 0.5720000000000001, 'lambda': 10.829847830079531, 'max_bin': 339}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:12:35,291]\u001b[0m Trial 297 finished with value: 0.8336800207127496 and parameters: {'n_estimators': 779, 'eta': 0.09154896944484865, 'max_depth': 12, 'alpha': 0.5671, 'lambda': 10.739646229245611, 'max_bin': 341}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:12:44,676]\u001b[0m Trial 298 finished with value: 0.8381544313556413 and parameters: {'n_estimators': 793, 'eta': 0.089764680433503, 'max_depth': 12, 'alpha': 0.6094, 'lambda': 12.324711475021537, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:12:53,577]\u001b[0m Trial 299 finished with value: 0.8356772146345535 and parameters: {'n_estimators': 790, 'eta': 0.08970627120973085, 'max_depth': 12, 'alpha': 0.6108, 'lambda': 11.95415558773838, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8396\n",
      "\tBest params:\n",
      "\t\tn_estimators: 822\n",
      "\t\teta: 0.08701851050696605\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.4616\n",
      "\t\tlambda: 9.108381395969195\n",
      "\t\tmax_bin: 326\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  393.000000  398.000000  417.000000  412.000000   \n",
      "1                    TN  358.000000  363.000000  347.000000  346.000000   \n",
      "2                    FP   76.000000   80.000000   77.000000   83.000000   \n",
      "3                    FN   72.000000   58.000000   58.000000   58.000000   \n",
      "4              Accuracy    0.835373    0.846496    0.849833    0.843159   \n",
      "5             Precision    0.837953    0.832636    0.844130    0.832323   \n",
      "6           Sensitivity    0.845161    0.872807    0.877895    0.876596   \n",
      "7           Specificity    0.824900    0.819400    0.818400    0.806500   \n",
      "8              F1 score    0.841542    0.852248    0.860681    0.853886   \n",
      "9   F1 score (weighted)    0.835344    0.846350    0.849585    0.842837   \n",
      "10     F1 score (macro)    0.835123    0.846263    0.848917    0.842309   \n",
      "11    Balanced Accuracy    0.835023    0.846110    0.848145    0.841561   \n",
      "12                  MCC    0.670279    0.693543    0.698601    0.685935   \n",
      "13                  NPV    0.832600    0.862200    0.856800    0.856400   \n",
      "14              ROC_AUC    0.835023    0.846110    0.848145    0.841561   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   400.000000  395.000000  \n",
      "1   348.000000  352.000000  \n",
      "2   103.000000   81.000000  \n",
      "3    48.000000   71.000000  \n",
      "4     0.832036    0.830923  \n",
      "5     0.795229    0.829832  \n",
      "6     0.892857    0.847639  \n",
      "7     0.771600    0.812900  \n",
      "8     0.841220    0.838641  \n",
      "9     0.831439    0.830833  \n",
      "10    0.831472    0.830536  \n",
      "11    0.832238    0.830286  \n",
      "12    0.669229    0.661277  \n",
      "13    0.878800    0.832200  \n",
      "14    0.832238    0.830286  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_xgb_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_xgb_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_xgb_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_xgb_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_xgb_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_xgb_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_xgb_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_xgb_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_xgb_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_xgb_5)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:13:03,670]\u001b[0m Trial 300 finished with value: 0.8348891893264909 and parameters: {'n_estimators': 758, 'eta': 0.08882433952427383, 'max_depth': 12, 'alpha': 0.6131, 'lambda': 12.36995431751809, 'max_bin': 343}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:13:13,050]\u001b[0m Trial 301 finished with value: 0.8320132196680703 and parameters: {'n_estimators': 796, 'eta': 0.08950572461389172, 'max_depth': 12, 'alpha': 0.6089, 'lambda': 13.214835951822735, 'max_bin': 340}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:13:21,709]\u001b[0m Trial 302 finished with value: 0.8353574248669122 and parameters: {'n_estimators': 756, 'eta': 0.0884154806437089, 'max_depth': 12, 'alpha': 0.6556000000000001, 'lambda': 11.057140052693253, 'max_bin': 343}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:13:30,650]\u001b[0m Trial 303 finished with value: 0.8311452777614615 and parameters: {'n_estimators': 765, 'eta': 0.09276206324174659, 'max_depth': 12, 'alpha': 0.6549, 'lambda': 12.033345925934144, 'max_bin': 345}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:13:39,346]\u001b[0m Trial 304 finished with value: 0.8350749437868762 and parameters: {'n_estimators': 755, 'eta': 0.08986503107537412, 'max_depth': 12, 'alpha': 0.6252, 'lambda': 12.735399827633636, 'max_bin': 340}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:13:48,148]\u001b[0m Trial 305 finished with value: 0.8320532533122307 and parameters: {'n_estimators': 753, 'eta': 0.08887228372198716, 'max_depth': 12, 'alpha': 0.6312, 'lambda': 12.594405555898286, 'max_bin': 339}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:13:56,890]\u001b[0m Trial 306 finished with value: 0.8325760104442217 and parameters: {'n_estimators': 755, 'eta': 0.09158574020597673, 'max_depth': 12, 'alpha': 0.6211, 'lambda': 12.204737821655375, 'max_bin': 343}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:14:06,007]\u001b[0m Trial 307 finished with value: 0.8322627098537005 and parameters: {'n_estimators': 787, 'eta': 0.0891415040182776, 'max_depth': 12, 'alpha': 0.6817000000000001, 'lambda': 11.330684224766303, 'max_bin': 337}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:14:14,121]\u001b[0m Trial 308 finished with value: 0.8323339276461773 and parameters: {'n_estimators': 776, 'eta': 0.0975703032528873, 'max_depth': 12, 'alpha': 0.5953, 'lambda': 12.274530226179657, 'max_bin': 346}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:14:22,932]\u001b[0m Trial 309 finished with value: 0.8328390891342712 and parameters: {'n_estimators': 796, 'eta': 0.0916431516727098, 'max_depth': 12, 'alpha': 0.5864, 'lambda': 13.420498612455372, 'max_bin': 337}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:14:32,656]\u001b[0m Trial 310 finished with value: 0.835406800554588 and parameters: {'n_estimators': 767, 'eta': 0.0840905503418594, 'max_depth': 12, 'alpha': 0.6415000000000001, 'lambda': 10.966061776433847, 'max_bin': 343}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:14:42,317]\u001b[0m Trial 311 finished with value: 0.8340954723345915 and parameters: {'n_estimators': 763, 'eta': 0.08846964564743814, 'max_depth': 12, 'alpha': 0.6447, 'lambda': 11.123302647555178, 'max_bin': 342}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:14:51,419]\u001b[0m Trial 312 finished with value: 0.8336186730781263 and parameters: {'n_estimators': 743, 'eta': 0.08460936081476889, 'max_depth': 12, 'alpha': 0.6648000000000001, 'lambda': 11.766964070097632, 'max_bin': 348}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:15:00,108]\u001b[0m Trial 313 finished with value: 0.8320279650119552 and parameters: {'n_estimators': 781, 'eta': 0.090301513225649, 'max_depth': 12, 'alpha': 0.6042000000000001, 'lambda': 14.216595927951117, 'max_bin': 334}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:15:09,532]\u001b[0m Trial 314 finished with value: 0.8347707202013523 and parameters: {'n_estimators': 795, 'eta': 0.08576128440158709, 'max_depth': 12, 'alpha': 0.5629000000000001, 'lambda': 13.065230983900287, 'max_bin': 336}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:15:19,226]\u001b[0m Trial 315 finished with value: 0.8372971673837954 and parameters: {'n_estimators': 799, 'eta': 0.08649760762173794, 'max_depth': 12, 'alpha': 0.6216, 'lambda': 12.46587484101693, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:15:28,497]\u001b[0m Trial 316 finished with value: 0.8327852115237265 and parameters: {'n_estimators': 799, 'eta': 0.08561063930196137, 'max_depth': 12, 'alpha': 0.6218, 'lambda': 13.109738493802478, 'max_bin': 343}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:15:37,877]\u001b[0m Trial 317 finished with value: 0.8297906355212505 and parameters: {'n_estimators': 762, 'eta': 0.08728657411095778, 'max_depth': 12, 'alpha': 0.649, 'lambda': 12.761441103725781, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:15:46,976]\u001b[0m Trial 318 finished with value: 0.8317666769365631 and parameters: {'n_estimators': 804, 'eta': 0.08418338803889487, 'max_depth': 12, 'alpha': 0.613, 'lambda': 11.895037694222019, 'max_bin': 350}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:15:56,296]\u001b[0m Trial 319 finished with value: 0.8314625351834997 and parameters: {'n_estimators': 800, 'eta': 0.08635739286581831, 'max_depth': 12, 'alpha': 0.6882, 'lambda': 13.500740622559048, 'max_bin': 334}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:16:04,818]\u001b[0m Trial 320 finished with value: 0.829551378305444 and parameters: {'n_estimators': 747, 'eta': 0.08823706211474817, 'max_depth': 12, 'alpha': 0.6251, 'lambda': 12.114307648610199, 'max_bin': 326}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:16:13,721]\u001b[0m Trial 321 finished with value: 0.8320836074912524 and parameters: {'n_estimators': 772, 'eta': 0.08354007459936334, 'max_depth': 12, 'alpha': 0.5943, 'lambda': 11.16127470638206, 'max_bin': 344}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:16:22,680]\u001b[0m Trial 322 finished with value: 0.8319944742955686 and parameters: {'n_estimators': 802, 'eta': 0.09357780658967162, 'max_depth': 12, 'alpha': 0.7133, 'lambda': 13.036933009269354, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:16:31,170]\u001b[0m Trial 323 finished with value: 0.8342366420231097 and parameters: {'n_estimators': 767, 'eta': 0.08830981179131037, 'max_depth': 12, 'alpha': 0.5492, 'lambda': 10.056949608946622, 'max_bin': 324}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:16:40,566]\u001b[0m Trial 324 finished with value: 0.8322866998023566 and parameters: {'n_estimators': 789, 'eta': 0.08444167574753153, 'max_depth': 12, 'alpha': 0.6435000000000001, 'lambda': 12.059948282011446, 'max_bin': 328}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:16:49,861]\u001b[0m Trial 325 finished with value: 0.8344840605240449 and parameters: {'n_estimators': 804, 'eta': 0.08558112846704766, 'max_depth': 12, 'alpha': 0.6686000000000001, 'lambda': 14.353608983180377, 'max_bin': 340}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:16:57,796]\u001b[0m Trial 326 finished with value: 0.8312390020356052 and parameters: {'n_estimators': 757, 'eta': 0.09583754848758827, 'max_depth': 12, 'alpha': 0.6343, 'lambda': 11.075942581250695, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:17:06,182]\u001b[0m Trial 327 finished with value: 0.8353653350915232 and parameters: {'n_estimators': 783, 'eta': 0.09033085016008259, 'max_depth': 12, 'alpha': 0.5493, 'lambda': 9.758973637859537, 'max_bin': 347}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:17:14,863]\u001b[0m Trial 328 finished with value: 0.8309344398605714 and parameters: {'n_estimators': 788, 'eta': 0.09000439921015292, 'max_depth': 12, 'alpha': 0.5840000000000001, 'lambda': 9.749393804975309, 'max_bin': 348}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:17:23,311]\u001b[0m Trial 329 finished with value: 0.8316612976373854 and parameters: {'n_estimators': 733, 'eta': 0.09324210474788992, 'max_depth': 12, 'alpha': 0.5542, 'lambda': 10.302201985608948, 'max_bin': 343}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:17:31,741]\u001b[0m Trial 330 finished with value: 0.833628667691527 and parameters: {'n_estimators': 806, 'eta': 0.0988316728362902, 'max_depth': 12, 'alpha': 0.606, 'lambda': 11.52274983966618, 'max_bin': 331}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:17:41,160]\u001b[0m Trial 331 finished with value: 0.8346272289638199 and parameters: {'n_estimators': 774, 'eta': 0.08770373293739328, 'max_depth': 12, 'alpha': 0.5604, 'lambda': 12.55117329675851, 'max_bin': 351}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:17:49,209]\u001b[0m Trial 332 finished with value: 0.8320448205843066 and parameters: {'n_estimators': 793, 'eta': 0.09198957961463798, 'max_depth': 12, 'alpha': 0.5363, 'lambda': 10.05046570032543, 'max_bin': 324}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:17:57,991]\u001b[0m Trial 333 finished with value: 0.8345062955641629 and parameters: {'n_estimators': 772, 'eta': 0.09037531453205525, 'max_depth': 12, 'alpha': 0.5894, 'lambda': 9.418105021566786, 'max_bin': 339}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:18:07,094]\u001b[0m Trial 334 finished with value: 0.8318233961438841 and parameters: {'n_estimators': 810, 'eta': 0.0865025183684894, 'max_depth': 12, 'alpha': 0.5322, 'lambda': 11.065630309836683, 'max_bin': 333}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:18:16,088]\u001b[0m Trial 335 finished with value: 0.8322126718671878 and parameters: {'n_estimators': 753, 'eta': 0.08955465042569563, 'max_depth': 12, 'alpha': 0.5715, 'lambda': 13.982651204686686, 'max_bin': 328}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:18:25,273]\u001b[0m Trial 336 finished with value: 0.835415212485886 and parameters: {'n_estimators': 788, 'eta': 0.08817117934280778, 'max_depth': 12, 'alpha': 0.5147, 'lambda': 12.631519243323176, 'max_bin': 345}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:18:33,956]\u001b[0m Trial 337 finished with value: 0.8320445532756029 and parameters: {'n_estimators': 779, 'eta': 0.09401190906542664, 'max_depth': 12, 'alpha': 0.6552, 'lambda': 9.45170710575438, 'max_bin': 346}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:18:42,279]\u001b[0m Trial 338 finished with value: 0.8303574903482346 and parameters: {'n_estimators': 743, 'eta': 0.08835285881046573, 'max_depth': 12, 'alpha': 0.5022, 'lambda': 10.65816135315181, 'max_bin': 344}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:18:51,775]\u001b[0m Trial 339 finished with value: 0.8323399330249114 and parameters: {'n_estimators': 806, 'eta': 0.0916995834591378, 'max_depth': 12, 'alpha': 0.5196000000000001, 'lambda': 12.300074425680757, 'max_bin': 353}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:19:00,307]\u001b[0m Trial 340 finished with value: 0.8317270700108829 and parameters: {'n_estimators': 358, 'eta': 0.09491451959262714, 'max_depth': 12, 'alpha': 0.5106, 'lambda': 11.38530065671837, 'max_bin': 324}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:19:08,655]\u001b[0m Trial 341 finished with value: 0.8303773520544263 and parameters: {'n_estimators': 766, 'eta': 0.08801806398493578, 'max_depth': 12, 'alpha': 0.6343, 'lambda': 9.29806527128245, 'max_bin': 340}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:19:17,041]\u001b[0m Trial 342 finished with value: 0.8298896611423832 and parameters: {'n_estimators': 790, 'eta': 0.09748337420960183, 'max_depth': 12, 'alpha': 0.6179, 'lambda': 10.331447655196321, 'max_bin': 329}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:19:25,418]\u001b[0m Trial 343 finished with value: 0.8345606876938287 and parameters: {'n_estimators': 455, 'eta': 0.09002673798715177, 'max_depth': 12, 'alpha': 0.5403, 'lambda': 9.096954418577615, 'max_bin': 347}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:19:34,179]\u001b[0m Trial 344 finished with value: 0.833020548422898 and parameters: {'n_estimators': 813, 'eta': 0.08655338556472768, 'max_depth': 12, 'alpha': 0.6689, 'lambda': 11.590576382269154, 'max_bin': 334}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:19:38,616]\u001b[0m Trial 345 finished with value: 0.8295821866993626 and parameters: {'n_estimators': 128, 'eta': 0.09263751530423422, 'max_depth': 12, 'alpha': 0.505, 'lambda': 9.921268720035103, 'max_bin': 324}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:19:48,464]\u001b[0m Trial 346 finished with value: 0.8336163439723592 and parameters: {'n_estimators': 778, 'eta': 0.0840490185958903, 'max_depth': 12, 'alpha': 0.5247, 'lambda': 12.096822168871809, 'max_bin': 340}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:20:00,604]\u001b[0m Trial 347 finished with value: 0.832345228968767 and parameters: {'n_estimators': 805, 'eta': 0.08943757778542324, 'max_depth': 12, 'alpha': 0.6055, 'lambda': 31.848696774513517, 'max_bin': 330}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:20:09,679]\u001b[0m Trial 348 finished with value: 0.8303771025716019 and parameters: {'n_estimators': 758, 'eta': 0.09133038619601189, 'max_depth': 12, 'alpha': 0.5065000000000001, 'lambda': 11.057191130841723, 'max_bin': 322}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:20:19,040]\u001b[0m Trial 349 finished with value: 0.8365414237264593 and parameters: {'n_estimators': 791, 'eta': 0.08796137398921744, 'max_depth': 12, 'alpha': 0.5831000000000001, 'lambda': 12.666941550341873, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8396\n",
      "\tBest params:\n",
      "\t\tn_estimators: 822\n",
      "\t\teta: 0.08701851050696605\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.4616\n",
      "\t\tlambda: 9.108381395969195\n",
      "\t\tmax_bin: 326\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  393.000000  398.000000  417.000000  412.000000   \n",
      "1                    TN  358.000000  363.000000  347.000000  346.000000   \n",
      "2                    FP   76.000000   80.000000   77.000000   83.000000   \n",
      "3                    FN   72.000000   58.000000   58.000000   58.000000   \n",
      "4              Accuracy    0.835373    0.846496    0.849833    0.843159   \n",
      "5             Precision    0.837953    0.832636    0.844130    0.832323   \n",
      "6           Sensitivity    0.845161    0.872807    0.877895    0.876596   \n",
      "7           Specificity    0.824900    0.819400    0.818400    0.806500   \n",
      "8              F1 score    0.841542    0.852248    0.860681    0.853886   \n",
      "9   F1 score (weighted)    0.835344    0.846350    0.849585    0.842837   \n",
      "10     F1 score (macro)    0.835123    0.846263    0.848917    0.842309   \n",
      "11    Balanced Accuracy    0.835023    0.846110    0.848145    0.841561   \n",
      "12                  MCC    0.670279    0.693543    0.698601    0.685935   \n",
      "13                  NPV    0.832600    0.862200    0.856800    0.856400   \n",
      "14              ROC_AUC    0.835023    0.846110    0.848145    0.841561   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   400.000000  395.000000  414.000000  \n",
      "1   348.000000  352.000000  341.000000  \n",
      "2   103.000000   81.000000   88.000000  \n",
      "3    48.000000   71.000000   56.000000  \n",
      "4     0.832036    0.830923    0.839822  \n",
      "5     0.795229    0.829832    0.824701  \n",
      "6     0.892857    0.847639    0.880851  \n",
      "7     0.771600    0.812900    0.794900  \n",
      "8     0.841220    0.838641    0.851852  \n",
      "9     0.831439    0.830833    0.839356  \n",
      "10    0.831472    0.830536    0.838759  \n",
      "11    0.832238    0.830286    0.837861  \n",
      "12    0.669229    0.661277    0.679672  \n",
      "13    0.878800    0.832200    0.858900  \n",
      "14    0.832238    0.830286    0.837861  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_xgb_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_xgb_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_xgb_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_xgb_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_xgb_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_xgb_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_xgb_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_xgb_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_xgb_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_xgb_6)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:20:28,734]\u001b[0m Trial 350 finished with value: 0.8281736526514413 and parameters: {'n_estimators': 785, 'eta': 0.08642941290831353, 'max_depth': 12, 'alpha': 0.5725, 'lambda': 13.047997013338506, 'max_bin': 342}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:20:36,929]\u001b[0m Trial 351 finished with value: 0.8273301336740868 and parameters: {'n_estimators': 768, 'eta': 0.08791555256333691, 'max_depth': 12, 'alpha': 0.5863, 'lambda': 13.691385799932831, 'max_bin': 350}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:20:45,712]\u001b[0m Trial 352 finished with value: 0.8307112930677418 and parameters: {'n_estimators': 730, 'eta': 0.08378359845121393, 'max_depth': 12, 'alpha': 0.5498000000000001, 'lambda': 12.429195636005186, 'max_bin': 345}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:20:53,071]\u001b[0m Trial 353 finished with value: 0.8296121564703492 and parameters: {'n_estimators': 785, 'eta': 0.096096760523859, 'max_depth': 12, 'alpha': 0.6278, 'lambda': 10.822235634281181, 'max_bin': 354}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:21:01,389]\u001b[0m Trial 354 finished with value: 0.8309205698156245 and parameters: {'n_estimators': 801, 'eta': 0.08605075021009313, 'max_depth': 12, 'alpha': 0.6014, 'lambda': 11.69449058159762, 'max_bin': 337}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:21:09,276]\u001b[0m Trial 355 finished with value: 0.8296716151295869 and parameters: {'n_estimators': 752, 'eta': 0.09310681157506058, 'max_depth': 12, 'alpha': 0.5504, 'lambda': 14.651968941834554, 'max_bin': 334}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:21:17,250]\u001b[0m Trial 356 finished with value: 0.8275105514255671 and parameters: {'n_estimators': 810, 'eta': 0.08769711308274511, 'max_depth': 12, 'alpha': 0.5245000000000001, 'lambda': 13.141030653425922, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:21:24,634]\u001b[0m Trial 357 finished with value: 0.8256612085598158 and parameters: {'n_estimators': 791, 'eta': 0.09091279976941614, 'max_depth': 12, 'alpha': 0.6461, 'lambda': 12.542714927597917, 'max_bin': 331}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:21:32,660]\u001b[0m Trial 358 finished with value: 0.8260188078155528 and parameters: {'n_estimators': 773, 'eta': 0.08390588797298099, 'max_depth': 12, 'alpha': 0.5865, 'lambda': 10.294820006741494, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:21:40,855]\u001b[0m Trial 359 finished with value: 0.829614540034757 and parameters: {'n_estimators': 810, 'eta': 0.088527495280685, 'max_depth': 12, 'alpha': 0.6298, 'lambda': 11.635471452629249, 'max_bin': 341}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:21:48,739]\u001b[0m Trial 360 finished with value: 0.8335606285083454 and parameters: {'n_estimators': 832, 'eta': 0.08591762279360785, 'max_depth': 12, 'alpha': 0.5365, 'lambda': 9.8704702832326, 'max_bin': 346}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:21:56,273]\u001b[0m Trial 361 finished with value: 0.8281972079920349 and parameters: {'n_estimators': 793, 'eta': 0.09492276220247539, 'max_depth': 12, 'alpha': 0.5671, 'lambda': 10.835676144509868, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:22:03,913]\u001b[0m Trial 362 finished with value: 0.8321291298736723 and parameters: {'n_estimators': 744, 'eta': 0.09080235712260745, 'max_depth': 12, 'alpha': 0.6124, 'lambda': 8.8784360311111, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:22:12,203]\u001b[0m Trial 363 finished with value: 0.8272341230165079 and parameters: {'n_estimators': 760, 'eta': 0.092712340549951, 'max_depth': 12, 'alpha': 0.4999, 'lambda': 13.586194986802573, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:22:19,939]\u001b[0m Trial 364 finished with value: 0.8271915114087243 and parameters: {'n_estimators': 774, 'eta': 0.08914821836571472, 'max_depth': 12, 'alpha': 0.5791000000000001, 'lambda': 12.061318817959668, 'max_bin': 332}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:22:27,988]\u001b[0m Trial 365 finished with value: 0.8271027970368422 and parameters: {'n_estimators': 813, 'eta': 0.08554572404194247, 'max_depth': 12, 'alpha': 0.6834, 'lambda': 10.46045204144757, 'max_bin': 348}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:22:35,707]\u001b[0m Trial 366 finished with value: 0.8319070962465892 and parameters: {'n_estimators': 793, 'eta': 0.09983215958809032, 'max_depth': 12, 'alpha': 0.5525, 'lambda': 12.699744637079483, 'max_bin': 341}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:22:45,581]\u001b[0m Trial 367 finished with value: 0.8259671057069549 and parameters: {'n_estimators': 829, 'eta': 0.08778527416190014, 'max_depth': 12, 'alpha': 0.5155000000000001, 'lambda': 25.852626387074878, 'max_bin': 325}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:22:52,932]\u001b[0m Trial 368 finished with value: 0.8332931215309181 and parameters: {'n_estimators': 779, 'eta': 0.09665018612934319, 'max_depth': 12, 'alpha': 0.6522, 'lambda': 9.445041850805858, 'max_bin': 332}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:23:00,822]\u001b[0m Trial 369 finished with value: 0.8258158442207387 and parameters: {'n_estimators': 811, 'eta': 0.08292473168962457, 'max_depth': 12, 'alpha': 0.6226, 'lambda': 11.4991542163574, 'max_bin': 352}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:23:07,773]\u001b[0m Trial 370 finished with value: 0.8312516853011853 and parameters: {'n_estimators': 835, 'eta': 0.09155101558183563, 'max_depth': 12, 'alpha': 0.49820000000000003, 'lambda': 8.655049267859791, 'max_bin': 321}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:23:14,952]\u001b[0m Trial 371 finished with value: 0.8282661504603357 and parameters: {'n_estimators': 795, 'eta': 0.09408404553289999, 'max_depth': 12, 'alpha': 0.6057, 'lambda': 10.863788081533313, 'max_bin': 344}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:23:24,542]\u001b[0m Trial 372 finished with value: 0.8296520330832287 and parameters: {'n_estimators': 760, 'eta': 0.08928209624413871, 'max_depth': 12, 'alpha': 0.5336000000000001, 'lambda': 23.990845681099458, 'max_bin': 337}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:23:32,523]\u001b[0m Trial 373 finished with value: 0.8316082530229378 and parameters: {'n_estimators': 739, 'eta': 0.08455217530053799, 'max_depth': 12, 'alpha': 0.5873, 'lambda': 9.86357474867663, 'max_bin': 358}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:23:40,555]\u001b[0m Trial 374 finished with value: 0.8318205641579997 and parameters: {'n_estimators': 775, 'eta': 0.08684830241973336, 'max_depth': 12, 'alpha': 0.49460000000000004, 'lambda': 11.76966914107693, 'max_bin': 329}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:23:48,529]\u001b[0m Trial 375 finished with value: 0.8299727607297159 and parameters: {'n_estimators': 816, 'eta': 0.09082148946179425, 'max_depth': 12, 'alpha': 0.6608, 'lambda': 12.66276026212742, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:23:58,314]\u001b[0m Trial 376 finished with value: 0.8305929536875251 and parameters: {'n_estimators': 795, 'eta': 0.0883052087710937, 'max_depth': 12, 'alpha': 0.5525, 'lambda': 27.840439043933483, 'max_bin': 320}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:24:05,896]\u001b[0m Trial 377 finished with value: 0.8290476937090631 and parameters: {'n_estimators': 835, 'eta': 0.09271155103422084, 'max_depth': 12, 'alpha': 0.5192, 'lambda': 8.856679669924308, 'max_bin': 342}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:24:13,085]\u001b[0m Trial 378 finished with value: 0.8292596762600652 and parameters: {'n_estimators': 783, 'eta': 0.09809562746184781, 'max_depth': 12, 'alpha': 0.46890000000000004, 'lambda': 10.423308553941862, 'max_bin': 325}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:24:22,363]\u001b[0m Trial 379 finished with value: 0.826833916160453 and parameters: {'n_estimators': 760, 'eta': 0.08522784694152535, 'max_depth': 12, 'alpha': 0.44370000000000004, 'lambda': 22.773415245090394, 'max_bin': 333}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:24:30,537]\u001b[0m Trial 380 finished with value: 0.8340782140811459 and parameters: {'n_estimators': 809, 'eta': 0.08997966912514294, 'max_depth': 12, 'alpha': 0.7059000000000001, 'lambda': 14.998876850143024, 'max_bin': 348}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:24:38,989]\u001b[0m Trial 381 finished with value: 0.8262426736609164 and parameters: {'n_estimators': 798, 'eta': 0.08711884672957673, 'max_depth': 12, 'alpha': 0.6414000000000001, 'lambda': 13.714112964351212, 'max_bin': 328}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:24:46,025]\u001b[0m Trial 382 finished with value: 0.8270381534463539 and parameters: {'n_estimators': 723, 'eta': 0.09495610180565725, 'max_depth': 12, 'alpha': 0.606, 'lambda': 9.587953683231204, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:24:54,284]\u001b[0m Trial 383 finished with value: 0.8299338037927356 and parameters: {'n_estimators': 820, 'eta': 0.0831086688514081, 'max_depth': 12, 'alpha': 0.5667, 'lambda': 11.30810852589784, 'max_bin': 321}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:25:01,726]\u001b[0m Trial 384 finished with value: 0.8290698143569296 and parameters: {'n_estimators': 315, 'eta': 0.08831445937216578, 'max_depth': 12, 'alpha': 0.5381, 'lambda': 8.660246857480878, 'max_bin': 343}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:25:09,454]\u001b[0m Trial 385 finished with value: 0.82731603294021 and parameters: {'n_estimators': 771, 'eta': 0.09237803929527443, 'max_depth': 12, 'alpha': 0.5059, 'lambda': 11.843274447097462, 'max_bin': 332}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:25:16,767]\u001b[0m Trial 386 finished with value: 0.830469279518514 and parameters: {'n_estimators': 747, 'eta': 0.09045564398863687, 'max_depth': 12, 'alpha': 0.4773, 'lambda': 10.293944764649279, 'max_bin': 326}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:25:24,907]\u001b[0m Trial 387 finished with value: 0.8310257883049651 and parameters: {'n_estimators': 786, 'eta': 0.0854675850627388, 'max_depth': 12, 'alpha': 0.5871000000000001, 'lambda': 12.246039746077978, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:25:32,257]\u001b[0m Trial 388 finished with value: 0.8313591109539891 and parameters: {'n_estimators': 252, 'eta': 0.08719175515309803, 'max_depth': 12, 'alpha': 0.4541, 'lambda': 11.12649893553895, 'max_bin': 354}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:25:40,312]\u001b[0m Trial 389 finished with value: 0.8287501925406395 and parameters: {'n_estimators': 804, 'eta': 0.08944708059385745, 'max_depth': 12, 'alpha': 0.6227, 'lambda': 9.483125888417149, 'max_bin': 321}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:25:48,017]\u001b[0m Trial 390 finished with value: 0.8296633641060909 and parameters: {'n_estimators': 836, 'eta': 0.09340663174055185, 'max_depth': 12, 'alpha': 0.5202, 'lambda': 12.650953176787434, 'max_bin': 346}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:25:55,931]\u001b[0m Trial 391 finished with value: 0.8247277713943688 and parameters: {'n_estimators': 764, 'eta': 0.08344134616909607, 'max_depth': 12, 'alpha': 0.4913, 'lambda': 10.747049234174732, 'max_bin': 331}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:26:04,570]\u001b[0m Trial 392 finished with value: 0.8294873715726764 and parameters: {'n_estimators': 821, 'eta': 0.09117760951294338, 'max_depth': 12, 'alpha': 0.5693, 'lambda': 14.216315394312042, 'max_bin': 341}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:26:14,672]\u001b[0m Trial 393 finished with value: 0.8323918318860626 and parameters: {'n_estimators': 790, 'eta': 0.09626400431373854, 'max_depth': 12, 'alpha': 0.44480000000000003, 'lambda': 36.82643742367344, 'max_bin': 336}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:26:22,068]\u001b[0m Trial 394 finished with value: 0.8298686976444436 and parameters: {'n_estimators': 779, 'eta': 0.08620365115375368, 'max_depth': 12, 'alpha': 0.5412, 'lambda': 8.733122724110144, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:26:30,459]\u001b[0m Trial 395 finished with value: 0.8295778006152501 and parameters: {'n_estimators': 808, 'eta': 0.08831176191252695, 'max_depth': 8, 'alpha': 0.678, 'lambda': 10.091889374250323, 'max_bin': 321}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:26:38,994]\u001b[0m Trial 396 finished with value: 0.8293383332603762 and parameters: {'n_estimators': 833, 'eta': 0.08511155324287273, 'max_depth': 12, 'alpha': 0.6373, 'lambda': 13.200696802715852, 'max_bin': 351}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:26:46,671]\u001b[0m Trial 397 finished with value: 0.8290925767390487 and parameters: {'n_estimators': 798, 'eta': 0.08945167932226919, 'max_depth': 12, 'alpha': 0.4862, 'lambda': 11.126655007092559, 'max_bin': 332}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:26:53,755]\u001b[0m Trial 398 finished with value: 0.8298453810075028 and parameters: {'n_estimators': 752, 'eta': 0.09217323272725192, 'max_depth': 12, 'alpha': 0.5983, 'lambda': 9.38305257674585, 'max_bin': 339}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:27:01,480]\u001b[0m Trial 399 finished with value: 0.828471333516515 and parameters: {'n_estimators': 771, 'eta': 0.09495483752028751, 'max_depth': 12, 'alpha': 0.46490000000000004, 'lambda': 12.105633749344184, 'max_bin': 344}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8396\n",
      "\tBest params:\n",
      "\t\tn_estimators: 822\n",
      "\t\teta: 0.08701851050696605\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.4616\n",
      "\t\tlambda: 9.108381395969195\n",
      "\t\tmax_bin: 326\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  393.000000  398.000000  417.000000  412.000000   \n",
      "1                    TN  358.000000  363.000000  347.000000  346.000000   \n",
      "2                    FP   76.000000   80.000000   77.000000   83.000000   \n",
      "3                    FN   72.000000   58.000000   58.000000   58.000000   \n",
      "4              Accuracy    0.835373    0.846496    0.849833    0.843159   \n",
      "5             Precision    0.837953    0.832636    0.844130    0.832323   \n",
      "6           Sensitivity    0.845161    0.872807    0.877895    0.876596   \n",
      "7           Specificity    0.824900    0.819400    0.818400    0.806500   \n",
      "8              F1 score    0.841542    0.852248    0.860681    0.853886   \n",
      "9   F1 score (weighted)    0.835344    0.846350    0.849585    0.842837   \n",
      "10     F1 score (macro)    0.835123    0.846263    0.848917    0.842309   \n",
      "11    Balanced Accuracy    0.835023    0.846110    0.848145    0.841561   \n",
      "12                  MCC    0.670279    0.693543    0.698601    0.685935   \n",
      "13                  NPV    0.832600    0.862200    0.856800    0.856400   \n",
      "14              ROC_AUC    0.835023    0.846110    0.848145    0.841561   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   400.000000  395.000000  414.000000  402.000000  \n",
      "1   348.000000  352.000000  341.000000  346.000000  \n",
      "2   103.000000   81.000000   88.000000   92.000000  \n",
      "3    48.000000   71.000000   56.000000   59.000000  \n",
      "4     0.832036    0.830923    0.839822    0.832036  \n",
      "5     0.795229    0.829832    0.824701    0.813765  \n",
      "6     0.892857    0.847639    0.880851    0.872017  \n",
      "7     0.771600    0.812900    0.794900    0.790000  \n",
      "8     0.841220    0.838641    0.851852    0.841885  \n",
      "9     0.831439    0.830833    0.839356    0.831650  \n",
      "10    0.831472    0.830536    0.838759    0.831381  \n",
      "11    0.832238    0.830286    0.837861    0.830986  \n",
      "12    0.669229    0.661277    0.679672    0.665022  \n",
      "13    0.878800    0.832200    0.858900    0.854300  \n",
      "14    0.832238    0.830286    0.837861    0.830986  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_xgb_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_xgb_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_xgb_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_xgb_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_xgb_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_xgb_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_xgb_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_xgb_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_xgb_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_xgb_7)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:27:11,106]\u001b[0m Trial 400 finished with value: 0.8316606787486698 and parameters: {'n_estimators': 811, 'eta': 0.08710980076637294, 'max_depth': 12, 'alpha': 0.5105000000000001, 'lambda': 10.04695854527425, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:27:19,925]\u001b[0m Trial 401 finished with value: 0.8315190639536771 and parameters: {'n_estimators': 736, 'eta': 0.08341073285869607, 'max_depth': 12, 'alpha': 0.5509000000000001, 'lambda': 8.397683001518448, 'max_bin': 319}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:27:29,222]\u001b[0m Trial 402 finished with value: 0.8278416483260994 and parameters: {'n_estimators': 786, 'eta': 0.09044923521372343, 'max_depth': 12, 'alpha': 0.6171, 'lambda': 11.76078616875162, 'max_bin': 336}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:27:36,781]\u001b[0m Trial 403 finished with value: 0.8273657432442837 and parameters: {'n_estimators': 819, 'eta': 0.09811274755474063, 'max_depth': 12, 'alpha': 0.4722, 'lambda': 10.784332201221261, 'max_bin': 331}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:27:44,173]\u001b[0m Trial 404 finished with value: 0.8308578209544335 and parameters: {'n_estimators': 797, 'eta': 0.08886538815018251, 'max_depth': 12, 'alpha': 0.443, 'lambda': 9.216496066200325, 'max_bin': 346}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:27:52,464]\u001b[0m Trial 405 finished with value: 0.8278346246163333 and parameters: {'n_estimators': 768, 'eta': 0.09347398172215574, 'max_depth': 12, 'alpha': 0.5857, 'lambda': 13.730926941652529, 'max_bin': 323}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:28:01,357]\u001b[0m Trial 406 finished with value: 0.8285446403317176 and parameters: {'n_estimators': 420, 'eta': 0.08683578166904275, 'max_depth': 12, 'alpha': 0.5038, 'lambda': 12.944228694118147, 'max_bin': 340}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:28:10,153]\u001b[0m Trial 407 finished with value: 0.8310306478170888 and parameters: {'n_estimators': 837, 'eta': 0.09084465419151411, 'max_depth': 12, 'alpha': 0.5217, 'lambda': 11.48313932467762, 'max_bin': 318}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:28:17,958]\u001b[0m Trial 408 finished with value: 0.8303588296385097 and parameters: {'n_estimators': 787, 'eta': 0.08468765069338323, 'max_depth': 12, 'alpha': 0.6674, 'lambda': 8.42615238359072, 'max_bin': 329}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:28:26,262]\u001b[0m Trial 409 finished with value: 0.8270696401304788 and parameters: {'n_estimators': 823, 'eta': 0.08851736387788202, 'max_depth': 12, 'alpha': 0.533, 'lambda': 10.035106533067658, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:28:34,238]\u001b[0m Trial 410 finished with value: 0.8284728574835517 and parameters: {'n_estimators': 516, 'eta': 0.0861695667085121, 'max_depth': 12, 'alpha': 0.5594, 'lambda': 12.483499671625035, 'max_bin': 351}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:28:43,017]\u001b[0m Trial 411 finished with value: 0.8302136444114616 and parameters: {'n_estimators': 755, 'eta': 0.08259095457824797, 'max_depth': 12, 'alpha': 0.4917, 'lambda': 10.743143242270737, 'max_bin': 325}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:28:50,734]\u001b[0m Trial 412 finished with value: 0.8324380366485586 and parameters: {'n_estimators': 801, 'eta': 0.09215392376448245, 'max_depth': 12, 'alpha': 0.4656, 'lambda': 9.24958263741729, 'max_bin': 342}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:28:58,882]\u001b[0m Trial 413 finished with value: 0.8279381456458378 and parameters: {'n_estimators': 778, 'eta': 0.09415776504406086, 'max_depth': 12, 'alpha': 0.6441, 'lambda': 11.53590080219196, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:29:06,253]\u001b[0m Trial 414 finished with value: 0.829382385825936 and parameters: {'n_estimators': 810, 'eta': 0.09646794700100608, 'max_depth': 12, 'alpha': 0.6099, 'lambda': 8.262986037803227, 'max_bin': 317}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:29:14,569]\u001b[0m Trial 415 finished with value: 0.8284599545717836 and parameters: {'n_estimators': 844, 'eta': 0.08906988188139663, 'max_depth': 12, 'alpha': 0.436, 'lambda': 9.952454626738898, 'max_bin': 347}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:29:27,175]\u001b[0m Trial 416 finished with value: 0.8289823379709855 and parameters: {'n_estimators': 726, 'eta': 0.08699000990720304, 'max_depth': 12, 'alpha': 0.48860000000000003, 'lambda': 39.77758241616715, 'max_bin': 330}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:29:34,744]\u001b[0m Trial 417 finished with value: 0.8307787251566406 and parameters: {'n_estimators': 765, 'eta': 0.09069583973334547, 'max_depth': 12, 'alpha': 0.5869, 'lambda': 7.386456073297613, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:29:43,640]\u001b[0m Trial 418 finished with value: 0.8276682201701681 and parameters: {'n_estimators': 793, 'eta': 0.08437697513753442, 'max_depth': 12, 'alpha': 0.4562, 'lambda': 12.798851520162547, 'max_bin': 357}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:29:52,231]\u001b[0m Trial 419 finished with value: 0.8267659473004201 and parameters: {'n_estimators': 826, 'eta': 0.08843264180183918, 'max_depth': 12, 'alpha': 0.627, 'lambda': 10.787282163101144, 'max_bin': 324}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:30:00,070]\u001b[0m Trial 420 finished with value: 0.8268318645532862 and parameters: {'n_estimators': 778, 'eta': 0.09141357758386272, 'max_depth': 12, 'alpha': 0.5710000000000001, 'lambda': 8.560163762723771, 'max_bin': 334}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:30:08,548]\u001b[0m Trial 421 finished with value: 0.8301550158516982 and parameters: {'n_estimators': 750, 'eta': 0.0858233465829623, 'max_depth': 12, 'alpha': 0.5281, 'lambda': 12.08823968861378, 'max_bin': 342}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:30:16,726]\u001b[0m Trial 422 finished with value: 0.8234096352007638 and parameters: {'n_estimators': 810, 'eta': 0.09491268300529729, 'max_depth': 12, 'alpha': 0.4713, 'lambda': 15.52163157748963, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:30:24,262]\u001b[0m Trial 423 finished with value: 0.830191740174049 and parameters: {'n_estimators': 836, 'eta': 0.09286720882447326, 'max_depth': 12, 'alpha': 0.5487000000000001, 'lambda': 9.470138935442606, 'max_bin': 320}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:30:33,299]\u001b[0m Trial 424 finished with value: 0.8345226045737878 and parameters: {'n_estimators': 796, 'eta': 0.08280964814977212, 'max_depth': 12, 'alpha': 0.5067, 'lambda': 11.295764228344593, 'max_bin': 331}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:30:41,766]\u001b[0m Trial 425 finished with value: 0.8293062101371484 and parameters: {'n_estimators': 778, 'eta': 0.08953717298757961, 'max_depth': 12, 'alpha': 0.6032000000000001, 'lambda': 13.775570312066, 'max_bin': 348}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:30:50,004]\u001b[0m Trial 426 finished with value: 0.8240229113719157 and parameters: {'n_estimators': 816, 'eta': 0.0878933248500179, 'max_depth': 12, 'alpha': 0.4359, 'lambda': 10.207191998276295, 'max_bin': 338}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:30:58,070]\u001b[0m Trial 427 finished with value: 0.8334381078423106 and parameters: {'n_estimators': 739, 'eta': 0.08474379955059548, 'max_depth': 12, 'alpha': 0.47700000000000004, 'lambda': 9.026059318391896, 'max_bin': 316}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:31:06,180]\u001b[0m Trial 428 finished with value: 0.8315750825782102 and parameters: {'n_estimators': 797, 'eta': 0.08718533722508637, 'max_depth': 12, 'alpha': 0.6535000000000001, 'lambda': 7.353312688199903, 'max_bin': 325}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:31:14,240]\u001b[0m Trial 429 finished with value: 0.828461807536103 and parameters: {'n_estimators': 760, 'eta': 0.09998149056303765, 'max_depth': 12, 'alpha': 0.5244, 'lambda': 12.090688366457162, 'max_bin': 343}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:31:22,562]\u001b[0m Trial 430 finished with value: 0.8289845095691584 and parameters: {'n_estimators': 823, 'eta': 0.0906078299139348, 'max_depth': 12, 'alpha': 0.5755, 'lambda': 11.095755906831446, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:31:31,459]\u001b[0m Trial 431 finished with value: 0.8304626806922819 and parameters: {'n_estimators': 783, 'eta': 0.09289743857335028, 'max_depth': 9, 'alpha': 0.45, 'lambda': 13.07983822102024, 'max_bin': 322}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:31:39,339]\u001b[0m Trial 432 finished with value: 0.8290021570335586 and parameters: {'n_estimators': 806, 'eta': 0.09722271627382112, 'max_depth': 12, 'alpha': 0.49260000000000004, 'lambda': 10.302579528987618, 'max_bin': 330}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:31:47,534]\u001b[0m Trial 433 finished with value: 0.8298934054532907 and parameters: {'n_estimators': 842, 'eta': 0.0895235768105143, 'max_depth': 12, 'alpha': 0.5569000000000001, 'lambda': 8.35167804009718, 'max_bin': 339}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:31:55,825]\u001b[0m Trial 434 finished with value: 0.8279235912078633 and parameters: {'n_estimators': 769, 'eta': 0.08592858348647721, 'max_depth': 12, 'alpha': 0.6354000000000001, 'lambda': 9.428539620206188, 'max_bin': 348}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:32:04,402]\u001b[0m Trial 435 finished with value: 0.8284719157502474 and parameters: {'n_estimators': 786, 'eta': 0.08819832713887248, 'max_depth': 12, 'alpha': 0.4214, 'lambda': 11.670346535397512, 'max_bin': 317}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:32:14,933]\u001b[0m Trial 436 finished with value: 0.8253553513101781 and parameters: {'n_estimators': 817, 'eta': 0.08229787173398738, 'max_depth': 12, 'alpha': 0.5056, 'lambda': 20.937737451723187, 'max_bin': 333}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:32:23,158]\u001b[0m Trial 437 finished with value: 0.8265936234598256 and parameters: {'n_estimators': 795, 'eta': 0.09463495489781393, 'max_depth': 12, 'alpha': 0.5987, 'lambda': 14.48888558852245, 'max_bin': 353}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:32:31,083]\u001b[0m Trial 438 finished with value: 0.8265500160517479 and parameters: {'n_estimators': 748, 'eta': 0.09219285693698231, 'max_depth': 12, 'alpha': 0.5394, 'lambda': 7.798459346499476, 'max_bin': 328}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:32:39,930]\u001b[0m Trial 439 finished with value: 0.8279005454728019 and parameters: {'n_estimators': 836, 'eta': 0.08475112713423365, 'max_depth': 12, 'alpha': 0.47350000000000003, 'lambda': 10.476123843948503, 'max_bin': 342}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:32:48,256]\u001b[0m Trial 440 finished with value: 0.8312581270845147 and parameters: {'n_estimators': 772, 'eta': 0.09041556185795552, 'max_depth': 12, 'alpha': 0.4504, 'lambda': 12.448349354945163, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:32:55,991]\u001b[0m Trial 441 finished with value: 0.8303667676780175 and parameters: {'n_estimators': 804, 'eta': 0.08727572170913898, 'max_depth': 12, 'alpha': 0.6269, 'lambda': 6.456100319168955, 'max_bin': 324}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:33:04,107]\u001b[0m Trial 442 finished with value: 0.8279084734255934 and parameters: {'n_estimators': 822, 'eta': 0.08916947351092054, 'max_depth': 12, 'alpha': 0.655, 'lambda': 8.899605298883728, 'max_bin': 345}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:33:12,994]\u001b[0m Trial 443 finished with value: 0.8301673906504632 and parameters: {'n_estimators': 786, 'eta': 0.08639312073658291, 'max_depth': 12, 'alpha': 0.6842, 'lambda': 9.699405842862326, 'max_bin': 318}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:33:20,973]\u001b[0m Trial 444 finished with value: 0.8295516850222308 and parameters: {'n_estimators': 764, 'eta': 0.09193254595623938, 'max_depth': 12, 'alpha': 0.5281, 'lambda': 10.882121591238228, 'max_bin': 360}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:33:28,648]\u001b[0m Trial 445 finished with value: 0.8289936085025851 and parameters: {'n_estimators': 800, 'eta': 0.09611863465080608, 'max_depth': 12, 'alpha': 0.49210000000000004, 'lambda': 11.643614398281452, 'max_bin': 330}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:33:37,899]\u001b[0m Trial 446 finished with value: 0.8287308002313217 and parameters: {'n_estimators': 715, 'eta': 0.08392780286274412, 'max_depth': 12, 'alpha': 0.5764, 'lambda': 13.241452326227359, 'max_bin': 459}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:33:45,958]\u001b[0m Trial 447 finished with value: 0.8318864777587492 and parameters: {'n_estimators': 843, 'eta': 0.09376859162273744, 'max_depth': 12, 'alpha': 0.613, 'lambda': 7.3588839321016115, 'max_bin': 340}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:33:53,762]\u001b[0m Trial 448 finished with value: 0.8263586783032324 and parameters: {'n_estimators': 817, 'eta': 0.08811874799516928, 'max_depth': 12, 'alpha': 0.46240000000000003, 'lambda': 9.971890609669414, 'max_bin': 336}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:34:02,157]\u001b[0m Trial 449 finished with value: 0.8326070286652927 and parameters: {'n_estimators': 484, 'eta': 0.09019797805574252, 'max_depth': 12, 'alpha': 0.5564, 'lambda': 8.899665270301682, 'max_bin': 323}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8396\n",
      "\tBest params:\n",
      "\t\tn_estimators: 822\n",
      "\t\teta: 0.08701851050696605\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.4616\n",
      "\t\tlambda: 9.108381395969195\n",
      "\t\tmax_bin: 326\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  393.000000  398.000000  417.000000  412.000000   \n",
      "1                    TN  358.000000  363.000000  347.000000  346.000000   \n",
      "2                    FP   76.000000   80.000000   77.000000   83.000000   \n",
      "3                    FN   72.000000   58.000000   58.000000   58.000000   \n",
      "4              Accuracy    0.835373    0.846496    0.849833    0.843159   \n",
      "5             Precision    0.837953    0.832636    0.844130    0.832323   \n",
      "6           Sensitivity    0.845161    0.872807    0.877895    0.876596   \n",
      "7           Specificity    0.824900    0.819400    0.818400    0.806500   \n",
      "8              F1 score    0.841542    0.852248    0.860681    0.853886   \n",
      "9   F1 score (weighted)    0.835344    0.846350    0.849585    0.842837   \n",
      "10     F1 score (macro)    0.835123    0.846263    0.848917    0.842309   \n",
      "11    Balanced Accuracy    0.835023    0.846110    0.848145    0.841561   \n",
      "12                  MCC    0.670279    0.693543    0.698601    0.685935   \n",
      "13                  NPV    0.832600    0.862200    0.856800    0.856400   \n",
      "14              ROC_AUC    0.835023    0.846110    0.848145    0.841561   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   400.000000  395.000000  414.000000  402.000000  424.000000  \n",
      "1   348.000000  352.000000  341.000000  346.000000  348.000000  \n",
      "2   103.000000   81.000000   88.000000   92.000000   66.000000  \n",
      "3    48.000000   71.000000   56.000000   59.000000   61.000000  \n",
      "4     0.832036    0.830923    0.839822    0.832036    0.858732  \n",
      "5     0.795229    0.829832    0.824701    0.813765    0.865306  \n",
      "6     0.892857    0.847639    0.880851    0.872017    0.874227  \n",
      "7     0.771600    0.812900    0.794900    0.790000    0.840600  \n",
      "8     0.841220    0.838641    0.851852    0.841885    0.869744  \n",
      "9     0.831439    0.830833    0.839356    0.831650    0.858665  \n",
      "10    0.831472    0.830536    0.838759    0.831381    0.857715  \n",
      "11    0.832238    0.830286    0.837861    0.830986    0.857403  \n",
      "12    0.669229    0.661277    0.679672    0.665022    0.715484  \n",
      "13    0.878800    0.832200    0.858900    0.854300    0.850900  \n",
      "14    0.832238    0.830286    0.837861    0.830986    0.857403  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_xgb_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_xgb_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_xgb_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_xgb_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_xgb_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_xgb_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_xgb_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_xgb_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_xgb_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_xgb_8)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:34:14,945]\u001b[0m Trial 450 finished with value: 0.8247980948672298 and parameters: {'n_estimators': 781, 'eta': 0.08569590911058096, 'max_depth': 7, 'alpha': 0.5131, 'lambda': 32.39949482914779, 'max_bin': 406}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:34:23,595]\u001b[0m Trial 451 finished with value: 0.8235217300855634 and parameters: {'n_estimators': 754, 'eta': 0.08231926033230899, 'max_depth': 12, 'alpha': 0.4772, 'lambda': 12.237995111294204, 'max_bin': 331}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:34:31,059]\u001b[0m Trial 452 finished with value: 0.82695747629929 and parameters: {'n_estimators': 806, 'eta': 0.09183734573564266, 'max_depth': 12, 'alpha': 0.44580000000000003, 'lambda': 8.26801295629074, 'max_bin': 350}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:34:38,940]\u001b[0m Trial 453 finished with value: 0.8288529567072358 and parameters: {'n_estimators': 831, 'eta': 0.09824739242934884, 'max_depth': 12, 'alpha': 0.5942000000000001, 'lambda': 11.228757987534415, 'max_bin': 340}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:34:47,227]\u001b[0m Trial 454 finished with value: 0.8251622259766584 and parameters: {'n_estimators': 741, 'eta': 0.08776164969951236, 'max_depth': 12, 'alpha': 0.41850000000000004, 'lambda': 10.625969092620377, 'max_bin': 316}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:34:53,808]\u001b[0m Trial 455 finished with value: 0.8272070381956121 and parameters: {'n_estimators': 788, 'eta': 0.08943935690513911, 'max_depth': 12, 'alpha': 0.5429, 'lambda': 6.643835112756353, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:35:01,955]\u001b[0m Trial 456 finished with value: 0.8285812687116783 and parameters: {'n_estimators': 769, 'eta': 0.0851303885435427, 'max_depth': 12, 'alpha': 0.4955, 'lambda': 12.838850069032972, 'max_bin': 344}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:35:09,912]\u001b[0m Trial 457 finished with value: 0.8316299557318629 and parameters: {'n_estimators': 806, 'eta': 0.09579270639899487, 'max_depth': 12, 'alpha': 0.6411, 'lambda': 9.511281681788805, 'max_bin': 334}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:35:18,148]\u001b[0m Trial 458 finished with value: 0.8246540456099997 and parameters: {'n_estimators': 819, 'eta': 0.09143046517551164, 'max_depth': 12, 'alpha': 0.5195000000000001, 'lambda': 13.56888754144817, 'max_bin': 321}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:35:23,821]\u001b[0m Trial 459 finished with value: 0.822586315155464 and parameters: {'n_estimators': 175, 'eta': 0.08688956783561569, 'max_depth': 12, 'alpha': 0.6201, 'lambda': 24.65914073420514, 'max_bin': 327}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:35:31,160]\u001b[0m Trial 460 finished with value: 0.8302452130872512 and parameters: {'n_estimators': 789, 'eta': 0.09398440850408939, 'max_depth': 12, 'alpha': 0.45990000000000003, 'lambda': 7.832265708866527, 'max_bin': 337}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:35:39,285]\u001b[0m Trial 461 finished with value: 0.8268413451191469 and parameters: {'n_estimators': 842, 'eta': 0.08891162697973896, 'max_depth': 12, 'alpha': 0.5666, 'lambda': 11.51955687426447, 'max_bin': 354}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:35:47,394]\u001b[0m Trial 462 finished with value: 0.8273573752492858 and parameters: {'n_estimators': 771, 'eta': 0.08348251201577578, 'max_depth': 12, 'alpha': 0.6594, 'lambda': 9.992651950567467, 'max_bin': 347}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:35:55,529]\u001b[0m Trial 463 finished with value: 0.8291288067315836 and parameters: {'n_estimators': 800, 'eta': 0.09063545673663292, 'max_depth': 12, 'alpha': 0.43160000000000004, 'lambda': 8.643623956244626, 'max_bin': 331}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:36:04,057]\u001b[0m Trial 464 finished with value: 0.829925161153646 and parameters: {'n_estimators': 732, 'eta': 0.0866958267242396, 'max_depth': 12, 'alpha': 0.48660000000000003, 'lambda': 10.678468466690719, 'max_bin': 321}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:36:12,235]\u001b[0m Trial 465 finished with value: 0.8276902621170027 and parameters: {'n_estimators': 758, 'eta': 0.09345588453032949, 'max_depth': 12, 'alpha': 0.5837, 'lambda': 12.096930640464532, 'max_bin': 340}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:36:20,396]\u001b[0m Trial 466 finished with value: 0.829108482023457 and parameters: {'n_estimators': 815, 'eta': 0.0884402494514411, 'max_depth': 12, 'alpha': 0.5137, 'lambda': 9.289558742665326, 'max_bin': 317}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:36:28,333]\u001b[0m Trial 467 finished with value: 0.8279126700884198 and parameters: {'n_estimators': 783, 'eta': 0.08485002632707067, 'max_depth': 12, 'alpha': 0.5458000000000001, 'lambda': 7.64021181928124, 'max_bin': 333}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:36:36,333]\u001b[0m Trial 468 finished with value: 0.8257348697281802 and parameters: {'n_estimators': 828, 'eta': 0.09173048692682473, 'max_depth': 12, 'alpha': 0.6144000000000001, 'lambda': 14.081372871769396, 'max_bin': 325}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:36:44,397]\u001b[0m Trial 469 finished with value: 0.8254141739613885 and parameters: {'n_estimators': 798, 'eta': 0.08951657517596771, 'max_depth': 12, 'alpha': 0.4726, 'lambda': 12.674545847157638, 'max_bin': 344}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:36:51,638]\u001b[0m Trial 470 finished with value: 0.8271778758257542 and parameters: {'n_estimators': 846, 'eta': 0.08221024856164896, 'max_depth': 12, 'alpha': 0.49960000000000004, 'lambda': 6.995465486432974, 'max_bin': 336}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:37:00,054]\u001b[0m Trial 471 finished with value: 0.830452769188682 and parameters: {'n_estimators': 776, 'eta': 0.0864813831209586, 'max_depth': 12, 'alpha': 0.5347000000000001, 'lambda': 11.035820072758169, 'max_bin': 328}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:37:07,416]\u001b[0m Trial 472 finished with value: 0.8285710087755749 and parameters: {'n_estimators': 758, 'eta': 0.0950264940511724, 'max_depth': 12, 'alpha': 0.44920000000000004, 'lambda': 10.020163321818009, 'max_bin': 339}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:37:14,829]\u001b[0m Trial 473 finished with value: 0.8307205824151216 and parameters: {'n_estimators': 811, 'eta': 0.09049582481639014, 'max_depth': 12, 'alpha': 0.6976, 'lambda': 8.41102674626606, 'max_bin': 315}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:37:23,133]\u001b[0m Trial 474 finished with value: 0.8287965606504389 and parameters: {'n_estimators': 794, 'eta': 0.08783859978243261, 'max_depth': 12, 'alpha': 0.7301000000000001, 'lambda': 11.80691997379443, 'max_bin': 350}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:37:30,634]\u001b[0m Trial 475 finished with value: 0.8267275574037564 and parameters: {'n_estimators': 834, 'eta': 0.09797169264634847, 'max_depth': 12, 'alpha': 0.5987, 'lambda': 10.517857452225787, 'max_bin': 322}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:37:39,336]\u001b[0m Trial 476 finished with value: 0.8302597028637818 and parameters: {'n_estimators': 777, 'eta': 0.06582905694555356, 'max_depth': 12, 'alpha': 0.6391, 'lambda': 6.187620649961218, 'max_bin': 331}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:37:46,892]\u001b[0m Trial 477 finished with value: 0.8251286289172587 and parameters: {'n_estimators': 742, 'eta': 0.08490255080537985, 'max_depth': 12, 'alpha': 0.47600000000000003, 'lambda': 8.948160014338733, 'max_bin': 343}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:37:54,876]\u001b[0m Trial 478 finished with value: 0.8300086598408283 and parameters: {'n_estimators': 802, 'eta': 0.09242172994821, 'max_depth': 12, 'alpha': 0.5716, 'lambda': 11.329070976981171, 'max_bin': 326}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:38:06,436]\u001b[0m Trial 479 finished with value: 0.8283170938379023 and parameters: {'n_estimators': 819, 'eta': 0.05732613953722297, 'max_depth': 12, 'alpha': 0.5134000000000001, 'lambda': 12.78435889111594, 'max_bin': 259}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:38:13,785]\u001b[0m Trial 480 finished with value: 0.829621140887474 and parameters: {'n_estimators': 786, 'eta': 0.08883969583643209, 'max_depth': 12, 'alpha': 0.42650000000000005, 'lambda': 9.586551095930005, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:38:21,524]\u001b[0m Trial 481 finished with value: 0.8266493885253041 and parameters: {'n_estimators': 767, 'eta': 0.08642424437470063, 'max_depth': 12, 'alpha': 0.4627, 'lambda': 8.020617583253944, 'max_bin': 346}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:38:30,065]\u001b[0m Trial 482 finished with value: 0.8265313430022425 and parameters: {'n_estimators': 849, 'eta': 0.08324180782547957, 'max_depth': 12, 'alpha': 0.6633, 'lambda': 11.953990529378249, 'max_bin': 319}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:38:37,977]\u001b[0m Trial 483 finished with value: 0.8295600540077217 and parameters: {'n_estimators': 829, 'eta': 0.090078063267183, 'max_depth': 12, 'alpha': 0.5604, 'lambda': 10.385237080765256, 'max_bin': 339}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:38:45,870]\u001b[0m Trial 484 finished with value: 0.8277194352933552 and parameters: {'n_estimators': 540, 'eta': 0.09316330179841587, 'max_depth': 12, 'alpha': 0.4872, 'lambda': 14.698758247680736, 'max_bin': 331}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:38:53,792]\u001b[0m Trial 485 finished with value: 0.824415231941767 and parameters: {'n_estimators': 801, 'eta': 0.09595291915889348, 'max_depth': 12, 'alpha': 0.534, 'lambda': 13.45110772743638, 'max_bin': 314}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:39:01,385]\u001b[0m Trial 486 finished with value: 0.8264147401552261 and parameters: {'n_estimators': 761, 'eta': 0.0874668143275523, 'max_depth': 12, 'alpha': 0.6015, 'lambda': 7.479885762834807, 'max_bin': 325}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:39:09,731]\u001b[0m Trial 487 finished with value: 0.8270297585489634 and parameters: {'n_estimators': 815, 'eta': 0.08472085766999896, 'max_depth': 12, 'alpha': 0.6239, 'lambda': 9.006985620060954, 'max_bin': 355}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:39:17,619]\u001b[0m Trial 488 finished with value: 0.8293958253479575 and parameters: {'n_estimators': 718, 'eta': 0.09080634058720947, 'max_depth': 12, 'alpha': 0.44220000000000004, 'lambda': 11.114227486709726, 'max_bin': 335}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:39:24,911]\u001b[0m Trial 489 finished with value: 0.8280072546411368 and parameters: {'n_estimators': 790, 'eta': 0.08844575384111907, 'max_depth': 12, 'alpha': 0.5133, 'lambda': 9.74007478008711, 'max_bin': 341}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:39:32,060]\u001b[0m Trial 490 finished with value: 0.8305097792777651 and parameters: {'n_estimators': 747, 'eta': 0.09364100518275388, 'max_depth': 12, 'alpha': 0.5815, 'lambda': 7.004528487357122, 'max_bin': 328}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:39:35,374]\u001b[0m Trial 491 finished with value: 0.8202859389446884 and parameters: {'n_estimators': 93, 'eta': 0.08603931770703883, 'max_depth': 12, 'alpha': 0.4612, 'lambda': 11.921160146489115, 'max_bin': 321}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:39:43,637]\u001b[0m Trial 492 finished with value: 0.829095226283361 and parameters: {'n_estimators': 810, 'eta': 0.08174582085145018, 'max_depth': 12, 'alpha': 0.5511, 'lambda': 8.50432873116728, 'max_bin': 347}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:39:52,197]\u001b[0m Trial 493 finished with value: 0.826311564063506 and parameters: {'n_estimators': 777, 'eta': 0.09138680739006697, 'max_depth': 12, 'alpha': 0.8479, 'lambda': 12.666739657785582, 'max_bin': 333}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:39:59,286]\u001b[0m Trial 494 finished with value: 0.8262880764991924 and parameters: {'n_estimators': 828, 'eta': 0.08952514021760744, 'max_depth': 12, 'alpha': 0.49660000000000004, 'lambda': 10.604812170183624, 'max_bin': 339}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:40:10,005]\u001b[0m Trial 495 finished with value: 0.8304201225933412 and parameters: {'n_estimators': 788, 'eta': 0.09720548255131767, 'max_depth': 11, 'alpha': 0.6749, 'lambda': 30.033673243927502, 'max_bin': 316}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:40:18,106]\u001b[0m Trial 496 finished with value: 0.8266317168146294 and parameters: {'n_estimators': 803, 'eta': 0.08728621489535585, 'max_depth': 12, 'alpha': 0.6305000000000001, 'lambda': 10.006210154458756, 'max_bin': 328}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:40:25,163]\u001b[0m Trial 497 finished with value: 0.8287883279379965 and parameters: {'n_estimators': 765, 'eta': 0.08421902069708305, 'max_depth': 12, 'alpha': 0.53, 'lambda': 6.318918745084455, 'max_bin': 343}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:40:32,997]\u001b[0m Trial 498 finished with value: 0.8274973926370348 and parameters: {'n_estimators': 851, 'eta': 0.09203349373384209, 'max_depth': 12, 'alpha': 0.4716, 'lambda': 11.458262643516093, 'max_bin': 350}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:40:41,139]\u001b[0m Trial 499 finished with value: 0.8218720088807994 and parameters: {'n_estimators': 826, 'eta': 0.09444395204427672, 'max_depth': 8, 'alpha': 0.4202, 'lambda': 9.300586827977803, 'max_bin': 323}. Best is trial 261 with value: 0.8396177830633036.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8396\n",
      "\tBest params:\n",
      "\t\tn_estimators: 822\n",
      "\t\teta: 0.08701851050696605\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.4616\n",
      "\t\tlambda: 9.108381395969195\n",
      "\t\tmax_bin: 326\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  393.000000  398.000000  417.000000  412.000000   \n",
      "1                    TN  358.000000  363.000000  347.000000  346.000000   \n",
      "2                    FP   76.000000   80.000000   77.000000   83.000000   \n",
      "3                    FN   72.000000   58.000000   58.000000   58.000000   \n",
      "4              Accuracy    0.835373    0.846496    0.849833    0.843159   \n",
      "5             Precision    0.837953    0.832636    0.844130    0.832323   \n",
      "6           Sensitivity    0.845161    0.872807    0.877895    0.876596   \n",
      "7           Specificity    0.824900    0.819400    0.818400    0.806500   \n",
      "8              F1 score    0.841542    0.852248    0.860681    0.853886   \n",
      "9   F1 score (weighted)    0.835344    0.846350    0.849585    0.842837   \n",
      "10     F1 score (macro)    0.835123    0.846263    0.848917    0.842309   \n",
      "11    Balanced Accuracy    0.835023    0.846110    0.848145    0.841561   \n",
      "12                  MCC    0.670279    0.693543    0.698601    0.685935   \n",
      "13                  NPV    0.832600    0.862200    0.856800    0.856400   \n",
      "14              ROC_AUC    0.835023    0.846110    0.848145    0.841561   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   400.000000  395.000000  414.000000  402.000000  424.000000  420.000000  \n",
      "1   348.000000  352.000000  341.000000  346.000000  348.000000  328.000000  \n",
      "2   103.000000   81.000000   88.000000   92.000000   66.000000   77.000000  \n",
      "3    48.000000   71.000000   56.000000   59.000000   61.000000   74.000000  \n",
      "4     0.832036    0.830923    0.839822    0.832036    0.858732    0.832036  \n",
      "5     0.795229    0.829832    0.824701    0.813765    0.865306    0.845070  \n",
      "6     0.892857    0.847639    0.880851    0.872017    0.874227    0.850202  \n",
      "7     0.771600    0.812900    0.794900    0.790000    0.840600    0.809900  \n",
      "8     0.841220    0.838641    0.851852    0.841885    0.869744    0.847629  \n",
      "9     0.831439    0.830833    0.839356    0.831650    0.858665    0.831978  \n",
      "10    0.831472    0.830536    0.838759    0.831381    0.857715    0.830258  \n",
      "11    0.832238    0.830286    0.837861    0.830986    0.857403    0.830039  \n",
      "12    0.669229    0.661277    0.679672    0.665022    0.715484    0.660535  \n",
      "13    0.878800    0.832200    0.858900    0.854300    0.850900    0.815900  \n",
      "14    0.832238    0.830286    0.837861    0.830986    0.857403    0.830039  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_xgb_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_xgb_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_xgb_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_xgb_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_xgb_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_xgb_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_xgb_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_xgb_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_xgb_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_xgb_9)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEaCAYAAADg2nttAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABeP0lEQVR4nO3deVxU9f4/8NeswLAJwyabIipqpWm4XHcEcUvllukt62ZW2mIut7qpmdk1zd3U1LL8qXWr26ZfM0uNNHcTF5RwA9RUQJYBAWGGYeZ8fn/gjAxzZuYMzAzb+/l49Mg5M2fm84HhvM9ne39EjDEGQgghxApxQxeAEEJI40fBghBCiE0ULAghhNhEwYIQQohNFCwIIYTYRMGCEEKITRQsSIMYPHgwXnjhhUbzPo3lc+yxdetWSKXShi6Gw02aNAkJCQkNXQxSCwULYiYvLw+vvfYa2rZtC7lcjsDAQIwbNw6pqal2v9f777+Ptm3bmh3fvn07Vq1aVe+yOup9DJxdXluuX78OkUiEI0eOmD23YMECtG/f3vh4woQJyM7OFvzeCQkJmDRpkiOKWWe///47RCKR8T+lUom4uDgcPny4Xu/bvn17LFiwwDGFJLwoWBATN2/eRGxsLI4dO4aNGzciMzMTu3fvhkwmQ58+fbBnzx6HfI6/vz98fHwazfs0ls+xh4eHB4KDg13+uYwxVFVV1es9zpw5g9zcXPz222/w8PDAiBEjcP36dccUkDgHI6SG0aNHs+DgYFZSUmL23IgRI1hwcDCrqKhgjDH27rvvsujoaPbll1+yqKgo5ubmxuLj49nVq1cZY4xt2bKFATD5791332WMMTZo0CD2/PPPG9970KBBbPLkyeztt99mgYGBzNfXl82dO5fp9Xr23nvvsaCgIBYQEMDmzp1rUqaa73PgwAGzzwPA2rRpwxhjjOM49sILL7B27doxd3d3FhUVxebMmcM0Go3d5dVqteytt95ioaGhTCaTsc6dO7Mvv/zSpGwA2Pr169nTTz/NvLy8WHh4OFu6dKnVn/+1a9cYAHb48GGz5ww/b4MtW7YwiURifFxSUsImTZrEgoODmVwuZ+Hh4WzWrFmMMcaeffZZs7odOHCAMcbYpUuX2MiRI5mnpyfz9PRkjz76KMvIyDD7nP3797OHH36YyWQytmbNGiYSidjRo0dNyvj7778zkUjEsrKyeOtn+B3dvHnTeOzWrVsMAPv444+NZY2Pjzc+z3EcW758OYuKimIymYy1a9eOrV692vj8oEGDzOp27do1qz9nYj8KFsSoqKiIicVitnDhQt7nDx06xACwnTt3MsaqL14KhYL169ePnTx5kp08eZL16tWLde3alXEcxyoqKthbb73FwsPDWW5uLsvNzWVlZWWMMf5g4ePjw/7973+zy5cvs82bNzMAbMSIEezNN99kly9fZlu3bmUA2M8//2xynuF9KisrjZ+Tm5vL0tPTWWhoKJs0aRJjjDG9Xs/efvttduLECXbt2jW2c+dOFhISwubPn88YY3aV94033mD+/v7s22+/ZZcvX2aLFi1iIpGIJScnG18DgAUFBbFNmzaxzMxMtmbNGgaA7d+/3+LvoD7B4rXXXmNdu3ZlJ06cYH/99Rc7evQo27RpE2OMsTt37rABAwaw8ePHG+tWWVnJKioqWGRkJBsyZAg7deoUO3XqFBs8eDCLjo5mlZWVxs8RiUQsNjaW/fbbbywrK4vl5+ezxMRE48/W4Omnn2YJCQkW68cXLFQqFQPA1q1bxxgzDxYfffQRc3d3Z5988gm7cuUK27hxI3Nzc2OfffaZ8fy2bduy119/3Vg3nU5nsQykbihYEKM//viDAWDbt2/nfd7wR71s2TLGWPXFC4DJXejly5cZAPbrr78yxhhbuHCh8c6+Jr5g0a1bN5PXdOnShT344IMmx7p27cpef/11i+9joNVq2eDBg1n//v2NLQc+q1atYu3btzc+FlLe8vJyJpfL2fr1601ek5SUxOLi4oyPAbDXXnvN5DUxMTFs9uzZFstjCBYeHh7GO33DfzKZzGqwGDNmDHv22Wctvnd8fLzZ85999hnz8PBgBQUFxmO3b99m7u7ubNu2bcbPAcAOHTpkcu4PP/zAFAoFu3PnDmOMseLiYubh4cG+/fZbi2WoHSxKS0vZCy+8wKRSKUtLS2OMmQeL8PBw9uabb5q8z8yZM1lUVJTxcXR0tLEVSJyDxiyIEbORU1IkEpkdCwwMNBl07dixIwICAnDhwgW7P79bt24mj0NCQtC1a1ezY/n5+Tbf6+WXX8bNmzexY8cOuLm5GY9/+umn6N27N4KDg+Hl5YU5c+bgr7/+squcmZmZ0Gq1GDhwoMnxQYMGIT093eTYww8/bPI4LCwMeXl5Nj9jy5YtSE1NNfnvpZdesnrOK6+8gu+//x4PPvggZsyYgV9++QUcx1k9Jz09HV26dEFAQIDxWHBwMGJiYszq0rNnT5PHY8aMga+vL7766isAwH//+194eXlh7NixNusXExMDLy8v+Pr6Yu/evfj888/x4IMPmr2utLQUt27d4v1ZX79+HRUVFTY/izgGBQti1KFDB4jFYvz555+8zxuOx8TEWH0fW0HHEplMZvJYJBLxHrN1AVy2bBm2b9+O3bt3m1wEv/vuO7z66quYMGECfv75Z5w9exbz58+v82Bt7eDJGDM7JpfL7S4/UB1U2rdvb/Kfv7+/1XOGDRuGGzdu4O2334ZGo8HTTz+NIUOGQK/X21UPvrpIJBK4u7ubvEYqleL555/Hp59+CgD47LPPMGnSJLM689m7dy/OnTuHwsJC3LhxA08++aRdZazrd4zUHQULYuTv748RI0Zg/fr1KC0tNXt+8eLFCA4OxtChQ43HCgoKkJWVZXx85coVqFQqdO7cGUD1xdLWxcqR/u///g/z58/H9u3bzYLaoUOH0L17d/zrX//CI488gg4dOpjNwBFS3vbt28PNzQ0HDx40e/8HHnjAIfWoK39/fzz55JP45JNPsHv3bhw8eNDYyuOr2wMPPID09HQUFhYaj+Xl5eHKlSuC6vLiiy/i3Llz+Pjjj3Hu3DnBa1Hatm2L6OhomwHQx8cH4eHhvD/rqKgoKBQKi3UjjkXBgphYv349JBIJhgwZgj179uDmzZtISUnBU089hQMHDmDr1q3w8PAwvl6hUOC5557D6dOncerUKTz77LN46KGHjIuqoqKicPv2bRw/fhyFhYVO7TZIT0/H008/jQULFqBTp064ffs2bt++jYKCAgDVLaK0tDTs3LkTWVlZWLNmDbZv327yHkLKq1AoMH36dLzzzjv47rvvkJGRgcWLF2Pnzp2YO3eu0+pny9tvv43t27fj8uXLyMjIwJdffgkvLy9ERkYCqK7b6dOnkZWVhcLCQlRVVeGpp55CYGAgJkyYgDNnzuD06dP4xz/+gbCwMEyYMMHmZ0ZGRmL48OGYMWMGBg8ejI4dOzq8XnPmzMG6devw6aefIiMjA5988gk2btxo8rOOiorC0aNHcePGDRQWFgpqvRH7ULAgJtq0aYNTp06hd+/emDp1KqKjozFixAhUVlbi+PHjGD58uMnrW7dujSlTpuDxxx9Hv3794OHhgR07dhi7DZKSkvDEE09g1KhRCAwMxLJly5xW9pSUFJSXl2POnDlo3bq18T9DX/vUqVPxzDPP4LnnnkP37t3xxx9/mC3kElreRYsW4cUXX8TMmTPxwAMP4L///S/++9//Ij4+3mn1s8Xd3R3z58/HI488gtjYWJw/fx6//PILfH19AQCvv/46AgIC0K1bNwQGBuLo0aPw8PDAvn374ObmhoEDB2LQoEHw9PTEnj17BHUnAcCUKVOg1WoxZcoUp9Tr5Zdfxn/+8x8sXrwYXbp0wdKlS7FkyRI8//zzxte89957KCkpQUxMDAIDA3Hjxg2nlKUlEzHq/CN1tGDBAvz3v/9FZmZmQxeFNKANGzZg/vz5yM7ONplMQJqX5pdYhhDiEnfv3kVmZiZWrFiBadOmUaBo5qgbihBSJ9OmTUOvXr3QuXNnvPXWWw1dHOJk1A1FCCHEJmpZEEIIsYmCBSGEEJua9QB3Tk5Onc4LCAgwWaTUElCdWwaqc8tQ1zqHhoZafI5aFoQQQmyiYEEIIcQmChaEEEJsomBBCCHEJpcNcKempmLLli3gOA7x8fFISkoyeb6iogJr166FSqWCXq/H6NGjERcXZ3ye4zjMnj0b/v7+mD17tquKTQghBC4KFhzHYfPmzZg3bx6USiXmzJmD2NhYhIeHG1+zZ88ehIeHY/bs2SgtLcWMGTMwYMAASKXVRfz5558RFhYGtVrtiiIT4lK55y7h5ob/h4hraZDrtBDDOc3+Eie8Z2PXouosFgNyOdTt2kE8bhzcBg9y3Fs77J2syMzMREhICIKDgyGVStG3b1+kpKSYvEYkEkGj0YAxBo1GAy8vL4jF1cVTqVQ4c+ZMg2b0JMRZcs9dwo3FKxCZcRZynRaGbX4otQKxG8cBGg20V69Cs2EDKn8/aPscgVzSsigqKoJSqTQ+ViqVyMjIMHnN8OHDsWzZMkydOhVqtRqzZs0yBoutW7fi6aefttmqSE5ORnJyMgBgyZIlJruk2UMqldb53KaK6txwft2RjLalKkjBIAKM/xFSZzodRBwHtns3AsY97pC3dEmw4Es/VXubxHPnzqFNmzaYP38+8vLysHDhQnTq1AkXL16Er68v2rVrZ7YncG0JCQnGTXcA1HkhDi3iaRkaqs6GLqewvy7ATavGI6AAQRyM48D0emhzc+36jltblOeSYKFUKqFSqYyPVSoV/Pz8TF5z4MABJCUlQSQSISQkBEFBQcjJycHly5dx6tQpnD17FlqtFmq1GmvXrsX06dNdUXRCHMrY5VR4E1Kmg+TecWcGCr7uLApMzZhIVD12IRJB5MCWs0uCRXR0NHJzc5Gfnw9/f38cO3bM7GIfEBCAtLQ0dO7cGXfu3EFOTg6CgoLw1FNP4amnngJQvW3mrl27KFCQJuvc93sRXaqCGMyhgcJSQKh9nNU4TgGjmWIMkEoBqRTyWrNO68MlwUIikWDy5MlYtGgROI5DXFwcIiIisG/fPgBAYmIiHn/8cWzYsAGvv/46AGDixInw8fFxRfEIcRm5qgDuXJXFC3XNi7vQ2Sfcvf9qv4fk3r8N/9UcOBffe540M/dmQ8mdMBuqWe9nQYkEhaM6u8aed9Yg+tTv8NXehZRxJmMVIgCQSACpFOKYGHh/uFrQey7Yex37LhebHU+M8UPh3Sqcyb5r9lyPcC989FgHk2M5JZXYdCIXhXerEOAlw5Q+rRHq2/R3v6PvtnANPmZBCKnWbdww3LiShlYFZeBw/+7f2NJgDPDwsKv7oPBuFf/x8uqLPp8AT9PjOSWVmLEjE9mlWuOx9NxyrPl7+2YRMEj9UbAgxIVad+sEzH0DxQvfh0+pCri3rkIEgJNIIW7dGp6TnzPrPrB21+8p5++wCvCsfl16brlJEAjzkWNKn9Ymr910ItfkNQCQXarFphO5WDCsbZ3r21xbKy0RBQtCXCzkoY7wG52IO+074+UMhdmFfE339gjF/QttdrEGV4sroa66PzJhuOsHgAt55WafEex9/8K85u/tqy/Y5VUI8/fCs939zS7Y1londUWtleaFggUhLmJYXxF68zLcqipR5u2PNu0TkR3ezfgaw938lD6tzS60NRlep9bqoarQmz0f4Ss3vyBbGZ0U2l1lD2e1VkjDoGBBiAsY1leEF+WgSiyBmAMUZcV4KW0nAOBYjYBRWF7Fe6GtrbC8CtdU/FkNTt8qx2Nb0uElFyG7tMrYKjmTfRd70/PQO9IbMwZW52YztF48ZGKT1kvN7ipL3UnWupmc0VohDYeCBSEuYFhfoReLIWN6MIkYWrEMYsZhzLUjJsHiz9xy6DnbkxQDPGW4ptLwPscA3C7jDzZaPcPha6U4+dcFeHtIUViuMz7nIRUjOsAdob5uJgGBrztpbkIkFiffsNjN5IzWCmk4FCwIuef2z8ko+eJL+BbkQMbpHJr5tfe9/xsWxGnFUlSKZRAxDgGaUpPXavW2A4VcIsKUPq1RodXjyLVSm6/nU8kBlTUCBQCodRwKy3VYMKytsYVgqTtp1s4ss7LW7GYSOrhOmgYKFoQAuL7kQ3ju+wmB95L5GThqpbOo1r9lnA5eVWpopHIUutu/+LRDgBumbc9EiVrLu1K7Pm6XaTFjR6ZxAP3Edf5gZCmopdwow7QfMhDgJcPchEjsTFdVT+P1bBqzoWgGFz8KFqTFu/1zMjx+/RliCE/oZyvfkq2LtwiAhOkh5Tj8GNVfYEmr+buLcSlfg5rXahEAhUyM8irO4nn2yC7V4sNDt3AlvwKlleYD6NYUq3UovrcQ0JGzn1xxEacZXJZRsCAtTu65S8jY+i0iMtPgXX4HbuDsaj1YCgQ18y4JOV8nkiDDN9RkvMIamRjo08YHl/IroGemQYEB0OocEygM0m9XoFits/1CK7JLtZi2PRMfPWZ6sbX3wu+qizjN4LKMggVpUXLPXcKVFesRln8T3lVlgvIj1bz42woqQgIFA6CRyHHDOxh/BrYXUIJqVVz1uUUV/BfwKocn7rHvDSUigK9nqma3lq0B853pKt4A4qqLuKUZXCk3ypBTUtmiWxcULGww3IWGZ5yHT0UJJOBcs72gi7WUrSfdAXS992+h3U2iWv+vnZSv9r+tPW/4f4XEDSVuXjga+pBd5a/rYHZtcokIcokId7X8rREPmRgPhnjisB2fF+gltzgDq+aF3dKF/41dV3kXHob6urlsGq6lGVzFap1JwGuJmuN1z2EMd6GhF87Ap6IYknu5PZtt5kUHYzz/NTQhO9EZyqqv8R8HQIf7GV51tY7reF7L92+tSIICDz+cCH0A27qMwHVfy4nbnMXHTYyurRWosDK+IQIDA6BUCMtNG+YjxztDIxHkafn+03Bht3ThV9cqjyHAAM5ZNMhnSp/WCPOR8z5XszzW5JRUYsHe65j2QwYW7L2OnJJKh5axoVDLwopz3+9Fq/IyeOo1xqjKd5Gpz+YyzXVjGmv9+jU1proayqYXi/FnZFfMffjpBi2Ps1RxwKlb5ilCaqqoYjhyrRT+7mL4uYtRpuWg5/h/rzKxCK19ZPjf2XxU6izfEhgu7JYu/HwMAUbINFx7xkEsvdaQHuXFby+jWG0+sG+rJdOcB8gpWFghVxVAzumMqaQB86mUti6KQoNL7ecb00XUmRpbXcVyOaSdOiGl6xjAPLN3kycWmd/BW1OkMX0t37hEFcdsBh8AGPuAEgD/hd+SnBKtoGm41i7StTeLs3VBD/V1Q89IH96079ZaMjkllZi2PdOsK665DJBTsLBCqwyE9vYtcBDV+aLG12fd1Dm6NeTKgFFztlLNejAAFVJ3oFsPhEyZhMILAO46ZnzAEj8PKd4f0RZfn83H8eulvIPDjiZgYbhV9SnjznQVuod7G+/e+S6std0u0xpfY+0O3dI4yNNfXoJMchmecgneGRqJ7uHeggbL+QKah1RsDHi1GQKQxTGbOxos2Hu9Sa/doGBhRbdxw3DleiYqS/MgZfwtCluDnbVfDxuvbWx32rXZu02nPT8fVwcMPUSoEkuR0SocH3f7O677hkIEoP8FyzOOrHETAxxEqBJ4Re4Z6Y1gbzmuqTQuCRQNrWYXTqivG0J9LA+I86l9Qa/ZlXStiD9HlkbHQaMDyir1mL4jE2v/3l7QYHmorxvmJkSaDLqrdRwWJ98wmdVl+PycUq3VulwtqkR63v0yNsWuKdopr5bcc5dw65MtCM88D7lOCzFMB2fFqB6oFHIRFLK9JV+rozFveamHsG06a2712djqzAHQicSokLrjXEB7/K9TgtlAs4dUDLUd6xbEIvvu2sN85MbU4XzdHc2Rj5sICrkMSoUUYa3coNbq7ZptBdzf4S+npBKvbs9AXpl9s6HEIsDbTYISjfl4hJ+HBD0jfYx3/dZ2ILSVFbgmS9+lxBg/k64pRy46pJ3ynMyQGbRNwV+Q1VioZVjZC6USma3CsDLS/OJiSf8oH1xTaQR9qYD7F5H63HHU5UunFiuw9JcLNs+Z9kOGzW06+fqEbeHb5tMZhJZNrePMsrBaIpeIeFNfeMlFcJdKUMUxMMbBTSpBsLfcJEmfpbvc2iQioHOwAoXlOrvuxhuT0kqG0srqO/D0vAoEeUoR7C2z64KvkFVPNVmy/4bdgQKoDuh8gQIAitV67LtcbLzrt7bmQq3VC/p+h3jLoVRIkZ5XYfZczZbM2VtlVqcO11bzb1whF0MEoFzLGf92a4/TOILLgkVqaiq2bNkCjuMQHx+PpFrbRlZUVGDt2rVQqVTQ6/UYPXo04uLioNVq8e6770Kn00Gv16NPnz4YP368U8poyAwqvZcfyGyKZUUF5JJS9MtJExQswnzkEAGCvlSG1bkzBobXO1DYOxsjp6QS/9p1CTeKbDeTLc1kuaZSY8He65jSp7Wg9Nq1uSoTqT1lY5ztQBHoKYVWx6DVm1+AZBIxfnzh/jqKmn/ghj0rhMwM8pCKsWJMO3QP97YZ7DykAESmQS7IUwqRWGT14uomEaHSxX1h+eU69I/yQYcAD5y8USYogWJGoRpnb5Xh9E3+2QdyiQgPtvbEhdsV0NRxRXt2qRZP/feixeeL1Tr8caPM5vvUbD3yBQtD4MspqcQbP141a31kl2rx5BcX0aeNt8l1wdZ3ID23HJ9P9oOHzRLaxyXrLDiOw+bNmzF37lysXr0aR48exa1bt0xes2fPHoSHh2P58uVYsGABPv/8c+h0OshkMrz77rtYvnw5li1bhtTUVFy5csUp5ZSrCuDOVd1vSdTCqnTgKisRpLbebeDnIUFijB/W/L09yi0seqqtigP+vF2OTSdy6zUv29rgnbVzbtTq87V0jqV56Ia7shk7MpF9h7/8lsYkxCJYHDh0NKF38gBg4QbURJCXzGJ3VbmWM861P3urDDN2ZGLf5WKcyb5r/FmNfUBp9vMM8pRiQJQPeoR7ITHGD19M7ITu4d4AYBwcDvHmXwswINoPXzzVCYkxfsbzN4zriPWPdbB4Tv8oH3z5dGckxvihvdINYjsGj+o7zlRRxWHZ6Gh89XRni+WrKa+sCq//eNXiZBEdx1BZxSFaafu9rNHqmdXgZe05EYABUT7Gm60pfVrzrj85dbMML35zGdO2Z1r8DlVx1enkX92eYbwu2LrhyS7V4sP9WRafryuXtCwyMzMREhKC4OBgAEDfvn2RkpKC8PBw42tEIhE0Gg0YY9BoNPDy8oJYLIZIJIK7uzsAQK/XQ6/XQyRyzlCoVhkIzTUZFKi+cNZeuatmIqhFUuR7+Fl9nyilBxYMa4uckkrk2HGHXbsZXJcWRl1WutpzTs1tOlNulJnlDsou1VocrBVbSAfBMZgMHDqTPXP8hcgsVENrIaho9czYZcfX951dqsXOdJXJtqdCMrOG+rrho8fam91dGtYdhPq68U7T/Oix9vjXrmsmNwZhPnLMvHfXauiHrzn2IgLwQIgHMgs10PCsoWjlIeFdjyBUZsH9FilfnfhYazFwDLx38a5kGNMz/A5Dfd0QE6RAfq3xGY2OCS5rXlkVnvziIrqFKpBRwL+HSU35ZY5fCOiSYFFUVASl8v6do1KpREZGhslrhg8fjmXLlmHq1KlQq9WYNWsWxOLqhg/HcXjrrbdw+/ZtDBs2DB068PdtJycnIzk5GQCwZMkSBNjZcTdg8hNIz/gTfvmlJoFChOqZM+UCUzSE+XtBLVbgX7su1al/ObtUi21ni7BynH2pIAAgTJnLO6YQ5u/F+/O4WVSBvHL+mT95d3V4+YcsFJZrEeglR4S/AjOHRKNrdADe8vPD+E9P8p7XSiFDYbkWNf+mRbA+7bI+dbbHWyMUuFRwxqwlVVd2JmQ1U6IFukaH4aPoMN7nbxZV4MP9WcgvrUSQjxtmDolGhL8CAQHA55P9qp8rq0SQ9/3nLAkIAL54LhArfr3Me84Hv6eZXagZgD9vqxHoKYNIpDfp3or090CHQE/8dtn+gVSD0srqG6RLBRps+WcPfD65J/659TSy79i+IDZmf/xVCrVYYfzZatn1er+n0PUsABDs42739c8WlwQLvglXtVsH586dQ5s2bTB//nzk5eVh4cKF6NSpExQKBcRiMZYvX47y8nKsWLECN27cQGRkpNl7JiQkICEhwfjY3tkAnlGhiPj3TJQu/gCKOwWQcbrqZrZMhhI3b5wI6Iifo/paHa+QioATWYV47EqBxYE0IbKL7tZpNsOz3f1x5nqR2R3ns939zd7PWt+nRFQ9N9zwR5t9R4PUW6U4c73IuEOaykJr5WphBWrf/AnpDa9rne3hAWDV6ChsOpGLEi3wV2F5nQeMpWIRdPVcuOArt/w95fv9nLleZGyBeQCYM7jGRkJcBQoLrd+ptg4IsHhOtsryKsSC8ir0j/KBQi4xaQEBwKXcUrvHqGq7UaTG0l8uYMGwtgj2lCL7Tr3ejpebGNBaWIVeV5ZmwVVxwPSvTyOslQeyizXIKnJdyg8PqRjje7RumrOhlEolVCqV8bFKpYKfn2lXzoEDB5CUlASRSISQkBAEBQUhJycH7dvfz8rp6emJLl26IDU1lTdY1FfuuUv487tfECGWIvuhvmg/aQJad+sEAFi8K8vqND9DS0THgAILd+r2qOuAb81uIlvdGpb6Pq21ArJLtXj9x6tWuwKEDFTycdUgt6GbJiAgAOezsu2euQVU/0Fq9fVLCW5r1zhXp8u21UVnGF+oTegCO1sM3Z6ecucMpfp5yuElE+FqcWW9FycadA5yR3oefysoPU9j8TlnUus4zN15EatGRzm0W9clA9zR0dHIzc1Ffn4+dDodjh07htjYWJPXBAQEIC0tDQBw584d5OTkICgoCKWlpSgvr256abVapKWlISyMv8leH7nnLiF13VaocgtRwEmRr7qL1HVbkXvuEgDhexTUhYfM9NdQ360nDRfDjx7rYLI9Zm2Wxips1aWus0ysaajtNg3BNTHGDw8Ee8CNZ7GHm0SE94ZFGl/jIaueN1+fyUMh3ranSLsq06qBtSR6gOVgbhhDsXauEAqZGDkllbic75wxh9tlWmQWOS5QAMCV/MbZXXajSC0o6aE9XNKykEgkmDx5MhYtWgSO4xAXF4eIiAjs27cPAJCYmIjHH38cGzZswOuvvw4AmDhxInx8fPDXX39h/fr14DgOjDH87W9/wyOPPOLwMp77fi/ymBvEEgaIRCiTe6KMMZz7fi9ad+uECoGzmuwll4juXYAkKK/iXLr1pKMHe+tCLhGhd6R3vacM10fNAeHpOzJwqta0zEo9Q/KVO1g6OhoL9l43WYlbF8HeMrPNgPi4KtOqgSFwfnjoltlUVlvB3Nq5QmUUqrHm0C3kO6BlXl/uUhHvgH5tjt9DxHEcfVPhsnUWPXr0QI8ePUyOJSYmGv/t7++PefPmmZ3Xpk0bLFu2zOnlk6sKUCHzRCtt9YVCJ5ZAK5ZCrioA4PgLq6HbSquvHrSquRjPkOLY2Xlk7Eno5mhecjH6Rvk2qhw5OSWVOJfNP4D45+3q4/ZMvTXwkQMavQh6jsFfIcX8oW0E1VlIplVHC/V1w7LR0ffXhNixd3btc/lmy1mTV1ZlcRzM1TzlYrhJgJL6zmBoQAqZYzuOaAX3PVplIBQ5RZBy1V+OKrEEiioNtKGBABx/Ya19Q2Loi+ZLI+CsPDI1xzdO3yqHqtyxQUOpkOBuJce72KtvlG+jyMJpuLBlF2twtbjSSl6n6gkZdblp0Ojvr/AuKNcJniZsz/iTo1mafmvPudN+yDDuxS2UpR5Oufhemh0OcJcBFVX83aVhPnK09pWbtQ7tparQI9BTCjThrSgyCtUO3d2PckPdYxizQEU5FFWVuObTGsGiSjz82iTjILfhwnLsWonFHcak99YSWPuhWtqnuUe4FwI8Zbxz8kO85Qj1kQtuadib8kMtVuDRj47blQ/JlkfCPXGtUG2W5jrYW4b1j3Vo8BaFWqzAP/9fiqAbgP5RPsa75roMiNdWOy+Qq9Q1Z1BdWMqtZC8hK9CB6r+Rjx6rnhDjiN+RVAQovexLRwJU58Cq4kR2pYLnI2Q/dwNLK/Dt/Z5Zmw1FO+Xd07pbJzz82iQofT2glDGEhPqbBArg/h1T9zAv3vfwcxfjb2194MM3SlqDpRWyAZ4yi90ct8u0Jit/ra3yNlzQaq8WtnZOhL8C7ZTuVsttr6uqSrNAAQAdAjwaPFAAwIf7swRdUII8pZg5sHoBac0B8R7hXugf5YNgb8utDbmE/5ftrEHqxmRKn9YW62+LXCIyrkCPCVIIumCH+spNNjASsiLcGh2r/q7W/F37ulmvj1IhgZtUUu9AAdg3acZSqhZHfs+oG6oWiV4HTnw/htbsplCp9VAqpMizMEVQrWOCsmjy/V7DfOQY+4ASC3+9YfN8W9Mn6zrlMqyVm0NXv9610N9rbTtPV8ovtd3HYLhbrRncanfR1OzfV8juJXW7N1nBUmZVV00Tbkihvm7oHeltd2ZZAKjSM3jKxJjSpzUWJ9v+mwBMf6aGGVrPfHnJZmuZb0Mng9rThd+yMoXez10MqURcpwSHzuLI7xkFi3sM3VCeJaUokXtBlVME/dot2NNlCE7LgoyvszaXnG/2hCGluSWGtMhjH1BicfINwXPVrbUS6jrlckqf1jiQUQxHXcst9f83lgtlkI/11o3QDMDW+vdzSipxVcWflqMlmDEw3Kz+QjAAh6+V4kphBjoG2E6Jx/czDfV1w4ox7fDmT9dQYSknCwAfd8spS2p/V63mehM1rkDh6O8ZdUPdY5g6y0RiVElkKJd7oADu6JR5tl7v2zm4uhnrZWGhkSGP1M50lV1/UJfz1RY3g7c15bL2hvJnb5Xh9e/TsDj5BsROyrtl0JgulDOHRJutDfCQivFgiMKYCLK+3WW1u60c9b5NRe36B/Ik1LMmr6wKDJa782on7aute7g3dr3Sx2SNTE1hPnI8GOLJ+94e91o2NVmf4GA9uaCfuxgBdta/rsJauTv8e0Yti3sMU2f/8g5Blbj6x1Ihc7eZYdbA0gYnYa3csWBYW4uDfYYL+FUbaRpqq+IY9l0uxuGsEmP6agNrUy75BmiFDEKKRMADwQpkl1Ty3oX5eUhRped4B/79PKSIUrq7dDaPEBH+CsGzjeqzMU19ZhY1BzXrP+rT87yvsTaYW1HFWezO6hdVndbf2u8mwl9hvrterZQltVs/HlIxVoxuZ/Y7HvuAEr9eLjYra4CnFJ2DFBa7qBiAYg2HIE8xBkT5oLyKwzWVul5JGC3xkInx+aRH4ME5dnEjBYt7DFNny+T3E7EpqjRWM8yGeMsR6itHgKfM2I1kqbvB1gX8moXcMbZmRKh1HN7YdRVfPNXJJMulpYvggr3X6zRLhDEYBw/5gotUDFjqdOsZ6d1oL5ZCLuR12SOEWMLfQpBZ2EAKgPH7y9ed92T3ILt+N5Z+30JvGnamq3j/HjsFKTBjYDgu51+xuqgwv1wHcaEGHz1W/10S+a4N7lIRVoxuhwh/hc08YfaiYHFPt3HDwNZtRZ62ukWhqNLAW6fGvra9eF8vEcG4AbyBtS+ctQv4W7uyLA6wuUsBW+ua1FUcXvz2MnpG+qBfW29sPHYbdyt1cJeK0VbpjuxiDaZtz4RSIUWOgEFdSwrLqzA3PhKpt8rM/iAs5cMybHLvyC0jXc3VOZqaswdCFDjCc/f9UGsFbhZXmn2vgr3vf1f4/n4c9bvhCyJ831lL44EVVZzFVOS13S7TYsaOTMxNiMThrBLB09WlouoZWgZ+7mK0C/BAlqoSAMODIZ5OzYRAweKe1t06Aa9Nwr4tuxB4pxD5Hn7Y17aXxQyzelZ9l1EzWBi+cIYv2eLkGyYXRr5ZNP/elYWjVr5cQhfAGvbCqHmnclfLobDi/uKk+iZ6C/CUCf6DMFDrOLy397rZPPmmdGfu6hxNzdnMgeG4UuvuO8hTijnxbQAAHx66hfTbFeC7+PFd0J31u7HUmrQ0vdzQnSx0szPDXiYrxrQz207VUpd27fkzRRoOsZ5yrH2so6DPrC8KFjW07tYJKQMkvPtB8OH7QgrtsnDU4i5XqdmlJvQPwoCvWd6U7sxdnaOpOQv1dcOGcR0ttsD5stpa4+jfjbVUJdmlWkQp3RHmI7fY3WzPCv/C8ip0D/fGF091Mvl58HVpW9rn3ZU3LBQsarHnl833hRTaLK7LPtWOZntaL//AtKPyZDWVO/OGyNHUnDlywN+RvxshN3AVVZzV7uYpfVrzdtPyMVw/+H4etT+jMazXoWBRy5Q+rXGpQGNzJzVLX0hL+0/XvjDWJSFdbVKx5Vw6QrRTuiG7tMriatMHQhS8d3qOypPlyi86X/+z0I3EGjJHE7HOkb8bITdwhq5YS8FOaDetYSzPEr4u64Zer0PBopZQXzds+WcPTNycwtvHb1hEx/eFrP6F8ue3V8jEJplkHbHBS582Pjh9826d8zndvKO1mCYAsJyIrPYfaM1VywqZGBfzyqGquD8l0N9dDJlMYjJmUd8vuj0D5pa6Bj+f7Afby72qtfTpr42Zo343tm7ghH5nLXXT1mzJq3WcXfvON4YbFgoWPCL8FQj1kfMGC8MiOj6bTuTyXrjdJNUX3poXyyBPKYK9TZOUSUWAVCIsj36YjxwzB4Yjr0xrNkAmhAiW88kY5JVVWRxXsPQHmlNSiVe3ZwC4HyxkMgnmD22DnemqOn3RawcGvj5dawPmlroGP9yfZbrFKGnRLHWvWrtBtOd9av+F2jtuZyso1vw7CVPm4tnu/rQozxUs3flb6zqxdGfiXuuuGqge9O0f5YNuoV42+yVrqv3FDfV1wxdPdcLL318RvJ2ru1QseLc7e8cVNp3INatrXlkVdqarBK1nqN1aAMwziPJNN7T2h2fp95Jf1oTzTxOHszT+Ye+sPb73cfYAde3W85nsuyb7tTsCBQseN4sqeLd2NMz5tsTSHYWlLLO1k5RN+yHDZtkMLZuaGyQp5GJwPJnmlQqJWWKzMB85WnlIBScMtBYc7ZmHbuuPgq+r6HBWCR5orTBrFVjqdrP0GZZ+L0HeNOZA7nNUVw/f+zh7gNoVa4EoWPD4cH8W72wGW6m1Ld2ZtFO6C/qiCJllFOApEzzttnOwJ2YaUiHUWsgkJFhY66Ot6zx0S/i+7Godh9N2bGJT8zNqBjKFXGzW5RfmI8fMIdGAg1MikKbNUeMfrh6gdsVaIAoWPCylrraVWtvSnQlgnnuG74tia5aR4Ryh024Nq0prf2krtHqz1aBAdUukS7CnoL3ALd3J2JqHzienpBIpNyzn1OHjIRObjNPU/Ay+QObvLkagpxTqKg5eblLMTYh0SkoEQvg4e4DaFWuBKFjwsJS6WsgPvj65Z0J93TA3IRILf71hkq6DYzAJPJYurLbKy3cRFYsAbzcJYtv44eU+QYK/vNbSHtjzR2Eok7WEarX7e8N85JibEFmdqfeOBqoKPTxkImNKE1WFzmxyQvUmTNXB5a5Wi8XJN9ApMljwbChC6suRM+r4Jn04ey2Qy4JFamoqtmzZAo7jEB8fj6SkJJPnKyoqsHbtWqhUKuj1eowePRpxcXEoLCzE+vXrcefOHYhEIiQkJGDkyJFOLevMIdE4c73IoT94oQnrau5pcVfLwU2qNQ5SCbmwGvCNr/C1BjgG9G7jg48mdrdru01rdzL2/FEIaSX1ivSGQi4xCz7B3nLM2JFpEhiEpjSh2VCkqbLUBWy4gSosr0KYv1fDzYbS6XTIyMhAcXEx+vbtC42mej2Bu7vtrTg5jsPmzZsxb948KJVKzJkzB7GxsQgPDze+Zs+ePQgPD8fs2bNRWlqKGTNmYMCAAZBIJHjmmWfQrl07qNVqzJ49G127djU519HsSV1dH7XvDtRavdVBKntWffONrziyX9NRK2dtzW2XS0QQoTo19M50FQrvVk/ntac7zhKaDUWaIktdwDVnHDpjr3VBweLGjRtYunQpZDIZVCoV+vbtiwsXLuDgwYOYNWuWzfMzMzMREhKC4OBgAEDfvn2RkpJicsEXiUTQaDRgjEGj0cDLywtisRh+fn7w86tOE+7h4YGwsDAUFRU5NVgAzl+ExXd3YGu/ZntWffONrziyX9NRfbBiC2sTDQuYtPrqrWqPXS81ycybnluOVh71axjTbCjSFDVUYktBf22ffvopJkyYgIEDB+K5554DAHTp0gWffPKJoA8pKiqCUnl/abtSqURGhuk00eHDh2PZsmWYOnUq1Go1Zs2aBXGtK0l+fj6uXbuG9u3b835OcnIykpOTAQBLlixBgNB8DrVIpdI6nyvUB7+nmd0dWMrnH+bvhYCAAIQpcwUnOTScU9NbIxS4VHDGJJVJpL8H3hrRpU51DggAPooOMz6+WVSBD/ZnIb+0EkE+bpg5JBoR/gqL55+8psIZntlOYlF191hNtX802aVaFNlIyRvkLYeeAyqrqqCuYibvEenvgTeGxiDAV275DZohV3y3G5vmVmdL14Gaf/POqLOgYHHr1i0MGDDA5Ji7uzu0WmFdAIxnDYCo1vad586dQ5s2bTB//nzk5eVh4cKF6NSpExSK6ouNRqPBypUrMWnSJOOx2hISEpCQkGB8XNdmmDOacLVlq/gv+nyDuc9290dhYSGe7e5vNpbCp+Y5NXkAWDU6yqw14MFVQKdT1KvOfC0lW4uC3vg+nTeRoYWtu83YWrVeUlHFu0rdQyrGW4PD0NpX7vTfc2Pjiu92Y9Pc6sx3Haj9N1/XOoeG8m/JAAjcgzswMBBXr141OWboWhJCqVRCpVIZH6tUKmPXksGBAwfQu3dviEQihISEICgoCDk5OQCqx0tWrlyJAQMGoHfv3oI+s7Gz1CXUK9Lb4n7NNfcz5ttPWC4RWd2POKekEh8euoWUG2W4plJDbWUTe3tZWxTEJ6ekEgV3nZt111I6E7WOw850Fe9zhDR2DbWvu6CWxYQJE7BkyRIMHToUOp0OO3bswK+//oqpU6cK+pDo6Gjk5uYiPz8f/v7+OHbsGKZPn27ymoCAAKSlpaFz5864c+cOcnJyEBQUBMYYPv74Y4SFheHRRx+1v4aNlKUB4pk2droK9XUzDu6CASq1HgGeUuNxa1NUX/nedNOZw9dKcaUwA+sf6yA4A6sl9vSjGlohNlJTOdVvV4rx8pdn7ZouTEhj0RCJLUWMr4+Ix9WrV7F//34UFBRAqVQiISEB7dq1E/xBZ86cwbZt28BxHOLi4vDYY49h3759AIDExEQUFRVhw4YNKC6u3ult7NixGDhwIC5duoT58+cjMjLS2HX15JNPokePHjY/09AysZermq18m8dbu3AZWgYnb5SZdVXZurNYsPe6xf1+E2P88NHEWJt1tpbp1dL7J8b4mX2prZXF1ZQKCT55IqbFBIzm1iUjBNVZOGvdUIKDRVPU2IOFPWyl+OC7KNc07YcMi4PjPcK98L8pf7NaZ77PrxmkbD0vtCx1ZWkrSiH6R/nYvUNbU9UYv9vORnUWzlqwENQN9c0331h8bsKECXYXiNjP1poCW9PmrOWdEjJ11laiMnum0jpqpz0DiQiIVsqRnqexmB7Emuo9nwkh1ggKFjUHpwHgzp07uHDhAnr16uWUQhFzttZY8F3wayfTUyokJpsSAbYz6dr6/JpBSmg/6pQ+rfF75h2LU4XtpWfAn3n8m04J02wb14Q4jKBg8corr5gdS01NxZEjRxxeIMLP2t0438ppvm6hIE8pYsM9kaWqBMDwYIgnZtgYULf1+XVd0Nc70tvm3h2u8mCIZ0MXgZBGr857e3bt2hUpKSmOLAuxYkqf1gjzMV1AZm2qLF+3UX65DlkqDXpGeuPT8TFYOjpa8MAu3+fXJ1/WjIHhZu/XEIK9ZZgx0LnZAAhpDgS1LPLy8kweV1ZW4siRI81qVWRjZ296DUvdRsVqPfZdLra6DakjPt/e91PIxPjjr1JYW2cnEZmv5K4rPw8J+ncIdHiyNUKaK0HBovaaCLlcjqioKLz66qtOKRThZ8/caluDyHXZRcvRc7trv99bu7Ksdk05KlAYZml1jQ5rcbNkCKmres+GIo2TrY2UAOcnHrPXjIHhZptEOZqfh9Qlq10JaW7qPGZBGreaKQH8LGRndeQuWo4gpMy1U5x4SMV4b1ikMfVBiLf1cZCekd4UKAipA4sti5dfflnQG2zcuNFhhSGOZejmsbRgzpG7aDmKrTLX3OCl5rjJ0JjqrMbWFi821joT0hRYDBavvfaaK8tBnMjZ+/86g6HMHx66dW/RHEM7pTuCveVWx01q1tWw5aqQ3FmEEOssBosuXbq4shzEyRoi8ZgjXFNpUHxv34rD10pxVZVpc8yhqdaVkMZM8FZj169fx8WLF1FWVmayPwWl+yDOYivFiIG1BIeEEMcQFCySk5Oxbds2dO3aFampqXj44Ydx/vx5xMbGOrt8DeJmUQWW7r1OF58GJiTFiKXN62nGEyGOJShY7Ny5E3PnzkXnzp3x3HPP4c0338TZs2dx9OhRZ5fP5XJKKvGvXZdMth5tahef5nKnLSTFiNDWByGkfgRNnS0tLUXnzp0BVG+HynEcunfvjtOnTzu1cA1h04lck0ABWN/xrbEx3Gnvu1yMM9l3se9yMWbsyEROSWVDF81uQlKMNNTm9YS0NIKChb+/P/Lz8wEArVu3xqlTp3Dx4kVIpYKHPJqMpn7xsXd708ZMyPaRllofOSVaTPshAwv2Xm+SgZKQxkbQ1X7s2LHIzs5GUFAQxo0bh1WrVkGn0+G5555zdvlczpHZVRtCUw92tdma2cS3Ul0iAm6XaXG7rPpYU+tGJKQxshosVq1ahcGDB2PgwIEQi6sbId27d8eWLVug0+ng7u7ukkK60pQ+rXGpQGPSFdWUFnMJCXZ8YxpNNSdk7TUkOSX3g4QBjWEQUn9Wg4W/vz8+/vhjMMbQv39/DB48GG3atIFUKm2WXVBA9cVnyz97YOkvF5rMAraa+O60awY7S7OHPp/sBw+Xl9YxarY+pv2QYRYsgKbbsiKksbB6xZ80aRL++c9/IjU1FYcPH8a8efMQEhKCQYMGoX///mjVqpXgD0pNTcWWLVvAcRzi4+ORlJRk8nxFRQXWrl0LlUoFvV6P0aNHIy4uDgCwYcMGnDlzBr6+vli5cqXdlbRXhL+iyd6F2lqtbWlM48P9WZgzuHG1nuoyq6updyMS0ljZbB6IxWL06NEDPXr0QEVFBU6cOIHDhw/j66+/xkMPPYTZs2fb/BCO47B582bMmzcPSqUSc+bMQWxsLMLD7286s2fPHoSHh2P27NkoLS3FjBkzMGDAAEilUgwePBjDhw/H+vXr61fbFsJaP7+lMY38ssY1CFzX9RO2WlaEkLqxK+usQqFA9+7d0b17d/j6+uLixYuCzsvMzERISAiCg4MhlUrRt29fs132RCIRNBoNGGPQaDTw8vIyjpN06dIFXl5e9hSVWGDpzjvIu3F1s9V1VpeQGVSEEPsJGnjQarU4efIkDh48iPT0dHTu3BkTJkxAnz59BH1IUVERlEql8bFSqURGRobJa4YPH45ly5Zh6tSpUKvVmDVrljFYCJWcnIzk5GQAwJIlS+q8k59UKm22uwC+NUKBSwVnTAbwI/098MbQGAT4Nvw2pwYlldf5j2th83cTEAB8FB1m8zOa8+/ZEqpzy+CMOlsNFunp6Th48CD++OMP+Pn5YeDAgZg6dardhaiZS8pAJBKZPD537hzatGmD+fPnIy8vDwsXLkSnTp2gUCgEf05CQgISEhKMj+u6C1pAQECz3UHNA8Cq0VFmYxqtfeWNqs6WGgK+8vu/1/quVG/Ov2dLqM4tQ13rHBoaavE5q8FixYoV6Nu3L95++2107NjR7g82UCqVUKlUxscqlQp+fn4mrzlw4ACSkpIgEokQEhKCoKAg5OTkoH379nX+XMKvKWRlreusLupyIsQ5rPbzbNq0CS+++GK9AgUAREdHIzc3F/n5+dDpdDh27JhZEsKAgACkpaUBAO7cuYOcnBwEBQXV63NJ02Vr7KE5rVQnpCmw2rKQyRwz3VAikWDy5MlYtGgROI5DXFwcIiIisG/fPgBAYmIiHn/8cWzYsAGvv/46AGDixInw8fEBAHz44Ye4cOECysrK8NJLL2H8+PEYMmSIQ8pGGq+6zOqi9RSEOIfLVtYZpt/WlJiYaPy3v78/5s2bx3vuzJkznVk00gTRegpCXMu+6UaENBJCMtISQhzHrpZFYWEhioqK6j2GQUh9NcV9xQlpygQFi8LCQqxZswbXr18HAHzxxRc4ceIEUlNT8dJLLzmzfIRY1BRmdRHSXAjqhtq0aRO6d++Obdu2GRMIdu3aFefPn3dq4QghhDQOgoJFZmYmkpKSTFZUKxQKVFRUOK1ghBBCGg9BwcLX1xe3b982OXbr1q0Wt4SeEEJaKkFjFqNHj8bSpUuRlJQEjuNw5MgR7NixwyzNOCGEkOZJULAYMmQIvLy88Ntvv0GpVOLQoUOYMGECevXq5ezyEYHqmyeJEEKsERQsOI5Dr169KDg0UpQniRDibILGLF588UV89tlnuHTpkrPLQ+qA8iQRQpxNUMti3rx5OHr0KNasWQOxWIx+/fqhf//+iIyMdHb5iACUJ4kQ4myCgkVUVBSioqLw9NNP48KFCzhy5Aj+85//oFWrVlixYoWzy0hsoDxJhBBnszs3VGhoKMLDw6FUKlFQUOCMMhE7UZ4kQoizCWpZlJeX448//sCRI0eQkZGBrl27YuzYsWZ7UpCGQXmSCCHOJihYTJ06FTExMejfvz/eeOMNu7Y6Ja5BeZIIIc4kKFisW7fObBtUQgghLYfFYHHhwgV06dIFAJCdnY3s7Gze1z344IPOKRkhhJBGw2Kw2Lx5M1auXAkA2LhxI+9rRCIRPvroI+eUjBBCSKNhMVgYAgUArF+/3iWFaWooxQYhpKUQNGaxbNky/Pvf/zY7vmLFCrzxxhuCPig1NRVbtmwBx3GIj483S0JYUVGBtWvXQqVSQa/XY/To0YiLixN0bkOgFBuEkJZE0DqL9PR0u47XxnEcNm/ejLlz52L16tU4evQobt26ZfKaPXv2IDw8HMuXL8eCBQvw+eefQ6fTCTq3IVCKDUJIS2K1ZfHNN98AAHQ6nfHfBnl5eQgMDBT0IZmZmQgJCUFwcDAAoG/fvkhJSUF4eLjxNSKRCBqNBowxaDQaeHl5QSwWCzq3IVCKDUJIS2I1WKhUKgDVLQPDvw0CAgIwfvx4QR9SVFQEpVJpfKxUKpGRkWHymuHDh2PZsmWYOnUq1Go1Zs2aBbFYLOhcg+TkZCQnJwMAlixZUufNmaRSqc1zw5S5OJN91/y4v1eT3BRKSJ2bG6pzy0B1dtB7WnvylVdeAQB07NgRCQkJdf4QxpjZMZFIZPL43LlzaNOmDebPn4+8vDwsXLgQnTp1EnSuQUJCgkk5CwsL61TegIAAm+c+290fZ64XmXRFhfnI8Wx3/zp/bkMSUufmhurcMlCdhQsNDbX4nKAxC5lMhr/++svk2PXr13Ho0CFBBVAqlSYtE5VKZbbI78CBA+jduzdEIhFCQkIQFBSEnJwcQec2BEOKjcQYP/QI90JijB8NbhNCmi1BweKbb74x6QoCqiPX//73P0EfEh0djdzcXOTn50On0+HYsWNmeaUCAgKQlpYGALhz5w5ycnIQFBQk6NyGYkix8dFjHbBgWFsKFISQZkvQ1Fm1Wm2WD0qhUKC8vFzQh0gkEkyePBmLFi0Cx3GIi4tDREQE9u3bBwBITEzE448/jg0bNuD1118HAEycOBE+Pj4AwHsuIYQQ1xEULMLDw3HixAn07dvXeOzkyZN2zUjq0aMHevToYXIsMTHR+G9/f3/MmzdP8LmEEEJcR1CwmDhxIj744AMcO3YMISEhuH37NtLS0jBnzhxnl48QQkgjIChYdOrUCStXrsSRI0dQWFiI9u3bY9KkSS1uOhohhLRUgoIFUD0APWbMGJSUlDSK2UiEEEJcR/BOeZ999hlOnDgBqVSKL774AqdOnUJmZib+8Y9/OLuMhBBCGpigqbOffvopFAoFNmzYAKm0Or507NgRx44dc2rhCCGENA6CWhZpaWn45JNPjIECAHx8fFBSUuK0ghFCCGk8BLUsFAoFysrKTI4VFhbS2AUhhLQQgoJFfHw8Vq5ciT///BOMMVy5cgXr16/H0KFDnV0+QgghjYCgbqixY8dCJpNh8+bN0Ov12LhxIxISEjBy5Ehnl48QQkgjIChYiEQijBo1CqNGjXJ2eQghhDRCFoPFhQsX0KVLFwDAn3/+afkNpFIEBgaaJRokhBDSfFgMFps3b8bKlSsBABs3brT4BowxlJWVYcSIEXjqqaccX0JCCCENzmKwMAQKAFi/fr3VNyktLcWMGTMoWBBCSDMlON0Hx3G4cuUKiouL4e/vjw4dOkAsrp5M5ePjYzFjLCGEkKZPULD466+/sHz5clRVVcHf3x9FRUWQyWR444030LZtWwDVGxwRQghpngQFi40bN2LYsGF49NFHIRKJwBjD7t27sXHjRixdutTZZSSEENLABAWL3NxcjBo1CiKRCED1VNqRI0fiu+++c2rhGlJOSSU2nchF4d0qBHjJMKVPa9o2lRDSYgkKFt27d8epU6fQq1cv47FTp06he/fuTitYQ7pZVIEZOzKRXao1HkvPLceav7engEEIaZEsBot169YZWxIcx+HDDz9Eu3btoFQqoVKpcPXqVcTGxrqsoK704f4sk0ABANmlWmw6kYsFw9o2TKEIIaQBWQwWISEhJo8jIiKM/w4PD0e3bt3s+qDU1FRs2bIFHMchPj4eSUlJJs//+OOPOHz4MIDq4HTr1i1s3rwZXl5e+Pnnn/Hbb7+BMYb4+HinryTPL63kPV5YXuXUzyWEkMbKYrB44oknHPYhHMdh8+bNmDdvHpRKJebMmYPY2FiEh4cbXzNmzBiMGTMGQHUX1+7du+Hl5YUbN27gt99+w+LFiyGVSrF48WL06NEDrVu3dlj5agvy4e9qCvCUOe0zCSGkMbM5ZqHX63H48GGcP38eZWVl8Pb2xkMPPYQBAwaY7G9hTWZmJkJCQhAcHAwA6Nu3L1JSUkyCRU1Hjx5Fv379AADZ2dno0KED3NyqL+CdO3fGyZMnMXbsWEGfXRczh0TjzPUik66oMB85pvRxXoAihJDGzGqK8oqKCsybNw9ffvklJBIJoqKiIJFI8NVXX+Gdd95BRUWFoA8pKioyyR2lVCpRVFTE+9rKykqkpqaiT58+AKq7vy5evIiysjJUVlbi7NmzUKlUQutXJxH+Cqz5e3skxvihR7gXEmP8aHCbENKiWW0afPXVV/Dx8cG7774Ld3d343GNRoPVq1fjq6++wgsvvGDzQxhjZscMg+e1nT59GjExMfDy8gJQPT4yduxYvP/++3B3d0ebNm2MK8drS05ORnJyMgBgyZIlCAgIsFk2PlKpFF2jw/BRdFidzm+KpFJpnX9eTRXVuWWgOjvoPa09mZKSgkWLFpkECgBwd3fH888/j3nz5gkKFoYZVAYqlcriLntHjx5F//79TY4NGTIEQ4YMAVAdwCxluE1ISEBCQoLxcWFhoc2y8QkICKjzuU0V1blloDq3DHWtc2hoqMXnbHZD+fv78z6nVCqhVqsFFSA6Ohq5ubnIz8+HTqfDsWPHeKfdVlRU4MKFC2bPGfb6LiwsxMmTJ43jGYQQQlzDassiODgYf/75J7p27Wr2XFpaGoKCggR9iEQiweTJk7Fo0SJwHIe4uDhERERg3759AIDExEQAwMmTJ9GtWzezlszKlStRVlYGqVSK559/3thFRQghxDVEjG9A4Z7ff/8dX331FSZPnoxevXpBLBaD4zicPHkS/+///T88+eSTiIuLc2V57ZKTk1On86jZ2jJQnVsGqrNw1rqhrLYsBg8ejLKyMmzYsAFr1qyBj48PSktLIZPJMG7cuEYdKAghhDiOzYUSo0ePRkJCAi5fvmxcZ9GxY0coFApXlI8QQkgjIGhVnYeHBx5++GEnF4UQQkhjZXU2FCGEEAJQsCCEECIABQtCCCE2UbAghBBiEwULQgghNlGwIIQQYhMFC0IIITZRsCCEEGITBQtCCCE2UbAghBBiEwULQgghNlGwIIQQYhMFC0IIITZRsCCEEGITBQtCCCE2UbAghBBiEwULQgghNgnaKc8RUlNTsWXLFnAch/j4eCQlJZk8/+OPP+Lw4cMAAI7jcOvWLWzevBleXl746aefsH//fohEIkREROCVV16BXC53VdEJIaTFc0mw4DgOmzdvxrx586BUKjFnzhzExsYiPDzc+JoxY8ZgzJgxAIBTp05h9+7d8PLyQlFREX755ResXr0acrkcq1atwrFjxzB48GBXFJ0QQghc1A2VmZmJkJAQBAcHQyqVom/fvkhJSbH4+qNHj6Jfv37GxxzHQavVQq/XQ6vVws/PzxXFJoQQco9LWhZFRUVQKpXGx0qlEhkZGbyvraysRGpqKp5//nkAgL+/P0aPHo2XX34Zcrkc3bp1Q7du3XjPTU5ORnJyMgBgyZIlCAgIqFN5pVJpnc9tqqjOLQPVuWVwRp1dEiwYY2bHRCIR72tPnz6NmJgYeHl5AQDu3r2LlJQUrF+/HgqFAqtWrcKhQ4cwcOBAs3MTEhKQkJBgfFxYWFin8gYEBNT53KaK6twyOKLOjDFoNBpwHGfx77gxcXNzQ2VlZUMXw6Ws1ZkxBrFYDHd3d7PfX2hoqMX3dEmwUCqVUKlUxscqlcpiV9LRo0fRv39/4+O0tDQEBQXBx8cHANC7d29cuXKFN1gQQpxPo9FAJpNBKnXZ/Jh6kUqlkEgkDV0Ml7JVZ51OB41GAw8PD8Hv6ZIxi+joaOTm5iI/Px86nQ7Hjh1DbGys2esqKipw4cIFk+cCAgKQkZGByspKMMaQlpaGsLAwVxSbEMKD47gmEygIP6lUCo7j7DvHSWUxIZFIMHnyZCxatAgcxyEuLg4RERHYt28fACAxMREAcPLkSXTr1g3u7u7Gczt06IA+ffrgrbfegkQiQdu2bU26mgghrtUUup6Ibfb+HkWMb0ChmcjJyanTedSX3TJQneumoqICCoXCQSVyPqlUCp1O19DFcCkhdeb7PVobs6AV3ISQJicnJwfPPfcc+vXrh759+2L+/PnQarUAgG+++QZvv/0273mGtVz22rNnD65cuWJ8vHz5chw6dKhO72XwzTff4JVXXjE5VlRUhIceesji4LS1ujkbBQtCiFPllFRiwd7rmPZDBhbsvY6ckvrNTGKM4cUXX8Tw4cNx9OhRHD58GOXl5Vi6dKnNc3/88cc6fWbtYPHmm2/We5LNyJEjcejQIajVauOxn376CYmJiXBzc6vXezsDBQtCiNPklFRixo5M7LtcjDPZd7HvcjFm7MisV8A4cuQI3NzcMGHCBADVY6ILFizA//73P+OFNycnBxMnTsSAAQOwYsUK47kdOnQw/nvjxo0YOXIkEhISTF7z3XffGafhv/baa0hJScGvv/6K999/H0OHDsX169cxc+ZMYxqiqVOnGs89duwYnn32WQDAwYMHMXr0aAwbNgxTpkxBeXm5ST28vb3Rp08f49gtUB3Mxo4di3379uHRRx9FYmIiJkyYgIKCArOfg6EMfHVbv349b93qg4IFIcRpNp3IRXap1uRYdqkWm07k1vk9r1y5goceesjkmLe3N8LCwnDt2jUA1bno1q1bh3379uHHH3/EuXPnTF5/8OBBXLt2Dbt378a+fftw/vx5nDhxApcvX8batWvx7bffIjk5Gf/5z3/Qs2dPDB06FPPmzcOvv/6Ktm3bGt9n4MCBOHPmDCoqKgBUX+zHjBmDoqIirFmzBt988w327t2Lbt26YdOmTWZ1GTt2rLG1c/v2bVy9ehX9+vVDr169sGvXLuzbtw9jx47Fhg0bBP98Dh48iKtXr5rVrb5o/hshxGkK71bxHy/nPy4EY4x3Jk/N4wMGDIC/vz8AYNSoUcaZlgYHDx7EwYMHjTMxKyoqcO3aNVy4cAGjRo0ynmsrtZBUKkVcXBx+/fVXjBo1Cr/99hvmzZuH48eP48qVKxg7diwAoKqqCo888ojZ+QkJCZg7dy7Kysqwa9cujBo1ChKJBLm5uXj55ZeRn58PrVaLyMhIwT8fS3Xr06eP4PfgrWu9ziaEECsCvGT8xz35jwvRsWNH/PzzzybHysrKkJOTg7Zt2+L8+fNmwaT2Y8YYpk2bhmeeecbk+ObNm+2eUjp69Ghs27YNrVq1wsMPPwwvLy8wxjBw4ECbLQIPDw8MHjwYv/zyC3bu3IkFCxYAAN555x1MmTIFiYmJOHbsGFatWmV2bs21EowxVFVVGf89ffp0PPXUU3bVwxbqhiKEOM2UPq0R5mO6nUCYjxxT+rSu83sOGDAAarUa3333HQBAr9fjP//5D8aPH29ckXz48GEUFxdDrVbjl19+Qc+ePU3eY/Dgwfjmm2+M4wi5ubkoLCxE//79sWvXLhQVFQEAiouLAQBeXl5mYw4Gffv2RVpaGr788kuMHj0aAPDII48gJSXF2C2mVquRlZXFe35SUhI2bdqEwsJCY+ujtLQUISEhAGCsZ23h4eFIS0sDAOzdu9cYLAYPHoyvvvrKrG71RcHCARw924OQ5iLU1w1r/t4eiTF+6BHuhcQYP6z5e3uE+tZ9to9IJMJnn32Gn376Cf369cOAAQPg5uaG2bNnG1/Ts2dPTJ8+HYmJiXj00UeNXVCGVsOgQYOQlJSEMWPGID4+HlOmTMHdu3cRExOD6dOnY9y4cUhISMB7770HoHpsYePGjUhMTMT169dNyiORSJCQkIADBw5g6NChAKpTHK1evRqvvvoqEhISMHr0aIvBYtCgQcjLy8OYMWOM5Xv99dcxdepU/P3vfzd2idU2ceJEHD9+HKNGjcLZs2eNayYGDRqExx57zKxu9UWL8njYs3DJMNuj5iBemI+83n8QrkYL1FqGlrwor6ioCMOHD8fJkycbukhOR4vyGiFnzPYghDjW7du3MWbMGLz00ksNXZQmiwa468kZsz0IIY4VEhKCI0eONHQxmjRqWdSTM2Z7EEJIY0PBop6cMduDEEIaG+qGqifDbI9NJ3JRWF6FAE8ZpvRp3aQGtwkhxBYKFg4Q6uuGBcPaNnQxCCHEaagbihDiVLqsLGi2bkPF0mXQbN0GnYX1BvaIiIjA0KFDkZCQgGHDhiElJaVO7/Ppp5+aZH01WLlyJT744AOTY3/++ScGDRpk8b1WrlyJjz/+uE7laAooWAhAi+4IqRtdVha0334HVlYGUWAgWFkZtN9+V++A4e7ujl9//RXJycmYM2cOlixZUqf3+eyzz3iDRc0EfwY//vgjkpKS6vQ5zQF1Q9nAt+guPbe8yS26I8QZqk6eBLuXGoP3+SNHwTQaiGqkymAaDSq3bAXXvx/vOSJ/f8h69RJchrKyMvj6+hofb9y4Ebt27YJWq8Xw4cMxe/ZsVFRUYOrUqcjNzQXHcZgxYwYKCwuRl5eHJ554An5+fvj++++N79G+fXv4+PjgzJkz6NGjBwBg165d+PLLL43/abVaREVFYe3atcY0Iwbjxo3DO++8g27duqGoqAgjRozAH3/8Ab1ej8WLF+P48ePQarV49tlnzfJTNVYULGywtuiOxikIsY6VlgLe3qYH3dyqj9eDRqPB0KFDUVlZifz8fHz77bcATFOPM8YwadIkHD9+HPn5+QgJCcEXX3wBoDr3ko+PDzZt2oTvvvuON6VGUlISdu7ciR49euD06dPw8/NDu3bt0KpVK0ycOBEAsHTpUnz99deYPHmyoHJ//fXX8Pb2xs8//4zKykokJSVh0KBBdmWVbSguCxapqanYsmULOI5DfHy8WXPuxx9/xOHDhwEAHMfh1q1b2Lx5M0pLS7F69Wrj6/Lz8zF+/HiMGjXKJeWmRXeEWGarBcDdzqvugqoRMFhZGUQdOkA+fHidP9fQDQUAp06dwowZM7B//37e9NxXr15FbGwsFi5ciEWLFiEhIQG9e/e2+RljxozB2LFj8e6772Lnzp3GdOOXL1/GsmXLUFpaivLycqvjGLUdPHgQFy9exO7duwFUt4quXbtGwcKA4zhs3rwZ8+bNg1KpxJw5cxAbG4vw8HDja8aMGWPcH/fUqVPYvXs3vLy84OXlheXLlxvfZ+rUqehlRxO1vmjRXeORU1JZPUX5bhUCvGiKclMgHdAf2m/vZU319ATKy8Hu3oVs5AiHfUZsbCyKioqgUql4U48b8iT98ssv2L9/Pz744AMMGjQIs2bNsvq+YWFhiIiIwPHjx/Hzzz8bxzBmzZqFzZs344EHHsA333yD48ePm50rkUiM6cM1Go3Jc++//z4GDx5cz1q7nksGuDMzMxESEoLg4GBIpVL07dvX6uyFo0ePol8/8/7MtLQ0hISEIDAw0JnFNUGL7hoHZ2zPSZxPGh0N+fgnIPL2BisogMjbG/LxT0AaHe2wz8jMzIRer4efnx9v6vGCggLcvn0bHh4eePzxx/HSSy8ZU3t7eXlZzcg6duxYLFiwAG3btjUm2bt79y6Cg4NRVVWFHTt28J4XERGB8+fPA4CxFQFUZ4T9/PPPjenEs7KyjLvsNXYuaVkUFRVBqVQaHyuVSmRkZPC+trKyEqmpqXj++efNnrMURAySk5ORnJwMAFiyZAkCAgLqVF6pVGo8NyAA+HyyHz7cn4X8skoEebth5pBoRPg3naybQtSsc2P0we9pvGNH284WYeW4hyycZV1jr7MzOKLOeXl5kEqFXzqkMTFATEy9PrM2jUZj7GpijGHdunVwc3NDfHw8srKyjL0Unp6e2LBhA65du4b33nsPYrEYMpkMS5cuhVQqxTPPPINnnnkGQUFBvBf+pKQkvPvuu1i8eLGxzrNnz8ajjz6KiIgIdOrUCeXl5ZBKpRCLxRCLxZBKpXj11Vfx4osvYvv27ejfvz9EIhGkUin++c9/Ijs7G8OHDwdjDEqlEtu2bbPr5ymUrfd0c3Oz67vgkhTlx48fx7lz54wZHw8dOoTMzEzeQaFjx47h0KFDJrnpAUCn02Hq1KlYuXIlWrVqJehzXZGivLlo7HWe9kMGzmSb3wH2CPfCR4914DnDtsZeZ2doySnKW5Imm6JcqVRCpVIZH6tUKot72x49ehT9+/c3O3727FlERUUJDhSkeaGxI0IalkuCRXR0NHJzc5Gfnw+dTodjx44hNjbW7HUVFRW4cOEC73O2uqBI80ZjR4Q0LJeMWUgkEkyePBmLFi0Cx3GIi4tDREQE9u3bBwDGvseTJ0+iW7ducHd3Nzm/srIS58+fx5QpU1xRXNIIUcLGxqMZb67Zotj7e6RtVXlQX3bLQHWuG7VaDZlM5pRBWWegMQtzOp0OVVVVZivPrY1ZNI3fNiGk0XB3d4dGo0FlZSVEIlFDF8cmNzc3VFa2rCnW1urMGINYLDbrwbGFggUhxC4ikcjsjrQxoxakY1DWWUIIITZRsCCEEGITBQtCCCE2NevZUIQQQhyDWhY8aqcaaQmozi0D1bllcEadKVgQQgixiYIFIYQQmyhY8EhISGjoIrgc1blloDq3DM6oMw1wE0IIsYlaFoQQQmyiYEEIIcQmyg1VQ2pqKrZs2QKO4xAfH4+kpKSGLpJDbNiwAWfOnIGvry9WrlwJoHof4dWrV6OgoACBgYGYNWsWvLy8AAA7duzA/v37IRaL8dxzz+Hhhx9uwNLXTWFhIdavX487d+5AJBIhISEBI0eObNb11mq1ePfdd6HT6aDX69GnTx+MHz++WdfZgOM4zJ49G/7+/pg9e3azr/Orr74Kd3d3iMViSCQSLFmyxPl1ZoQxxpher2fTpk1jt2/fZlVVVeyNN95gN2/ebOhiOUR6ejrLyspi//rXv4zHvvjiC7Zjxw7GGGM7duxgX3zxBWOMsZs3b7I33niDabValpeXx6ZNm8b0en1DFLteioqKWFZWFmOMsYqKCjZ9+nR28+bNZl1vjuOYWq1mjDFWVVXF5syZwy5fvtys62ywa9cu9uGHH7IPPviAMdb8v9+vvPIKKykpMTnm7DpTN9Q9mZmZCAkJQXBwMKRSKfr27YuUlJSGLpZDdOnSxXiHYZCSkoJBgwYBAAYNGmSsa0pKCvr27QuZTIagoCCEhIQgMzPT5WWuLz8/P7Rr1w4A4OHhgbCwMBQVFTXreotEImPaab1eD71eD5FI1KzrDFRv03zmzBnEx8cbjzX3OvNxdp0pWNxTVFQEpVJpfKxUKlFUVNSAJXKukpIS4z7ofn5+KC0tBWD+c/D392/yP4f8/Hxcu3YN7du3b/b15jgOb775Jl544QU89NBD6NChQ7Ov89atW/H000+b7K3R3OsMAIsWLcJbb72F5ORkAM6vM41Z3MN4ZhA3hY1dHI3v59CUaTQarFy5EpMmTYJCobD4uuZSb7FYjOXLl6O8vBwrVqzAjRs3LL62OdT59OnT8PX1Rbt27ZCenm7z9c2hzgCwcOFC+Pv7o6SkBO+//77VHe4cVWcKFvcolUqoVCrjY5VKZYzSzZGvry+Ki4vh5+eH4uJi+Pj4ADD/ORQVFcHf37+hilkvOp0OK1euxIABA9C7d28ALaPeAODp6YkuXbogNTW1Wdf58uXLOHXqFM6ePQutVgu1Wo21a9c26zoDMJbZ19cXPXv2RGZmptPrTN1Q90RHRyM3Nxf5+fnQ6XQ4duwYYmNjG7pYThMbG4uDBw8CAA4ePIiePXsajx87dgxVVVXIz89Hbm4u2rdv35BFrRPGGD7++GOEhYXh0UcfNR5vzvUuLS1FeXk5gOqZUWlpaQgLC2vWdX7qqafw8ccfY/369Zg5cyYefPBBTJ8+vVnXWaPRQK1WG/99/vx5REZGOr3OtIK7hjNnzmDbtm3gOA5xcXF47LHHGrpIDvHhhx/iwoULKCsrg6+vL8aPH4+ePXti9erVKCwsREBAAP71r38ZB8G3b9+OAwcOQCwWY9KkSejevXsD18B+ly5dwvz58xEZGWnsTnzyySfRoUOHZlvvv/76C+vXrwfHcWCM4W9/+xvGjRuHsrKyZlvnmtLT07Fr1y7Mnj27Wdc5Ly8PK1asAFA9kaF///547LHHnF5nChaEEEJsom4oQgghNlGwIIQQYhMFC0IIITZRsCCEEGITBQtCCCE2UbAgxMUuXryIGTNmCHrt77//jnfeecfJJSLENlrBTYid5syZg+nTp0MsFmPVqlVYunQpnnnmGePzWq0WUqkUYnH1vdiUKVMwYMAA4/OdO3fGmjVrXF5uQuqDggUhdtDpdCgsLERISAhOnDiBqKgoAMAXX3xhfM2rr76KqVOnomvXrmbn6/V6SCQSl5WXEEehYEGIHW7evInw8HCIRCJkZWUZg4Ul6enpWLduHYYPH47du3eja9euGDJkCNatW4ePP/4YAPB///d/+O2331BSUgKlUoknn3wSvXr1Mnsvxhi2bduGI0eOoKqqCoGBgZg+fToiIyOdUldCaqJgQYgABw4cwLZt26DT6cAYw6RJk6DRaCCXy/H1119j2bJlCAoK4j33zp07uHv3LjZs2ADGGDIyMkyeDw4OxnvvvYdWrVrhxIkTWLduHdauXWuWyPLcuXO4ePEi1qxZA4VCgezsbHh6ejqtzoTURAPchAgQFxeHrVu3ol27dli0aBFWrFiBiIgIbNu2DVu3brUYKIDqVPfjx4+HTCaDXC43e/5vf/sb/P39IRaL0bdvX4ub00ilUmg0GmRnZ4MxhvDw8GadGZk0LtSyIMSGu3fvYtq0aWCMQaPRYMGCBaiqqgIAPPfcc3jiiScwatQoi+f7+PjwBgmDgwcP4qeffkJBQQGA6kyiZWVlZq978MEHMWzYMGzevBmFhYXo1asXnnnmGav7dBDiKBQsCLHBy8sLW7duxdGjR5Geno4pU6Zg+fLlGDZsGO8gdm3WNtEqKCjAJ598gvnz56Njx44Qi8V48803LW5YM3LkSIwcORIlJSVYvXo1fvzxR/zjH/+oc90IEYqCBSECXb161Tigff36deMe3/VRWVkJkUhk3KjmwIEDuHnzJu9rMzMzwRhDVFQU3NzcIJPJjNNzCXE2ChaECHT16lX87W9/Q1lZGcRisXGvgPoIDw/Ho48+irfffhtisRgDBw5ETEwM72vVajW2bduGvLw8yOVydOvWDWPGjKl3GQgRgvazIIQQYhO1YQkhhNhEwYIQQohNFCwIIYTYRMGCEEKITRQsCCGE2ETBghBCiE0ULAghhNhEwYIQQohN/x/SyENaC7Qr7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>204.500000</td>\n",
       "      <td>5.930149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>9.104334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>8.653837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>31.700000</td>\n",
       "      <td>6.600505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.842606</td>\n",
       "      <td>0.014768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.840770</td>\n",
       "      <td>0.029839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.866291</td>\n",
       "      <td>0.024475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.817240</td>\n",
       "      <td>0.037863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.852690</td>\n",
       "      <td>0.012290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.842383</td>\n",
       "      <td>0.014941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.841707</td>\n",
       "      <td>0.015226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.841766</td>\n",
       "      <td>0.015308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.685311</td>\n",
       "      <td>0.029308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.846320</td>\n",
       "      <td>0.027509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.841766</td>\n",
       "      <td>0.015308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       204.500000     5.930149\n",
       "1                    TN       174.000000     9.104334\n",
       "2                    FP        39.000000     8.653837\n",
       "3                    FN        31.700000     6.600505\n",
       "4              Accuracy         0.842606     0.014768\n",
       "5             Precision         0.840770     0.029839\n",
       "6           Sensitivity         0.866291     0.024475\n",
       "7           Specificity         0.817240     0.037863\n",
       "8              F1 score         0.852690     0.012290\n",
       "9   F1 score (weighted)         0.842383     0.014941\n",
       "10     F1 score (macro)         0.841707     0.015226\n",
       "11    Balanced Accuracy         0.841766     0.015308\n",
       "12                  MCC         0.685311     0.029308\n",
       "13                  NPV         0.846320     0.027509\n",
       "14              ROC_AUC         0.841766     0.015308"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>407.500000</td>\n",
       "      <td>11.177855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>347.700000</td>\n",
       "      <td>9.416888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>82.300000</td>\n",
       "      <td>10.133004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>8.249579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.835373</td>\n",
       "      <td>0.846496</td>\n",
       "      <td>0.849833</td>\n",
       "      <td>0.843159</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.830923</td>\n",
       "      <td>0.839822</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.858732</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.840044</td>\n",
       "      <td>0.009392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.837953</td>\n",
       "      <td>0.832636</td>\n",
       "      <td>0.844130</td>\n",
       "      <td>0.832323</td>\n",
       "      <td>0.795229</td>\n",
       "      <td>0.829832</td>\n",
       "      <td>0.824701</td>\n",
       "      <td>0.813765</td>\n",
       "      <td>0.865306</td>\n",
       "      <td>0.845070</td>\n",
       "      <td>0.832095</td>\n",
       "      <td>0.018873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.845161</td>\n",
       "      <td>0.872807</td>\n",
       "      <td>0.877895</td>\n",
       "      <td>0.876596</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.847639</td>\n",
       "      <td>0.880851</td>\n",
       "      <td>0.872017</td>\n",
       "      <td>0.874227</td>\n",
       "      <td>0.850202</td>\n",
       "      <td>0.869025</td>\n",
       "      <td>0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.824900</td>\n",
       "      <td>0.819400</td>\n",
       "      <td>0.818400</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.771600</td>\n",
       "      <td>0.812900</td>\n",
       "      <td>0.794900</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.840600</td>\n",
       "      <td>0.809900</td>\n",
       "      <td>0.808910</td>\n",
       "      <td>0.019518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.841542</td>\n",
       "      <td>0.852248</td>\n",
       "      <td>0.860681</td>\n",
       "      <td>0.853886</td>\n",
       "      <td>0.841220</td>\n",
       "      <td>0.838641</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.841885</td>\n",
       "      <td>0.869744</td>\n",
       "      <td>0.847629</td>\n",
       "      <td>0.849933</td>\n",
       "      <td>0.009869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.835344</td>\n",
       "      <td>0.846350</td>\n",
       "      <td>0.849585</td>\n",
       "      <td>0.842837</td>\n",
       "      <td>0.831439</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.839356</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.858665</td>\n",
       "      <td>0.831978</td>\n",
       "      <td>0.839804</td>\n",
       "      <td>0.009438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.835123</td>\n",
       "      <td>0.846263</td>\n",
       "      <td>0.848917</td>\n",
       "      <td>0.842309</td>\n",
       "      <td>0.831472</td>\n",
       "      <td>0.830536</td>\n",
       "      <td>0.838759</td>\n",
       "      <td>0.831381</td>\n",
       "      <td>0.857715</td>\n",
       "      <td>0.830258</td>\n",
       "      <td>0.839273</td>\n",
       "      <td>0.009366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.835023</td>\n",
       "      <td>0.846110</td>\n",
       "      <td>0.848145</td>\n",
       "      <td>0.841561</td>\n",
       "      <td>0.832238</td>\n",
       "      <td>0.830286</td>\n",
       "      <td>0.837861</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.857403</td>\n",
       "      <td>0.830039</td>\n",
       "      <td>0.838965</td>\n",
       "      <td>0.009206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.670279</td>\n",
       "      <td>0.693543</td>\n",
       "      <td>0.698601</td>\n",
       "      <td>0.685935</td>\n",
       "      <td>0.669229</td>\n",
       "      <td>0.661277</td>\n",
       "      <td>0.679672</td>\n",
       "      <td>0.665022</td>\n",
       "      <td>0.715484</td>\n",
       "      <td>0.660535</td>\n",
       "      <td>0.679958</td>\n",
       "      <td>0.018221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.832600</td>\n",
       "      <td>0.862200</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.878800</td>\n",
       "      <td>0.832200</td>\n",
       "      <td>0.858900</td>\n",
       "      <td>0.854300</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>0.815900</td>\n",
       "      <td>0.849900</td>\n",
       "      <td>0.018093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.835023</td>\n",
       "      <td>0.846110</td>\n",
       "      <td>0.848145</td>\n",
       "      <td>0.841561</td>\n",
       "      <td>0.832238</td>\n",
       "      <td>0.830286</td>\n",
       "      <td>0.837861</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.857403</td>\n",
       "      <td>0.830039</td>\n",
       "      <td>0.838965</td>\n",
       "      <td>0.009206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  393.000000  398.000000  417.000000  412.000000   \n",
       "1                    TN  358.000000  363.000000  347.000000  346.000000   \n",
       "2                    FP   76.000000   80.000000   77.000000   83.000000   \n",
       "3                    FN   72.000000   58.000000   58.000000   58.000000   \n",
       "4              Accuracy    0.835373    0.846496    0.849833    0.843159   \n",
       "5             Precision    0.837953    0.832636    0.844130    0.832323   \n",
       "6           Sensitivity    0.845161    0.872807    0.877895    0.876596   \n",
       "7           Specificity    0.824900    0.819400    0.818400    0.806500   \n",
       "8              F1 score    0.841542    0.852248    0.860681    0.853886   \n",
       "9   F1 score (weighted)    0.835344    0.846350    0.849585    0.842837   \n",
       "10     F1 score (macro)    0.835123    0.846263    0.848917    0.842309   \n",
       "11    Balanced Accuracy    0.835023    0.846110    0.848145    0.841561   \n",
       "12                  MCC    0.670279    0.693543    0.698601    0.685935   \n",
       "13                  NPV    0.832600    0.862200    0.856800    0.856400   \n",
       "14              ROC_AUC    0.835023    0.846110    0.848145    0.841561   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   400.000000  395.000000  414.000000  402.000000  424.000000  420.000000   \n",
       "1   348.000000  352.000000  341.000000  346.000000  348.000000  328.000000   \n",
       "2   103.000000   81.000000   88.000000   92.000000   66.000000   77.000000   \n",
       "3    48.000000   71.000000   56.000000   59.000000   61.000000   74.000000   \n",
       "4     0.832036    0.830923    0.839822    0.832036    0.858732    0.832036   \n",
       "5     0.795229    0.829832    0.824701    0.813765    0.865306    0.845070   \n",
       "6     0.892857    0.847639    0.880851    0.872017    0.874227    0.850202   \n",
       "7     0.771600    0.812900    0.794900    0.790000    0.840600    0.809900   \n",
       "8     0.841220    0.838641    0.851852    0.841885    0.869744    0.847629   \n",
       "9     0.831439    0.830833    0.839356    0.831650    0.858665    0.831978   \n",
       "10    0.831472    0.830536    0.838759    0.831381    0.857715    0.830258   \n",
       "11    0.832238    0.830286    0.837861    0.830986    0.857403    0.830039   \n",
       "12    0.669229    0.661277    0.679672    0.665022    0.715484    0.660535   \n",
       "13    0.878800    0.832200    0.858900    0.854300    0.850900    0.815900   \n",
       "14    0.832238    0.830286    0.837861    0.830986    0.857403    0.830039   \n",
       "\n",
       "           ave        std  \n",
       "0   407.500000  11.177855  \n",
       "1   347.700000   9.416888  \n",
       "2    82.300000  10.133004  \n",
       "3    61.500000   8.249579  \n",
       "4     0.840044   0.009392  \n",
       "5     0.832095   0.018873  \n",
       "6     0.869025   0.015900  \n",
       "7     0.808910   0.019518  \n",
       "8     0.849933   0.009869  \n",
       "9     0.839804   0.009438  \n",
       "10    0.839273   0.009366  \n",
       "11    0.838965   0.009206  \n",
       "12    0.679958   0.018221  \n",
       "13    0.849900   0.018093  \n",
       "14    0.838965   0.009206  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_xgb0</th>\n",
       "      <th>y_pred_xgb1</th>\n",
       "      <th>y_pred_xgb2</th>\n",
       "      <th>y_pred_xgb3</th>\n",
       "      <th>y_pred_xgb4</th>\n",
       "      <th>y_pred_xgb_ave</th>\n",
       "      <th>y_pred_xgb_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>4487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>4488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>4489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>4490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>4491</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.489898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_test_idx0  y_test0  y_pred_xgb0  y_pred_xgb1  y_pred_xgb2  \\\n",
       "0               0      0.0          0.0          0.0          0.0   \n",
       "1               1      1.0          1.0          1.0          1.0   \n",
       "2               2      1.0          1.0          1.0          1.0   \n",
       "3               3      1.0          1.0          1.0          1.0   \n",
       "4               4      1.0          0.0          0.0          1.0   \n",
       "...           ...      ...          ...          ...          ...   \n",
       "4487         4487      1.0          1.0          1.0          1.0   \n",
       "4488         4488      1.0          1.0          1.0          1.0   \n",
       "4489         4489      0.0          0.0          1.0          0.0   \n",
       "4490         4490      0.0          0.0          0.0          0.0   \n",
       "4491         4491      1.0          0.0          0.0          1.0   \n",
       "\n",
       "      y_pred_xgb3  y_pred_xgb4  y_pred_xgb_ave  y_pred_xgb_std  \n",
       "0             0.0          0.0             0.0        0.000000  \n",
       "1             1.0          1.0             1.0        0.000000  \n",
       "2             1.0          1.0             1.0        0.000000  \n",
       "3             1.0          1.0             1.0        0.000000  \n",
       "4             0.0          0.0             0.2        0.400000  \n",
       "...           ...          ...             ...             ...  \n",
       "4487          1.0          1.0             1.0        0.000000  \n",
       "4488          1.0          1.0             1.0        0.000000  \n",
       "4489          0.0          0.0             0.2        0.400000  \n",
       "4490          0.0          0.0             0.0        0.000000  \n",
       "4491          1.0          1.0             0.6        0.489898  \n",
       "\n",
       "[4492 rows x 9 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"logloss\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_xgb)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_xgb))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_xgb))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_xgb))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_xgb))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_xgb, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_xgb, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_xgb))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_xgb))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_xgb))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "xgb_5preds = pd.concat([xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "xgb_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c2883ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_xgb.to_csv('mat_met_xgb_opt.csv')\n",
    "xgb_5preds.to_csv('xgb_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:42:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:42:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost baseline model f1_score 0.8267 with a standard deviation of 0.0183\n",
      "XGBoost optimized model f1_score 0.8442 with a standard deviation of 0.0181\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_clf, X, Y, cv=10, scoring=\"f1_macro\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"f1_macro\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"f1_macro\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_xgb_clf.joblib']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_clf, \"./xgb_clf.joblib\")\n",
    "#joblib.dump(optimized_xgb, \"./optimized_xgb.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"./optimizedCV_xgb_clf.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       197.400000     9.559172\n",
      "1                    TN       169.200000    10.612362\n",
      "2                    FP        43.800000     9.425733\n",
      "3                    FN        38.800000     6.892830\n",
      "4              Accuracy         0.816115     0.015921\n",
      "5             Precision         0.819460     0.030795\n",
      "6           Sensitivity         0.835778     0.028490\n",
      "7           Specificity         0.794510     0.042712\n",
      "8              F1 score         0.826811     0.015336\n",
      "9   F1 score (weighted)         0.815843     0.016221\n",
      "10     F1 score (macro)         0.815062     0.016419\n",
      "11    Balanced Accuracy         0.815149     0.016671\n",
      "12                  MCC         0.631940     0.031574\n",
      "13                  NPV         0.814150     0.024311\n",
      "14              ROC_AUC         0.815149     0.016671\n",
      "CPU times: user 21.5 s, sys: 4 ms, total: 21.5 s\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    \n",
    "    knn_clf.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_clf.predict(X_test) \n",
    "    \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average='macro')\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "  \n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:42:56,980]\u001b[0m A new study created in memory with name: KNNClassifier\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:00,034]\u001b[0m Trial 0 finished with value: 0.7692126983239562 and parameters: {'n_neighbors': 25, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 0 with value: 0.7692126983239562.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:03,125]\u001b[0m Trial 1 finished with value: 0.7585872343519913 and parameters: {'n_neighbors': 28, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 0 with value: 0.7692126983239562.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:06,761]\u001b[0m Trial 2 finished with value: 0.7619091149780555 and parameters: {'n_neighbors': 26, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 0 with value: 0.7692126983239562.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:10,374]\u001b[0m Trial 3 finished with value: 0.7810035840817872 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 3 with value: 0.7810035840817872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:14,840]\u001b[0m Trial 4 finished with value: 0.7972739067614629 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 4 with value: 0.7972739067614629.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:18,479]\u001b[0m Trial 5 finished with value: 0.805297805119239 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 49}. Best is trial 5 with value: 0.805297805119239.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:22,087]\u001b[0m Trial 6 finished with value: 0.8013722442767806 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 5 with value: 0.805297805119239.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:25,152]\u001b[0m Trial 7 finished with value: 0.7960260288620165 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 5 with value: 0.805297805119239.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:28,154]\u001b[0m Trial 8 finished with value: 0.8104536596864254 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:31,719]\u001b[0m Trial 9 finished with value: 0.7760303237509563 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:34,729]\u001b[0m Trial 10 finished with value: 0.8089128035906217 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:37,750]\u001b[0m Trial 11 finished with value: 0.8089128035906217 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:40,770]\u001b[0m Trial 12 finished with value: 0.8081170691136954 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:43,571]\u001b[0m Trial 13 finished with value: 0.802476665793464 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:46,541]\u001b[0m Trial 14 finished with value: 0.8098155899865083 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:49,464]\u001b[0m Trial 15 finished with value: 0.8098155899865083 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:52,508]\u001b[0m Trial 16 finished with value: 0.8104536596864254 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:55,428]\u001b[0m Trial 17 finished with value: 0.7942383299823921 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:43:58,520]\u001b[0m Trial 18 finished with value: 0.7881882363870417 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 57}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:01,483]\u001b[0m Trial 19 finished with value: 0.7933084962318758 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:04,396]\u001b[0m Trial 20 finished with value: 0.8049781769741738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:07,250]\u001b[0m Trial 21 finished with value: 0.810343920900286 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:10,278]\u001b[0m Trial 22 finished with value: 0.810343920900286 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:13,315]\u001b[0m Trial 23 finished with value: 0.8073980507482104 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:16,233]\u001b[0m Trial 24 finished with value: 0.8029744912042742 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:19,050]\u001b[0m Trial 25 finished with value: 0.810343920900286 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:22,473]\u001b[0m Trial 26 finished with value: 0.8006554153492564 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:25,514]\u001b[0m Trial 27 finished with value: 0.7758494751900423 and parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:28,525]\u001b[0m Trial 28 finished with value: 0.8089128035906217 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:31,470]\u001b[0m Trial 29 finished with value: 0.7868807234090419 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:34,451]\u001b[0m Trial 30 finished with value: 0.8006058066851194 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 86}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:37,497]\u001b[0m Trial 31 finished with value: 0.810343920900286 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:40,490]\u001b[0m Trial 32 finished with value: 0.8098155899865083 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:43,473]\u001b[0m Trial 33 finished with value: 0.8104536596864254 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:46,504]\u001b[0m Trial 34 finished with value: 0.7881882363870417 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:44:49,553]\u001b[0m Trial 35 finished with value: 0.8072705364833259 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:52,546]\u001b[0m Trial 36 finished with value: 0.802453728906471 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 62}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:56,219]\u001b[0m Trial 37 finished with value: 0.8098967269508073 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:44:59,238]\u001b[0m Trial 38 finished with value: 0.7960260288620165 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 77}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:02,934]\u001b[0m Trial 39 finished with value: 0.785759018376547 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:07,471]\u001b[0m Trial 40 finished with value: 0.8101484910025288 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:10,349]\u001b[0m Trial 41 finished with value: 0.810343920900286 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:13,290]\u001b[0m Trial 42 finished with value: 0.8081170691136954 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 8 with value: 0.8104536596864254.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:16,152]\u001b[0m Trial 43 finished with value: 0.8150073404039212 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:19,096]\u001b[0m Trial 44 finished with value: 0.8150073404039212 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:21,970]\u001b[0m Trial 45 finished with value: 0.8098155899865083 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:24,927]\u001b[0m Trial 46 finished with value: 0.8150073404039212 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:27,864]\u001b[0m Trial 47 finished with value: 0.8150073404039212 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:30,786]\u001b[0m Trial 48 finished with value: 0.8150073404039212 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:33,773]\u001b[0m Trial 49 finished with value: 0.7960260288620165 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8150\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 64\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNClassifier\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  385.000000\n",
      "1                    TN  350.000000\n",
      "2                    FP   84.000000\n",
      "3                    FN   80.000000\n",
      "4              Accuracy    0.817575\n",
      "5             Precision    0.820896\n",
      "6           Sensitivity    0.827957\n",
      "7           Specificity    0.806500\n",
      "8              F1 score    0.824411\n",
      "9   F1 score (weighted)    0.817543\n",
      "10     F1 score (macro)    0.817298\n",
      "11    Balanced Accuracy    0.817204\n",
      "12                  MCC    0.634629\n",
      "13                  NPV    0.814000\n",
      "14              ROC_AUC    0.817204\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_knn_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_knn_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_knn_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_knn_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_knn_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_knn_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_knn_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_knn_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_knn_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_knn_0)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:45:37,205]\u001b[0m Trial 50 finished with value: 0.776685611765544 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:40,119]\u001b[0m Trial 51 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:43,034]\u001b[0m Trial 52 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:45,920]\u001b[0m Trial 53 finished with value: 0.8127864783570565 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:48,863]\u001b[0m Trial 54 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:51,821]\u001b[0m Trial 55 finished with value: 0.8072591235241158 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:54,760]\u001b[0m Trial 56 finished with value: 0.7917540351287307 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:45:57,827]\u001b[0m Trial 57 finished with value: 0.8124132800973205 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:00,911]\u001b[0m Trial 58 finished with value: 0.8127864783570565 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:03,995]\u001b[0m Trial 59 finished with value: 0.8073199892318719 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:07,073]\u001b[0m Trial 60 finished with value: 0.8072591235241158 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:10,127]\u001b[0m Trial 61 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:13,150]\u001b[0m Trial 62 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:16,199]\u001b[0m Trial 63 finished with value: 0.8127864783570565 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:19,285]\u001b[0m Trial 64 finished with value: 0.8080455440771237 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:22,370]\u001b[0m Trial 65 finished with value: 0.8101418445014609 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:25,365]\u001b[0m Trial 66 finished with value: 0.8101418445014609 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:28,270]\u001b[0m Trial 67 finished with value: 0.7867495752012562 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:31,177]\u001b[0m Trial 68 finished with value: 0.7935405193854347 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:34,004]\u001b[0m Trial 69 finished with value: 0.8042750625011618 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:36,932]\u001b[0m Trial 70 finished with value: 0.8085211851263802 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:39,828]\u001b[0m Trial 71 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:42,645]\u001b[0m Trial 72 finished with value: 0.8080455440771237 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:45,494]\u001b[0m Trial 73 finished with value: 0.8080455440771237 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:48,359]\u001b[0m Trial 74 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:51,174]\u001b[0m Trial 75 finished with value: 0.8101418445014609 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:54,043]\u001b[0m Trial 76 finished with value: 0.8127864783570565 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:56,954]\u001b[0m Trial 77 finished with value: 0.8072591235241158 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:46:59,901]\u001b[0m Trial 78 finished with value: 0.7921588730221081 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:02,850]\u001b[0m Trial 79 finished with value: 0.803246305017902 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:05,737]\u001b[0m Trial 80 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:08,564]\u001b[0m Trial 81 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:11,370]\u001b[0m Trial 82 finished with value: 0.8127864783570565 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:14,303]\u001b[0m Trial 83 finished with value: 0.8080455440771237 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:17,145]\u001b[0m Trial 84 finished with value: 0.8101418445014609 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:47:19,978]\u001b[0m Trial 85 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:22,852]\u001b[0m Trial 86 finished with value: 0.8085211851263802 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 84}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:25,696]\u001b[0m Trial 87 finished with value: 0.8124132800973205 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:28,614]\u001b[0m Trial 88 finished with value: 0.8080455440771237 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:31,615]\u001b[0m Trial 89 finished with value: 0.8072591235241158 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:35,326]\u001b[0m Trial 90 finished with value: 0.7860741690662469 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:38,341]\u001b[0m Trial 91 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:41,400]\u001b[0m Trial 92 finished with value: 0.8127864783570565 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:44,360]\u001b[0m Trial 93 finished with value: 0.8080455440771237 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:47,448]\u001b[0m Trial 94 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:50,387]\u001b[0m Trial 95 finished with value: 0.7995175432320917 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:53,311]\u001b[0m Trial 96 finished with value: 0.8101418445014609 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:56,223]\u001b[0m Trial 97 finished with value: 0.8127864783570565 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:47:59,209]\u001b[0m Trial 98 finished with value: 0.8080455440771237 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:02,227]\u001b[0m Trial 99 finished with value: 0.814944868103338 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8150\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 64\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  385.000000  389.000000\n",
      "1                    TN  350.000000  350.000000\n",
      "2                    FP   84.000000   93.000000\n",
      "3                    FN   80.000000   67.000000\n",
      "4              Accuracy    0.817575    0.822024\n",
      "5             Precision    0.820896    0.807054\n",
      "6           Sensitivity    0.827957    0.853070\n",
      "7           Specificity    0.806500    0.790100\n",
      "8              F1 score    0.824411    0.829424\n",
      "9   F1 score (weighted)    0.817543    0.821801\n",
      "10     F1 score (macro)    0.817298    0.821689\n",
      "11    Balanced Accuracy    0.817204    0.821569\n",
      "12                  MCC    0.634629    0.644758\n",
      "13                  NPV    0.814000    0.839300\n",
      "14              ROC_AUC    0.817204    0.821569\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_knn_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_knn_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_knn_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_knn_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_knn_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_knn_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_knn_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_knn_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_knn_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_knn_1)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:48:05,819]\u001b[0m Trial 100 finished with value: 0.8117705189975244 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 43 with value: 0.8150073404039212.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:08,714]\u001b[0m Trial 101 finished with value: 0.8165803003134405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 101 with value: 0.8165803003134405.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:11,626]\u001b[0m Trial 102 finished with value: 0.8165803003134405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 101 with value: 0.8165803003134405.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:14,693]\u001b[0m Trial 103 finished with value: 0.8117858851797047 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 101 with value: 0.8165803003134405.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:17,818]\u001b[0m Trial 104 finished with value: 0.8074065040466127 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 101 with value: 0.8165803003134405.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:20,930]\u001b[0m Trial 105 finished with value: 0.8165803003134405 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 101 with value: 0.8165803003134405.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:24,040]\u001b[0m Trial 106 finished with value: 0.8188488317295033 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:27,056]\u001b[0m Trial 107 finished with value: 0.803196132265169 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:30,007]\u001b[0m Trial 108 finished with value: 0.8074065040466127 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:33,052]\u001b[0m Trial 109 finished with value: 0.8025814163563092 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:36,058]\u001b[0m Trial 110 finished with value: 0.8117705189975244 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:39,098]\u001b[0m Trial 111 finished with value: 0.8188488317295033 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:42,113]\u001b[0m Trial 112 finished with value: 0.8127900144732226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:45,128]\u001b[0m Trial 113 finished with value: 0.8188488317295033 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:48,162]\u001b[0m Trial 114 finished with value: 0.8079194312251307 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 64}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:51,175]\u001b[0m Trial 115 finished with value: 0.8127900144732226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 91}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:54,146]\u001b[0m Trial 116 finished with value: 0.8079194312251307 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:48:57,149]\u001b[0m Trial 117 finished with value: 0.8188488317295033 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:00,090]\u001b[0m Trial 118 finished with value: 0.8188488317295033 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:03,687]\u001b[0m Trial 119 finished with value: 0.8109195406120548 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:07,116]\u001b[0m Trial 120 finished with value: 0.8185397156694855 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:10,661]\u001b[0m Trial 121 finished with value: 0.8185397156694855 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:14,270]\u001b[0m Trial 122 finished with value: 0.8076524836572657 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:17,879]\u001b[0m Trial 123 finished with value: 0.8136350267106277 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:21,483]\u001b[0m Trial 124 finished with value: 0.8185397156694855 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:25,106]\u001b[0m Trial 125 finished with value: 0.8185397156694855 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:28,690]\u001b[0m Trial 126 finished with value: 0.8185397156694855 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:32,304]\u001b[0m Trial 127 finished with value: 0.8076524836572657 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:35,902]\u001b[0m Trial 128 finished with value: 0.7870817247298209 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:39,495]\u001b[0m Trial 129 finished with value: 0.8109195406120548 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:43,081]\u001b[0m Trial 130 finished with value: 0.8136350267106277 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 34}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:47,500]\u001b[0m Trial 131 finished with value: 0.8179818474036281 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:51,885]\u001b[0m Trial 132 finished with value: 0.8179818474036281 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:49:56,244]\u001b[0m Trial 133 finished with value: 0.813891147779569 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:00,648]\u001b[0m Trial 134 finished with value: 0.8179818474036281 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:50:05,089]\u001b[0m Trial 135 finished with value: 0.8079238235388141 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:08,579]\u001b[0m Trial 136 finished with value: 0.8185397156694855 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:13,089]\u001b[0m Trial 137 finished with value: 0.8065927897111467 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:17,577]\u001b[0m Trial 138 finished with value: 0.8079238235388141 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:21,212]\u001b[0m Trial 139 finished with value: 0.8185397156694855 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:24,818]\u001b[0m Trial 140 finished with value: 0.8076524836572657 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:29,296]\u001b[0m Trial 141 finished with value: 0.8179818474036281 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:33,788]\u001b[0m Trial 142 finished with value: 0.7948933260437914 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:37,426]\u001b[0m Trial 143 finished with value: 0.8185397156694855 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:41,055]\u001b[0m Trial 144 finished with value: 0.8185397156694855 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:44,670]\u001b[0m Trial 145 finished with value: 0.8136350267106277 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:48,300]\u001b[0m Trial 146 finished with value: 0.8185397156694855 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:51,985]\u001b[0m Trial 147 finished with value: 0.8109195406120548 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:55,639]\u001b[0m Trial 148 finished with value: 0.8136350267106277 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:50:59,275]\u001b[0m Trial 149 finished with value: 0.8076524836572657 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8188\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 96\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  385.000000  389.000000  386.000000\n",
      "1                    TN  350.000000  350.000000  341.000000\n",
      "2                    FP   84.000000   93.000000   83.000000\n",
      "3                    FN   80.000000   67.000000   89.000000\n",
      "4              Accuracy    0.817575    0.822024    0.808676\n",
      "5             Precision    0.820896    0.807054    0.823028\n",
      "6           Sensitivity    0.827957    0.853070    0.812632\n",
      "7           Specificity    0.806500    0.790100    0.804200\n",
      "8              F1 score    0.824411    0.829424    0.817797\n",
      "9   F1 score (weighted)    0.817543    0.821801    0.808740\n",
      "10     F1 score (macro)    0.817298    0.821689    0.808196\n",
      "11    Balanced Accuracy    0.817204    0.821569    0.808438\n",
      "12                  MCC    0.634629    0.644758    0.616464\n",
      "13                  NPV    0.814000    0.839300    0.793000\n",
      "14              ROC_AUC    0.817204    0.821569    0.808438\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "#predict\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_knn_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_knn_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_knn_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_knn_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_knn_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_knn_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_knn_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_knn_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_knn_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_knn_2)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:51:03,355]\u001b[0m Trial 150 finished with value: 0.7855410540228824 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:07,606]\u001b[0m Trial 151 finished with value: 0.8146895153989157 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:11,120]\u001b[0m Trial 152 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 37}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:14,604]\u001b[0m Trial 153 finished with value: 0.7958294095683748 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:18,841]\u001b[0m Trial 154 finished with value: 0.8146895153989157 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:22,374]\u001b[0m Trial 155 finished with value: 0.8093029804276493 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 35}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:25,917]\u001b[0m Trial 156 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:29,437]\u001b[0m Trial 157 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:32,965]\u001b[0m Trial 158 finished with value: 0.8019171640134992 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:35,905]\u001b[0m Trial 159 finished with value: 0.8084774319416642 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 51}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:39,435]\u001b[0m Trial 160 finished with value: 0.7900285118588785 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:43,682]\u001b[0m Trial 161 finished with value: 0.8146895153989157 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:47,168]\u001b[0m Trial 162 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:50,640]\u001b[0m Trial 163 finished with value: 0.8093029804276493 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:54,970]\u001b[0m Trial 164 finished with value: 0.7963235422665447 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:51:58,403]\u001b[0m Trial 165 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:01,941]\u001b[0m Trial 166 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:06,269]\u001b[0m Trial 167 finished with value: 0.8090416997315975 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:10,460]\u001b[0m Trial 168 finished with value: 0.7963235422665447 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:13,899]\u001b[0m Trial 169 finished with value: 0.8093029804276493 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:18,056]\u001b[0m Trial 170 finished with value: 0.7963235422665447 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:22,163]\u001b[0m Trial 171 finished with value: 0.8146895153989157 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:26,371]\u001b[0m Trial 172 finished with value: 0.8146895153989157 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:29,960]\u001b[0m Trial 173 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:33,417]\u001b[0m Trial 174 finished with value: 0.7958294095683748 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:36,980]\u001b[0m Trial 175 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:40,458]\u001b[0m Trial 176 finished with value: 0.7827412570885902 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 46}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:43,975]\u001b[0m Trial 177 finished with value: 0.8093029804276493 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 35}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:47,565]\u001b[0m Trial 178 finished with value: 0.7958294095683748 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:51,137]\u001b[0m Trial 179 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:55,434]\u001b[0m Trial 180 finished with value: 0.8090416997315975 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:52:59,545]\u001b[0m Trial 181 finished with value: 0.8146895153989157 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:03,594]\u001b[0m Trial 182 finished with value: 0.8146895153989157 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:07,612]\u001b[0m Trial 183 finished with value: 0.8146895153989157 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:11,580]\u001b[0m Trial 184 finished with value: 0.8090416997315975 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:53:14,913]\u001b[0m Trial 185 finished with value: 0.8019171640134992 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:18,289]\u001b[0m Trial 186 finished with value: 0.7958294095683748 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:22,575]\u001b[0m Trial 187 finished with value: 0.7963235422665447 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:25,974]\u001b[0m Trial 188 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:30,319]\u001b[0m Trial 189 finished with value: 0.8030597716771919 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:33,752]\u001b[0m Trial 190 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:36,607]\u001b[0m Trial 191 finished with value: 0.8149634938486019 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:40,536]\u001b[0m Trial 192 finished with value: 0.7963235422665447 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:44,011]\u001b[0m Trial 193 finished with value: 0.8152426263779494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:47,550]\u001b[0m Trial 194 finished with value: 0.8093029804276493 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:50,588]\u001b[0m Trial 195 finished with value: 0.8149634938486019 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:54,893]\u001b[0m Trial 196 finished with value: 0.7807988993408506 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:53:58,336]\u001b[0m Trial 197 finished with value: 0.7958294095683748 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:01,856]\u001b[0m Trial 198 finished with value: 0.8093029804276493 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:06,023]\u001b[0m Trial 199 finished with value: 0.8146895153989157 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8188\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 96\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  385.000000  389.000000  386.000000  405.000000\n",
      "1                    TN  350.000000  350.000000  341.000000  340.000000\n",
      "2                    FP   84.000000   93.000000   83.000000   89.000000\n",
      "3                    FN   80.000000   67.000000   89.000000   65.000000\n",
      "4              Accuracy    0.817575    0.822024    0.808676    0.828699\n",
      "5             Precision    0.820896    0.807054    0.823028    0.819838\n",
      "6           Sensitivity    0.827957    0.853070    0.812632    0.861702\n",
      "7           Specificity    0.806500    0.790100    0.804200    0.792500\n",
      "8              F1 score    0.824411    0.829424    0.817797    0.840249\n",
      "9   F1 score (weighted)    0.817543    0.821801    0.808740    0.828366\n",
      "10     F1 score (macro)    0.817298    0.821689    0.808196    0.827798\n",
      "11    Balanced Accuracy    0.817204    0.821569    0.808438    0.827121\n",
      "12                  MCC    0.634629    0.644758    0.616464    0.656789\n",
      "13                  NPV    0.814000    0.839300    0.793000    0.839500\n",
      "14              ROC_AUC    0.817204    0.821569    0.808438    0.827121\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_knn_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_knn_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_knn_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_knn_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_knn_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_knn_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_knn_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_knn_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_knn_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_knn_3)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:54:09,896]\u001b[0m Trial 200 finished with value: 0.807816218210565 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:12,841]\u001b[0m Trial 201 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 94}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:15,774]\u001b[0m Trial 202 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:18,733]\u001b[0m Trial 203 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:21,725]\u001b[0m Trial 204 finished with value: 0.8162560035000341 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:24,777]\u001b[0m Trial 205 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:27,811]\u001b[0m Trial 206 finished with value: 0.8061835182448798 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 52}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:30,767]\u001b[0m Trial 207 finished with value: 0.7848684642082122 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 55}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:34,220]\u001b[0m Trial 208 finished with value: 0.8162373557135438 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 49}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:37,131]\u001b[0m Trial 209 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:40,094]\u001b[0m Trial 210 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:43,067]\u001b[0m Trial 211 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:46,032]\u001b[0m Trial 212 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:48,928]\u001b[0m Trial 213 finished with value: 0.8061835182448798 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:51,846]\u001b[0m Trial 214 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:54,780]\u001b[0m Trial 215 finished with value: 0.8162560035000341 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:54:57,685]\u001b[0m Trial 216 finished with value: 0.8061835182448798 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:00,507]\u001b[0m Trial 217 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:03,477]\u001b[0m Trial 218 finished with value: 0.8162560035000341 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 94}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:07,039]\u001b[0m Trial 219 finished with value: 0.807816218210565 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:11,246]\u001b[0m Trial 220 finished with value: 0.8029997195931096 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:14,151]\u001b[0m Trial 221 finished with value: 0.8167409524517943 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:17,440]\u001b[0m Trial 222 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:20,797]\u001b[0m Trial 223 finished with value: 0.807816218210565 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:24,119]\u001b[0m Trial 224 finished with value: 0.8162373557135438 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:27,418]\u001b[0m Trial 225 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:30,729]\u001b[0m Trial 226 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:34,144]\u001b[0m Trial 227 finished with value: 0.807816218210565 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:37,664]\u001b[0m Trial 228 finished with value: 0.8162373557135438 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:41,162]\u001b[0m Trial 229 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:44,597]\u001b[0m Trial 230 finished with value: 0.8162373557135438 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:48,512]\u001b[0m Trial 231 finished with value: 0.8167296413683477 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:51,744]\u001b[0m Trial 232 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:55,154]\u001b[0m Trial 233 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:55:58,703]\u001b[0m Trial 234 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:56:01,932]\u001b[0m Trial 235 finished with value: 0.807816218210565 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:05,243]\u001b[0m Trial 236 finished with value: 0.8162373557135438 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:08,477]\u001b[0m Trial 237 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:11,760]\u001b[0m Trial 238 finished with value: 0.7976662438642981 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:15,137]\u001b[0m Trial 239 finished with value: 0.7964537875994547 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:19,213]\u001b[0m Trial 240 finished with value: 0.8167296413683477 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:22,535]\u001b[0m Trial 241 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:25,864]\u001b[0m Trial 242 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:29,147]\u001b[0m Trial 243 finished with value: 0.807816218210565 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:32,435]\u001b[0m Trial 244 finished with value: 0.8162373557135438 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 34}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:36,493]\u001b[0m Trial 245 finished with value: 0.8167296413683477 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:39,783]\u001b[0m Trial 246 finished with value: 0.8170002543130718 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:43,192]\u001b[0m Trial 247 finished with value: 0.807816218210565 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:47,331]\u001b[0m Trial 248 finished with value: 0.8167296413683477 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:51,368]\u001b[0m Trial 249 finished with value: 0.8081243454753144 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 106 with value: 0.8188488317295033.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8188\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 96\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  389.000000  386.000000  405.000000   \n",
      "1                    TN  350.000000  350.000000  341.000000  340.000000   \n",
      "2                    FP   84.000000   93.000000   83.000000   89.000000   \n",
      "3                    FN   80.000000   67.000000   89.000000   65.000000   \n",
      "4              Accuracy    0.817575    0.822024    0.808676    0.828699   \n",
      "5             Precision    0.820896    0.807054    0.823028    0.819838   \n",
      "6           Sensitivity    0.827957    0.853070    0.812632    0.861702   \n",
      "7           Specificity    0.806500    0.790100    0.804200    0.792500   \n",
      "8              F1 score    0.824411    0.829424    0.817797    0.840249   \n",
      "9   F1 score (weighted)    0.817543    0.821801    0.808740    0.828366   \n",
      "10     F1 score (macro)    0.817298    0.821689    0.808196    0.827798   \n",
      "11    Balanced Accuracy    0.817204    0.821569    0.808438    0.827121   \n",
      "12                  MCC    0.634629    0.644758    0.616464    0.656789   \n",
      "13                  NPV    0.814000    0.839300    0.793000    0.839500   \n",
      "14              ROC_AUC    0.817204    0.821569    0.808438    0.827121   \n",
      "\n",
      "          Set4  \n",
      "0   390.000000  \n",
      "1   341.000000  \n",
      "2   110.000000  \n",
      "3    58.000000  \n",
      "4     0.813126  \n",
      "5     0.780000  \n",
      "6     0.870536  \n",
      "7     0.756100  \n",
      "8     0.822785  \n",
      "9     0.812535  \n",
      "10    0.812569  \n",
      "11    0.813317  \n",
      "12    0.630622  \n",
      "13    0.854600  \n",
      "14    0.813317  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_knn_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_knn_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_knn_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_knn_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_knn_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_knn_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_knn_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_knn_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_knn_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_knn_4)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:56:55,408]\u001b[0m Trial 250 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:56:58,762]\u001b[0m Trial 251 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:02,187]\u001b[0m Trial 252 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:05,676]\u001b[0m Trial 253 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:09,288]\u001b[0m Trial 254 finished with value: 0.8144635169644013 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:12,853]\u001b[0m Trial 255 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:16,426]\u001b[0m Trial 256 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:20,009]\u001b[0m Trial 257 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:23,535]\u001b[0m Trial 258 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:27,148]\u001b[0m Trial 259 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:30,761]\u001b[0m Trial 260 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:34,357]\u001b[0m Trial 261 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:37,819]\u001b[0m Trial 262 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:41,175]\u001b[0m Trial 263 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:44,674]\u001b[0m Trial 264 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:48,085]\u001b[0m Trial 265 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:51,407]\u001b[0m Trial 266 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:54,866]\u001b[0m Trial 267 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:57:58,242]\u001b[0m Trial 268 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:01,643]\u001b[0m Trial 269 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:05,083]\u001b[0m Trial 270 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:08,495]\u001b[0m Trial 271 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:11,902]\u001b[0m Trial 272 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:15,299]\u001b[0m Trial 273 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:18,797]\u001b[0m Trial 274 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:22,241]\u001b[0m Trial 275 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:25,692]\u001b[0m Trial 276 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:29,088]\u001b[0m Trial 277 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:32,542]\u001b[0m Trial 278 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:36,071]\u001b[0m Trial 279 finished with value: 0.8144635169644013 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:39,484]\u001b[0m Trial 280 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:42,894]\u001b[0m Trial 281 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:46,288]\u001b[0m Trial 282 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:49,682]\u001b[0m Trial 283 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:53,106]\u001b[0m Trial 284 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:58:56,568]\u001b[0m Trial 285 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:58:59,980]\u001b[0m Trial 286 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:03,456]\u001b[0m Trial 287 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:06,875]\u001b[0m Trial 288 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:10,350]\u001b[0m Trial 289 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:13,766]\u001b[0m Trial 290 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 47}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:17,181]\u001b[0m Trial 291 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:20,530]\u001b[0m Trial 292 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:23,901]\u001b[0m Trial 293 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:27,282]\u001b[0m Trial 294 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:30,688]\u001b[0m Trial 295 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:34,046]\u001b[0m Trial 296 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:37,523]\u001b[0m Trial 297 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:41,063]\u001b[0m Trial 298 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:44,516]\u001b[0m Trial 299 finished with value: 0.8190194057010226 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8190\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 32\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  389.000000  386.000000  405.000000   \n",
      "1                    TN  350.000000  350.000000  341.000000  340.000000   \n",
      "2                    FP   84.000000   93.000000   83.000000   89.000000   \n",
      "3                    FN   80.000000   67.000000   89.000000   65.000000   \n",
      "4              Accuracy    0.817575    0.822024    0.808676    0.828699   \n",
      "5             Precision    0.820896    0.807054    0.823028    0.819838   \n",
      "6           Sensitivity    0.827957    0.853070    0.812632    0.861702   \n",
      "7           Specificity    0.806500    0.790100    0.804200    0.792500   \n",
      "8              F1 score    0.824411    0.829424    0.817797    0.840249   \n",
      "9   F1 score (weighted)    0.817543    0.821801    0.808740    0.828366   \n",
      "10     F1 score (macro)    0.817298    0.821689    0.808196    0.827798   \n",
      "11    Balanced Accuracy    0.817204    0.821569    0.808438    0.827121   \n",
      "12                  MCC    0.634629    0.644758    0.616464    0.656789   \n",
      "13                  NPV    0.814000    0.839300    0.793000    0.839500   \n",
      "14              ROC_AUC    0.817204    0.821569    0.808438    0.827121   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   390.000000  376.000000  \n",
      "1   341.000000  343.000000  \n",
      "2   110.000000   90.000000  \n",
      "3    58.000000   90.000000  \n",
      "4     0.813126    0.799778  \n",
      "5     0.780000    0.806867  \n",
      "6     0.870536    0.806867  \n",
      "7     0.756100    0.792100  \n",
      "8     0.822785    0.806867  \n",
      "9     0.812535    0.799778  \n",
      "10    0.812569    0.799507  \n",
      "11    0.813317    0.799507  \n",
      "12    0.630622    0.599015  \n",
      "13    0.854600    0.792100  \n",
      "14    0.813317    0.799507  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_knn_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_knn_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_knn_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_knn_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_knn_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_knn_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_knn_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_knn_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_knn_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_knn_5)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:59:48,514]\u001b[0m Trial 300 finished with value: 0.8060453820869883 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:51,898]\u001b[0m Trial 301 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:55,190]\u001b[0m Trial 302 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 14:59:58,557]\u001b[0m Trial 303 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:01,899]\u001b[0m Trial 304 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:05,211]\u001b[0m Trial 305 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:08,576]\u001b[0m Trial 306 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:11,989]\u001b[0m Trial 307 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 47}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:15,535]\u001b[0m Trial 308 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:19,027]\u001b[0m Trial 309 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:22,554]\u001b[0m Trial 310 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:26,113]\u001b[0m Trial 311 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:29,654]\u001b[0m Trial 312 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:33,216]\u001b[0m Trial 313 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:36,774]\u001b[0m Trial 314 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:40,330]\u001b[0m Trial 315 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:43,837]\u001b[0m Trial 316 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:47,380]\u001b[0m Trial 317 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:50,892]\u001b[0m Trial 318 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:54,262]\u001b[0m Trial 319 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:00:57,677]\u001b[0m Trial 320 finished with value: 0.8060453820869883 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:00,915]\u001b[0m Trial 321 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:04,256]\u001b[0m Trial 322 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:07,585]\u001b[0m Trial 323 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:10,896]\u001b[0m Trial 324 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:14,286]\u001b[0m Trial 325 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:17,834]\u001b[0m Trial 326 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:21,164]\u001b[0m Trial 327 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:24,583]\u001b[0m Trial 328 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:28,114]\u001b[0m Trial 329 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:31,662]\u001b[0m Trial 330 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:35,200]\u001b[0m Trial 331 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:38,686]\u001b[0m Trial 332 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:42,296]\u001b[0m Trial 333 finished with value: 0.7844668090738689 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:45,707]\u001b[0m Trial 334 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:01:49,014]\u001b[0m Trial 335 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:52,315]\u001b[0m Trial 336 finished with value: 0.7929136725348949 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 34}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:55,645]\u001b[0m Trial 337 finished with value: 0.7845581685365792 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:01:58,926]\u001b[0m Trial 338 finished with value: 0.7798034673854415 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 49}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:02,244]\u001b[0m Trial 339 finished with value: 0.8060453820869883 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:05,559]\u001b[0m Trial 340 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:08,897]\u001b[0m Trial 341 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:12,218]\u001b[0m Trial 342 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:15,530]\u001b[0m Trial 343 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:18,773]\u001b[0m Trial 344 finished with value: 0.8133327745831046 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:22,106]\u001b[0m Trial 345 finished with value: 0.7975578436428916 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:25,444]\u001b[0m Trial 346 finished with value: 0.7935116759301982 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:28,871]\u001b[0m Trial 347 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:32,386]\u001b[0m Trial 348 finished with value: 0.8133327745831046 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:35,907]\u001b[0m Trial 349 finished with value: 0.8105346882338418 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8190\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 32\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  389.000000  386.000000  405.000000   \n",
      "1                    TN  350.000000  350.000000  341.000000  340.000000   \n",
      "2                    FP   84.000000   93.000000   83.000000   89.000000   \n",
      "3                    FN   80.000000   67.000000   89.000000   65.000000   \n",
      "4              Accuracy    0.817575    0.822024    0.808676    0.828699   \n",
      "5             Precision    0.820896    0.807054    0.823028    0.819838   \n",
      "6           Sensitivity    0.827957    0.853070    0.812632    0.861702   \n",
      "7           Specificity    0.806500    0.790100    0.804200    0.792500   \n",
      "8              F1 score    0.824411    0.829424    0.817797    0.840249   \n",
      "9   F1 score (weighted)    0.817543    0.821801    0.808740    0.828366   \n",
      "10     F1 score (macro)    0.817298    0.821689    0.808196    0.827798   \n",
      "11    Balanced Accuracy    0.817204    0.821569    0.808438    0.827121   \n",
      "12                  MCC    0.634629    0.644758    0.616464    0.656789   \n",
      "13                  NPV    0.814000    0.839300    0.793000    0.839500   \n",
      "14              ROC_AUC    0.817204    0.821569    0.808438    0.827121   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   390.000000  376.000000  396.000000  \n",
      "1   341.000000  343.000000  332.000000  \n",
      "2   110.000000   90.000000   97.000000  \n",
      "3    58.000000   90.000000   74.000000  \n",
      "4     0.813126    0.799778    0.809789  \n",
      "5     0.780000    0.806867    0.803245  \n",
      "6     0.870536    0.806867    0.842553  \n",
      "7     0.756100    0.792100    0.773900  \n",
      "8     0.822785    0.806867    0.822430  \n",
      "9     0.812535    0.799778    0.809440  \n",
      "10    0.812569    0.799507    0.808820  \n",
      "11    0.813317    0.799507    0.808223  \n",
      "12    0.630622    0.599015    0.618709  \n",
      "13    0.854600    0.792100    0.817700  \n",
      "14    0.813317    0.799507    0.808223  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_knn_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_knn_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_knn_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_knn_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_knn_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_knn_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_knn_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_knn_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_knn_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_knn_6)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:02:40,315]\u001b[0m Trial 350 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:43,870]\u001b[0m Trial 351 finished with value: 0.7899187715679732 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:47,301]\u001b[0m Trial 352 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 47}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:50,713]\u001b[0m Trial 353 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:54,172]\u001b[0m Trial 354 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:02:57,549]\u001b[0m Trial 355 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 47}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:00,990]\u001b[0m Trial 356 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:04,656]\u001b[0m Trial 357 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:08,290]\u001b[0m Trial 358 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:11,922]\u001b[0m Trial 359 finished with value: 0.7992299817571186 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:15,333]\u001b[0m Trial 360 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:18,796]\u001b[0m Trial 361 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:22,184]\u001b[0m Trial 362 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:25,705]\u001b[0m Trial 363 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:29,128]\u001b[0m Trial 364 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:32,559]\u001b[0m Trial 365 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:36,210]\u001b[0m Trial 366 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:39,902]\u001b[0m Trial 367 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:43,495]\u001b[0m Trial 368 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:47,095]\u001b[0m Trial 369 finished with value: 0.791458363191033 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:50,762]\u001b[0m Trial 370 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:54,408]\u001b[0m Trial 371 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:03:57,963]\u001b[0m Trial 372 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:01,476]\u001b[0m Trial 373 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:04,903]\u001b[0m Trial 374 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:08,332]\u001b[0m Trial 375 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:11,880]\u001b[0m Trial 376 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:15,514]\u001b[0m Trial 377 finished with value: 0.812289665337034 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:19,042]\u001b[0m Trial 378 finished with value: 0.7999927318057654 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:22,565]\u001b[0m Trial 379 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:26,150]\u001b[0m Trial 380 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:29,645]\u001b[0m Trial 381 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:33,210]\u001b[0m Trial 382 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:36,798]\u001b[0m Trial 383 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:40,270]\u001b[0m Trial 384 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:04:43,819]\u001b[0m Trial 385 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:47,315]\u001b[0m Trial 386 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:50,847]\u001b[0m Trial 387 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:54,425]\u001b[0m Trial 388 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 47}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:04:57,915]\u001b[0m Trial 389 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:01,410]\u001b[0m Trial 390 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:05,067]\u001b[0m Trial 391 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:08,712]\u001b[0m Trial 392 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:12,343]\u001b[0m Trial 393 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:15,872]\u001b[0m Trial 394 finished with value: 0.806617976522008 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:19,393]\u001b[0m Trial 395 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:22,876]\u001b[0m Trial 396 finished with value: 0.8154568589532895 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:26,395]\u001b[0m Trial 397 finished with value: 0.7992299817571186 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:29,853]\u001b[0m Trial 398 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:33,207]\u001b[0m Trial 399 finished with value: 0.813407511868651 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8190\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 32\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  389.000000  386.000000  405.000000   \n",
      "1                    TN  350.000000  350.000000  341.000000  340.000000   \n",
      "2                    FP   84.000000   93.000000   83.000000   89.000000   \n",
      "3                    FN   80.000000   67.000000   89.000000   65.000000   \n",
      "4              Accuracy    0.817575    0.822024    0.808676    0.828699   \n",
      "5             Precision    0.820896    0.807054    0.823028    0.819838   \n",
      "6           Sensitivity    0.827957    0.853070    0.812632    0.861702   \n",
      "7           Specificity    0.806500    0.790100    0.804200    0.792500   \n",
      "8              F1 score    0.824411    0.829424    0.817797    0.840249   \n",
      "9   F1 score (weighted)    0.817543    0.821801    0.808740    0.828366   \n",
      "10     F1 score (macro)    0.817298    0.821689    0.808196    0.827798   \n",
      "11    Balanced Accuracy    0.817204    0.821569    0.808438    0.827121   \n",
      "12                  MCC    0.634629    0.644758    0.616464    0.656789   \n",
      "13                  NPV    0.814000    0.839300    0.793000    0.839500   \n",
      "14              ROC_AUC    0.817204    0.821569    0.808438    0.827121   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   390.000000  376.000000  396.000000  384.000000  \n",
      "1   341.000000  343.000000  332.000000  341.000000  \n",
      "2   110.000000   90.000000   97.000000   97.000000  \n",
      "3    58.000000   90.000000   74.000000   77.000000  \n",
      "4     0.813126    0.799778    0.809789    0.806452  \n",
      "5     0.780000    0.806867    0.803245    0.798337  \n",
      "6     0.870536    0.806867    0.842553    0.832972  \n",
      "7     0.756100    0.792100    0.773900    0.778500  \n",
      "8     0.822785    0.806867    0.822430    0.815287  \n",
      "9     0.812535    0.799778    0.809440    0.806245  \n",
      "10    0.812569    0.799507    0.808820    0.806008  \n",
      "11    0.813317    0.799507    0.808223    0.805755  \n",
      "12    0.630622    0.599015    0.618709    0.612817  \n",
      "13    0.854600    0.792100    0.817700    0.815800  \n",
      "14    0.813317    0.799507    0.808223    0.805755  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_knn_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_knn_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_knn_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_knn_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_knn_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_knn_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_knn_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_knn_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_knn_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_knn_7)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:05:37,411]\u001b[0m Trial 400 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:40,737]\u001b[0m Trial 401 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:44,089]\u001b[0m Trial 402 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:47,432]\u001b[0m Trial 403 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:50,759]\u001b[0m Trial 404 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:54,224]\u001b[0m Trial 405 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:05:57,805]\u001b[0m Trial 406 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:01,354]\u001b[0m Trial 407 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:04,910]\u001b[0m Trial 408 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:08,442]\u001b[0m Trial 409 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:11,993]\u001b[0m Trial 410 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:15,534]\u001b[0m Trial 411 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:19,042]\u001b[0m Trial 412 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:22,562]\u001b[0m Trial 413 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:26,171]\u001b[0m Trial 414 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:29,734]\u001b[0m Trial 415 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:33,386]\u001b[0m Trial 416 finished with value: 0.8012729685805503 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:36,895]\u001b[0m Trial 417 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:40,257]\u001b[0m Trial 418 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 47}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:43,573]\u001b[0m Trial 419 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:46,836]\u001b[0m Trial 420 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:50,119]\u001b[0m Trial 421 finished with value: 0.7813427674939447 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 49}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:53,502]\u001b[0m Trial 422 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:06:56,852]\u001b[0m Trial 423 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:00,173]\u001b[0m Trial 424 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:03,490]\u001b[0m Trial 425 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:06,799]\u001b[0m Trial 426 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:10,113]\u001b[0m Trial 427 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:13,505]\u001b[0m Trial 428 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:16,804]\u001b[0m Trial 429 finished with value: 0.7970455158937164 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:20,119]\u001b[0m Trial 430 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:23,416]\u001b[0m Trial 431 finished with value: 0.7883653242952968 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:26,719]\u001b[0m Trial 432 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:30,223]\u001b[0m Trial 433 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:33,610]\u001b[0m Trial 434 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:07:37,246]\u001b[0m Trial 435 finished with value: 0.7930420875533678 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:40,846]\u001b[0m Trial 436 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:44,350]\u001b[0m Trial 437 finished with value: 0.778863855632811 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:47,749]\u001b[0m Trial 438 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:51,256]\u001b[0m Trial 439 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:54,744]\u001b[0m Trial 440 finished with value: 0.7755873144205705 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:07:58,132]\u001b[0m Trial 441 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:01,662]\u001b[0m Trial 442 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:05,244]\u001b[0m Trial 443 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:08,756]\u001b[0m Trial 444 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:12,095]\u001b[0m Trial 445 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:15,493]\u001b[0m Trial 446 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:18,821]\u001b[0m Trial 447 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:22,318]\u001b[0m Trial 448 finished with value: 0.8055839930353444 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:25,639]\u001b[0m Trial 449 finished with value: 0.8100340030336397 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8190\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 32\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  389.000000  386.000000  405.000000   \n",
      "1                    TN  350.000000  350.000000  341.000000  340.000000   \n",
      "2                    FP   84.000000   93.000000   83.000000   89.000000   \n",
      "3                    FN   80.000000   67.000000   89.000000   65.000000   \n",
      "4              Accuracy    0.817575    0.822024    0.808676    0.828699   \n",
      "5             Precision    0.820896    0.807054    0.823028    0.819838   \n",
      "6           Sensitivity    0.827957    0.853070    0.812632    0.861702   \n",
      "7           Specificity    0.806500    0.790100    0.804200    0.792500   \n",
      "8              F1 score    0.824411    0.829424    0.817797    0.840249   \n",
      "9   F1 score (weighted)    0.817543    0.821801    0.808740    0.828366   \n",
      "10     F1 score (macro)    0.817298    0.821689    0.808196    0.827798   \n",
      "11    Balanced Accuracy    0.817204    0.821569    0.808438    0.827121   \n",
      "12                  MCC    0.634629    0.644758    0.616464    0.656789   \n",
      "13                  NPV    0.814000    0.839300    0.793000    0.839500   \n",
      "14              ROC_AUC    0.817204    0.821569    0.808438    0.827121   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   390.000000  376.000000  396.000000  384.000000  409.000000  \n",
      "1   341.000000  343.000000  332.000000  341.000000  345.000000  \n",
      "2   110.000000   90.000000   97.000000   97.000000   69.000000  \n",
      "3    58.000000   90.000000   74.000000   77.000000   76.000000  \n",
      "4     0.813126    0.799778    0.809789    0.806452    0.838710  \n",
      "5     0.780000    0.806867    0.803245    0.798337    0.855649  \n",
      "6     0.870536    0.806867    0.842553    0.832972    0.843299  \n",
      "7     0.756100    0.792100    0.773900    0.778500    0.833300  \n",
      "8     0.822785    0.806867    0.822430    0.815287    0.849429  \n",
      "9     0.812535    0.799778    0.809440    0.806245    0.838800  \n",
      "10    0.812569    0.799507    0.808820    0.806008    0.837888  \n",
      "11    0.813317    0.799507    0.808223    0.805755    0.838316  \n",
      "12    0.630622    0.599015    0.618709    0.612817    0.675879  \n",
      "13    0.854600    0.792100    0.817700    0.815800    0.819500  \n",
      "14    0.813317    0.799507    0.808223    0.805755    0.838316  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_knn_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_knn_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_knn_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_knn_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_knn_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_knn_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_knn_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_knn_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_knn_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_knn_8)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:08:29,572]\u001b[0m Trial 450 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:32,838]\u001b[0m Trial 451 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:36,395]\u001b[0m Trial 452 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:39,970]\u001b[0m Trial 453 finished with value: 0.7896899693196122 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:43,609]\u001b[0m Trial 454 finished with value: 0.7618861582245471 and parameters: {'n_neighbors': 24, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:47,117]\u001b[0m Trial 455 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:50,673]\u001b[0m Trial 456 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 35}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:54,235]\u001b[0m Trial 457 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:08:57,818]\u001b[0m Trial 458 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:00,823]\u001b[0m Trial 459 finished with value: 0.8030935156296601 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 51}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:04,240]\u001b[0m Trial 460 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:07,659]\u001b[0m Trial 461 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:11,092]\u001b[0m Trial 462 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:14,545]\u001b[0m Trial 463 finished with value: 0.7936635384473042 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:17,452]\u001b[0m Trial 464 finished with value: 0.8091554902520428 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 55}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:20,820]\u001b[0m Trial 465 finished with value: 0.7769765145517169 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:24,239]\u001b[0m Trial 466 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:27,613]\u001b[0m Trial 467 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:31,049]\u001b[0m Trial 468 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:34,488]\u001b[0m Trial 469 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:37,443]\u001b[0m Trial 470 finished with value: 0.8030935156296601 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 60}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:40,874]\u001b[0m Trial 471 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:44,309]\u001b[0m Trial 472 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:47,780]\u001b[0m Trial 473 finished with value: 0.7947900240334487 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:51,200]\u001b[0m Trial 474 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:54,703]\u001b[0m Trial 475 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:09:58,104]\u001b[0m Trial 476 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:01,495]\u001b[0m Trial 477 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:04,909]\u001b[0m Trial 478 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 47}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:07,810]\u001b[0m Trial 479 finished with value: 0.7950813138915215 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:11,199]\u001b[0m Trial 480 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:14,641]\u001b[0m Trial 481 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:18,190]\u001b[0m Trial 482 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:21,700]\u001b[0m Trial 483 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:25,137]\u001b[0m Trial 484 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:10:28,483]\u001b[0m Trial 485 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:31,828]\u001b[0m Trial 486 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:35,132]\u001b[0m Trial 487 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:38,472]\u001b[0m Trial 488 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:41,831]\u001b[0m Trial 489 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:45,211]\u001b[0m Trial 490 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:48,556]\u001b[0m Trial 491 finished with value: 0.7936635384473042 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:51,895]\u001b[0m Trial 492 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:55,151]\u001b[0m Trial 493 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:10:58,556]\u001b[0m Trial 494 finished with value: 0.7947900240334487 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 41}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:11:01,847]\u001b[0m Trial 495 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:11:05,174]\u001b[0m Trial 496 finished with value: 0.7978905816378654 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:11:08,553]\u001b[0m Trial 497 finished with value: 0.8077522246323617 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:11:11,892]\u001b[0m Trial 498 finished with value: 0.8036511479856191 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:11:15,269]\u001b[0m Trial 499 finished with value: 0.7948948584457327 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 250 with value: 0.8190194057010226.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8190\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 32\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  385.000000  389.000000  386.000000  405.000000   \n",
      "1                    TN  350.000000  350.000000  341.000000  340.000000   \n",
      "2                    FP   84.000000   93.000000   83.000000   89.000000   \n",
      "3                    FN   80.000000   67.000000   89.000000   65.000000   \n",
      "4              Accuracy    0.817575    0.822024    0.808676    0.828699   \n",
      "5             Precision    0.820896    0.807054    0.823028    0.819838   \n",
      "6           Sensitivity    0.827957    0.853070    0.812632    0.861702   \n",
      "7           Specificity    0.806500    0.790100    0.804200    0.792500   \n",
      "8              F1 score    0.824411    0.829424    0.817797    0.840249   \n",
      "9   F1 score (weighted)    0.817543    0.821801    0.808740    0.828366   \n",
      "10     F1 score (macro)    0.817298    0.821689    0.808196    0.827798   \n",
      "11    Balanced Accuracy    0.817204    0.821569    0.808438    0.827121   \n",
      "12                  MCC    0.634629    0.644758    0.616464    0.656789   \n",
      "13                  NPV    0.814000    0.839300    0.793000    0.839500   \n",
      "14              ROC_AUC    0.817204    0.821569    0.808438    0.827121   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   390.000000  376.000000  396.000000  384.000000  409.000000  410.000000  \n",
      "1   341.000000  343.000000  332.000000  341.000000  345.000000  325.000000  \n",
      "2   110.000000   90.000000   97.000000   97.000000   69.000000   80.000000  \n",
      "3    58.000000   90.000000   74.000000   77.000000   76.000000   84.000000  \n",
      "4     0.813126    0.799778    0.809789    0.806452    0.838710    0.817575  \n",
      "5     0.780000    0.806867    0.803245    0.798337    0.855649    0.836735  \n",
      "6     0.870536    0.806867    0.842553    0.832972    0.843299    0.829960  \n",
      "7     0.756100    0.792100    0.773900    0.778500    0.833300    0.802500  \n",
      "8     0.822785    0.806867    0.822430    0.815287    0.849429    0.833333  \n",
      "9     0.812535    0.799778    0.809440    0.806245    0.838800    0.817653  \n",
      "10    0.812569    0.799507    0.808820    0.806008    0.837888    0.815930  \n",
      "11    0.813317    0.799507    0.808223    0.805755    0.838316    0.816214  \n",
      "12    0.630622    0.599015    0.618709    0.612817    0.675879    0.631892  \n",
      "13    0.854600    0.792100    0.817700    0.815800    0.819500    0.794600  \n",
      "14    0.813317    0.799507    0.808223    0.805755    0.838316    0.816214  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_knn_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_knn_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_knn_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_knn_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_knn_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_knn_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_knn_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_knn_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_knn_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_knn_9)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>197.700000</td>\n",
       "      <td>9.672986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>171.200000</td>\n",
       "      <td>9.402127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>8.664102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>6.186904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.821234</td>\n",
       "      <td>0.015189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.826236</td>\n",
       "      <td>0.030217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.836954</td>\n",
       "      <td>0.025808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.804010</td>\n",
       "      <td>0.038189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.830960</td>\n",
       "      <td>0.015891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.821060</td>\n",
       "      <td>0.015347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.820348</td>\n",
       "      <td>0.015341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.820488</td>\n",
       "      <td>0.015432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.642105</td>\n",
       "      <td>0.029788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.021385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.820488</td>\n",
       "      <td>0.015432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       197.700000     9.672986\n",
       "1                    TN       171.200000     9.402127\n",
       "2                    FP        41.800000     8.664102\n",
       "3                    FN        38.500000     6.186904\n",
       "4              Accuracy         0.821234     0.015189\n",
       "5             Precision         0.826236     0.030217\n",
       "6           Sensitivity         0.836954     0.025808\n",
       "7           Specificity         0.804010     0.038189\n",
       "8              F1 score         0.830960     0.015891\n",
       "9   F1 score (weighted)         0.821060     0.015347\n",
       "10     F1 score (macro)         0.820348     0.015341\n",
       "11    Balanced Accuracy         0.820488     0.015432\n",
       "12                  MCC         0.642105     0.029788\n",
       "13                  NPV         0.817000     0.021385\n",
       "14              ROC_AUC         0.820488     0.015432"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>386.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>11.575837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>340.800000</td>\n",
       "      <td>7.598245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>89.200000</td>\n",
       "      <td>11.193252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>10.413666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.817575</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.808676</td>\n",
       "      <td>0.828699</td>\n",
       "      <td>0.813126</td>\n",
       "      <td>0.799778</td>\n",
       "      <td>0.809789</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.817575</td>\n",
       "      <td>0.816240</td>\n",
       "      <td>0.011414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.807054</td>\n",
       "      <td>0.823028</td>\n",
       "      <td>0.819838</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.806867</td>\n",
       "      <td>0.803245</td>\n",
       "      <td>0.798337</td>\n",
       "      <td>0.855649</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.815165</td>\n",
       "      <td>0.021127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.827957</td>\n",
       "      <td>0.853070</td>\n",
       "      <td>0.812632</td>\n",
       "      <td>0.861702</td>\n",
       "      <td>0.870536</td>\n",
       "      <td>0.806867</td>\n",
       "      <td>0.842553</td>\n",
       "      <td>0.832972</td>\n",
       "      <td>0.843299</td>\n",
       "      <td>0.829960</td>\n",
       "      <td>0.838155</td>\n",
       "      <td>0.020275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.790100</td>\n",
       "      <td>0.804200</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>0.756100</td>\n",
       "      <td>0.792100</td>\n",
       "      <td>0.773900</td>\n",
       "      <td>0.778500</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.802500</td>\n",
       "      <td>0.792970</td>\n",
       "      <td>0.020998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.824411</td>\n",
       "      <td>0.829424</td>\n",
       "      <td>0.817797</td>\n",
       "      <td>0.840249</td>\n",
       "      <td>0.822785</td>\n",
       "      <td>0.806867</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>0.815287</td>\n",
       "      <td>0.849429</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.826201</td>\n",
       "      <td>0.012431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.817543</td>\n",
       "      <td>0.821801</td>\n",
       "      <td>0.808740</td>\n",
       "      <td>0.828366</td>\n",
       "      <td>0.812535</td>\n",
       "      <td>0.799778</td>\n",
       "      <td>0.809440</td>\n",
       "      <td>0.806245</td>\n",
       "      <td>0.838800</td>\n",
       "      <td>0.817653</td>\n",
       "      <td>0.816090</td>\n",
       "      <td>0.011438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.817298</td>\n",
       "      <td>0.821689</td>\n",
       "      <td>0.808196</td>\n",
       "      <td>0.827798</td>\n",
       "      <td>0.812569</td>\n",
       "      <td>0.799507</td>\n",
       "      <td>0.808820</td>\n",
       "      <td>0.806008</td>\n",
       "      <td>0.837888</td>\n",
       "      <td>0.815930</td>\n",
       "      <td>0.815570</td>\n",
       "      <td>0.011287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.817204</td>\n",
       "      <td>0.821569</td>\n",
       "      <td>0.808438</td>\n",
       "      <td>0.827121</td>\n",
       "      <td>0.813317</td>\n",
       "      <td>0.799507</td>\n",
       "      <td>0.808223</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.838316</td>\n",
       "      <td>0.816214</td>\n",
       "      <td>0.815567</td>\n",
       "      <td>0.011324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.634629</td>\n",
       "      <td>0.644758</td>\n",
       "      <td>0.616464</td>\n",
       "      <td>0.656789</td>\n",
       "      <td>0.630622</td>\n",
       "      <td>0.599015</td>\n",
       "      <td>0.618709</td>\n",
       "      <td>0.612817</td>\n",
       "      <td>0.675879</td>\n",
       "      <td>0.631892</td>\n",
       "      <td>0.632157</td>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.839300</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.839500</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>0.792100</td>\n",
       "      <td>0.817700</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.819500</td>\n",
       "      <td>0.794600</td>\n",
       "      <td>0.818010</td>\n",
       "      <td>0.021401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.817204</td>\n",
       "      <td>0.821569</td>\n",
       "      <td>0.808438</td>\n",
       "      <td>0.827121</td>\n",
       "      <td>0.813317</td>\n",
       "      <td>0.799507</td>\n",
       "      <td>0.808223</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.838316</td>\n",
       "      <td>0.816214</td>\n",
       "      <td>0.815567</td>\n",
       "      <td>0.011324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  385.000000  389.000000  386.000000  405.000000   \n",
       "1                    TN  350.000000  350.000000  341.000000  340.000000   \n",
       "2                    FP   84.000000   93.000000   83.000000   89.000000   \n",
       "3                    FN   80.000000   67.000000   89.000000   65.000000   \n",
       "4              Accuracy    0.817575    0.822024    0.808676    0.828699   \n",
       "5             Precision    0.820896    0.807054    0.823028    0.819838   \n",
       "6           Sensitivity    0.827957    0.853070    0.812632    0.861702   \n",
       "7           Specificity    0.806500    0.790100    0.804200    0.792500   \n",
       "8              F1 score    0.824411    0.829424    0.817797    0.840249   \n",
       "9   F1 score (weighted)    0.817543    0.821801    0.808740    0.828366   \n",
       "10     F1 score (macro)    0.817298    0.821689    0.808196    0.827798   \n",
       "11    Balanced Accuracy    0.817204    0.821569    0.808438    0.827121   \n",
       "12                  MCC    0.634629    0.644758    0.616464    0.656789   \n",
       "13                  NPV    0.814000    0.839300    0.793000    0.839500   \n",
       "14              ROC_AUC    0.817204    0.821569    0.808438    0.827121   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   390.000000  376.000000  396.000000  384.000000  409.000000  410.000000   \n",
       "1   341.000000  343.000000  332.000000  341.000000  345.000000  325.000000   \n",
       "2   110.000000   90.000000   97.000000   97.000000   69.000000   80.000000   \n",
       "3    58.000000   90.000000   74.000000   77.000000   76.000000   84.000000   \n",
       "4     0.813126    0.799778    0.809789    0.806452    0.838710    0.817575   \n",
       "5     0.780000    0.806867    0.803245    0.798337    0.855649    0.836735   \n",
       "6     0.870536    0.806867    0.842553    0.832972    0.843299    0.829960   \n",
       "7     0.756100    0.792100    0.773900    0.778500    0.833300    0.802500   \n",
       "8     0.822785    0.806867    0.822430    0.815287    0.849429    0.833333   \n",
       "9     0.812535    0.799778    0.809440    0.806245    0.838800    0.817653   \n",
       "10    0.812569    0.799507    0.808820    0.806008    0.837888    0.815930   \n",
       "11    0.813317    0.799507    0.808223    0.805755    0.838316    0.816214   \n",
       "12    0.630622    0.599015    0.618709    0.612817    0.675879    0.631892   \n",
       "13    0.854600    0.792100    0.817700    0.815800    0.819500    0.794600   \n",
       "14    0.813317    0.799507    0.808223    0.805755    0.838316    0.816214   \n",
       "\n",
       "           ave        std  \n",
       "0   393.000000  11.575837  \n",
       "1   340.800000   7.598245  \n",
       "2    89.200000  11.193252  \n",
       "3    76.000000  10.413666  \n",
       "4     0.816240   0.011414  \n",
       "5     0.815165   0.021127  \n",
       "6     0.838155   0.020275  \n",
       "7     0.792970   0.020998  \n",
       "8     0.826201   0.012431  \n",
       "9     0.816090   0.011438  \n",
       "10    0.815570   0.011287  \n",
       "11    0.815567   0.011324  \n",
       "12    0.632157   0.022571  \n",
       "13    0.818010   0.021401  \n",
       "14    0.815567   0.011324  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_knn0</th>\n",
       "      <th>y_pred_knn1</th>\n",
       "      <th>y_pred_knn2</th>\n",
       "      <th>y_pred_knn3</th>\n",
       "      <th>y_pred_knn4</th>\n",
       "      <th>y_pred_knn_ave</th>\n",
       "      <th>y_pred_knn_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>4487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>4488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>4489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>4490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>4491</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_test_idx0  y_test0  y_pred_knn0  y_pred_knn1  y_pred_knn2  \\\n",
       "0               0      0.0          0.0          0.0          0.0   \n",
       "1               1      1.0          1.0          1.0          1.0   \n",
       "2               2      1.0          1.0          1.0          1.0   \n",
       "3               3      1.0          1.0          1.0          1.0   \n",
       "4               4      1.0          0.0          0.0          0.0   \n",
       "...           ...      ...          ...          ...          ...   \n",
       "4487         4487      1.0          1.0          1.0          1.0   \n",
       "4488         4488      1.0          1.0          1.0          1.0   \n",
       "4489         4489      0.0          0.0          0.0          0.0   \n",
       "4490         4490      0.0          0.0          0.0          0.0   \n",
       "4491         4491      1.0          1.0          1.0          1.0   \n",
       "\n",
       "      y_pred_knn3  y_pred_knn4  y_pred_knn_ave  y_pred_knn_std  \n",
       "0             0.0          0.0             0.0             0.0  \n",
       "1             1.0          1.0             1.0             0.0  \n",
       "2             1.0          1.0             1.0             0.0  \n",
       "3             1.0          1.0             1.0             0.0  \n",
       "4             0.0          0.0             0.0             0.0  \n",
       "...           ...          ...             ...             ...  \n",
       "4487          1.0          1.0             1.0             0.0  \n",
       "4488          1.0          1.0             1.0             0.0  \n",
       "4489          0.0          0.0             0.0             0.0  \n",
       "4490          0.0          0.0             0.0             0.0  \n",
       "4491          1.0          1.0             1.0             0.0  \n",
       "\n",
       "[4492 rows x 9 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_knn)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_knn))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_knn))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_knn))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_knn))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_knn, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_knn, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_knn))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_knn))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_knn))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "knn_5preds = pd.concat([knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "knn_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c149767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_knn.to_csv('mat_met_knn_opt.csv')\n",
    "knn_5preds.to_csv('knn_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model f1_score 0.8176 with a standard deviation of 0.0186\n",
      "KNN optimized model f1_score 0.8201 with a standard deviation of 0.0172\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"KNN baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_knn_clf.joblib']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_clf, \"./knn_clf.joblib\")\n",
    "#joblib.dump(optimized_knn, \"./optimized_knn.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"./optimizedCV_knn_clf.joblib\")\n",
    "#loaded_rf = joblib.load(\"./optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       205.100000     7.607745\n",
      "1                    TN       164.800000     7.786027\n",
      "2                    FP        48.200000     6.746192\n",
      "3                    FN        31.100000     5.606544\n",
      "4              Accuracy         0.823458     0.012771\n",
      "5             Precision         0.810024     0.022964\n",
      "6           Sensitivity         0.868548     0.021922\n",
      "7           Specificity         0.773920     0.028068\n",
      "8              F1 score         0.837881     0.012442\n",
      "9   F1 score (weighted)         0.822829     0.012893\n",
      "10     F1 score (macro)         0.821854     0.013006\n",
      "11    Balanced Accuracy         0.821237     0.012830\n",
      "12                  MCC         0.647035     0.025368\n",
      "13                  NPV         0.841600     0.025224\n",
      "14              ROC_AUC         0.821237     0.012830\n",
      "CPU times: user 1min 29s, sys: 32 ms, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_clf = SVC()\n",
    "    \n",
    "    svm_clf.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_clf.predict(X_test) \n",
    "   \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggestegorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVC(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average='macro')\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggestegorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "  \n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVC(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:13:45,412]\u001b[0m A new study created in memory with name: SVM_classifier\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:15:06,234]\u001b[0m Trial 0 finished with value: 0.36950212982311054 and parameters: {'C': 1.0, 'gamma': 4.0}. Best is trial 0 with value: 0.36950212982311054.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:16:26,433]\u001b[0m Trial 1 finished with value: 0.36950212982311054 and parameters: {'C': 64.0, 'gamma': 4.0}. Best is trial 0 with value: 0.36950212982311054.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:17:45,687]\u001b[0m Trial 2 finished with value: 0.8304670110339071 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:19:04,949]\u001b[0m Trial 3 finished with value: 0.7663801751050385 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:20:02,740]\u001b[0m Trial 4 finished with value: 0.7725456790368395 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:21:20,878]\u001b[0m Trial 5 finished with value: 0.3454466791085817 and parameters: {'C': 0.0625, 'gamma': 0.0009765625}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:22:23,405]\u001b[0m Trial 6 finished with value: 0.7582398312598555 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:23:08,378]\u001b[0m Trial 7 finished with value: 0.7968643501013045 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:24:27,347]\u001b[0m Trial 8 finished with value: 0.38531234166901457 and parameters: {'C': 2.0, 'gamma': 2.0}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:25:47,900]\u001b[0m Trial 9 finished with value: 0.3454466791085817 and parameters: {'C': 0.0625, 'gamma': 4.0}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:27:06,182]\u001b[0m Trial 10 finished with value: 0.6412162638268042 and parameters: {'C': 0.25, 'gamma': 0.0625}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:28:23,381]\u001b[0m Trial 11 finished with value: 0.3454466791085817 and parameters: {'C': 0.03125, 'gamma': 0.001953125}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:29:41,739]\u001b[0m Trial 12 finished with value: 0.8304670110339071 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 2 with value: 0.8304670110339071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:30:59,894]\u001b[0m Trial 13 finished with value: 0.8307481576674849 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:32:17,969]\u001b[0m Trial 14 finished with value: 0.8307481576674849 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:33:38,279]\u001b[0m Trial 15 finished with value: 0.39816192877688794 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:35:01,190]\u001b[0m Trial 16 finished with value: 0.4260567525448372 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:36:23,669]\u001b[0m Trial 17 finished with value: 0.3454466791085817 and parameters: {'C': 0.125, 'gamma': 0.00048828125}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:37:16,472]\u001b[0m Trial 18 finished with value: 0.8247774847881933 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:38:40,173]\u001b[0m Trial 19 finished with value: 0.3454466791085817 and parameters: {'C': 0.0078125, 'gamma': 8.0}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:39:37,358]\u001b[0m Trial 20 finished with value: 0.7734349869613564 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:40:55,768]\u001b[0m Trial 21 finished with value: 0.8304670110339071 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:42:13,241]\u001b[0m Trial 22 finished with value: 0.3454466791085817 and parameters: {'C': 0.015625, 'gamma': 0.0001220703125}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:43:29,783]\u001b[0m Trial 23 finished with value: 0.7699134575146852 and parameters: {'C': 0.5, 'gamma': 0.0625}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:44:37,720]\u001b[0m Trial 24 finished with value: 0.8274532528800647 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:45:22,714]\u001b[0m Trial 25 finished with value: 0.8012824142809446 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:46:41,379]\u001b[0m Trial 26 finished with value: 0.5150690674189614 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 13 with value: 0.8307481576674849.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:47:59,191]\u001b[0m Trial 27 finished with value: 0.83200074587592 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 27 with value: 0.83200074587592.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:49:09,226]\u001b[0m Trial 28 finished with value: 0.7996616927688567 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 27 with value: 0.83200074587592.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:50:19,851]\u001b[0m Trial 29 finished with value: 0.8247540877030941 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 27 with value: 0.83200074587592.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:51:35,488]\u001b[0m Trial 30 finished with value: 0.8332502357027721 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:52:47,691]\u001b[0m Trial 31 finished with value: 0.7394137301072006 and parameters: {'C': 2.0, 'gamma': 0.00048828125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:54:06,027]\u001b[0m Trial 32 finished with value: 0.8332502357027721 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:55:24,493]\u001b[0m Trial 33 finished with value: 0.8332502357027721 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:56:42,863]\u001b[0m Trial 34 finished with value: 0.8332502357027721 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:58:01,379]\u001b[0m Trial 35 finished with value: 0.8332502357027721 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 15:59:19,943]\u001b[0m Trial 36 finished with value: 0.8332502357027721 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:00:34,155]\u001b[0m Trial 37 finished with value: 0.8332502357027721 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:01:47,458]\u001b[0m Trial 38 finished with value: 0.7121202152402676 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:03:06,886]\u001b[0m Trial 39 finished with value: 0.7663801751050385 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:04:22,545]\u001b[0m Trial 40 finished with value: 0.8308680587746995 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:05:39,329]\u001b[0m Trial 41 finished with value: 0.8332502357027721 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:06:57,487]\u001b[0m Trial 42 finished with value: 0.8332502357027721 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:08:01,303]\u001b[0m Trial 43 finished with value: 0.7574572231068708 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:09:18,616]\u001b[0m Trial 44 finished with value: 0.3474490748966581 and parameters: {'C': 2.0, 'gamma': 6.103515625e-05}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:10:30,698]\u001b[0m Trial 45 finished with value: 0.7638472787585462 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:11:51,716]\u001b[0m Trial 46 finished with value: 0.3454466791085817 and parameters: {'C': 0.03125, 'gamma': 4.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:13:11,776]\u001b[0m Trial 47 finished with value: 0.38531234166901457 and parameters: {'C': 2.0, 'gamma': 2.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:14:30,214]\u001b[0m Trial 48 finished with value: 0.5656955431044904 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:15:50,971]\u001b[0m Trial 49 finished with value: 0.3454466791085817 and parameters: {'C': 0.125, 'gamma': 1.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8333\n",
      "\tBest params:\n",
      "\t\tC: 2.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_classifier\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  401.000000\n",
      "1                    TN  357.000000\n",
      "2                    FP   77.000000\n",
      "3                    FN   64.000000\n",
      "4              Accuracy    0.843159\n",
      "5             Precision    0.838912\n",
      "6           Sensitivity    0.862366\n",
      "7           Specificity    0.822600\n",
      "8              F1 score    0.850477\n",
      "9   F1 score (weighted)    0.843048\n",
      "10     F1 score (macro)    0.842782\n",
      "11    Balanced Accuracy    0.842473\n",
      "12                  MCC    0.685919\n",
      "13                  NPV    0.848000\n",
      "14              ROC_AUC    0.842473\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_svm_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_svm_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_svm_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_svm_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_svm_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_svm_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_svm_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_svm_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_svm_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_svm_0)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:17:21,991]\u001b[0m Trial 50 finished with value: 0.41862243064264615 and parameters: {'C': 2.0, 'gamma': 0.5}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:18:37,454]\u001b[0m Trial 51 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:19:52,842]\u001b[0m Trial 52 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:20:56,474]\u001b[0m Trial 53 finished with value: 0.8081412209542158 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:22:14,915]\u001b[0m Trial 54 finished with value: 0.3465164117244387 and parameters: {'C': 0.015625, 'gamma': 0.001953125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:23:39,386]\u001b[0m Trial 55 finished with value: 0.3465164117244387 and parameters: {'C': 0.0078125, 'gamma': 8.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:24:57,090]\u001b[0m Trial 56 finished with value: 0.6121247696553621 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:26:13,950]\u001b[0m Trial 57 finished with value: 0.8303882473442228 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:27:10,674]\u001b[0m Trial 58 finished with value: 0.8237250395395558 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:28:09,918]\u001b[0m Trial 59 finished with value: 0.7780345502345611 and parameters: {'C': 1.0, 'gamma': 0.00390625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:29:30,032]\u001b[0m Trial 60 finished with value: 0.49246032074766655 and parameters: {'C': 2.0, 'gamma': 0.25}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:30:47,945]\u001b[0m Trial 61 finished with value: 0.3465164117244387 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:32:04,081]\u001b[0m Trial 62 finished with value: 0.8242514173619739 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:33:18,764]\u001b[0m Trial 63 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:34:33,502]\u001b[0m Trial 64 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:35:51,994]\u001b[0m Trial 65 finished with value: 0.37317793878798267 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:36:55,040]\u001b[0m Trial 66 finished with value: 0.7611172968302753 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:38:13,355]\u001b[0m Trial 67 finished with value: 0.3465164117244387 and parameters: {'C': 0.25, 'gamma': 0.000244140625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:39:33,979]\u001b[0m Trial 68 finished with value: 0.3465164117244387 and parameters: {'C': 0.125, 'gamma': 0.125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:40:31,401]\u001b[0m Trial 69 finished with value: 0.7689362231836719 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:41:40,404]\u001b[0m Trial 70 finished with value: 0.80606343885321 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:42:55,237]\u001b[0m Trial 71 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:44:10,105]\u001b[0m Trial 72 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:45:24,842]\u001b[0m Trial 73 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:46:45,784]\u001b[0m Trial 74 finished with value: 0.3465164117244387 and parameters: {'C': 0.0625, 'gamma': 2.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:48:06,249]\u001b[0m Trial 75 finished with value: 0.37348964834579307 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:48:55,742]\u001b[0m Trial 76 finished with value: 0.7920921760446804 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:50:10,715]\u001b[0m Trial 77 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:51:31,163]\u001b[0m Trial 78 finished with value: 0.39637526436754905 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:52:52,801]\u001b[0m Trial 79 finished with value: 0.3465164117244387 and parameters: {'C': 0.0078125, 'gamma': 0.5}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:54:15,732]\u001b[0m Trial 80 finished with value: 0.3465164117244387 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:55:34,314]\u001b[0m Trial 81 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:56:52,458]\u001b[0m Trial 82 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:57:44,875]\u001b[0m Trial 83 finished with value: 0.8118859377583977 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:58:58,477]\u001b[0m Trial 84 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:00:00,913]\u001b[0m Trial 85 finished with value: 0.7584527167034292 and parameters: {'C': 1.0, 'gamma': 0.001953125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:01:27,159]\u001b[0m Trial 86 finished with value: 0.37348964834579307 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:02:46,612]\u001b[0m Trial 87 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:03:52,329]\u001b[0m Trial 88 finished with value: 0.7592661314226652 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:05:16,046]\u001b[0m Trial 89 finished with value: 0.3465164117244387 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:06:16,073]\u001b[0m Trial 90 finished with value: 0.8237250395395558 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:07:36,224]\u001b[0m Trial 91 finished with value: 0.8259107404235806 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:08:56,487]\u001b[0m Trial 92 finished with value: 0.8259107404235806 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:09:44,949]\u001b[0m Trial 93 finished with value: 0.8077973607823756 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 17:10:58,546]\u001b[0m Trial 94 finished with value: 0.7670535099637923 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:12:18,413]\u001b[0m Trial 95 finished with value: 0.3465164117244387 and parameters: {'C': 0.03125, 'gamma': 0.25}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:13:34,116]\u001b[0m Trial 96 finished with value: 0.8262037198916369 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:14:48,299]\u001b[0m Trial 97 finished with value: 0.8289104738579999 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:15:56,759]\u001b[0m Trial 98 finished with value: 0.80606343885321 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:17:09,613]\u001b[0m Trial 99 finished with value: 0.7188403133344993 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8333\n",
      "\tBest params:\n",
      "\t\tC: 2.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  401.000000  402.000000\n",
      "1                    TN  357.000000  358.000000\n",
      "2                    FP   77.000000   85.000000\n",
      "3                    FN   64.000000   54.000000\n",
      "4              Accuracy    0.843159    0.845384\n",
      "5             Precision    0.838912    0.825462\n",
      "6           Sensitivity    0.862366    0.881579\n",
      "7           Specificity    0.822600    0.808100\n",
      "8              F1 score    0.850477    0.852598\n",
      "9   F1 score (weighted)    0.843048    0.845122\n",
      "10     F1 score (macro)    0.842782    0.845012\n",
      "11    Balanced Accuracy    0.842473    0.844853\n",
      "12                  MCC    0.685919    0.692046\n",
      "13                  NPV    0.848000    0.868900\n",
      "14              ROC_AUC    0.842473    0.844853\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_svm_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_svm_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_svm_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_svm_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_svm_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_svm_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_svm_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_svm_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_svm_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_svm_1)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 17:18:22,417]\u001b[0m Trial 100 finished with value: 0.7626873221003316 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:19:37,266]\u001b[0m Trial 101 finished with value: 0.7190860273730104 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:20:53,440]\u001b[0m Trial 102 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:22:08,331]\u001b[0m Trial 103 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:23:27,287]\u001b[0m Trial 104 finished with value: 0.7673398298842178 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:24:45,797]\u001b[0m Trial 105 finished with value: 0.8223562311678606 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:26:00,832]\u001b[0m Trial 106 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:27:15,899]\u001b[0m Trial 107 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:28:33,554]\u001b[0m Trial 108 finished with value: 0.344233332726529 and parameters: {'C': 0.0625, 'gamma': 0.00048828125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:29:41,434]\u001b[0m Trial 109 finished with value: 0.7413719195212897 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:31:01,186]\u001b[0m Trial 110 finished with value: 0.37601635430477576 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:32:16,336]\u001b[0m Trial 111 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:33:31,385]\u001b[0m Trial 112 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:34:46,620]\u001b[0m Trial 113 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:36:00,601]\u001b[0m Trial 114 finished with value: 0.829080081918294 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:37:20,071]\u001b[0m Trial 115 finished with value: 0.39296003481773106 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:38:38,553]\u001b[0m Trial 116 finished with value: 0.344233332726529 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:39:58,465]\u001b[0m Trial 117 finished with value: 0.344233332726529 and parameters: {'C': 0.0078125, 'gamma': 1.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:41:17,572]\u001b[0m Trial 118 finished with value: 0.43202632338861324 and parameters: {'C': 2.0, 'gamma': 0.5}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:42:32,636]\u001b[0m Trial 119 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:43:25,216]\u001b[0m Trial 120 finished with value: 0.8137596438537242 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:44:40,162]\u001b[0m Trial 121 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:45:55,331]\u001b[0m Trial 122 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:47:10,525]\u001b[0m Trial 123 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:48:20,838]\u001b[0m Trial 124 finished with value: 0.8193776919993938 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:49:43,207]\u001b[0m Trial 125 finished with value: 0.37601635430477576 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:50:57,014]\u001b[0m Trial 126 finished with value: 0.829080081918294 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:51:43,811]\u001b[0m Trial 127 finished with value: 0.8045088773361453 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:53:00,436]\u001b[0m Trial 128 finished with value: 0.6316958650699143 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:53:56,765]\u001b[0m Trial 129 finished with value: 0.8228511139489256 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:55:08,104]\u001b[0m Trial 130 finished with value: 0.7725251840962039 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:56:23,142]\u001b[0m Trial 131 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:57:38,292]\u001b[0m Trial 132 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 17:58:53,261]\u001b[0m Trial 133 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:00:10,738]\u001b[0m Trial 134 finished with value: 0.344233332726529 and parameters: {'C': 0.03125, 'gamma': 3.0517578125e-05}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:01:26,183]\u001b[0m Trial 135 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:02:20,860]\u001b[0m Trial 136 finished with value: 0.7961366517131483 and parameters: {'C': 2.0, 'gamma': 0.00390625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:03:40,777]\u001b[0m Trial 137 finished with value: 0.36725045299825054 and parameters: {'C': 0.5, 'gamma': 0.25}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:04:54,511]\u001b[0m Trial 138 finished with value: 0.829080081918294 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:06:07,506]\u001b[0m Trial 139 finished with value: 0.829080081918294 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:07:21,487]\u001b[0m Trial 140 finished with value: 0.7190860273730104 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:08:38,371]\u001b[0m Trial 141 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:09:52,818]\u001b[0m Trial 142 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:11:07,336]\u001b[0m Trial 143 finished with value: 0.8325459112649531 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 18:12:04,426]\u001b[0m Trial 144 finished with value: 0.7822980516549195 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:12:53,021]\u001b[0m Trial 145 finished with value: 0.7950466220743264 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:14:11,193]\u001b[0m Trial 146 finished with value: 0.7673398298842178 and parameters: {'C': 64.0, 'gamma': 0.125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:15:27,927]\u001b[0m Trial 147 finished with value: 0.5818101005544056 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:16:40,938]\u001b[0m Trial 148 finished with value: 0.829080081918294 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:17:57,227]\u001b[0m Trial 149 finished with value: 0.3613328561872839 and parameters: {'C': 2.0, 'gamma': 6.103515625e-05}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8333\n",
      "\tBest params:\n",
      "\t\tC: 2.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  401.000000  402.000000  409.000000\n",
      "1                    TN  357.000000  358.000000  349.000000\n",
      "2                    FP   77.000000   85.000000   75.000000\n",
      "3                    FN   64.000000   54.000000   66.000000\n",
      "4              Accuracy    0.843159    0.845384    0.843159\n",
      "5             Precision    0.838912    0.825462    0.845041\n",
      "6           Sensitivity    0.862366    0.881579    0.861053\n",
      "7           Specificity    0.822600    0.808100    0.823100\n",
      "8              F1 score    0.850477    0.852598    0.852972\n",
      "9   F1 score (weighted)    0.843048    0.845122    0.843054\n",
      "10     F1 score (macro)    0.842782    0.845012    0.842457\n",
      "11    Balanced Accuracy    0.842473    0.844853    0.842083\n",
      "12                  MCC    0.685919    0.692046    0.685085\n",
      "13                  NPV    0.848000    0.868900    0.841000\n",
      "14              ROC_AUC    0.842473    0.844853    0.842083\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_svm_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_svm_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_svm_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_svm_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_svm_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_svm_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_svm_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_svm_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_svm_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_svm_2)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 18:19:24,731]\u001b[0m Trial 150 finished with value: 0.8227239203396299 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:20:39,313]\u001b[0m Trial 151 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:21:53,822]\u001b[0m Trial 152 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:23:08,290]\u001b[0m Trial 153 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:24:22,810]\u001b[0m Trial 154 finished with value: 0.8247872680335551 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:25:37,413]\u001b[0m Trial 155 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:26:54,503]\u001b[0m Trial 156 finished with value: 0.3447997546465653 and parameters: {'C': 0.015625, 'gamma': 0.00048828125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:28:13,481]\u001b[0m Trial 157 finished with value: 0.3447997546465653 and parameters: {'C': 0.0078125, 'gamma': 4.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:29:31,740]\u001b[0m Trial 158 finished with value: 0.3853159418251028 and parameters: {'C': 2.0, 'gamma': 2.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:30:46,274]\u001b[0m Trial 159 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:32:04,503]\u001b[0m Trial 160 finished with value: 0.3994208147822838 and parameters: {'C': 2.0, 'gamma': 1.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:33:19,150]\u001b[0m Trial 161 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:34:33,848]\u001b[0m Trial 162 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:35:48,438]\u001b[0m Trial 163 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:36:58,134]\u001b[0m Trial 164 finished with value: 0.8147001665884686 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:38:16,234]\u001b[0m Trial 165 finished with value: 0.42338789779163266 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:39:30,470]\u001b[0m Trial 166 finished with value: 0.8250957412536725 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:40:44,458]\u001b[0m Trial 167 finished with value: 0.828075432442839 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:41:59,145]\u001b[0m Trial 168 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:42:56,881]\u001b[0m Trial 169 finished with value: 0.7752957093386009 and parameters: {'C': 2.0, 'gamma': 0.001953125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:44:14,191]\u001b[0m Trial 170 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:45:33,658]\u001b[0m Trial 171 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:46:35,567]\u001b[0m Trial 172 finished with value: 0.8120811202404015 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:47:50,158]\u001b[0m Trial 173 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:49:04,313]\u001b[0m Trial 174 finished with value: 0.828075432442839 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:50:26,681]\u001b[0m Trial 175 finished with value: 0.3447997546465653 and parameters: {'C': 0.25, 'gamma': 8.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:51:34,277]\u001b[0m Trial 176 finished with value: 0.8181522258071725 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:52:48,995]\u001b[0m Trial 177 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:54:05,033]\u001b[0m Trial 178 finished with value: 0.6348018168865305 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:55:22,284]\u001b[0m Trial 179 finished with value: 0.3732839142707277 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:56:36,999]\u001b[0m Trial 180 finished with value: 0.8267481220661373 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:57:51,728]\u001b[0m Trial 181 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 18:59:06,472]\u001b[0m Trial 182 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:00:18,630]\u001b[0m Trial 183 finished with value: 0.710098084339292 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:01:32,862]\u001b[0m Trial 184 finished with value: 0.828075432442839 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:02:19,672]\u001b[0m Trial 185 finished with value: 0.8081638807166355 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:03:38,935]\u001b[0m Trial 186 finished with value: 0.37336039268116517 and parameters: {'C': 0.5, 'gamma': 0.25}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:04:53,279]\u001b[0m Trial 187 finished with value: 0.7056947289951834 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:06:07,286]\u001b[0m Trial 188 finished with value: 0.828075432442839 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:07:24,182]\u001b[0m Trial 189 finished with value: 0.5681112340735306 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:08:21,384]\u001b[0m Trial 190 finished with value: 0.7691974932391421 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:09:36,070]\u001b[0m Trial 191 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:10:50,861]\u001b[0m Trial 192 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:12:05,865]\u001b[0m Trial 193 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 19:13:20,770]\u001b[0m Trial 194 finished with value: 0.8259607348706034 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:14:34,921]\u001b[0m Trial 195 finished with value: 0.828075432442839 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:15:54,428]\u001b[0m Trial 196 finished with value: 0.8267481220661373 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:17:09,895]\u001b[0m Trial 197 finished with value: 0.8247872680335551 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:18:27,826]\u001b[0m Trial 198 finished with value: 0.7599301304458782 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:19:43,858]\u001b[0m Trial 199 finished with value: 0.3447997546465653 and parameters: {'C': 0.0078125, 'gamma': 6.103515625e-05}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8333\n",
      "\tBest params:\n",
      "\t\tC: 2.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  401.000000  402.000000  409.000000  418.000000\n",
      "1                    TN  357.000000  358.000000  349.000000  347.000000\n",
      "2                    FP   77.000000   85.000000   75.000000   82.000000\n",
      "3                    FN   64.000000   54.000000   66.000000   52.000000\n",
      "4              Accuracy    0.843159    0.845384    0.843159    0.850945\n",
      "5             Precision    0.838912    0.825462    0.845041    0.836000\n",
      "6           Sensitivity    0.862366    0.881579    0.861053    0.889362\n",
      "7           Specificity    0.822600    0.808100    0.823100    0.808900\n",
      "8              F1 score    0.850477    0.852598    0.852972    0.861856\n",
      "9   F1 score (weighted)    0.843048    0.845122    0.843054    0.850550\n",
      "10     F1 score (macro)    0.842782    0.845012    0.842457    0.850010\n",
      "11    Balanced Accuracy    0.842473    0.844853    0.842083    0.849110\n",
      "12                  MCC    0.685919    0.692046    0.685085    0.701937\n",
      "13                  NPV    0.848000    0.868900    0.841000    0.869700\n",
      "14              ROC_AUC    0.842473    0.844853    0.842083    0.849110\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_svm_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_svm_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_svm_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_svm_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_svm_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_svm_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_svm_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_svm_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_svm_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_svm_3)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 19:21:10,598]\u001b[0m Trial 200 finished with value: 0.8249190115114388 and parameters: {'C': 2.0, 'gamma': 0.0625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:21:58,378]\u001b[0m Trial 201 finished with value: 0.8028863342964053 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:23:11,390]\u001b[0m Trial 202 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:24:24,252]\u001b[0m Trial 203 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:25:37,090]\u001b[0m Trial 204 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:26:52,122]\u001b[0m Trial 205 finished with value: 0.34747884783716254 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:28:03,549]\u001b[0m Trial 206 finished with value: 0.8297196052414115 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:29:20,131]\u001b[0m Trial 207 finished with value: 0.38964962832490596 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:30:07,286]\u001b[0m Trial 208 finished with value: 0.7992828914988651 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:31:15,218]\u001b[0m Trial 209 finished with value: 0.8258656290341951 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:32:32,085]\u001b[0m Trial 210 finished with value: 0.3718283716843949 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:33:44,745]\u001b[0m Trial 211 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:34:57,341]\u001b[0m Trial 212 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:36:09,987]\u001b[0m Trial 213 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:37:22,552]\u001b[0m Trial 214 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:38:38,920]\u001b[0m Trial 215 finished with value: 0.3953806510850648 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:39:51,288]\u001b[0m Trial 216 finished with value: 0.8283844199803454 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:41:02,568]\u001b[0m Trial 217 finished with value: 0.8297196052414115 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:42:15,115]\u001b[0m Trial 218 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:43:17,039]\u001b[0m Trial 219 finished with value: 0.8161940639832952 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:44:28,298]\u001b[0m Trial 220 finished with value: 0.8297196052414115 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:45:40,917]\u001b[0m Trial 221 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:46:53,451]\u001b[0m Trial 222 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:48:09,851]\u001b[0m Trial 223 finished with value: 0.42381612072774094 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:49:22,437]\u001b[0m Trial 224 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:50:30,701]\u001b[0m Trial 225 finished with value: 0.7692240918221813 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:51:26,575]\u001b[0m Trial 226 finished with value: 0.7852740856164272 and parameters: {'C': 2.0, 'gamma': 0.001953125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:52:41,213]\u001b[0m Trial 227 finished with value: 0.3782026894508145 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:53:53,746]\u001b[0m Trial 228 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:55:13,291]\u001b[0m Trial 229 finished with value: 0.3718283716843949 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:56:04,019]\u001b[0m Trial 230 finished with value: 0.7920777131186414 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:57:16,537]\u001b[0m Trial 231 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:58:29,088]\u001b[0m Trial 232 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 19:59:41,600]\u001b[0m Trial 233 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:00:52,936]\u001b[0m Trial 234 finished with value: 0.8297196052414115 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:01:51,902]\u001b[0m Trial 235 finished with value: 0.8003794106033327 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:03:04,474]\u001b[0m Trial 236 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:04:17,034]\u001b[0m Trial 237 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:05:28,409]\u001b[0m Trial 238 finished with value: 0.8297196052414115 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:06:39,975]\u001b[0m Trial 239 finished with value: 0.704069717478751 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:07:57,151]\u001b[0m Trial 240 finished with value: 0.34747884783716254 and parameters: {'C': 0.0625, 'gamma': 0.25}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:09:09,685]\u001b[0m Trial 241 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:10:22,231]\u001b[0m Trial 242 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:11:34,766]\u001b[0m Trial 243 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 20:12:47,364]\u001b[0m Trial 244 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:13:59,937]\u001b[0m Trial 245 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:15:14,076]\u001b[0m Trial 246 finished with value: 0.34747884783716254 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:16:26,660]\u001b[0m Trial 247 finished with value: 0.827038671508825 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:17:39,164]\u001b[0m Trial 248 finished with value: 0.8284180473942045 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:18:49,136]\u001b[0m Trial 249 finished with value: 0.7139631317494478 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 30 with value: 0.8332502357027721.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8333\n",
      "\tBest params:\n",
      "\t\tC: 2.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  401.000000  402.000000  409.000000  418.000000   \n",
      "1                    TN  357.000000  358.000000  349.000000  347.000000   \n",
      "2                    FP   77.000000   85.000000   75.000000   82.000000   \n",
      "3                    FN   64.000000   54.000000   66.000000   52.000000   \n",
      "4              Accuracy    0.843159    0.845384    0.843159    0.850945   \n",
      "5             Precision    0.838912    0.825462    0.845041    0.836000   \n",
      "6           Sensitivity    0.862366    0.881579    0.861053    0.889362   \n",
      "7           Specificity    0.822600    0.808100    0.823100    0.808900   \n",
      "8              F1 score    0.850477    0.852598    0.852972    0.861856   \n",
      "9   F1 score (weighted)    0.843048    0.845122    0.843054    0.850550   \n",
      "10     F1 score (macro)    0.842782    0.845012    0.842457    0.850010   \n",
      "11    Balanced Accuracy    0.842473    0.844853    0.842083    0.849110   \n",
      "12                  MCC    0.685919    0.692046    0.685085    0.701937   \n",
      "13                  NPV    0.848000    0.868900    0.841000    0.869700   \n",
      "14              ROC_AUC    0.842473    0.844853    0.842083    0.849110   \n",
      "\n",
      "          Set4  \n",
      "0   409.000000  \n",
      "1   351.000000  \n",
      "2   100.000000  \n",
      "3    39.000000  \n",
      "4     0.845384  \n",
      "5     0.803536  \n",
      "6     0.912946  \n",
      "7     0.778300  \n",
      "8     0.854754  \n",
      "9     0.844704  \n",
      "10    0.844738  \n",
      "11    0.845608  \n",
      "12    0.697349  \n",
      "13    0.900000  \n",
      "14    0.845608  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_svm_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_svm_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_svm_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_svm_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_svm_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_svm_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_svm_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_svm_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_svm_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_svm_4)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 20:20:11,498]\u001b[0m Trial 250 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:21:24,300]\u001b[0m Trial 251 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:22:15,603]\u001b[0m Trial 252 finished with value: 0.7916191821510965 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:23:32,344]\u001b[0m Trial 253 finished with value: 0.34535792787014735 and parameters: {'C': 0.015625, 'gamma': 0.125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:24:45,118]\u001b[0m Trial 254 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:25:57,935]\u001b[0m Trial 255 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:27:13,755]\u001b[0m Trial 256 finished with value: 0.8278653470745263 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:28:26,584]\u001b[0m Trial 257 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:29:36,603]\u001b[0m Trial 258 finished with value: 0.7168415426239815 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:30:49,452]\u001b[0m Trial 259 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:32:02,261]\u001b[0m Trial 260 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:33:15,081]\u001b[0m Trial 261 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:34:27,920]\u001b[0m Trial 262 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:35:40,744]\u001b[0m Trial 263 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:36:53,517]\u001b[0m Trial 264 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:38:06,327]\u001b[0m Trial 265 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:39:22,894]\u001b[0m Trial 266 finished with value: 0.38178346002463004 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:40:35,724]\u001b[0m Trial 267 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:41:31,214]\u001b[0m Trial 268 finished with value: 0.7760976820722099 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:42:48,257]\u001b[0m Trial 269 finished with value: 0.3674065255395263 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:44:01,044]\u001b[0m Trial 270 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:45:13,927]\u001b[0m Trial 271 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:46:26,746]\u001b[0m Trial 272 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:47:39,559]\u001b[0m Trial 273 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:48:56,078]\u001b[0m Trial 274 finished with value: 0.3987243938371997 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:50:08,872]\u001b[0m Trial 275 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:51:21,678]\u001b[0m Trial 276 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:52:34,472]\u001b[0m Trial 277 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:53:47,263]\u001b[0m Trial 278 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:55:00,078]\u001b[0m Trial 279 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:56:12,874]\u001b[0m Trial 280 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:57:25,666]\u001b[0m Trial 281 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:58:38,255]\u001b[0m Trial 282 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 20:59:50,681]\u001b[0m Trial 283 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:01:03,121]\u001b[0m Trial 284 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:02:19,151]\u001b[0m Trial 285 finished with value: 0.43249594363351457 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:03:31,555]\u001b[0m Trial 286 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:04:19,494]\u001b[0m Trial 287 finished with value: 0.8271328849366991 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:05:31,845]\u001b[0m Trial 288 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:06:44,301]\u001b[0m Trial 289 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:07:32,150]\u001b[0m Trial 290 finished with value: 0.8049153098197349 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:08:44,511]\u001b[0m Trial 291 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:09:56,876]\u001b[0m Trial 292 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:11:16,125]\u001b[0m Trial 293 finished with value: 0.3674065255395263 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 21:12:20,576]\u001b[0m Trial 294 finished with value: 0.7415672333206864 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:13:32,893]\u001b[0m Trial 295 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:14:38,708]\u001b[0m Trial 296 finished with value: 0.826924593962001 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:15:51,072]\u001b[0m Trial 297 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:17:03,490]\u001b[0m Trial 298 finished with value: 0.8333334230916009 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:17:49,865]\u001b[0m Trial 299 finished with value: 0.816546844893019 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8333\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  401.000000  402.000000  409.000000  418.000000   \n",
      "1                    TN  357.000000  358.000000  349.000000  347.000000   \n",
      "2                    FP   77.000000   85.000000   75.000000   82.000000   \n",
      "3                    FN   64.000000   54.000000   66.000000   52.000000   \n",
      "4              Accuracy    0.843159    0.845384    0.843159    0.850945   \n",
      "5             Precision    0.838912    0.825462    0.845041    0.836000   \n",
      "6           Sensitivity    0.862366    0.881579    0.861053    0.889362   \n",
      "7           Specificity    0.822600    0.808100    0.823100    0.808900   \n",
      "8              F1 score    0.850477    0.852598    0.852972    0.861856   \n",
      "9   F1 score (weighted)    0.843048    0.845122    0.843054    0.850550   \n",
      "10     F1 score (macro)    0.842782    0.845012    0.842457    0.850010   \n",
      "11    Balanced Accuracy    0.842473    0.844853    0.842083    0.849110   \n",
      "12                  MCC    0.685919    0.692046    0.685085    0.701937   \n",
      "13                  NPV    0.848000    0.868900    0.841000    0.869700   \n",
      "14              ROC_AUC    0.842473    0.844853    0.842083    0.849110   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   409.000000  393.000000  \n",
      "1   351.000000  343.000000  \n",
      "2   100.000000   90.000000  \n",
      "3    39.000000   73.000000  \n",
      "4     0.845384    0.818687  \n",
      "5     0.803536    0.813665  \n",
      "6     0.912946    0.843348  \n",
      "7     0.778300    0.792100  \n",
      "8     0.854754    0.828240  \n",
      "9     0.844704    0.818496  \n",
      "10    0.844738    0.818125  \n",
      "11    0.845608    0.817748  \n",
      "12    0.697349    0.636838  \n",
      "13    0.900000    0.824500  \n",
      "14    0.845608    0.817748  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_svm_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_svm_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_svm_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_svm_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_svm_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_svm_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_svm_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_svm_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_svm_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_svm_5)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 21:19:13,022]\u001b[0m Trial 300 finished with value: 0.636245945164387 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 250 with value: 0.8333334230916009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:20:24,962]\u001b[0m Trial 301 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:21:36,936]\u001b[0m Trial 302 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:22:48,908]\u001b[0m Trial 303 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:24:00,870]\u001b[0m Trial 304 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:25:12,855]\u001b[0m Trial 305 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:26:28,653]\u001b[0m Trial 306 finished with value: 0.5139183988555047 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:27:40,610]\u001b[0m Trial 307 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:28:52,585]\u001b[0m Trial 308 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:29:52,196]\u001b[0m Trial 309 finished with value: 0.7621876492328049 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:31:04,118]\u001b[0m Trial 310 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:32:19,914]\u001b[0m Trial 311 finished with value: 0.772582597183084 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:33:31,879]\u001b[0m Trial 312 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:34:47,071]\u001b[0m Trial 313 finished with value: 0.8272198461279545 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:35:59,035]\u001b[0m Trial 314 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:37:10,990]\u001b[0m Trial 315 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:38:20,769]\u001b[0m Trial 316 finished with value: 0.7204466798869754 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:39:32,748]\u001b[0m Trial 317 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:40:23,540]\u001b[0m Trial 318 finished with value: 0.7896389439351623 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:41:35,469]\u001b[0m Trial 319 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:42:30,078]\u001b[0m Trial 320 finished with value: 0.7792356032433798 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:43:40,461]\u001b[0m Trial 321 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:44:50,839]\u001b[0m Trial 322 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:46:01,263]\u001b[0m Trial 323 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:47:16,014]\u001b[0m Trial 324 finished with value: 0.37382557606794287 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:48:26,730]\u001b[0m Trial 325 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:49:37,710]\u001b[0m Trial 326 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:50:52,665]\u001b[0m Trial 327 finished with value: 0.40417620009249255 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:52:07,052]\u001b[0m Trial 328 finished with value: 0.38472038949649606 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:53:17,432]\u001b[0m Trial 329 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:54:31,654]\u001b[0m Trial 330 finished with value: 0.428472432988361 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:55:42,007]\u001b[0m Trial 331 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:56:52,354]\u001b[0m Trial 332 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:57:38,305]\u001b[0m Trial 333 finished with value: 0.8231744853360597 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:58:48,623]\u001b[0m Trial 334 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 21:59:35,016]\u001b[0m Trial 335 finished with value: 0.798130060827801 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:00:45,332]\u001b[0m Trial 336 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:01:55,674]\u001b[0m Trial 337 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:03:06,029]\u001b[0m Trial 338 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:04:23,385]\u001b[0m Trial 339 finished with value: 0.37382557606794287 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:05:27,090]\u001b[0m Trial 340 finished with value: 0.8322107151206181 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:06:30,211]\u001b[0m Trial 341 finished with value: 0.7397489926429122 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:07:41,508]\u001b[0m Trial 342 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:08:53,467]\u001b[0m Trial 343 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 22:10:05,380]\u001b[0m Trial 344 finished with value: 0.8326373394807846 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:11:18,751]\u001b[0m Trial 345 finished with value: 0.636245945164387 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:12:33,415]\u001b[0m Trial 346 finished with value: 0.344830023966733 and parameters: {'C': 0.0078125, 'gamma': 0.00390625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:13:45,380]\u001b[0m Trial 347 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:15:01,159]\u001b[0m Trial 348 finished with value: 0.5139183988555047 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:16:12,826]\u001b[0m Trial 349 finished with value: 0.8337448573549443 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8337\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  401.000000  402.000000  409.000000  418.000000   \n",
      "1                    TN  357.000000  358.000000  349.000000  347.000000   \n",
      "2                    FP   77.000000   85.000000   75.000000   82.000000   \n",
      "3                    FN   64.000000   54.000000   66.000000   52.000000   \n",
      "4              Accuracy    0.843159    0.845384    0.843159    0.850945   \n",
      "5             Precision    0.838912    0.825462    0.845041    0.836000   \n",
      "6           Sensitivity    0.862366    0.881579    0.861053    0.889362   \n",
      "7           Specificity    0.822600    0.808100    0.823100    0.808900   \n",
      "8              F1 score    0.850477    0.852598    0.852972    0.861856   \n",
      "9   F1 score (weighted)    0.843048    0.845122    0.843054    0.850550   \n",
      "10     F1 score (macro)    0.842782    0.845012    0.842457    0.850010   \n",
      "11    Balanced Accuracy    0.842473    0.844853    0.842083    0.849110   \n",
      "12                  MCC    0.685919    0.692046    0.685085    0.701937   \n",
      "13                  NPV    0.848000    0.868900    0.841000    0.869700   \n",
      "14              ROC_AUC    0.842473    0.844853    0.842083    0.849110   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   409.000000  393.000000  411.000000  \n",
      "1   351.000000  343.000000  334.000000  \n",
      "2   100.000000   90.000000   95.000000  \n",
      "3    39.000000   73.000000   59.000000  \n",
      "4     0.845384    0.818687    0.828699  \n",
      "5     0.803536    0.813665    0.812253  \n",
      "6     0.912946    0.843348    0.874468  \n",
      "7     0.778300    0.792100    0.778600  \n",
      "8     0.854754    0.828240    0.842213  \n",
      "9     0.844704    0.818496    0.828107  \n",
      "10    0.844738    0.818125    0.827433  \n",
      "11    0.845608    0.817748    0.826511  \n",
      "12    0.697349    0.636838    0.657559  \n",
      "13    0.900000    0.824500    0.849900  \n",
      "14    0.845608    0.817748    0.826511  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_svm_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_svm_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_svm_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_svm_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_svm_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_svm_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_svm_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_svm_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_svm_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_svm_6)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 22:17:29,961]\u001b[0m Trial 350 finished with value: 0.8264039272012319 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:18:42,069]\u001b[0m Trial 351 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:19:54,176]\u001b[0m Trial 352 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:21:06,291]\u001b[0m Trial 353 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:22:10,103]\u001b[0m Trial 354 finished with value: 0.7442252347148595 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:23:22,220]\u001b[0m Trial 355 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:24:37,456]\u001b[0m Trial 356 finished with value: 0.8259691998848624 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:25:49,564]\u001b[0m Trial 357 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:27:02,639]\u001b[0m Trial 358 finished with value: 0.6064979892168536 and parameters: {'C': 0.25, 'gamma': 0.0009765625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:28:18,438]\u001b[0m Trial 359 finished with value: 0.7689932445218226 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:29:32,974]\u001b[0m Trial 360 finished with value: 0.38941466459051555 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:30:45,087]\u001b[0m Trial 361 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:31:57,212]\u001b[0m Trial 362 finished with value: 0.8333507174522667 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:33:11,451]\u001b[0m Trial 363 finished with value: 0.34583299308500387 and parameters: {'C': 0.5, 'gamma': 6.103515625e-05}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:34:23,536]\u001b[0m Trial 364 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:35:17,616]\u001b[0m Trial 365 finished with value: 0.7805443677410863 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:36:28,976]\u001b[0m Trial 366 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:37:44,245]\u001b[0m Trial 367 finished with value: 0.3867916792849527 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:38:56,444]\u001b[0m Trial 368 finished with value: 0.5929476697300744 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:40:11,677]\u001b[0m Trial 369 finished with value: 0.34583299308500387 and parameters: {'C': 0.125, 'gamma': 4.0}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:41:22,131]\u001b[0m Trial 370 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:42:32,607]\u001b[0m Trial 371 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:43:46,864]\u001b[0m Trial 372 finished with value: 0.39624844941203385 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:44:57,336]\u001b[0m Trial 373 finished with value: 0.8333507174522667 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:46:07,812]\u001b[0m Trial 374 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:47:20,782]\u001b[0m Trial 375 finished with value: 0.34583299308500387 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:48:35,009]\u001b[0m Trial 376 finished with value: 0.41804236346957807 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:49:20,792]\u001b[0m Trial 377 finished with value: 0.8226481918970497 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:50:31,281]\u001b[0m Trial 378 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:51:41,778]\u001b[0m Trial 379 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:52:52,299]\u001b[0m Trial 380 finished with value: 0.8332999605702283 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:53:38,171]\u001b[0m Trial 381 finished with value: 0.8018909422269912 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:54:51,323]\u001b[0m Trial 382 finished with value: 0.34583299308500387 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:55:53,597]\u001b[0m Trial 383 finished with value: 0.7450720106666725 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:57:10,933]\u001b[0m Trial 384 finished with value: 0.373633485283004 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:58:21,417]\u001b[0m Trial 385 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 22:59:25,212]\u001b[0m Trial 386 finished with value: 0.8259655128373057 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:00:35,648]\u001b[0m Trial 387 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:01:41,523]\u001b[0m Trial 388 finished with value: 0.8264039272012319 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:02:52,719]\u001b[0m Trial 389 finished with value: 0.623468074475247 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:03:39,537]\u001b[0m Trial 390 finished with value: 0.8064108921633959 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:04:49,946]\u001b[0m Trial 391 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:06:04,033]\u001b[0m Trial 392 finished with value: 0.5041585879548333 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:07:14,479]\u001b[0m Trial 393 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 23:08:24,930]\u001b[0m Trial 394 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:09:35,373]\u001b[0m Trial 395 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:10:41,341]\u001b[0m Trial 396 finished with value: 0.774066742499279 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:11:26,772]\u001b[0m Trial 397 finished with value: 0.7925885062208401 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:12:39,592]\u001b[0m Trial 398 finished with value: 0.38941466459051555 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:13:49,990]\u001b[0m Trial 399 finished with value: 0.8318777355459721 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8337\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  401.000000  402.000000  409.000000  418.000000   \n",
      "1                    TN  357.000000  358.000000  349.000000  347.000000   \n",
      "2                    FP   77.000000   85.000000   75.000000   82.000000   \n",
      "3                    FN   64.000000   54.000000   66.000000   52.000000   \n",
      "4              Accuracy    0.843159    0.845384    0.843159    0.850945   \n",
      "5             Precision    0.838912    0.825462    0.845041    0.836000   \n",
      "6           Sensitivity    0.862366    0.881579    0.861053    0.889362   \n",
      "7           Specificity    0.822600    0.808100    0.823100    0.808900   \n",
      "8              F1 score    0.850477    0.852598    0.852972    0.861856   \n",
      "9   F1 score (weighted)    0.843048    0.845122    0.843054    0.850550   \n",
      "10     F1 score (macro)    0.842782    0.845012    0.842457    0.850010   \n",
      "11    Balanced Accuracy    0.842473    0.844853    0.842083    0.849110   \n",
      "12                  MCC    0.685919    0.692046    0.685085    0.701937   \n",
      "13                  NPV    0.848000    0.868900    0.841000    0.869700   \n",
      "14              ROC_AUC    0.842473    0.844853    0.842083    0.849110   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   409.000000  393.000000  411.000000  406.000000  \n",
      "1   351.000000  343.000000  334.000000  349.000000  \n",
      "2   100.000000   90.000000   95.000000   89.000000  \n",
      "3    39.000000   73.000000   59.000000   55.000000  \n",
      "4     0.845384    0.818687    0.828699    0.839822  \n",
      "5     0.803536    0.813665    0.812253    0.820202  \n",
      "6     0.912946    0.843348    0.874468    0.880694  \n",
      "7     0.778300    0.792100    0.778600    0.796800  \n",
      "8     0.854754    0.828240    0.842213    0.849372  \n",
      "9     0.844704    0.818496    0.828107    0.839436  \n",
      "10    0.844738    0.818125    0.827433    0.839176  \n",
      "11    0.845608    0.817748    0.826511    0.838749  \n",
      "12    0.697349    0.636838    0.657559    0.680773  \n",
      "13    0.900000    0.824500    0.849900    0.863900  \n",
      "14    0.845608    0.817748    0.826511    0.838749  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_svm_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_svm_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_svm_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_svm_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_svm_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_svm_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_svm_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_svm_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_svm_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_svm_7)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 23:15:14,392]\u001b[0m Trial 400 finished with value: 0.4471590340776109 and parameters: {'C': 0.5, 'gamma': 0.125}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:16:04,454]\u001b[0m Trial 401 finished with value: 0.7843814229997015 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:17:18,162]\u001b[0m Trial 402 finished with value: 0.8290314720154228 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 301 with value: 0.8337448573549443.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:18:28,849]\u001b[0m Trial 403 finished with value: 0.8342080131693039 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 403 with value: 0.8342080131693039.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:19:39,369]\u001b[0m Trial 404 finished with value: 0.7164079979469611 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 403 with value: 0.8342080131693039.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:20:52,766]\u001b[0m Trial 405 finished with value: 0.34294123656544195 and parameters: {'C': 0.0625, 'gamma': 6.103515625e-05}. Best is trial 403 with value: 0.8342080131693039.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:22:03,439]\u001b[0m Trial 406 finished with value: 0.8342080131693039 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 403 with value: 0.8342080131693039.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:23:14,141]\u001b[0m Trial 407 finished with value: 0.8342080131693039 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 403 with value: 0.8342080131693039.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:24:24,846]\u001b[0m Trial 408 finished with value: 0.8342080131693039 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 403 with value: 0.8342080131693039.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:25:19,032]\u001b[0m Trial 409 finished with value: 0.7734558078912328 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 403 with value: 0.8342080131693039.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:26:30,963]\u001b[0m Trial 410 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:27:45,697]\u001b[0m Trial 411 finished with value: 0.3685780017055635 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:28:57,645]\u001b[0m Trial 412 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:30:12,040]\u001b[0m Trial 413 finished with value: 0.37887079647818406 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:31:24,024]\u001b[0m Trial 414 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:32:35,959]\u001b[0m Trial 415 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:33:47,896]\u001b[0m Trial 416 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:35:02,200]\u001b[0m Trial 417 finished with value: 0.3982582136253464 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:36:14,124]\u001b[0m Trial 418 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:37:26,302]\u001b[0m Trial 419 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:38:41,277]\u001b[0m Trial 420 finished with value: 0.41572876941881914 and parameters: {'C': 1.0, 'gamma': 0.5}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:39:53,210]\u001b[0m Trial 421 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:41:05,120]\u001b[0m Trial 422 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:42:17,026]\u001b[0m Trial 423 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:43:16,622]\u001b[0m Trial 424 finished with value: 0.8186234234760613 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:43:58,936]\u001b[0m Trial 425 finished with value: 0.795702047396053 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:45:12,450]\u001b[0m Trial 426 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:46:25,723]\u001b[0m Trial 427 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:47:43,549]\u001b[0m Trial 428 finished with value: 0.3679069342350191 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:48:56,756]\u001b[0m Trial 429 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:49:43,752]\u001b[0m Trial 430 finished with value: 0.7859442585428253 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:50:56,825]\u001b[0m Trial 431 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:52:09,217]\u001b[0m Trial 432 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:53:21,899]\u001b[0m Trial 433 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:54:27,077]\u001b[0m Trial 434 finished with value: 0.8302039463301017 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:55:40,528]\u001b[0m Trial 435 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:56:54,161]\u001b[0m Trial 436 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:58:07,459]\u001b[0m Trial 437 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 23:59:20,260]\u001b[0m Trial 438 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:00:33,507]\u001b[0m Trial 439 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:01:21,448]\u001b[0m Trial 440 finished with value: 0.8038314423485465 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:02:16,072]\u001b[0m Trial 441 finished with value: 0.7732397583648118 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:03:28,102]\u001b[0m Trial 442 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:04:44,053]\u001b[0m Trial 443 finished with value: 0.5384629662373033 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-18 00:05:57,701]\u001b[0m Trial 444 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:06:42,381]\u001b[0m Trial 445 finished with value: 0.783917891149222 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:07:56,042]\u001b[0m Trial 446 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:09:09,685]\u001b[0m Trial 447 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:10:25,133]\u001b[0m Trial 448 finished with value: 0.8287495942964356 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:11:38,501]\u001b[0m Trial 449 finished with value: 0.8347464210531648 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8347\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  401.000000  402.000000  409.000000  418.000000   \n",
      "1                    TN  357.000000  358.000000  349.000000  347.000000   \n",
      "2                    FP   77.000000   85.000000   75.000000   82.000000   \n",
      "3                    FN   64.000000   54.000000   66.000000   52.000000   \n",
      "4              Accuracy    0.843159    0.845384    0.843159    0.850945   \n",
      "5             Precision    0.838912    0.825462    0.845041    0.836000   \n",
      "6           Sensitivity    0.862366    0.881579    0.861053    0.889362   \n",
      "7           Specificity    0.822600    0.808100    0.823100    0.808900   \n",
      "8              F1 score    0.850477    0.852598    0.852972    0.861856   \n",
      "9   F1 score (weighted)    0.843048    0.845122    0.843054    0.850550   \n",
      "10     F1 score (macro)    0.842782    0.845012    0.842457    0.850010   \n",
      "11    Balanced Accuracy    0.842473    0.844853    0.842083    0.849110   \n",
      "12                  MCC    0.685919    0.692046    0.685085    0.701937   \n",
      "13                  NPV    0.848000    0.868900    0.841000    0.869700   \n",
      "14              ROC_AUC    0.842473    0.844853    0.842083    0.849110   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   409.000000  393.000000  411.000000  406.000000  420.000000  \n",
      "1   351.000000  343.000000  334.000000  349.000000  351.000000  \n",
      "2   100.000000   90.000000   95.000000   89.000000   63.000000  \n",
      "3    39.000000   73.000000   59.000000   55.000000   65.000000  \n",
      "4     0.845384    0.818687    0.828699    0.839822    0.857620  \n",
      "5     0.803536    0.813665    0.812253    0.820202    0.869565  \n",
      "6     0.912946    0.843348    0.874468    0.880694    0.865979  \n",
      "7     0.778300    0.792100    0.778600    0.796800    0.847800  \n",
      "8     0.854754    0.828240    0.842213    0.849372    0.867769  \n",
      "9     0.844704    0.818496    0.828107    0.839436    0.857644  \n",
      "10    0.844738    0.818125    0.827433    0.839176    0.856776  \n",
      "11    0.845608    0.817748    0.826511    0.838749    0.856903  \n",
      "12    0.697349    0.636838    0.657559    0.680773    0.713560  \n",
      "13    0.900000    0.824500    0.849900    0.863900    0.843800  \n",
      "14    0.845608    0.817748    0.826511    0.838749    0.856903  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_svm_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_svm_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_svm_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_svm_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_svm_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_svm_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_svm_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_svm_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_svm_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_svm_8)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-18 00:13:04,087]\u001b[0m Trial 450 finished with value: 0.7819691875977799 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:14:16,280]\u001b[0m Trial 451 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:15:06,857]\u001b[0m Trial 452 finished with value: 0.787202171234717 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:16:19,066]\u001b[0m Trial 453 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:17:31,350]\u001b[0m Trial 454 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:18:13,894]\u001b[0m Trial 455 finished with value: 0.7931476003823907 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:19:26,080]\u001b[0m Trial 456 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:20:38,264]\u001b[0m Trial 457 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:21:21,227]\u001b[0m Trial 458 finished with value: 0.7869531872187704 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:22:37,774]\u001b[0m Trial 459 finished with value: 0.37114983536046403 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:23:49,919]\u001b[0m Trial 460 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:25:02,094]\u001b[0m Trial 461 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:26:18,283]\u001b[0m Trial 462 finished with value: 0.38157398122414726 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:27:30,456]\u001b[0m Trial 463 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:28:42,526]\u001b[0m Trial 464 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:29:54,601]\u001b[0m Trial 465 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:31:10,636]\u001b[0m Trial 466 finished with value: 0.39377325951764697 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:32:22,812]\u001b[0m Trial 467 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:33:38,898]\u001b[0m Trial 468 finished with value: 0.42105345363118996 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:34:51,049]\u001b[0m Trial 469 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:36:03,241]\u001b[0m Trial 470 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:37:15,232]\u001b[0m Trial 471 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:37:56,842]\u001b[0m Trial 472 finished with value: 0.788476572863811 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:38:55,128]\u001b[0m Trial 473 finished with value: 0.8073095273862527 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:40:05,620]\u001b[0m Trial 474 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:41:16,139]\u001b[0m Trial 475 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:42:26,624]\u001b[0m Trial 476 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:43:37,121]\u001b[0m Trial 477 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:44:54,490]\u001b[0m Trial 478 finished with value: 0.37114983536046403 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:46:04,974]\u001b[0m Trial 479 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:46:51,047]\u001b[0m Trial 480 finished with value: 0.7903858644053511 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:47:54,198]\u001b[0m Trial 481 finished with value: 0.8147192698700781 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:49:08,552]\u001b[0m Trial 482 finished with value: 0.5394538758955338 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:50:01,943]\u001b[0m Trial 483 finished with value: 0.7762942431695625 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:50:47,996]\u001b[0m Trial 484 finished with value: 0.7926496629470126 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:51:58,467]\u001b[0m Trial 485 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:53:08,964]\u001b[0m Trial 486 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:54:19,450]\u001b[0m Trial 487 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:55:29,917]\u001b[0m Trial 488 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:56:40,398]\u001b[0m Trial 489 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:57:50,757]\u001b[0m Trial 490 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:59:03,411]\u001b[0m Trial 491 finished with value: 0.8241763819856155 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 00:59:46,158]\u001b[0m Trial 492 finished with value: 0.7906287980751726 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 01:00:55,654]\u001b[0m Trial 493 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-18 01:01:36,679]\u001b[0m Trial 494 finished with value: 0.7931476003823907 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 01:02:51,152]\u001b[0m Trial 495 finished with value: 0.7819691875977799 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 01:04:01,648]\u001b[0m Trial 496 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 01:05:12,096]\u001b[0m Trial 497 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 01:06:22,530]\u001b[0m Trial 498 finished with value: 0.8257935078530696 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n",
      "\u001b[32m[I 2023-02-18 01:07:11,877]\u001b[0m Trial 499 finished with value: 0.787202171234717 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 410 with value: 0.8347464210531648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8347\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  401.000000  402.000000  409.000000  418.000000   \n",
      "1                    TN  357.000000  358.000000  349.000000  347.000000   \n",
      "2                    FP   77.000000   85.000000   75.000000   82.000000   \n",
      "3                    FN   64.000000   54.000000   66.000000   52.000000   \n",
      "4              Accuracy    0.843159    0.845384    0.843159    0.850945   \n",
      "5             Precision    0.838912    0.825462    0.845041    0.836000   \n",
      "6           Sensitivity    0.862366    0.881579    0.861053    0.889362   \n",
      "7           Specificity    0.822600    0.808100    0.823100    0.808900   \n",
      "8              F1 score    0.850477    0.852598    0.852972    0.861856   \n",
      "9   F1 score (weighted)    0.843048    0.845122    0.843054    0.850550   \n",
      "10     F1 score (macro)    0.842782    0.845012    0.842457    0.850010   \n",
      "11    Balanced Accuracy    0.842473    0.844853    0.842083    0.849110   \n",
      "12                  MCC    0.685919    0.692046    0.685085    0.701937   \n",
      "13                  NPV    0.848000    0.868900    0.841000    0.869700   \n",
      "14              ROC_AUC    0.842473    0.844853    0.842083    0.849110   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   409.000000  393.000000  411.000000  406.000000  420.000000  427.000000  \n",
      "1   351.000000  343.000000  334.000000  349.000000  351.000000  330.000000  \n",
      "2   100.000000   90.000000   95.000000   89.000000   63.000000   75.000000  \n",
      "3    39.000000   73.000000   59.000000   55.000000   65.000000   67.000000  \n",
      "4     0.845384    0.818687    0.828699    0.839822    0.857620    0.842047  \n",
      "5     0.803536    0.813665    0.812253    0.820202    0.869565    0.850598  \n",
      "6     0.912946    0.843348    0.874468    0.880694    0.865979    0.864372  \n",
      "7     0.778300    0.792100    0.778600    0.796800    0.847800    0.814800  \n",
      "8     0.854754    0.828240    0.842213    0.849372    0.867769    0.857430  \n",
      "9     0.844704    0.818496    0.828107    0.839436    0.857644    0.841893  \n",
      "10    0.844738    0.818125    0.827433    0.839176    0.856776    0.840186  \n",
      "11    0.845608    0.817748    0.826511    0.838749    0.856903    0.839594  \n",
      "12    0.697349    0.636838    0.657559    0.680773    0.713560    0.680508  \n",
      "13    0.900000    0.824500    0.849900    0.863900    0.843800    0.831200  \n",
      "14    0.845608    0.817748    0.826511    0.838749    0.856903    0.839594  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_svm_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_svm_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_svm_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_svm_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_svm_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_svm_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_svm_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_svm_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_svm_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_svm_9)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>205.200000</td>\n",
       "      <td>6.844300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>172.800000</td>\n",
       "      <td>8.904431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>40.200000</td>\n",
       "      <td>10.982815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>5.696002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.841491</td>\n",
       "      <td>0.017411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.837477</td>\n",
       "      <td>0.037982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.869071</td>\n",
       "      <td>0.021360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.812100</td>\n",
       "      <td>0.046704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.852175</td>\n",
       "      <td>0.015481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.841171</td>\n",
       "      <td>0.017763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.840502</td>\n",
       "      <td>0.017744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.840576</td>\n",
       "      <td>0.017630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.683534</td>\n",
       "      <td>0.032719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.848460</td>\n",
       "      <td>0.021741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.840576</td>\n",
       "      <td>0.017630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       205.200000     6.844300\n",
       "1                    TN       172.800000     8.904431\n",
       "2                    FP        40.200000    10.982815\n",
       "3                    FN        31.000000     5.696002\n",
       "4              Accuracy         0.841491     0.017411\n",
       "5             Precision         0.837477     0.037982\n",
       "6           Sensitivity         0.869071     0.021360\n",
       "7           Specificity         0.812100     0.046704\n",
       "8              F1 score         0.852175     0.015481\n",
       "9   F1 score (weighted)         0.841171     0.017763\n",
       "10     F1 score (macro)         0.840502     0.017744\n",
       "11    Balanced Accuracy         0.840576     0.017630\n",
       "12                  MCC         0.683534     0.032719\n",
       "13                  NPV         0.848460     0.021741\n",
       "14              ROC_AUC         0.840576     0.017630"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>409.600000</td>\n",
       "      <td>10.024415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>346.900000</td>\n",
       "      <td>9.036346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>83.100000</td>\n",
       "      <td>10.989389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>59.400000</td>\n",
       "      <td>9.766155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.843159</td>\n",
       "      <td>0.845384</td>\n",
       "      <td>0.843159</td>\n",
       "      <td>0.850945</td>\n",
       "      <td>0.845384</td>\n",
       "      <td>0.818687</td>\n",
       "      <td>0.828699</td>\n",
       "      <td>0.839822</td>\n",
       "      <td>0.857620</td>\n",
       "      <td>0.842047</td>\n",
       "      <td>0.841491</td>\n",
       "      <td>0.010914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.838912</td>\n",
       "      <td>0.825462</td>\n",
       "      <td>0.845041</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.803536</td>\n",
       "      <td>0.813665</td>\n",
       "      <td>0.812253</td>\n",
       "      <td>0.820202</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.850598</td>\n",
       "      <td>0.831523</td>\n",
       "      <td>0.020295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.862366</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.861053</td>\n",
       "      <td>0.889362</td>\n",
       "      <td>0.912946</td>\n",
       "      <td>0.843348</td>\n",
       "      <td>0.874468</td>\n",
       "      <td>0.880694</td>\n",
       "      <td>0.865979</td>\n",
       "      <td>0.864372</td>\n",
       "      <td>0.873617</td>\n",
       "      <td>0.019013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.822600</td>\n",
       "      <td>0.808100</td>\n",
       "      <td>0.823100</td>\n",
       "      <td>0.808900</td>\n",
       "      <td>0.778300</td>\n",
       "      <td>0.792100</td>\n",
       "      <td>0.778600</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>0.847800</td>\n",
       "      <td>0.814800</td>\n",
       "      <td>0.807110</td>\n",
       "      <td>0.021574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.850477</td>\n",
       "      <td>0.852598</td>\n",
       "      <td>0.852972</td>\n",
       "      <td>0.861856</td>\n",
       "      <td>0.854754</td>\n",
       "      <td>0.828240</td>\n",
       "      <td>0.842213</td>\n",
       "      <td>0.849372</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>0.857430</td>\n",
       "      <td>0.851768</td>\n",
       "      <td>0.010817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.843048</td>\n",
       "      <td>0.845122</td>\n",
       "      <td>0.843054</td>\n",
       "      <td>0.850550</td>\n",
       "      <td>0.844704</td>\n",
       "      <td>0.818496</td>\n",
       "      <td>0.828107</td>\n",
       "      <td>0.839436</td>\n",
       "      <td>0.857644</td>\n",
       "      <td>0.841893</td>\n",
       "      <td>0.841205</td>\n",
       "      <td>0.010969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.842782</td>\n",
       "      <td>0.845012</td>\n",
       "      <td>0.842457</td>\n",
       "      <td>0.850010</td>\n",
       "      <td>0.844738</td>\n",
       "      <td>0.818125</td>\n",
       "      <td>0.827433</td>\n",
       "      <td>0.839176</td>\n",
       "      <td>0.856776</td>\n",
       "      <td>0.840186</td>\n",
       "      <td>0.840669</td>\n",
       "      <td>0.010932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.842473</td>\n",
       "      <td>0.844853</td>\n",
       "      <td>0.842083</td>\n",
       "      <td>0.849110</td>\n",
       "      <td>0.845608</td>\n",
       "      <td>0.817748</td>\n",
       "      <td>0.826511</td>\n",
       "      <td>0.838749</td>\n",
       "      <td>0.856903</td>\n",
       "      <td>0.839594</td>\n",
       "      <td>0.840363</td>\n",
       "      <td>0.011114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.685919</td>\n",
       "      <td>0.692046</td>\n",
       "      <td>0.685085</td>\n",
       "      <td>0.701937</td>\n",
       "      <td>0.697349</td>\n",
       "      <td>0.636838</td>\n",
       "      <td>0.657559</td>\n",
       "      <td>0.680773</td>\n",
       "      <td>0.713560</td>\n",
       "      <td>0.680508</td>\n",
       "      <td>0.683157</td>\n",
       "      <td>0.022066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.868900</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.869700</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.824500</td>\n",
       "      <td>0.849900</td>\n",
       "      <td>0.863900</td>\n",
       "      <td>0.843800</td>\n",
       "      <td>0.831200</td>\n",
       "      <td>0.854090</td>\n",
       "      <td>0.022107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.842473</td>\n",
       "      <td>0.844853</td>\n",
       "      <td>0.842083</td>\n",
       "      <td>0.849110</td>\n",
       "      <td>0.845608</td>\n",
       "      <td>0.817748</td>\n",
       "      <td>0.826511</td>\n",
       "      <td>0.838749</td>\n",
       "      <td>0.856903</td>\n",
       "      <td>0.839594</td>\n",
       "      <td>0.840363</td>\n",
       "      <td>0.011114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  401.000000  402.000000  409.000000  418.000000   \n",
       "1                    TN  357.000000  358.000000  349.000000  347.000000   \n",
       "2                    FP   77.000000   85.000000   75.000000   82.000000   \n",
       "3                    FN   64.000000   54.000000   66.000000   52.000000   \n",
       "4              Accuracy    0.843159    0.845384    0.843159    0.850945   \n",
       "5             Precision    0.838912    0.825462    0.845041    0.836000   \n",
       "6           Sensitivity    0.862366    0.881579    0.861053    0.889362   \n",
       "7           Specificity    0.822600    0.808100    0.823100    0.808900   \n",
       "8              F1 score    0.850477    0.852598    0.852972    0.861856   \n",
       "9   F1 score (weighted)    0.843048    0.845122    0.843054    0.850550   \n",
       "10     F1 score (macro)    0.842782    0.845012    0.842457    0.850010   \n",
       "11    Balanced Accuracy    0.842473    0.844853    0.842083    0.849110   \n",
       "12                  MCC    0.685919    0.692046    0.685085    0.701937   \n",
       "13                  NPV    0.848000    0.868900    0.841000    0.869700   \n",
       "14              ROC_AUC    0.842473    0.844853    0.842083    0.849110   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   409.000000  393.000000  411.000000  406.000000  420.000000  427.000000   \n",
       "1   351.000000  343.000000  334.000000  349.000000  351.000000  330.000000   \n",
       "2   100.000000   90.000000   95.000000   89.000000   63.000000   75.000000   \n",
       "3    39.000000   73.000000   59.000000   55.000000   65.000000   67.000000   \n",
       "4     0.845384    0.818687    0.828699    0.839822    0.857620    0.842047   \n",
       "5     0.803536    0.813665    0.812253    0.820202    0.869565    0.850598   \n",
       "6     0.912946    0.843348    0.874468    0.880694    0.865979    0.864372   \n",
       "7     0.778300    0.792100    0.778600    0.796800    0.847800    0.814800   \n",
       "8     0.854754    0.828240    0.842213    0.849372    0.867769    0.857430   \n",
       "9     0.844704    0.818496    0.828107    0.839436    0.857644    0.841893   \n",
       "10    0.844738    0.818125    0.827433    0.839176    0.856776    0.840186   \n",
       "11    0.845608    0.817748    0.826511    0.838749    0.856903    0.839594   \n",
       "12    0.697349    0.636838    0.657559    0.680773    0.713560    0.680508   \n",
       "13    0.900000    0.824500    0.849900    0.863900    0.843800    0.831200   \n",
       "14    0.845608    0.817748    0.826511    0.838749    0.856903    0.839594   \n",
       "\n",
       "           ave        std  \n",
       "0   409.600000  10.024415  \n",
       "1   346.900000   9.036346  \n",
       "2    83.100000  10.989389  \n",
       "3    59.400000   9.766155  \n",
       "4     0.841491   0.010914  \n",
       "5     0.831523   0.020295  \n",
       "6     0.873617   0.019013  \n",
       "7     0.807110   0.021574  \n",
       "8     0.851768   0.010817  \n",
       "9     0.841205   0.010969  \n",
       "10    0.840669   0.010932  \n",
       "11    0.840363   0.011114  \n",
       "12    0.683157   0.022066  \n",
       "13    0.854090   0.022107  \n",
       "14    0.840363   0.011114  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_svm0</th>\n",
       "      <th>y_pred_svm1</th>\n",
       "      <th>y_pred_svm2</th>\n",
       "      <th>y_pred_svm3</th>\n",
       "      <th>y_pred_svm4</th>\n",
       "      <th>y_pred_svm_ave</th>\n",
       "      <th>y_pred_svm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>4487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>4488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>4489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>4490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>4491</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4492 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_test_idx0  y_test0  y_pred_svm0  y_pred_svm1  y_pred_svm2  \\\n",
       "0               0      0.0          0.0          0.0          0.0   \n",
       "1               1      1.0          1.0          1.0          1.0   \n",
       "2               2      1.0          1.0          1.0          1.0   \n",
       "3               3      1.0          1.0          1.0          1.0   \n",
       "4               4      1.0          0.0          0.0          0.0   \n",
       "...           ...      ...          ...          ...          ...   \n",
       "4487         4487      1.0          1.0          1.0          1.0   \n",
       "4488         4488      1.0          1.0          1.0          1.0   \n",
       "4489         4489      0.0          0.0          0.0          0.0   \n",
       "4490         4490      0.0          0.0          0.0          0.0   \n",
       "4491         4491      1.0          1.0          1.0          1.0   \n",
       "\n",
       "      y_pred_svm3  y_pred_svm4  y_pred_svm_ave  y_pred_svm_std  \n",
       "0             0.0          0.0             0.0             0.0  \n",
       "1             1.0          1.0             1.0             0.0  \n",
       "2             1.0          1.0             1.0             0.0  \n",
       "3             1.0          1.0             1.0             0.0  \n",
       "4             0.0          0.0             0.0             0.0  \n",
       "...           ...          ...             ...             ...  \n",
       "4487          1.0          1.0             1.0             0.0  \n",
       "4488          1.0          1.0             1.0             0.0  \n",
       "4489          0.0          0.0             0.0             0.0  \n",
       "4490          0.0          0.0             0.0             0.0  \n",
       "4491          1.0          1.0             1.0             0.0  \n",
       "\n",
       "[4492 rows x 9 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_svm)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_svm))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_svm))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_svm))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_svm))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_svm, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_svm, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_svm))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_svm))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_svm))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "svm_5preds = pd.concat([svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "svm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6394fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_svm.to_csv('mat_met_svm_opt.csv')\n",
    "svm_5preds.to_csv('svm_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model f1_score 0.8252 with a standard deviation of 0.0208\n",
      "SVM optimized model f1_score 0.8429 with a standard deviation of 0.0165\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVC \n",
    "svm_baseline_CVscore = cross_val_score(svm_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"SVM baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_svm_clf.joblib']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_clf, \"./svm_clf.joblib\")\n",
    "#joblib.dump(optimized_svm, \"./optimized_svm.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"./optimizedCV_svm_clf.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
