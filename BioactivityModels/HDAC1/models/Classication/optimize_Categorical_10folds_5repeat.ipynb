{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "levels_up = 2\n",
    "HDAC1= HERE.parents[levels_up-1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3335306</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[13007771, 3877094, 5834832, 1003699, 8143647,...</td>\n",
       "      <td>6.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2047614</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[18646422, 3821889, 7998790, 4833708, 16951091...</td>\n",
       "      <td>5.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3983635</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2123993, 1354952, 1098538, 6502576, 5044047, ...</td>\n",
       "      <td>5.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3770484</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[59160858, 22495760, 28570269, 26472156, 77802...</td>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3689795</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3671935, 4046075, 6095547, 7378093, 2598690, ...</td>\n",
       "      <td>6.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL3335306  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL2047614  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3983635  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3770484  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3689795  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "2  [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, ...   \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC1  \n",
       "0  [13007771, 3877094, 5834832, 1003699, 8143647,...           6.71  \n",
       "1  [18646422, 3821889, 7998790, 4833708, 16951091...           5.53  \n",
       "2  [2123993, 1354952, 1098538, 6502576, 5044047, ...           5.54  \n",
       "3  [59160858, 22495760, 28570269, 26472156, 77802...           4.32  \n",
       "4  [3671935, 4046075, 6095547, 7378093, 2598690, ...           6.85  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1/\"HDAC1_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type</th>\n",
       "      <th>Standard_Value_HDAC1</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL327146</td>\n",
       "      <td>O=C(CCCCCC(C(=O)Nc1ccc2ncccc2c1)C(=O)Nc1ccc2nc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL116620</td>\n",
       "      <td>O=C(/C=C/c1cccc(C(C(=O)Nc2ccccc2)C(=O)Nc2ccccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2093007</td>\n",
       "      <td>C/C=C1\\NC(=O)[C@@H](CSC)NC(=O)[C@@H](C(C)C)CC(...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL316457</td>\n",
       "      <td>CC(C)c1cc(C(C)C)c(S(=O)(=O)Nc2ccc(/C=C/C(=O)NO...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>600.0</td>\n",
       "      <td>6.22</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                             smiles  type  \\\n",
       "0       CHEMBL327146  O=C(CCCCCC(C(=O)Nc1ccc2ncccc2c1)C(=O)Nc1ccc2nc...  IC50   \n",
       "1       CHEMBL116620  O=C(/C=C/c1cccc(C(C(=O)Nc2ccccc2)C(=O)Nc2ccccc...  IC50   \n",
       "2      CHEMBL2093007  C/C=C1\\NC(=O)[C@@H](CSC)NC(=O)[C@@H](C(C)C)CC(...  IC50   \n",
       "3       CHEMBL316457  CC(C)c1cc(C(C)C)c(S(=O)(=O)Nc2ccc(/C=C/C(=O)NO...  IC50   \n",
       "\n",
       "   Standard_Value_HDAC1  pChEMBL_HDAC1          label  \n",
       "0                   1.0           9.00  Single points  \n",
       "1                   1.0           9.00  Single points  \n",
       "2                6300.0           5.20  Single points  \n",
       "3                 600.0           6.22  Single points  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1/\"HDAC1_dataset.csv\", )\n",
    "df_labeled.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3335306</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[13007771, 3877094, 5834832, 1003699, 8143647,...</td>\n",
       "      <td>6.71</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2047614</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[18646422, 3821889, 7998790, 4833708, 16951091...</td>\n",
       "      <td>5.53</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3983635</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2123993, 1354952, 1098538, 6502576, 5044047, ...</td>\n",
       "      <td>5.54</td>\n",
       "      <td>Semi-selective</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3770484</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[59160858, 22495760, 28570269, 26472156, 77802...</td>\n",
       "      <td>4.32</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL3335306  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL2047614  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3983635  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3770484  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "2  [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, ...   \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC1  \\\n",
       "0  [13007771, 3877094, 5834832, 1003699, 8143647,...           6.71   \n",
       "1  [18646422, 3821889, 7998790, 4833708, 16951091...           5.53   \n",
       "2  [2123993, 1354952, 1098538, 6502576, 5044047, ...           5.54   \n",
       "3  [59160858, 22495760, 28570269, 26472156, 77802...           4.32   \n",
       "\n",
       "            label  Class  \n",
       "0   Single points    0.0  \n",
       "1   Single points    0.0  \n",
       "2  Semi-selective    5.0  \n",
       "3      Non-binder    4.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'HDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'HDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'HDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"activity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.pChEMBL_HDAC1 >= 6.6].index, \"activity\"] = 1.0\n",
    "\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y =  df[\"activity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['activity'])).astype(float)\n",
    "    \n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['activity'])).astype(float)\n",
    "    \n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       208.600000     8.262364\n",
      "1                    TN       176.200000     9.704524\n",
      "2                    FP        42.600000     7.662318\n",
      "3                    FN        31.800000     2.699794\n",
      "4              Accuracy         0.837980     0.013767\n",
      "5             Precision         0.830775     0.026964\n",
      "6           Sensitivity         0.867812     0.008454\n",
      "7           Specificity         0.805550     0.031093\n",
      "8              F1 score         0.848600     0.012695\n",
      "9   F1 score (weighted)         0.837700     0.014013\n",
      "10     F1 score (macro)         0.836962     0.013889\n",
      "11    Balanced Accuracy         0.836676     0.013675\n",
      "12                  MCC         0.675494     0.025728\n",
      "13                  NPV         0.846860     0.014216\n",
      "14              ROC_AUC         0.836676     0.013675\n",
      "CPU times: user 3min 8s, sys: 119 ms, total: 3min 9s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_clf =  RandomForestClassifier(random_state=1121218, max_features = None, n_jobs=16,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_clf.fit(x_train, y_train)\n",
    "        y_pred = rf_clf.predict(x_test)  \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggestegorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggestegorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestClassifier(**param_grid, n_jobs=16, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred,  average=\"macro\")\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggestegorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggestegorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestClassifier(**param_grid, n_jobs=16, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 11:45:05,154] A new study created in memory with name: RFclassifier\n",
      "[I 2023-12-01 11:47:06,038] Trial 0 finished with value: 0.8420106351217139 and parameters: {'n_estimators': 609}. Best is trial 0 with value: 0.8420106351217139.\n",
      "[I 2023-12-01 11:48:47,211] Trial 1 finished with value: 0.8414641873878491 and parameters: {'n_estimators': 500}. Best is trial 0 with value: 0.8420106351217139.\n",
      "[I 2023-12-01 11:50:59,848] Trial 2 finished with value: 0.8419923215642328 and parameters: {'n_estimators': 653}. Best is trial 0 with value: 0.8420106351217139.\n",
      "[I 2023-12-01 11:53:48,837] Trial 3 finished with value: 0.8428664748941438 and parameters: {'n_estimators': 853}. Best is trial 3 with value: 0.8428664748941438.\n",
      "[I 2023-12-01 11:54:15,073] Trial 4 finished with value: 0.8392979515021353 and parameters: {'n_estimators': 125}. Best is trial 3 with value: 0.8428664748941438.\n",
      "[I 2023-12-01 11:55:02,445] Trial 5 finished with value: 0.8384183263301915 and parameters: {'n_estimators': 235}. Best is trial 3 with value: 0.8428664748941438.\n",
      "[I 2023-12-01 11:57:33,299] Trial 6 finished with value: 0.8431127713402405 and parameters: {'n_estimators': 740}. Best is trial 6 with value: 0.8431127713402405.\n",
      "[I 2023-12-01 11:57:58,119] Trial 7 finished with value: 0.838494802878681 and parameters: {'n_estimators': 119}. Best is trial 6 with value: 0.8431127713402405.\n",
      "[I 2023-12-01 12:01:04,245] Trial 8 finished with value: 0.8425639079935452 and parameters: {'n_estimators': 937}. Best is trial 6 with value: 0.8431127713402405.\n",
      "[I 2023-12-01 12:03:08,939] Trial 9 finished with value: 0.841755885247278 and parameters: {'n_estimators': 616}. Best is trial 6 with value: 0.8431127713402405.\n",
      "[I 2023-12-01 12:04:31,006] Trial 10 finished with value: 0.8417406663020817 and parameters: {'n_estimators': 410}. Best is trial 6 with value: 0.8431127713402405.\n",
      "[I 2023-12-01 12:07:21,539] Trial 11 finished with value: 0.843145391220825 and parameters: {'n_estimators': 845}. Best is trial 11 with value: 0.843145391220825.\n",
      "[I 2023-12-01 12:10:01,785] Trial 12 finished with value: 0.8434172049334091 and parameters: {'n_estimators': 794}. Best is trial 12 with value: 0.8434172049334091.\n",
      "[I 2023-12-01 12:12:44,959] Trial 13 finished with value: 0.8428783336774994 and parameters: {'n_estimators': 807}. Best is trial 12 with value: 0.8434172049334091.\n",
      "[I 2023-12-01 12:15:56,183] Trial 14 finished with value: 0.8425458655934441 and parameters: {'n_estimators': 948}. Best is trial 12 with value: 0.8434172049334091.\n",
      "[I 2023-12-01 12:18:31,102] Trial 15 finished with value: 0.8425876224537439 and parameters: {'n_estimators': 762}. Best is trial 12 with value: 0.8434172049334091.\n",
      "[I 2023-12-01 12:21:47,440] Trial 16 finished with value: 0.8428301395741373 and parameters: {'n_estimators': 981}. Best is trial 12 with value: 0.8434172049334091.\n",
      "[I 2023-12-01 12:23:20,734] Trial 17 finished with value: 0.8434179678150471 and parameters: {'n_estimators': 460}. Best is trial 17 with value: 0.8434179678150471.\n",
      "[I 2023-12-01 12:24:41,401] Trial 18 finished with value: 0.8420003706268204 and parameters: {'n_estimators': 397}. Best is trial 17 with value: 0.8434179678150471.\n",
      "[I 2023-12-01 12:26:17,859] Trial 19 finished with value: 0.8414268160208908 and parameters: {'n_estimators': 485}. Best is trial 17 with value: 0.8434179678150471.\n",
      "[I 2023-12-01 12:27:20,285] Trial 20 finished with value: 0.8409192419888735 and parameters: {'n_estimators': 307}. Best is trial 17 with value: 0.8434179678150471.\n",
      "[I 2023-12-01 12:29:35,924] Trial 21 finished with value: 0.8425377818672544 and parameters: {'n_estimators': 686}. Best is trial 17 with value: 0.8434179678150471.\n",
      "[I 2023-12-01 12:32:32,513] Trial 22 finished with value: 0.8414774321164614 and parameters: {'n_estimators': 877}. Best is trial 17 with value: 0.8434179678150471.\n",
      "[I 2023-12-01 12:34:24,343] Trial 23 finished with value: 0.8428338491183613 and parameters: {'n_estimators': 554}. Best is trial 17 with value: 0.8434179678150471.\n",
      "[I 2023-12-01 12:36:54,064] Trial 24 finished with value: 0.8431359276362128 and parameters: {'n_estimators': 752}. Best is trial 17 with value: 0.8434179678150471.\n",
      "[I 2023-12-01 12:39:45,514] Trial 25 finished with value: 0.8431493447886984 and parameters: {'n_estimators': 852}. Best is trial 17 with value: 0.8434179678150471.\n",
      "[I 2023-12-01 12:40:56,386] Trial 26 finished with value: 0.8417399320595769 and parameters: {'n_estimators': 351}. Best is trial 17 with value: 0.8434179678150471.\n",
      "[I 2023-12-01 12:42:43,997] Trial 27 finished with value: 0.8442177239059274 and parameters: {'n_estimators': 528}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 12:44:21,092] Trial 28 finished with value: 0.8414268160208908 and parameters: {'n_estimators': 485}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 12:46:13,578] Trial 29 finished with value: 0.8430873114566297 and parameters: {'n_estimators': 553}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 12:47:06,119] Trial 30 finished with value: 0.8400793266595992 and parameters: {'n_estimators': 261}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 12:49:23,969] Trial 31 finished with value: 0.8419910192398318 and parameters: {'n_estimators': 677}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 12:50:51,824] Trial 32 finished with value: 0.8417373601321902 and parameters: {'n_estimators': 439}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 12:52:51,978] Trial 33 finished with value: 0.8425658305436844 and parameters: {'n_estimators': 591}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 12:54:36,111] Trial 34 finished with value: 0.8422626869587667 and parameters: {'n_estimators': 511}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 12:56:39,167] Trial 35 finished with value: 0.8425570435881052 and parameters: {'n_estimators': 604}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 12:59:38,592] Trial 36 finished with value: 0.8425998964366203 and parameters: {'n_estimators': 888}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:02:03,828] Trial 37 finished with value: 0.8425484449610876 and parameters: {'n_estimators': 707}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:04:46,999] Trial 38 finished with value: 0.8428783336774994 and parameters: {'n_estimators': 812}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:06:58,799] Trial 39 finished with value: 0.8420176752684249 and parameters: {'n_estimators': 639}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:07:38,471] Trial 40 finished with value: 0.8395301665578417 and parameters: {'n_estimators': 194}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:10:43,917] Trial 41 finished with value: 0.8423012627604664 and parameters: {'n_estimators': 912}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:13:30,152] Trial 42 finished with value: 0.843145391220825 and parameters: {'n_estimators': 833}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:16:52,450] Trial 43 finished with value: 0.8428601353913459 and parameters: {'n_estimators': 1000}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:19:32,280] Trial 44 finished with value: 0.8431462314787381 and parameters: {'n_estimators': 803}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:22:10,326] Trial 45 finished with value: 0.8431343350388547 and parameters: {'n_estimators': 778}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:24:32,560] Trial 46 finished with value: 0.8420085803978477 and parameters: {'n_estimators': 718}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:25:59,633] Trial 47 finished with value: 0.8420419889355932 and parameters: {'n_estimators': 434}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:28:36,480] Trial 48 finished with value: 0.8434172049334091 and parameters: {'n_estimators': 792}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:30:18,993] Trial 49 finished with value: 0.8425493133955186 and parameters: {'n_estimators': 512}. Best is trial 27 with value: 0.8442177239059274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFclassifier\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  421.000000\n",
      "1                    TN  344.000000\n",
      "2                    FP   83.000000\n",
      "3                    FN   71.000000\n",
      "4              Accuracy    0.832427\n",
      "5             Precision    0.835317\n",
      "6           Sensitivity    0.855691\n",
      "7           Specificity    0.805600\n",
      "8              F1 score    0.845382\n",
      "9   F1 score (weighted)    0.832242\n",
      "10     F1 score (macro)    0.831242\n",
      "11    Balanced Accuracy    0.830656\n",
      "12                  MCC    0.662771\n",
      "13                  NPV    0.828900\n",
      "14              ROC_AUC    0.830656\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_rf_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_rf_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_rf_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_rf_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_rf_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_rf_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_rf_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_rf_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_rf_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_rf_0)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP), np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                           np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 13:31:46,069] Trial 50 finished with value: 0.8233340606304242 and parameters: {'n_estimators': 367}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:34:25,728] Trial 51 finished with value: 0.8257763779840372 and parameters: {'n_estimators': 802}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:37:24,041] Trial 52 finished with value: 0.8249574167042016 and parameters: {'n_estimators': 857}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:40:33,098] Trial 53 finished with value: 0.8271273791917599 and parameters: {'n_estimators': 948}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:42:50,850] Trial 54 finished with value: 0.8260459837706522 and parameters: {'n_estimators': 650}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:45:29,235] Trial 55 finished with value: 0.8262946248692238 and parameters: {'n_estimators': 784}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:48:39,609] Trial 56 finished with value: 0.8260536737633084 and parameters: {'n_estimators': 902}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:50:35,700] Trial 57 finished with value: 0.8247076840609575 and parameters: {'n_estimators': 581}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:53:05,975] Trial 58 finished with value: 0.8268598304688128 and parameters: {'n_estimators': 724}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:54:38,059] Trial 59 finished with value: 0.8241247893244112 and parameters: {'n_estimators': 459}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 13:57:31,415] Trial 60 finished with value: 0.8260259647861542 and parameters: {'n_estimators': 839}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:00:27,350] Trial 61 finished with value: 0.8260212751947039 and parameters: {'n_estimators': 866}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:02:59,377] Trial 62 finished with value: 0.8268407195207436 and parameters: {'n_estimators': 751}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:05:46,818] Trial 63 finished with value: 0.8263013966150652 and parameters: {'n_estimators': 826}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:08:59,547] Trial 64 finished with value: 0.8266090573152896 and parameters: {'n_estimators': 927}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:11:17,842] Trial 65 finished with value: 0.826309445061189 and parameters: {'n_estimators': 683}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:14:37,129] Trial 66 finished with value: 0.8254653666107277 and parameters: {'n_estimators': 965}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:17:16,506] Trial 67 finished with value: 0.8262946248692238 and parameters: {'n_estimators': 787}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:20:14,079] Trial 68 finished with value: 0.8263039244439717 and parameters: {'n_estimators': 881}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:22:47,707] Trial 69 finished with value: 0.8268407195207436 and parameters: {'n_estimators': 757}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:24:36,376] Trial 70 finished with value: 0.8241778199251961 and parameters: {'n_estimators': 526}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:27:28,177] Trial 71 finished with value: 0.8249574167042016 and parameters: {'n_estimators': 855}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:30:18,602] Trial 72 finished with value: 0.8257813496691776 and parameters: {'n_estimators': 820}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:33:08,493] Trial 73 finished with value: 0.8260522197734032 and parameters: {'n_estimators': 836}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:36:16,534] Trial 74 finished with value: 0.8265997255636378 and parameters: {'n_estimators': 916}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:38:12,350] Trial 75 finished with value: 0.8247216136662991 and parameters: {'n_estimators': 570}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:40:20,696] Trial 76 finished with value: 0.8266185682576761 and parameters: {'n_estimators': 628}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:41:59,007] Trial 77 finished with value: 0.8230542769104741 and parameters: {'n_estimators': 476}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:44:26,745] Trial 78 finished with value: 0.8268360502773398 and parameters: {'n_estimators': 735}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:45:51,245] Trial 79 finished with value: 0.8246686490130848 and parameters: {'n_estimators': 408}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:48:30,689] Trial 80 finished with value: 0.8260347812352966 and parameters: {'n_estimators': 792}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:51:08,890] Trial 81 finished with value: 0.8263108850520184 and parameters: {'n_estimators': 767}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:53:29,063] Trial 82 finished with value: 0.8271249371693452 and parameters: {'n_estimators': 697}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:56:32,009] Trial 83 finished with value: 0.8265775717032373 and parameters: {'n_estimators': 890}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 14:59:18,691] Trial 84 finished with value: 0.8260635265185279 and parameters: {'n_estimators': 814}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:02:12,530] Trial 85 finished with value: 0.8263212664354269 and parameters: {'n_estimators': 842}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:04:25,382] Trial 86 finished with value: 0.8271312758207477 and parameters: {'n_estimators': 662}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:07:01,420] Trial 87 finished with value: 0.8263074749575543 and parameters: {'n_estimators': 746}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:09:58,124] Trial 88 finished with value: 0.8260212751947039 and parameters: {'n_estimators': 865}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:11:50,207] Trial 89 finished with value: 0.8244409241108581 and parameters: {'n_estimators': 533}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:14:34,046] Trial 90 finished with value: 0.8265737977124739 and parameters: {'n_estimators': 809}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:17:13,184] Trial 91 finished with value: 0.8265853641483177 and parameters: {'n_estimators': 774}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:19:52,372] Trial 92 finished with value: 0.8260131231819987 and parameters: {'n_estimators': 780}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:22:36,685] Trial 93 finished with value: 0.825496673176873 and parameters: {'n_estimators': 801}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:24:07,460] Trial 94 finished with value: 0.8244220943630832 and parameters: {'n_estimators': 444}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:26:34,649] Trial 95 finished with value: 0.8262970825544818 and parameters: {'n_estimators': 715}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:29:26,328] Trial 96 finished with value: 0.8260259647861542 and parameters: {'n_estimators': 839}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:30:40,778] Trial 97 finished with value: 0.8236128683111643 and parameters: {'n_estimators': 363}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:33:11,163] Trial 98 finished with value: 0.8265542720847725 and parameters: {'n_estimators': 739}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:36:08,143] Trial 99 finished with value: 0.8263039244439717 and parameters: {'n_estimators': 882}. Best is trial 27 with value: 0.8442177239059274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  421.000000  422.000000\n",
      "1                    TN  344.000000  357.000000\n",
      "2                    FP   83.000000   78.000000\n",
      "3                    FN   71.000000   62.000000\n",
      "4              Accuracy    0.832427    0.847661\n",
      "5             Precision    0.835317    0.844000\n",
      "6           Sensitivity    0.855691    0.871901\n",
      "7           Specificity    0.805600    0.820700\n",
      "8              F1 score    0.845382    0.857724\n",
      "9   F1 score (weighted)    0.832242    0.847472\n",
      "10     F1 score (macro)    0.831242    0.846895\n",
      "11    Balanced Accuracy    0.830656    0.846295\n",
      "12                  MCC    0.662771    0.694307\n",
      "13                  NPV    0.828900    0.852000\n",
      "14              ROC_AUC    0.830656    0.846295\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_rf_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_rf_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_rf_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_rf_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_rf_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_rf_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_rf_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_rf_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_rf_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_rf_1)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 15:36:51,969] Trial 100 finished with value: 0.8308455270885938 and parameters: {'n_estimators': 148}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:39:27,064] Trial 101 finished with value: 0.8296853113662145 and parameters: {'n_estimators': 755}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:42:04,644] Trial 102 finished with value: 0.829985123232581 and parameters: {'n_estimators': 768}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:43:43,542] Trial 103 finished with value: 0.829485747231011 and parameters: {'n_estimators': 491}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:46:30,720] Trial 104 finished with value: 0.8297132943272725 and parameters: {'n_estimators': 819}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:49:09,694] Trial 105 finished with value: 0.829995491879154 and parameters: {'n_estimators': 794}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:52:02,697] Trial 106 finished with value: 0.8297154341553243 and parameters: {'n_estimators': 851}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:54:30,890] Trial 107 finished with value: 0.8297016747910894 and parameters: {'n_estimators': 727}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:56:37,812] Trial 108 finished with value: 0.8300023694526655 and parameters: {'n_estimators': 607}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 15:59:38,287] Trial 109 finished with value: 0.8294380131834057 and parameters: {'n_estimators': 898}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:01:59,131] Trial 110 finished with value: 0.8302251761179346 and parameters: {'n_estimators': 697}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:03:50,570] Trial 111 finished with value: 0.8302784497155358 and parameters: {'n_estimators': 553}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:04:52,666] Trial 112 finished with value: 0.8297664499242989 and parameters: {'n_estimators': 305}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:06:27,124] Trial 113 finished with value: 0.830041188976127 and parameters: {'n_estimators': 466}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:09:14,384] Trial 114 finished with value: 0.8297132943272725 and parameters: {'n_estimators': 828}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:11:28,884] Trial 115 finished with value: 0.8294074493794772 and parameters: {'n_estimators': 666}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:14:24,333] Trial 116 finished with value: 0.8294277881797856 and parameters: {'n_estimators': 870}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:16:07,825] Trial 117 finished with value: 0.8289440937805072 and parameters: {'n_estimators': 511}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:19:18,344] Trial 118 finished with value: 0.8299819944744158 and parameters: {'n_estimators': 937}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:21:54,416] Trial 119 finished with value: 0.8299743450340348 and parameters: {'n_estimators': 779}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:23:15,426] Trial 120 finished with value: 0.8322111194171928 and parameters: {'n_estimators': 396}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:26:01,280] Trial 121 finished with value: 0.8302506889818899 and parameters: {'n_estimators': 811}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:28:44,771] Trial 122 finished with value: 0.829995491879154 and parameters: {'n_estimators': 794}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:31:42,936] Trial 123 finished with value: 0.8297210590957679 and parameters: {'n_estimators': 850}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:34:30,354] Trial 124 finished with value: 0.8294360531771012 and parameters: {'n_estimators': 827}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:37:02,623] Trial 125 finished with value: 0.8302506606790085 and parameters: {'n_estimators': 765}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:39:03,520] Trial 126 finished with value: 0.8297241169241897 and parameters: {'n_estimators': 582}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:41:34,304] Trial 127 finished with value: 0.8302506606790085 and parameters: {'n_estimators': 744}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:43:22,878] Trial 128 finished with value: 0.829206742824331 and parameters: {'n_estimators': 536}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:46:02,133] Trial 129 finished with value: 0.8297212026130476 and parameters: {'n_estimators': 795}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:47:28,532] Trial 130 finished with value: 0.8313784746068515 and parameters: {'n_estimators': 425}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:50:14,996] Trial 131 finished with value: 0.8297171919345896 and parameters: {'n_estimators': 808}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:53:05,531] Trial 132 finished with value: 0.8299987186221898 and parameters: {'n_estimators': 854}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:55:52,776] Trial 133 finished with value: 0.8294360531771012 and parameters: {'n_estimators': 827}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 16:58:53,029] Trial 134 finished with value: 0.8297094260268061 and parameters: {'n_estimators': 874}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:01:32,911] Trial 135 finished with value: 0.8299743450340348 and parameters: {'n_estimators': 773}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:04:42,561] Trial 136 finished with value: 0.8297156524909433 and parameters: {'n_estimators': 912}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:07:13,613] Trial 137 finished with value: 0.8297016747910894 and parameters: {'n_estimators': 727}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:09:47,163] Trial 138 finished with value: 0.8296960895647608 and parameters: {'n_estimators': 757}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:12:38,283] Trial 139 finished with value: 0.8294411016972664 and parameters: {'n_estimators': 836}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:15:02,907] Trial 140 finished with value: 0.8294409470223216 and parameters: {'n_estimators': 710}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:17:41,156] Trial 141 finished with value: 0.8299872052884985 and parameters: {'n_estimators': 799}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:20:39,756] Trial 142 finished with value: 0.8288748775951875 and parameters: {'n_estimators': 865}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:23:23,787] Trial 143 finished with value: 0.8297178814260088 and parameters: {'n_estimators': 818}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:26:04,370] Trial 144 finished with value: 0.8302611838089081 and parameters: {'n_estimators': 786}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:28:54,537] Trial 145 finished with value: 0.8302759597723611 and parameters: {'n_estimators': 846}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:31:53,724] Trial 146 finished with value: 0.8297027562366788 and parameters: {'n_estimators': 903}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:34:51,521] Trial 147 finished with value: 0.8300111050030283 and parameters: {'n_estimators': 890}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:37:33,971] Trial 148 finished with value: 0.8297178814260088 and parameters: {'n_estimators': 815}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:40:05,622] Trial 149 finished with value: 0.8296853113662145 and parameters: {'n_estimators': 749}. Best is trial 27 with value: 0.8442177239059274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  421.000000  422.000000  423.000000\n",
      "1                    TN  344.000000  357.000000  338.000000\n",
      "2                    FP   83.000000   78.000000   84.000000\n",
      "3                    FN   71.000000   62.000000   74.000000\n",
      "4              Accuracy    0.832427    0.847661    0.828074\n",
      "5             Precision    0.835317    0.844000    0.834320\n",
      "6           Sensitivity    0.855691    0.871901    0.851107\n",
      "7           Specificity    0.805600    0.820700    0.800900\n",
      "8              F1 score    0.845382    0.857724    0.842629\n",
      "9   F1 score (weighted)    0.832242    0.847472    0.827899\n",
      "10     F1 score (macro)    0.831242    0.846895    0.826591\n",
      "11    Balanced Accuracy    0.830656    0.846295    0.826027\n",
      "12                  MCC    0.662771    0.694307    0.653380\n",
      "13                  NPV    0.828900    0.852000    0.820400\n",
      "14              ROC_AUC    0.830656    0.846295    0.826027\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_rf_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_rf_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_rf_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_rf_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_rf_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_rf_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_rf_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_rf_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_rf_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_rf_2)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 17:41:59,078] Trial 150 finished with value: 0.8274825730794599 and parameters: {'n_estimators': 500}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:45:17,203] Trial 151 finished with value: 0.8302027031076703 and parameters: {'n_estimators': 994}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:47:51,009] Trial 152 finished with value: 0.8283059166494052 and parameters: {'n_estimators': 764}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:51:06,405] Trial 153 finished with value: 0.8293909521342183 and parameters: {'n_estimators': 966}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:53:17,215] Trial 154 finished with value: 0.8278057961630066 and parameters: {'n_estimators': 643}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:56:26,199] Trial 155 finished with value: 0.8290895931066193 and parameters: {'n_estimators': 932}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 17:58:32,272] Trial 156 finished with value: 0.8288949452558482 and parameters: {'n_estimators': 625}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:01:45,338] Trial 157 finished with value: 0.8293720501294874 and parameters: {'n_estimators': 959}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:04:22,996] Trial 158 finished with value: 0.8288619020418444 and parameters: {'n_estimators': 784}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:06:15,386] Trial 159 finished with value: 0.828577564370096 and parameters: {'n_estimators': 561}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:09:03,092] Trial 160 finished with value: 0.8293899440049444 and parameters: {'n_estimators': 833}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:10:58,032] Trial 161 finished with value: 0.8285824967995279 and parameters: {'n_estimators': 570}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:12:45,708] Trial 162 finished with value: 0.8272138674363809 and parameters: {'n_estimators': 527}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:14:22,581] Trial 163 finished with value: 0.8274713678379548 and parameters: {'n_estimators': 477}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:16:22,615] Trial 164 finished with value: 0.8288737322886455 and parameters: {'n_estimators': 591}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:19:06,732] Trial 165 finished with value: 0.8288503705532749 and parameters: {'n_estimators': 811}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:20:38,693] Trial 166 finished with value: 0.8264203849307297 and parameters: {'n_estimators': 450}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:23:21,177] Trial 167 finished with value: 0.8288528336217545 and parameters: {'n_estimators': 799}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:26:16,473] Trial 168 finished with value: 0.8302126792725091 and parameters: {'n_estimators': 863}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:28:09,181] Trial 169 finished with value: 0.8280270941607781 and parameters: {'n_estimators': 548}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:29:26,368] Trial 170 finished with value: 0.8267486768073301 and parameters: {'n_estimators': 382}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:32:42,288] Trial 171 finished with value: 0.8288356078554932 and parameters: {'n_estimators': 961}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:35:56,911] Trial 172 finished with value: 0.8296387271357218 and parameters: {'n_estimators': 971}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:38:33,716] Trial 173 finished with value: 0.8280296375857106 and parameters: {'n_estimators': 778}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:41:51,965] Trial 174 finished with value: 0.8299201404326668 and parameters: {'n_estimators': 980}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:44:42,946] Trial 175 finished with value: 0.8291121699514139 and parameters: {'n_estimators': 847}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:48:05,187] Trial 176 finished with value: 0.8299290401219901 and parameters: {'n_estimators': 998}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:50:49,222] Trial 177 finished with value: 0.8291236355040423 and parameters: {'n_estimators': 827}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:53:18,933] Trial 178 finished with value: 0.828324857400658 and parameters: {'n_estimators': 741}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:55:59,911] Trial 179 finished with value: 0.8293901490287885 and parameters: {'n_estimators': 804}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 18:58:56,638] Trial 180 finished with value: 0.8299164798870562 and parameters: {'n_estimators': 883}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:00:01,653] Trial 181 finished with value: 0.8272480182159352 and parameters: {'n_estimators': 321}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:02:51,356] Trial 182 finished with value: 0.8291121699514139 and parameters: {'n_estimators': 842}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:05:56,416] Trial 183 finished with value: 0.8293690014882777 and parameters: {'n_estimators': 917}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:09:03,807] Trial 184 finished with value: 0.8296488425900547 and parameters: {'n_estimators': 940}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:11:59,599] Trial 185 finished with value: 0.8304930620798346 and parameters: {'n_estimators': 871}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:14:43,844] Trial 186 finished with value: 0.8288528336217545 and parameters: {'n_estimators': 821}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:18:06,526] Trial 187 finished with value: 0.8299314599796406 and parameters: {'n_estimators': 999}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:20:45,223] Trial 188 finished with value: 0.8288586139550917 and parameters: {'n_estimators': 783}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:23:34,725] Trial 189 finished with value: 0.8291227864860797 and parameters: {'n_estimators': 851}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:25:22,519] Trial 190 finished with value: 0.8277709111708147 and parameters: {'n_estimators': 518}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:28:03,671] Trial 191 finished with value: 0.8291236355040423 and parameters: {'n_estimators': 800}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:30:41,218] Trial 192 finished with value: 0.828312543332897 and parameters: {'n_estimators': 762}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:33:16,679] Trial 193 finished with value: 0.8280312136862704 and parameters: {'n_estimators': 771}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:35:39,850] Trial 194 finished with value: 0.8285991812020865 and parameters: {'n_estimators': 724}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:38:25,692] Trial 195 finished with value: 0.8288503705532749 and parameters: {'n_estimators': 816}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:39:50,441] Trial 196 finished with value: 0.8269343499602158 and parameters: {'n_estimators': 423}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:42:51,896] Trial 197 finished with value: 0.8299280414606043 and parameters: {'n_estimators': 893}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:45:08,149] Trial 198 finished with value: 0.8275091360746283 and parameters: {'n_estimators': 684}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:45:56,967] Trial 199 finished with value: 0.8248271669644656 and parameters: {'n_estimators': 237}. Best is trial 27 with value: 0.8442177239059274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  421.000000  422.000000  423.000000  448.000000\n",
      "1                    TN  344.000000  357.000000  338.000000  320.000000\n",
      "2                    FP   83.000000   78.000000   84.000000   84.000000\n",
      "3                    FN   71.000000   62.000000   74.000000   67.000000\n",
      "4              Accuracy    0.832427    0.847661    0.828074    0.835691\n",
      "5             Precision    0.835317    0.844000    0.834320    0.842105\n",
      "6           Sensitivity    0.855691    0.871901    0.851107    0.869903\n",
      "7           Specificity    0.805600    0.820700    0.800900    0.792100\n",
      "8              F1 score    0.845382    0.857724    0.842629    0.855778\n",
      "9   F1 score (weighted)    0.832242    0.847472    0.827899    0.835259\n",
      "10     F1 score (macro)    0.831242    0.846895    0.826591    0.832440\n",
      "11    Balanced Accuracy    0.830656    0.846295    0.826027    0.830991\n",
      "12                  MCC    0.662771    0.694307    0.653380    0.665471\n",
      "13                  NPV    0.828900    0.852000    0.820400    0.826900\n",
      "14              ROC_AUC    0.830656    0.846295    0.826027    0.830991\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_rf_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_rf_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_rf_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_rf_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_rf_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_rf_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_rf_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_rf_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_rf_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_rf_3)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 19:48:47,197] Trial 200 finished with value: 0.8287048214304642 and parameters: {'n_estimators': 791}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:50:36,104] Trial 201 finished with value: 0.8257488828892263 and parameters: {'n_estimators': 543}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:52:35,231] Trial 202 finished with value: 0.8284439855817105 and parameters: {'n_estimators': 588}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:54:28,876] Trial 203 finished with value: 0.8276635486507995 and parameters: {'n_estimators': 569}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:56:10,243] Trial 204 finished with value: 0.8270866819579368 and parameters: {'n_estimators': 500}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 19:58:10,617] Trial 205 finished with value: 0.8276253773463752 and parameters: {'n_estimators': 602}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:00:58,605] Trial 206 finished with value: 0.8281698012925099 and parameters: {'n_estimators': 832}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:03:29,612] Trial 207 finished with value: 0.8281516732035697 and parameters: {'n_estimators': 748}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:06:24,271] Trial 208 finished with value: 0.8284569016176041 and parameters: {'n_estimators': 861}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:09:07,328] Trial 209 finished with value: 0.828443180524396 and parameters: {'n_estimators': 807}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:11:00,389] Trial 210 finished with value: 0.8268339640881512 and parameters: {'n_estimators': 553}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:14:18,407] Trial 211 finished with value: 0.8287050570074037 and parameters: {'n_estimators': 974}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:17:29,034] Trial 212 finished with value: 0.8281775370591611 and parameters: {'n_estimators': 950}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:20:49,177] Trial 213 finished with value: 0.8289856897192406 and parameters: {'n_estimators': 983}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:23:21,915] Trial 214 finished with value: 0.8289723686316813 and parameters: {'n_estimators': 772}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:26:26,175] Trial 215 finished with value: 0.8278916938488742 and parameters: {'n_estimators': 921}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:29:36,940] Trial 216 finished with value: 0.8281775370591611 and parameters: {'n_estimators': 946}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:32:23,776] Trial 217 finished with value: 0.828165668353454 and parameters: {'n_estimators': 845}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:35:22,388] Trial 218 finished with value: 0.8281667683857702 and parameters: {'n_estimators': 900}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:38:15,312] Trial 219 finished with value: 0.8281778144461406 and parameters: {'n_estimators': 873}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:40:54,202] Trial 220 finished with value: 0.8287048214304642 and parameters: {'n_estimators': 794}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:42:59,114] Trial 221 finished with value: 0.8292070257206093 and parameters: {'n_estimators': 633}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:44:58,459] Trial 222 finished with value: 0.8284293519349722 and parameters: {'n_estimators': 603}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:46:43,035] Trial 223 finished with value: 0.8273698673984843 and parameters: {'n_estimators': 533}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:48:39,805] Trial 224 finished with value: 0.8281854008455547 and parameters: {'n_estimators': 583}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:51:24,628] Trial 225 finished with value: 0.8290058777549255 and parameters: {'n_estimators': 825}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:53:16,144] Trial 226 finished with value: 0.8268201421866477 and parameters: {'n_estimators': 561}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:56:35,254] Trial 227 finished with value: 0.8289856897192406 and parameters: {'n_estimators': 983}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 20:59:12,521] Trial 228 finished with value: 0.8278649311368182 and parameters: {'n_estimators': 781}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:01:15,745] Trial 229 finished with value: 0.8281441628450812 and parameters: {'n_estimators': 612}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:02:47,769] Trial 230 finished with value: 0.8262702340554913 and parameters: {'n_estimators': 462}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:04:28,986] Trial 231 finished with value: 0.827620656141496 and parameters: {'n_estimators': 501}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:06:06,875] Trial 232 finished with value: 0.8262661839844556 and parameters: {'n_estimators': 486}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:07:57,448] Trial 233 finished with value: 0.8262954799539349 and parameters: {'n_estimators': 552}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:09:39,607] Trial 234 finished with value: 0.8276347972411553 and parameters: {'n_estimators': 511}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:11:24,820] Trial 235 finished with value: 0.8268327821668912 and parameters: {'n_estimators': 530}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:13:54,014] Trial 236 finished with value: 0.8278567167158544 and parameters: {'n_estimators': 754}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:16:44,875] Trial 237 finished with value: 0.8289920908594448 and parameters: {'n_estimators': 811}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:19:28,528] Trial 238 finished with value: 0.8278845410610884 and parameters: {'n_estimators': 837}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:21:18,893] Trial 239 finished with value: 0.827634515226797 and parameters: {'n_estimators': 522}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:23:56,179] Trial 240 finished with value: 0.8287109635670793 and parameters: {'n_estimators': 805}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:26:27,112] Trial 241 finished with value: 0.8284323059154067 and parameters: {'n_estimators': 727}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:28:47,619] Trial 242 finished with value: 0.8281796529448163 and parameters: {'n_estimators': 715}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:31:12,067] Trial 243 finished with value: 0.8287237474339604 and parameters: {'n_estimators': 705}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:33:43,997] Trial 244 finished with value: 0.8281383518124981 and parameters: {'n_estimators': 751}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:35:40,682] Trial 245 finished with value: 0.8273829717065114 and parameters: {'n_estimators': 574}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:38:13,878] Trial 246 finished with value: 0.8289723686316813 and parameters: {'n_estimators': 769}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:40:53,596] Trial 247 finished with value: 0.8281465662334618 and parameters: {'n_estimators': 787}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:43:20,722] Trial 248 finished with value: 0.8278990202329796 and parameters: {'n_estimators': 740}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:46:14,526] Trial 249 finished with value: 0.8284569016176041 and parameters: {'n_estimators': 855}. Best is trial 27 with value: 0.8442177239059274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  421.000000  422.000000  423.000000  448.000000   \n",
      "1                    TN  344.000000  357.000000  338.000000  320.000000   \n",
      "2                    FP   83.000000   78.000000   84.000000   84.000000   \n",
      "3                    FN   71.000000   62.000000   74.000000   67.000000   \n",
      "4              Accuracy    0.832427    0.847661    0.828074    0.835691   \n",
      "5             Precision    0.835317    0.844000    0.834320    0.842105   \n",
      "6           Sensitivity    0.855691    0.871901    0.851107    0.869903   \n",
      "7           Specificity    0.805600    0.820700    0.800900    0.792100   \n",
      "8              F1 score    0.845382    0.857724    0.842629    0.855778   \n",
      "9   F1 score (weighted)    0.832242    0.847472    0.827899    0.835259   \n",
      "10     F1 score (macro)    0.831242    0.846895    0.826591    0.832440   \n",
      "11    Balanced Accuracy    0.830656    0.846295    0.826027    0.830991   \n",
      "12                  MCC    0.662771    0.694307    0.653380    0.665471   \n",
      "13                  NPV    0.828900    0.852000    0.820400    0.826900   \n",
      "14              ROC_AUC    0.830656    0.846295    0.826027    0.830991   \n",
      "\n",
      "          Set4  \n",
      "0   408.000000  \n",
      "1   347.000000  \n",
      "2    83.000000  \n",
      "3    81.000000  \n",
      "4     0.821545  \n",
      "5     0.830957  \n",
      "6     0.834356  \n",
      "7     0.807000  \n",
      "8     0.832653  \n",
      "9     0.821519  \n",
      "10    0.820755  \n",
      "11    0.820666  \n",
      "12    0.641519  \n",
      "13    0.810700  \n",
      "14    0.820666  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_rf_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_rf_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_rf_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_rf_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_rf_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_rf_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_rf_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_rf_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_rf_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_rf_4)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 21:49:12,339] Trial 250 finished with value: 0.8282885815002963 and parameters: {'n_estimators': 821}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:51:23,271] Trial 251 finished with value: 0.8277660212628237 and parameters: {'n_estimators': 654}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:54:20,302] Trial 252 finished with value: 0.8268846316973553 and parameters: {'n_estimators': 883}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:56:54,616] Trial 253 finished with value: 0.8272374388350123 and parameters: {'n_estimators': 766}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 21:59:36,024] Trial 254 finished with value: 0.8272117297319225 and parameters: {'n_estimators': 792}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:02:56,288] Trial 255 finished with value: 0.8273866747965808 and parameters: {'n_estimators': 997}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:04:47,161] Trial 256 finished with value: 0.8275084036749026 and parameters: {'n_estimators': 542}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:07:34,926] Trial 257 finished with value: 0.8271810249562961 and parameters: {'n_estimators': 839}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:10:31,313] Trial 258 finished with value: 0.827991184637835 and parameters: {'n_estimators': 863}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:11:59,130] Trial 259 finished with value: 0.8266720466615027 and parameters: {'n_estimators': 437}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:14:44,734] Trial 260 finished with value: 0.828289898035283 and parameters: {'n_estimators': 812}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:16:19,783] Trial 261 finished with value: 0.827526392511912 and parameters: {'n_estimators': 472}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:19:36,279] Trial 262 finished with value: 0.8290695472571503 and parameters: {'n_estimators': 964}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:21:28,530] Trial 263 finished with value: 0.8280245549616595 and parameters: {'n_estimators': 565}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:23:58,626] Trial 264 finished with value: 0.8269561148614921 and parameters: {'n_estimators': 734}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:27:01,712] Trial 265 finished with value: 0.8285381229675309 and parameters: {'n_estimators': 924}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:29:02,655] Trial 266 finished with value: 0.828028192669184 and parameters: {'n_estimators': 591}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:31:47,949] Trial 267 finished with value: 0.8280359173153627 and parameters: {'n_estimators': 832}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:33:35,581] Trial 268 finished with value: 0.8280311581992013 and parameters: {'n_estimators': 517}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:36:07,522] Trial 269 finished with value: 0.8277717483796698 and parameters: {'n_estimators': 764}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:38:50,077] Trial 270 finished with value: 0.8275049159313899 and parameters: {'n_estimators': 784}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:41:28,487] Trial 271 finished with value: 0.8280190405257887 and parameters: {'n_estimators': 797}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:44:14,506] Trial 272 finished with value: 0.8285876914759658 and parameters: {'n_estimators': 824}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:47:30,754] Trial 273 finished with value: 0.8285190156188085 and parameters: {'n_estimators': 981}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:49:49,908] Trial 274 finished with value: 0.8267049951802614 and parameters: {'n_estimators': 690}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:52:40,421] Trial 275 finished with value: 0.8277352243718076 and parameters: {'n_estimators': 852}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:54:29,198] Trial 276 finished with value: 0.8280282789076276 and parameters: {'n_estimators': 539}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:57:23,461] Trial 277 finished with value: 0.8277211666605346 and parameters: {'n_estimators': 880}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 22:59:53,732] Trial 278 finished with value: 0.8266717638422723 and parameters: {'n_estimators': 753}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:01:44,332] Trial 279 finished with value: 0.8277874882157491 and parameters: {'n_estimators': 558}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:03:48,876] Trial 280 finished with value: 0.8283064382123866 and parameters: {'n_estimators': 624}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:06:47,963] Trial 281 finished with value: 0.8279880201588579 and parameters: {'n_estimators': 901}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:08:26,601] Trial 282 finished with value: 0.8264065631312457 and parameters: {'n_estimators': 491}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:11:34,178] Trial 283 finished with value: 0.8287956334719084 and parameters: {'n_estimators': 937}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:14:16,449] Trial 284 finished with value: 0.828023065587003 and parameters: {'n_estimators': 807}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:16:53,284] Trial 285 finished with value: 0.8277734866137868 and parameters: {'n_estimators': 779}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:19:41,575] Trial 286 finished with value: 0.8274659411990368 and parameters: {'n_estimators': 835}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:22:54,019] Trial 287 finished with value: 0.829331414549792 and parameters: {'n_estimators': 955}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:26:15,789] Trial 288 finished with value: 0.8271355340075492 and parameters: {'n_estimators': 1000}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:28:56,130] Trial 289 finished with value: 0.8274627967727655 and parameters: {'n_estimators': 794}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:31:11,184] Trial 290 finished with value: 0.8272268173560766 and parameters: {'n_estimators': 665}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:33:42,290] Trial 291 finished with value: 0.8272011043262953 and parameters: {'n_estimators': 739}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:36:37,392] Trial 292 finished with value: 0.8277519141990572 and parameters: {'n_estimators': 858}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:39:14,722] Trial 293 finished with value: 0.8272374388350123 and parameters: {'n_estimators': 766}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:41:12,571] Trial 294 finished with value: 0.8283535671032605 and parameters: {'n_estimators': 586}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:44:00,790] Trial 295 finished with value: 0.8280042304810766 and parameters: {'n_estimators': 819}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:45:49,802] Trial 296 finished with value: 0.8280282789076276 and parameters: {'n_estimators': 539}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:47:34,463] Trial 297 finished with value: 0.8285946695136566 and parameters: {'n_estimators': 518}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:49:08,223] Trial 298 finished with value: 0.8264228839715599 and parameters: {'n_estimators': 454}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:51:30,681] Trial 299 finished with value: 0.8280551366050621 and parameters: {'n_estimators': 708}. Best is trial 27 with value: 0.8442177239059274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  421.000000  422.000000  423.000000  448.000000   \n",
      "1                    TN  344.000000  357.000000  338.000000  320.000000   \n",
      "2                    FP   83.000000   78.000000   84.000000   84.000000   \n",
      "3                    FN   71.000000   62.000000   74.000000   67.000000   \n",
      "4              Accuracy    0.832427    0.847661    0.828074    0.835691   \n",
      "5             Precision    0.835317    0.844000    0.834320    0.842105   \n",
      "6           Sensitivity    0.855691    0.871901    0.851107    0.869903   \n",
      "7           Specificity    0.805600    0.820700    0.800900    0.792100   \n",
      "8              F1 score    0.845382    0.857724    0.842629    0.855778   \n",
      "9   F1 score (weighted)    0.832242    0.847472    0.827899    0.835259   \n",
      "10     F1 score (macro)    0.831242    0.846895    0.826591    0.832440   \n",
      "11    Balanced Accuracy    0.830656    0.846295    0.826027    0.830991   \n",
      "12                  MCC    0.662771    0.694307    0.653380    0.665471   \n",
      "13                  NPV    0.828900    0.852000    0.820400    0.826900   \n",
      "14              ROC_AUC    0.830656    0.846295    0.826027    0.830991   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   408.000000  415.000000  \n",
      "1   347.000000  357.000000  \n",
      "2    83.000000   78.000000  \n",
      "3    81.000000   69.000000  \n",
      "4     0.821545    0.840044  \n",
      "5     0.830957    0.841785  \n",
      "6     0.834356    0.857438  \n",
      "7     0.807000    0.820700  \n",
      "8     0.832653    0.849539  \n",
      "9     0.821519    0.839944  \n",
      "10    0.820755    0.839404  \n",
      "11    0.820666    0.839064  \n",
      "12    0.641519    0.678970  \n",
      "13    0.810700    0.838000  \n",
      "14    0.820666    0.839064  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_rf_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_rf_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_rf_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_rf_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_rf_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_rf_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_rf_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_rf_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_rf_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_rf_5)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 23:53:44,634] Trial 300 finished with value: 0.836886892286645 and parameters: {'n_estimators': 604}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:56:34,786] Trial 301 finished with value: 0.8366137657300314 and parameters: {'n_estimators': 844}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-01 23:59:15,742] Trial 302 finished with value: 0.836891460815402 and parameters: {'n_estimators': 804}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:01:51,603] Trial 303 finished with value: 0.8358098269667995 and parameters: {'n_estimators': 775}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:03:31,698] Trial 304 finished with value: 0.8368591589911656 and parameters: {'n_estimators': 504}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:05:59,952] Trial 305 finished with value: 0.8349663686524614 and parameters: {'n_estimators': 727}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:08:29,788] Trial 306 finished with value: 0.83579012060707 and parameters: {'n_estimators': 753}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:08:54,231] Trial 307 finished with value: 0.8371684174686344 and parameters: {'n_estimators': 117}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:11:45,946] Trial 308 finished with value: 0.8366163447246924 and parameters: {'n_estimators': 869}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:13:40,587] Trial 309 finished with value: 0.8371393624521556 and parameters: {'n_estimators': 569}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:16:26,261] Trial 310 finished with value: 0.8363374103657346 and parameters: {'n_estimators': 831}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:19:08,580] Trial 311 finished with value: 0.8357895631658213 and parameters: {'n_estimators': 795}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:20:31,333] Trial 312 finished with value: 0.8368681133827334 and parameters: {'n_estimators': 413}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:23:39,014] Trial 313 finished with value: 0.8363245959401574 and parameters: {'n_estimators': 916}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:26:23,609] Trial 314 finished with value: 0.8366099776271498 and parameters: {'n_estimators': 818}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:29:43,788] Trial 315 finished with value: 0.8363299773308531 and parameters: {'n_estimators': 975}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:32:20,452] Trial 316 finished with value: 0.836075855539139 and parameters: {'n_estimators': 782}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:35:13,899] Trial 317 finished with value: 0.83635315383769 and parameters: {'n_estimators': 848}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:38:21,014] Trial 318 finished with value: 0.8363269923342754 and parameters: {'n_estimators': 889}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:40:12,378] Trial 319 finished with value: 0.8365888042150204 and parameters: {'n_estimators': 555}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:42:48,311] Trial 320 finished with value: 0.8355259496043393 and parameters: {'n_estimators': 758}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:45:24,413] Trial 321 finished with value: 0.836075855539139 and parameters: {'n_estimators': 781}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:47:02,458] Trial 322 finished with value: 0.8371383171358054 and parameters: {'n_estimators': 485}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:49:44,760] Trial 323 finished with value: 0.8366148310138286 and parameters: {'n_estimators': 815}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:51:31,062] Trial 324 finished with value: 0.836038388517163 and parameters: {'n_estimators': 525}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:53:59,126] Trial 325 finished with value: 0.8352397207861355 and parameters: {'n_estimators': 742}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:57:08,193] Trial 326 finished with value: 0.8368652516254015 and parameters: {'n_estimators': 945}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 00:59:48,910] Trial 327 finished with value: 0.836891460815402 and parameters: {'n_estimators': 804}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:02:36,811] Trial 328 finished with value: 0.8363374103657346 and parameters: {'n_estimators': 833}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:04:26,069] Trial 329 finished with value: 0.8371255220635723 and parameters: {'n_estimators': 541}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:06:20,824] Trial 330 finished with value: 0.8374289134993148 and parameters: {'n_estimators': 576}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:09:16,548] Trial 331 finished with value: 0.8360711430484933 and parameters: {'n_estimators': 873}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:12:11,975] Trial 332 finished with value: 0.8360750919566471 and parameters: {'n_estimators': 860}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:14:49,164] Trial 333 finished with value: 0.8358098269667995 and parameters: {'n_estimators': 770}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:16:58,841] Trial 334 finished with value: 0.8357884476282056 and parameters: {'n_estimators': 647}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:19:01,854] Trial 335 finished with value: 0.8368966753628431 and parameters: {'n_estimators': 621}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:21:41,857] Trial 336 finished with value: 0.8357895631658213 and parameters: {'n_estimators': 794}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:22:18,916] Trial 337 finished with value: 0.8369365766369679 and parameters: {'n_estimators': 184}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:25:40,105] Trial 338 finished with value: 0.8363130225369859 and parameters: {'n_estimators': 1000}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:28:03,252] Trial 339 finished with value: 0.8352363272536643 and parameters: {'n_estimators': 721}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:31:13,413] Trial 340 finished with value: 0.8366072095476917 and parameters: {'n_estimators': 963}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:33:58,612] Trial 341 finished with value: 0.8363345721941448 and parameters: {'n_estimators': 822}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:36:46,421] Trial 342 finished with value: 0.8360750919566471 and parameters: {'n_estimators': 846}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:39:51,378] Trial 343 finished with value: 0.8363285191595822 and parameters: {'n_estimators': 933}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:42:50,424] Trial 344 finished with value: 0.8363202690259147 and parameters: {'n_estimators': 906}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:44:47,686] Trial 345 finished with value: 0.8371584742268816 and parameters: {'n_estimators': 591}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:46:38,259] Trial 346 finished with value: 0.8365888042150204 and parameters: {'n_estimators': 555}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:48:52,804] Trial 347 finished with value: 0.8366392596794539 and parameters: {'n_estimators': 678}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:51:23,576] Trial 348 finished with value: 0.8358061009673143 and parameters: {'n_estimators': 760}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:54:38,729] Trial 349 finished with value: 0.8366032350716898 and parameters: {'n_estimators': 984}. Best is trial 27 with value: 0.8442177239059274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  421.000000  422.000000  423.000000  448.000000   \n",
      "1                    TN  344.000000  357.000000  338.000000  320.000000   \n",
      "2                    FP   83.000000   78.000000   84.000000   84.000000   \n",
      "3                    FN   71.000000   62.000000   74.000000   67.000000   \n",
      "4              Accuracy    0.832427    0.847661    0.828074    0.835691   \n",
      "5             Precision    0.835317    0.844000    0.834320    0.842105   \n",
      "6           Sensitivity    0.855691    0.871901    0.851107    0.869903   \n",
      "7           Specificity    0.805600    0.820700    0.800900    0.792100   \n",
      "8              F1 score    0.845382    0.857724    0.842629    0.855778   \n",
      "9   F1 score (weighted)    0.832242    0.847472    0.827899    0.835259   \n",
      "10     F1 score (macro)    0.831242    0.846895    0.826591    0.832440   \n",
      "11    Balanced Accuracy    0.830656    0.846295    0.826027    0.830991   \n",
      "12                  MCC    0.662771    0.694307    0.653380    0.665471   \n",
      "13                  NPV    0.828900    0.852000    0.820400    0.826900   \n",
      "14              ROC_AUC    0.830656    0.846295    0.826027    0.830991   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   408.000000  415.000000  458.000000  \n",
      "1   347.000000  357.000000  320.000000  \n",
      "2    83.000000   78.000000   84.000000  \n",
      "3    81.000000   69.000000   57.000000  \n",
      "4     0.821545    0.840044    0.846572  \n",
      "5     0.830957    0.841785    0.845018  \n",
      "6     0.834356    0.857438    0.889320  \n",
      "7     0.807000    0.820700    0.792100  \n",
      "8     0.832653    0.849539    0.866604  \n",
      "9     0.821519    0.839944    0.845880  \n",
      "10    0.820755    0.839404    0.843033  \n",
      "11    0.820666    0.839064    0.840700  \n",
      "12    0.641519    0.678970    0.687584  \n",
      "13    0.810700    0.838000    0.848800  \n",
      "14    0.820666    0.839064    0.840700  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_rf_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_rf_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_rf_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_rf_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_rf_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_rf_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_rf_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_rf_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_rf_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_rf_6)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 01:56:08,968] Trial 350 finished with value: 0.8307444449744011 and parameters: {'n_estimators': 388}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 01:57:52,814] Trial 351 finished with value: 0.832076007738524 and parameters: {'n_estimators': 507}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:00:33,392] Trial 352 finished with value: 0.8315643690334884 and parameters: {'n_estimators': 800}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:03:34,228] Trial 353 finished with value: 0.8320915930540735 and parameters: {'n_estimators': 890}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:06:11,445] Trial 354 finished with value: 0.8318216807506454 and parameters: {'n_estimators': 785}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:08:58,921] Trial 355 finished with value: 0.8315365184672325 and parameters: {'n_estimators': 825}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:11:27,626] Trial 356 finished with value: 0.8310252859996126 and parameters: {'n_estimators': 743}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:13:13,119] Trial 357 finished with value: 0.8329158422278938 and parameters: {'n_estimators': 526}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:15:35,462] Trial 358 finished with value: 0.8312798536551582 and parameters: {'n_estimators': 705}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:18:17,078] Trial 359 finished with value: 0.831832176530195 and parameters: {'n_estimators': 808}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:19:51,375] Trial 360 finished with value: 0.8326793471603757 and parameters: {'n_estimators': 466}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:22:40,953] Trial 361 finished with value: 0.8320943464383669 and parameters: {'n_estimators': 844}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:25:34,960] Trial 362 finished with value: 0.8329290897428134 and parameters: {'n_estimators': 866}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:28:11,115] Trial 363 finished with value: 0.8312845210165578 and parameters: {'n_estimators': 779}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:30:42,628] Trial 364 finished with value: 0.8312879551183722 and parameters: {'n_estimators': 767}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:32:10,827] Trial 365 finished with value: 0.831336429496665 and parameters: {'n_estimators': 440}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:34:03,493] Trial 366 finished with value: 0.831548991700563 and parameters: {'n_estimators': 568}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:36:48,887] Trial 367 finished with value: 0.8323635820379975 and parameters: {'n_estimators': 832}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:38:47,365] Trial 368 finished with value: 0.8315452488924988 and parameters: {'n_estimators': 598}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:41:12,495] Trial 369 finished with value: 0.8315692297647239 and parameters: {'n_estimators': 727}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:43:53,371] Trial 370 finished with value: 0.8318325122652312 and parameters: {'n_estimators': 810}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:47:04,871] Trial 371 finished with value: 0.8331752372905641 and parameters: {'n_estimators': 963}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:49:36,196] Trial 372 finished with value: 0.831289251772648 and parameters: {'n_estimators': 754}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:52:26,424] Trial 373 finished with value: 0.8329198353570193 and parameters: {'n_estimators': 853}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:54:15,459] Trial 374 finished with value: 0.8318652754937984 and parameters: {'n_estimators': 544}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:55:53,736] Trial 375 finished with value: 0.8326582842323346 and parameters: {'n_estimators': 490}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 02:58:49,168] Trial 376 finished with value: 0.8332050938363846 and parameters: {'n_estimators': 878}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:01:27,199] Trial 377 finished with value: 0.8321070925276695 and parameters: {'n_estimators': 790}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:04:08,476] Trial 378 finished with value: 0.8320992316658412 and parameters: {'n_estimators': 809}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:07:13,251] Trial 379 finished with value: 0.832394438185937 and parameters: {'n_estimators': 913}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:09:15,474] Trial 380 finished with value: 0.8315068209605213 and parameters: {'n_estimators': 611}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:12:01,154] Trial 381 finished with value: 0.8318074425596903 and parameters: {'n_estimators': 826}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:15:20,843] Trial 382 finished with value: 0.8323566172090107 and parameters: {'n_estimators': 988}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:17:18,377] Trial 383 finished with value: 0.8309859303116062 and parameters: {'n_estimators': 577}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:19:54,368] Trial 384 finished with value: 0.8312844717140466 and parameters: {'n_estimators': 774}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:23:05,712] Trial 385 finished with value: 0.8329237389048633 and parameters: {'n_estimators': 948}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:24:48,863] Trial 386 finished with value: 0.8323720915610572 and parameters: {'n_estimators': 510}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:27:17,210] Trial 387 finished with value: 0.8323768723342548 and parameters: {'n_estimators': 735}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:29:04,334] Trial 388 finished with value: 0.8334317730308817 and parameters: {'n_estimators': 531}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:31:43,410] Trial 389 finished with value: 0.832373898539925 and parameters: {'n_estimators': 794}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:34:33,676] Trial 390 finished with value: 0.8320943464383669 and parameters: {'n_estimators': 836}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:37:24,700] Trial 391 finished with value: 0.8334707003050473 and parameters: {'n_estimators': 855}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:39:17,302] Trial 392 finished with value: 0.8323870700361594 and parameters: {'n_estimators': 553}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:42:34,143] Trial 393 finished with value: 0.8331621352021406 and parameters: {'n_estimators': 972}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:45:18,182] Trial 394 finished with value: 0.8321038146933178 and parameters: {'n_estimators': 815}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:47:49,607] Trial 395 finished with value: 0.8310064563233901 and parameters: {'n_estimators': 757}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:49:01,434] Trial 396 finished with value: 0.8315690853793377 and parameters: {'n_estimators': 349}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:50:36,764] Trial 397 finished with value: 0.8332055101203982 and parameters: {'n_estimators': 475}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:53:37,678] Trial 398 finished with value: 0.8329188198277663 and parameters: {'n_estimators': 895}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 03:56:15,022] Trial 399 finished with value: 0.8312766094558863 and parameters: {'n_estimators': 781}. Best is trial 27 with value: 0.8442177239059274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  421.000000  422.000000  423.000000  448.000000   \n",
      "1                    TN  344.000000  357.000000  338.000000  320.000000   \n",
      "2                    FP   83.000000   78.000000   84.000000   84.000000   \n",
      "3                    FN   71.000000   62.000000   74.000000   67.000000   \n",
      "4              Accuracy    0.832427    0.847661    0.828074    0.835691   \n",
      "5             Precision    0.835317    0.844000    0.834320    0.842105   \n",
      "6           Sensitivity    0.855691    0.871901    0.851107    0.869903   \n",
      "7           Specificity    0.805600    0.820700    0.800900    0.792100   \n",
      "8              F1 score    0.845382    0.857724    0.842629    0.855778   \n",
      "9   F1 score (weighted)    0.832242    0.847472    0.827899    0.835259   \n",
      "10     F1 score (macro)    0.831242    0.846895    0.826591    0.832440   \n",
      "11    Balanced Accuracy    0.830656    0.846295    0.826027    0.830991   \n",
      "12                  MCC    0.662771    0.694307    0.653380    0.665471   \n",
      "13                  NPV    0.828900    0.852000    0.820400    0.826900   \n",
      "14              ROC_AUC    0.830656    0.846295    0.826027    0.830991   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   408.000000  415.000000  458.000000  429.000000  \n",
      "1   347.000000  357.000000  320.000000  343.000000  \n",
      "2    83.000000   78.000000   84.000000   87.000000  \n",
      "3    81.000000   69.000000   57.000000   60.000000  \n",
      "4     0.821545    0.840044    0.846572    0.840044  \n",
      "5     0.830957    0.841785    0.845018    0.831395  \n",
      "6     0.834356    0.857438    0.889320    0.877301  \n",
      "7     0.807000    0.820700    0.792100    0.797700  \n",
      "8     0.832653    0.849539    0.866604    0.853731  \n",
      "9     0.821519    0.839944    0.845880    0.839600  \n",
      "10    0.820755    0.839404    0.843033    0.838630  \n",
      "11    0.820666    0.839064    0.840700    0.837488  \n",
      "12    0.641519    0.678970    0.687584    0.678733  \n",
      "13    0.810700    0.838000    0.848800    0.851100  \n",
      "14    0.820666    0.839064    0.840700    0.837488  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_rf_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_rf_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_rf_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_rf_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_rf_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_rf_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_rf_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_rf_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_rf_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_rf_7)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 03:59:27,194] Trial 400 finished with value: 0.8271308782333012 and parameters: {'n_estimators': 877}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:02:08,789] Trial 401 finished with value: 0.8265705428132624 and parameters: {'n_estimators': 798}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:05:17,402] Trial 402 finished with value: 0.8263154844982379 and parameters: {'n_estimators': 933}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:08:08,238] Trial 403 finished with value: 0.8263111767994109 and parameters: {'n_estimators': 835}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:10:15,210] Trial 404 finished with value: 0.8274391424461893 and parameters: {'n_estimators': 628}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:12:48,269] Trial 405 finished with value: 0.8260303994572146 and parameters: {'n_estimators': 745}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:15:10,719] Trial 406 finished with value: 0.8268976446862213 and parameters: {'n_estimators': 704}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:17:56,848] Trial 407 finished with value: 0.8265831665395792 and parameters: {'n_estimators': 816}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:20:32,517] Trial 408 finished with value: 0.8265697481504143 and parameters: {'n_estimators': 764}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:23:27,131] Trial 409 finished with value: 0.8265863520752855 and parameters: {'n_estimators': 860}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:25:13,995] Trial 410 finished with value: 0.8254552655456339 and parameters: {'n_estimators': 527}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:27:12,088] Trial 411 finished with value: 0.8255718060636428 and parameters: {'n_estimators': 578}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:29:50,400] Trial 412 finished with value: 0.8260246893571332 and parameters: {'n_estimators': 784}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:31:41,722] Trial 413 finished with value: 0.8255056263900563 and parameters: {'n_estimators': 555}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:34:05,439] Trial 414 finished with value: 0.8263323972322165 and parameters: {'n_estimators': 718}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:36:56,737] Trial 415 finished with value: 0.8268652184605456 and parameters: {'n_estimators': 843}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:39:40,172] Trial 416 finished with value: 0.8263048845730954 and parameters: {'n_estimators': 800}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:42:59,040] Trial 417 finished with value: 0.826617812285552 and parameters: {'n_estimators': 997}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:45:46,072] Trial 418 finished with value: 0.8263064190036473 and parameters: {'n_estimators': 822}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:47:10,786] Trial 419 finished with value: 0.8238305317462122 and parameters: {'n_estimators': 415}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:48:06,644] Trial 420 finished with value: 0.8257375273650289 and parameters: {'n_estimators': 269}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:50:37,015] Trial 421 finished with value: 0.8251986054696839 and parameters: {'n_estimators': 737}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:52:18,931] Trial 422 finished with value: 0.8257522944771383 and parameters: {'n_estimators': 493}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:54:54,856] Trial 423 finished with value: 0.8270778260409214 and parameters: {'n_estimators': 767}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 04:57:51,655] Trial 424 finished with value: 0.8266026476106397 and parameters: {'n_estimators': 872}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:00:08,180] Trial 425 finished with value: 0.8260083051721576 and parameters: {'n_estimators': 667}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:02:10,326] Trial 426 finished with value: 0.8261051776699615 and parameters: {'n_estimators': 594}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:05:27,888] Trial 427 finished with value: 0.8271901272558353 and parameters: {'n_estimators': 971}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:07:47,674] Trial 428 finished with value: 0.826582111116098 and parameters: {'n_estimators': 688}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:09:37,657] Trial 429 finished with value: 0.8250141328784434 and parameters: {'n_estimators': 542}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:11:10,060] Trial 430 finished with value: 0.8252505643904307 and parameters: {'n_estimators': 454}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:12:57,605] Trial 431 finished with value: 0.8255336638148367 and parameters: {'n_estimators': 510}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:15:54,686] Trial 432 finished with value: 0.8268665889343433 and parameters: {'n_estimators': 849}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:19:13,666] Trial 433 finished with value: 0.8265938518180619 and parameters: {'n_estimators': 917}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:22:00,559] Trial 434 finished with value: 0.8263008107684797 and parameters: {'n_estimators': 806}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:24:44,607] Trial 435 finished with value: 0.8257486636251011 and parameters: {'n_estimators': 787}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:27:59,780] Trial 436 finished with value: 0.8266167288081302 and parameters: {'n_estimators': 948}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:30:40,031] Trial 437 finished with value: 0.8263166532701408 and parameters: {'n_estimators': 752}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:33:31,308] Trial 438 finished with value: 0.8263096178103014 and parameters: {'n_estimators': 829}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:36:11,074] Trial 439 finished with value: 0.8273762971476284 and parameters: {'n_estimators': 771}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:38:07,187] Trial 440 finished with value: 0.825826985004281 and parameters: {'n_estimators': 566}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:40:54,735] Trial 441 finished with value: 0.8265831665395792 and parameters: {'n_estimators': 816}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:43:56,743] Trial 442 finished with value: 0.8263355560238317 and parameters: {'n_estimators': 886}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:46:38,211] Trial 443 finished with value: 0.8257460307643825 and parameters: {'n_estimators': 792}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:49:31,432] Trial 444 finished with value: 0.826866278384219 and parameters: {'n_estimators': 862}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:51:33,661] Trial 445 finished with value: 0.826366301396857 and parameters: {'n_estimators': 610}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:53:44,262] Trial 446 finished with value: 0.8265360256227721 and parameters: {'n_estimators': 647}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:57:00,285] Trial 447 finished with value: 0.8274486536869798 and parameters: {'n_estimators': 977}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 05:59:47,518] Trial 448 finished with value: 0.8268664867876486 and parameters: {'n_estimators': 838}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:02:15,818] Trial 449 finished with value: 0.8260525656313501 and parameters: {'n_estimators': 742}. Best is trial 27 with value: 0.8442177239059274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  421.000000  422.000000  423.000000  448.000000   \n",
      "1                    TN  344.000000  357.000000  338.000000  320.000000   \n",
      "2                    FP   83.000000   78.000000   84.000000   84.000000   \n",
      "3                    FN   71.000000   62.000000   74.000000   67.000000   \n",
      "4              Accuracy    0.832427    0.847661    0.828074    0.835691   \n",
      "5             Precision    0.835317    0.844000    0.834320    0.842105   \n",
      "6           Sensitivity    0.855691    0.871901    0.851107    0.869903   \n",
      "7           Specificity    0.805600    0.820700    0.800900    0.792100   \n",
      "8              F1 score    0.845382    0.857724    0.842629    0.855778   \n",
      "9   F1 score (weighted)    0.832242    0.847472    0.827899    0.835259   \n",
      "10     F1 score (macro)    0.831242    0.846895    0.826591    0.832440   \n",
      "11    Balanced Accuracy    0.830656    0.846295    0.826027    0.830991   \n",
      "12                  MCC    0.662771    0.694307    0.653380    0.665471   \n",
      "13                  NPV    0.828900    0.852000    0.820400    0.826900   \n",
      "14              ROC_AUC    0.830656    0.846295    0.826027    0.830991   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   408.000000  415.000000  458.000000  429.000000  397.000000  \n",
      "1   347.000000  357.000000  320.000000  343.000000  359.000000  \n",
      "2    83.000000   78.000000   84.000000   87.000000   90.000000  \n",
      "3    81.000000   69.000000   57.000000   60.000000   73.000000  \n",
      "4     0.821545    0.840044    0.846572    0.840044    0.822633  \n",
      "5     0.830957    0.841785    0.845018    0.831395    0.815195  \n",
      "6     0.834356    0.857438    0.889320    0.877301    0.844681  \n",
      "7     0.807000    0.820700    0.792100    0.797700    0.799600  \n",
      "8     0.832653    0.849539    0.866604    0.853731    0.829676  \n",
      "9     0.821519    0.839944    0.845880    0.839600    0.822497  \n",
      "10    0.820755    0.839404    0.843033    0.838630    0.822330  \n",
      "11    0.820666    0.839064    0.840700    0.837488    0.822118  \n",
      "12    0.641519    0.678970    0.687584    0.678733    0.645224  \n",
      "13    0.810700    0.838000    0.848800    0.851100    0.831000  \n",
      "14    0.820666    0.839064    0.840700    0.837488    0.822118  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_rf_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_rf_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_rf_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_rf_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_rf_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_rf_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_rf_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_rf_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_rf_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_rf_8)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 06:04:19,072] Trial 450 finished with value: 0.8321713822975513 and parameters: {'n_estimators': 522}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:06:58,784] Trial 451 finished with value: 0.8316196888564742 and parameters: {'n_estimators': 780}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:10:27,988] Trial 452 finished with value: 0.830809360469367 and parameters: {'n_estimators': 1000}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:13:33,963] Trial 453 finished with value: 0.8326765158691565 and parameters: {'n_estimators': 907}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:15:33,058] Trial 454 finished with value: 0.8327130531499611 and parameters: {'n_estimators': 580}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:17:24,347] Trial 455 finished with value: 0.8340913501028885 and parameters: {'n_estimators': 542}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:19:51,383] Trial 456 finished with value: 0.8316081925818649 and parameters: {'n_estimators': 722}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:22:39,344] Trial 457 finished with value: 0.8315900827796809 and parameters: {'n_estimators': 804}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:25:26,952] Trial 458 finished with value: 0.8318690022692404 and parameters: {'n_estimators': 824}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:28:04,320] Trial 459 finished with value: 0.8313436967803657 and parameters: {'n_estimators': 761}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:30:56,631] Trial 460 finished with value: 0.831591624889952 and parameters: {'n_estimators': 851}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:33:38,452] Trial 461 finished with value: 0.831325795079605 and parameters: {'n_estimators': 794}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:35:20,482] Trial 462 finished with value: 0.8321310155892794 and parameters: {'n_estimators': 499}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:38:29,448] Trial 463 finished with value: 0.8316235814708776 and parameters: {'n_estimators': 930}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:41:26,976] Trial 464 finished with value: 0.8321695950838999 and parameters: {'n_estimators': 870}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:43:23,395] Trial 465 finished with value: 0.832997424711923 and parameters: {'n_estimators': 565}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:46:00,542] Trial 466 finished with value: 0.83106867226749 and parameters: {'n_estimators': 773}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:47:31,354] Trial 467 finished with value: 0.8324160122854982 and parameters: {'n_estimators': 438}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:49:09,183] Trial 468 finished with value: 0.8326479461738983 and parameters: {'n_estimators': 481}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:52:02,468] Trial 469 finished with value: 0.8321473339214494 and parameters: {'n_estimators': 840}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:54:47,943] Trial 470 finished with value: 0.831320202109552 and parameters: {'n_estimators': 809}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:56:39,804] Trial 471 finished with value: 0.8335375379365104 and parameters: {'n_estimators': 540}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 06:59:15,085] Trial 472 finished with value: 0.8321385656682031 and parameters: {'n_estimators': 753}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:02:32,169] Trial 473 finished with value: 0.8310724657080542 and parameters: {'n_estimators': 981}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:05:21,714] Trial 474 finished with value: 0.8315909503386665 and parameters: {'n_estimators': 827}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:07:49,426] Trial 475 finished with value: 0.8315833216178706 and parameters: {'n_estimators': 729}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:11:06,102] Trial 476 finished with value: 0.8308065960386282 and parameters: {'n_estimators': 956}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:13:45,873] Trial 477 finished with value: 0.8316196888564742 and parameters: {'n_estimators': 780}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:16:45,554] Trial 478 finished with value: 0.8324223691228344 and parameters: {'n_estimators': 884}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:18:47,212] Trial 479 finished with value: 0.8327376045262849 and parameters: {'n_estimators': 598}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:20:33,661] Trial 480 finished with value: 0.8327108833722067 and parameters: {'n_estimators': 519}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:23:18,408] Trial 481 finished with value: 0.831320202109552 and parameters: {'n_estimators': 812}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:26:21,481] Trial 482 finished with value: 0.8327080933891822 and parameters: {'n_estimators': 898}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:29:16,705] Trial 483 finished with value: 0.831591624889952 and parameters: {'n_estimators': 855}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:32:01,842] Trial 484 finished with value: 0.8316115175960291 and parameters: {'n_estimators': 785}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:33:59,955] Trial 485 finished with value: 0.8332758062916581 and parameters: {'n_estimators': 560}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:35:37,602] Trial 486 finished with value: 0.8318427819448978 and parameters: {'n_estimators': 465}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:38:33,786] Trial 487 finished with value: 0.8318608310087955 and parameters: {'n_estimators': 839}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:41:19,106] Trial 488 finished with value: 0.831325795079605 and parameters: {'n_estimators': 796}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:43:56,026] Trial 489 finished with value: 0.8321385656682031 and parameters: {'n_estimators': 753}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:46:33,224] Trial 490 finished with value: 0.8318764758988995 and parameters: {'n_estimators': 763}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:48:56,018] Trial 491 finished with value: 0.831848032200637 and parameters: {'n_estimators': 699}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:50:12,406] Trial 492 finished with value: 0.8308272714229938 and parameters: {'n_estimators': 368}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:53:01,640] Trial 493 finished with value: 0.8318690022692404 and parameters: {'n_estimators': 821}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:55:13,044] Trial 494 finished with value: 0.8335461072763417 and parameters: {'n_estimators': 636}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 07:58:27,926] Trial 495 finished with value: 0.8305201643537581 and parameters: {'n_estimators': 958}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 08:01:28,615] Trial 496 finished with value: 0.8321695950838999 and parameters: {'n_estimators': 870}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 08:04:12,922] Trial 497 finished with value: 0.8315900827796809 and parameters: {'n_estimators': 799}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 08:06:05,826] Trial 498 finished with value: 0.8324182280610432 and parameters: {'n_estimators': 533}. Best is trial 27 with value: 0.8442177239059274.\n",
      "[I 2023-12-02 08:08:08,320] Trial 499 finished with value: 0.8326964441063209 and parameters: {'n_estimators': 577}. Best is trial 27 with value: 0.8442177239059274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  421.000000  422.000000  423.000000  448.000000   \n",
      "1                    TN  344.000000  357.000000  338.000000  320.000000   \n",
      "2                    FP   83.000000   78.000000   84.000000   84.000000   \n",
      "3                    FN   71.000000   62.000000   74.000000   67.000000   \n",
      "4              Accuracy    0.832427    0.847661    0.828074    0.835691   \n",
      "5             Precision    0.835317    0.844000    0.834320    0.842105   \n",
      "6           Sensitivity    0.855691    0.871901    0.851107    0.869903   \n",
      "7           Specificity    0.805600    0.820700    0.800900    0.792100   \n",
      "8              F1 score    0.845382    0.857724    0.842629    0.855778   \n",
      "9   F1 score (weighted)    0.832242    0.847472    0.827899    0.835259   \n",
      "10     F1 score (macro)    0.831242    0.846895    0.826591    0.832440   \n",
      "11    Balanced Accuracy    0.830656    0.846295    0.826027    0.830991   \n",
      "12                  MCC    0.662771    0.694307    0.653380    0.665471   \n",
      "13                  NPV    0.828900    0.852000    0.820400    0.826900   \n",
      "14              ROC_AUC    0.830656    0.846295    0.826027    0.830991   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   408.000000  415.000000  458.000000  429.000000  397.000000  420.000000  \n",
      "1   347.000000  357.000000  320.000000  343.000000  359.000000  375.000000  \n",
      "2    83.000000   78.000000   84.000000   87.000000   90.000000   75.000000  \n",
      "3    81.000000   69.000000   57.000000   60.000000   73.000000   49.000000  \n",
      "4     0.821545    0.840044    0.846572    0.840044    0.822633    0.865071  \n",
      "5     0.830957    0.841785    0.845018    0.831395    0.815195    0.848485  \n",
      "6     0.834356    0.857438    0.889320    0.877301    0.844681    0.895522  \n",
      "7     0.807000    0.820700    0.792100    0.797700    0.799600    0.833300  \n",
      "8     0.832653    0.849539    0.866604    0.853731    0.829676    0.871369  \n",
      "9     0.821519    0.839944    0.845880    0.839600    0.822497    0.864883  \n",
      "10    0.820755    0.839404    0.843033    0.838630    0.822330    0.864746  \n",
      "11    0.820666    0.839064    0.840700    0.837488    0.822118    0.864428  \n",
      "12    0.641519    0.678970    0.687584    0.678733    0.645224    0.730884  \n",
      "13    0.810700    0.838000    0.848800    0.851100    0.831000    0.884400  \n",
      "14    0.820666    0.839064    0.840700    0.837488    0.822118    0.864428  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_rf_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_rf_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_rf_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_rf_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_rf_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_rf_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_rf_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_rf_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_rf_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_rf_9)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })  \n",
    "\n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8442\n",
      "\tBest params:\n",
      "\t\tn_estimators: 528\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Optimization History Plot'}, xlabel='Trial', ylabel='Objective Value'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>208.800000</td>\n",
       "      <td>9.077445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>175.900000</td>\n",
       "      <td>9.960477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>42.900000</td>\n",
       "      <td>8.116513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>3.169297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.837767</td>\n",
       "      <td>0.017906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.829880</td>\n",
       "      <td>0.029032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.868559</td>\n",
       "      <td>0.011850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.804190</td>\n",
       "      <td>0.033103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.848517</td>\n",
       "      <td>0.016894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.837468</td>\n",
       "      <td>0.018120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.836724</td>\n",
       "      <td>0.017944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.836375</td>\n",
       "      <td>0.017637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.675041</td>\n",
       "      <td>0.034042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.847460</td>\n",
       "      <td>0.016125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.836375</td>\n",
       "      <td>0.017637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       208.800000     9.077445\n",
       "1                    TN       175.900000     9.960477\n",
       "2                    FP        42.900000     8.116513\n",
       "3                    FN        31.600000     3.169297\n",
       "4              Accuracy         0.837767     0.017906\n",
       "5             Precision         0.829880     0.029032\n",
       "6           Sensitivity         0.868559     0.011850\n",
       "7           Specificity         0.804190     0.033103\n",
       "8              F1 score         0.848517     0.016894\n",
       "9   F1 score (weighted)         0.837468     0.018120\n",
       "10     F1 score (macro)         0.836724     0.017944\n",
       "11    Balanced Accuracy         0.836375     0.017637\n",
       "12                  MCC         0.675041     0.034042\n",
       "13                  NPV         0.847460     0.016125\n",
       "14              ROC_AUC         0.836375     0.017637"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>458.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>424.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>346.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>66.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.832427</td>\n",
       "      <td>0.847661</td>\n",
       "      <td>0.828074</td>\n",
       "      <td>0.835691</td>\n",
       "      <td>0.821545</td>\n",
       "      <td>0.840044</td>\n",
       "      <td>0.846572</td>\n",
       "      <td>0.840044</td>\n",
       "      <td>0.822633</td>\n",
       "      <td>0.865071</td>\n",
       "      <td>0.837976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.835317</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.834320</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.830957</td>\n",
       "      <td>0.841785</td>\n",
       "      <td>0.845018</td>\n",
       "      <td>0.831395</td>\n",
       "      <td>0.815195</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.836858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.855691</td>\n",
       "      <td>0.871901</td>\n",
       "      <td>0.851107</td>\n",
       "      <td>0.869903</td>\n",
       "      <td>0.834356</td>\n",
       "      <td>0.857438</td>\n",
       "      <td>0.889320</td>\n",
       "      <td>0.877301</td>\n",
       "      <td>0.844681</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.864722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.805600</td>\n",
       "      <td>0.820700</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.792100</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.820700</td>\n",
       "      <td>0.792100</td>\n",
       "      <td>0.797700</td>\n",
       "      <td>0.799600</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.806970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.845382</td>\n",
       "      <td>0.857724</td>\n",
       "      <td>0.842629</td>\n",
       "      <td>0.855778</td>\n",
       "      <td>0.832653</td>\n",
       "      <td>0.849539</td>\n",
       "      <td>0.866604</td>\n",
       "      <td>0.853731</td>\n",
       "      <td>0.829676</td>\n",
       "      <td>0.871369</td>\n",
       "      <td>0.850509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.832242</td>\n",
       "      <td>0.847472</td>\n",
       "      <td>0.827899</td>\n",
       "      <td>0.835259</td>\n",
       "      <td>0.821519</td>\n",
       "      <td>0.839944</td>\n",
       "      <td>0.845880</td>\n",
       "      <td>0.839600</td>\n",
       "      <td>0.822497</td>\n",
       "      <td>0.864883</td>\n",
       "      <td>0.837720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.831242</td>\n",
       "      <td>0.846895</td>\n",
       "      <td>0.826591</td>\n",
       "      <td>0.832440</td>\n",
       "      <td>0.820755</td>\n",
       "      <td>0.839404</td>\n",
       "      <td>0.843033</td>\n",
       "      <td>0.838630</td>\n",
       "      <td>0.822330</td>\n",
       "      <td>0.864746</td>\n",
       "      <td>0.836607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.830656</td>\n",
       "      <td>0.846295</td>\n",
       "      <td>0.826027</td>\n",
       "      <td>0.830991</td>\n",
       "      <td>0.820666</td>\n",
       "      <td>0.839064</td>\n",
       "      <td>0.840700</td>\n",
       "      <td>0.837488</td>\n",
       "      <td>0.822118</td>\n",
       "      <td>0.864428</td>\n",
       "      <td>0.835843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.662771</td>\n",
       "      <td>0.694307</td>\n",
       "      <td>0.653380</td>\n",
       "      <td>0.665471</td>\n",
       "      <td>0.641519</td>\n",
       "      <td>0.678970</td>\n",
       "      <td>0.687584</td>\n",
       "      <td>0.678733</td>\n",
       "      <td>0.645224</td>\n",
       "      <td>0.730884</td>\n",
       "      <td>0.673884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.828900</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.820400</td>\n",
       "      <td>0.826900</td>\n",
       "      <td>0.810700</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.848800</td>\n",
       "      <td>0.851100</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.884400</td>\n",
       "      <td>0.839220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.830656</td>\n",
       "      <td>0.846295</td>\n",
       "      <td>0.826027</td>\n",
       "      <td>0.830991</td>\n",
       "      <td>0.820666</td>\n",
       "      <td>0.839064</td>\n",
       "      <td>0.840700</td>\n",
       "      <td>0.837488</td>\n",
       "      <td>0.822118</td>\n",
       "      <td>0.864428</td>\n",
       "      <td>0.835843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  421.000000  422.000000  423.000000  448.000000   \n",
       "1                    TN  344.000000  357.000000  338.000000  320.000000   \n",
       "2                    FP   83.000000   78.000000   84.000000   84.000000   \n",
       "3                    FN   71.000000   62.000000   74.000000   67.000000   \n",
       "4              Accuracy    0.832427    0.847661    0.828074    0.835691   \n",
       "5             Precision    0.835317    0.844000    0.834320    0.842105   \n",
       "6           Sensitivity    0.855691    0.871901    0.851107    0.869903   \n",
       "7           Specificity    0.805600    0.820700    0.800900    0.792100   \n",
       "8              F1 score    0.845382    0.857724    0.842629    0.855778   \n",
       "9   F1 score (weighted)    0.832242    0.847472    0.827899    0.835259   \n",
       "10     F1 score (macro)    0.831242    0.846895    0.826591    0.832440   \n",
       "11    Balanced Accuracy    0.830656    0.846295    0.826027    0.830991   \n",
       "12                  MCC    0.662771    0.694307    0.653380    0.665471   \n",
       "13                  NPV    0.828900    0.852000    0.820400    0.826900   \n",
       "14              ROC_AUC    0.830656    0.846295    0.826027    0.830991   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   408.000000  415.000000  458.000000  429.000000  397.000000  420.000000   \n",
       "1   347.000000  357.000000  320.000000  343.000000  359.000000  375.000000   \n",
       "2    83.000000   78.000000   84.000000   87.000000   90.000000   75.000000   \n",
       "3    81.000000   69.000000   57.000000   60.000000   73.000000   49.000000   \n",
       "4     0.821545    0.840044    0.846572    0.840044    0.822633    0.865071   \n",
       "5     0.830957    0.841785    0.845018    0.831395    0.815195    0.848485   \n",
       "6     0.834356    0.857438    0.889320    0.877301    0.844681    0.895522   \n",
       "7     0.807000    0.820700    0.792100    0.797700    0.799600    0.833300   \n",
       "8     0.832653    0.849539    0.866604    0.853731    0.829676    0.871369   \n",
       "9     0.821519    0.839944    0.845880    0.839600    0.822497    0.864883   \n",
       "10    0.820755    0.839404    0.843033    0.838630    0.822330    0.864746   \n",
       "11    0.820666    0.839064    0.840700    0.837488    0.822118    0.864428   \n",
       "12    0.641519    0.678970    0.687584    0.678733    0.645224    0.730884   \n",
       "13    0.810700    0.838000    0.848800    0.851100    0.831000    0.884400   \n",
       "14    0.820666    0.839064    0.840700    0.837488    0.822118    0.864428   \n",
       "\n",
       "           ave  \n",
       "0   424.100000  \n",
       "1   346.000000  \n",
       "2    82.600000  \n",
       "3    66.300000  \n",
       "4     0.837976  \n",
       "5     0.836858  \n",
       "6     0.864722  \n",
       "7     0.806970  \n",
       "8     0.850509  \n",
       "9     0.837720  \n",
       "10    0.836607  \n",
       "11    0.835843  \n",
       "12    0.673884  \n",
       "13    0.839220  \n",
       "14    0.835843  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.841246</td>\n",
       "      <td>0.016030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.830379</td>\n",
       "      <td>0.022069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.875894</td>\n",
       "      <td>0.021630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.803220</td>\n",
       "      <td>0.026590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.840876</td>\n",
       "      <td>0.016088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.840126</td>\n",
       "      <td>0.016147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.839558</td>\n",
       "      <td>0.016183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.682271</td>\n",
       "      <td>0.032250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.855076</td>\n",
       "      <td>0.023764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.839558</td>\n",
       "      <td>0.016183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.841246     0.016030\n",
       "1             Precision         0.830379     0.022069\n",
       "2           Sensitivity         0.875894     0.021630\n",
       "3           Specificity         0.803220     0.026590\n",
       "4              F1 score         0.852273     0.015900\n",
       "5   F1 score (weighted)         0.840876     0.016088\n",
       "6      F1 score (macro)         0.840126     0.016147\n",
       "7     Balanced Accuracy         0.839558     0.016183\n",
       "8                   MCC         0.682271     0.032250\n",
       "9                   NPV         0.855076     0.023764\n",
       "10              ROC_AUC         0.839558     0.016183"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=1121218, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "   \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_rf)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "    \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_rf))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_rf))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_rf))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_rf))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_rf, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_rf, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_rf))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_rf))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_rf))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model f1_score 0.8290 with a standard deviation of 0.0218\n",
      "RF optimized model f1_score 0.8322 with a standard deviation of 0.0220\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"RF baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf_clf.joblib']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the modesls, both the one with optimized hyperparameters and the initial one\n",
    "joblib.dump(rf_clf, \"OUTPUT/rf_clf.joblib\")\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf_clf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       206.500000     9.119576\n",
      "1                    TN       177.500000     8.897565\n",
      "2                    FP        41.300000     6.549809\n",
      "3                    FN        33.900000     4.724640\n",
      "4              Accuracy         0.836245     0.014583\n",
      "5             Precision         0.833452     0.024372\n",
      "6           Sensitivity         0.859077     0.018360\n",
      "7           Specificity         0.811500     0.026295\n",
      "8              F1 score         0.845795     0.014945\n",
      "9   F1 score (weighted)         0.836079     0.014711\n",
      "10     F1 score (macro)         0.835397     0.014524\n",
      "11    Balanced Accuracy         0.835285     0.014555\n",
      "12                  MCC         0.671828     0.028234\n",
      "13                  NPV         0.839640     0.020181\n",
      "14              ROC_AUC         0.835285     0.014555\n",
      "CPU times: user 29.7 s, sys: 176 ms, total: 29.9 s\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_clf = lgbm.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=16,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_clf.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"logloss\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_clf.predict(X_test) \n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggestegorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=16,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggestegorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "  \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=16,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:21:54,537] A new study created in memory with name: LGBMClassifier\n",
      "[I 2023-12-04 09:21:56,216] Trial 0 finished with value: 0.8310635802908269 and parameters: {'n_estimators': 764, 'learning_rate': 0.19464600839496699, 'max_depth': 6, 'max_bin': 230, 'num_leaves': 291}. Best is trial 0 with value: 0.8310635802908269.\n",
      "[I 2023-12-04 09:21:57,718] Trial 1 finished with value: 0.7930928373904333 and parameters: {'n_estimators': 236, 'learning_rate': 0.04133121877691464, 'max_depth': 4, 'max_bin': 280, 'num_leaves': 286}. Best is trial 0 with value: 0.8310635802908269.\n",
      "[I 2023-12-04 09:21:59,875] Trial 2 finished with value: 0.832571410803211 and parameters: {'n_estimators': 773, 'learning_rate': 0.1289507840886797, 'max_depth': 7, 'max_bin': 288, 'num_leaves': 196}. Best is trial 2 with value: 0.832571410803211.\n",
      "[I 2023-12-04 09:22:06,113] Trial 3 finished with value: 0.8307280139601657 and parameters: {'n_estimators': 650, 'learning_rate': 0.030073005085240856, 'max_depth': 9, 'max_bin': 196, 'num_leaves': 286}. Best is trial 2 with value: 0.832571410803211.\n",
      "[I 2023-12-04 09:22:09,114] Trial 4 finished with value: 0.837144196851335 and parameters: {'n_estimators': 531, 'learning_rate': 0.10611300930201889, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 727}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:12,432] Trial 5 finished with value: 0.7871696161061928 and parameters: {'n_estimators': 584, 'learning_rate': 0.012386679158611694, 'max_depth': 4, 'max_bin': 203, 'num_leaves': 492}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:18,203] Trial 6 finished with value: 0.8279805210220254 and parameters: {'n_estimators': 595, 'learning_rate': 0.03091941073557726, 'max_depth': 9, 'max_bin': 151, 'num_leaves': 656}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:19,811] Trial 7 finished with value: 0.8241180926275575 and parameters: {'n_estimators': 283, 'learning_rate': 0.16256228111147503, 'max_depth': 5, 'max_bin': 217, 'num_leaves': 644}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:21,324] Trial 8 finished with value: 0.8246943042112967 and parameters: {'n_estimators': 451, 'learning_rate': 0.19704462772002923, 'max_depth': 4, 'max_bin': 207, 'num_leaves': 697}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:28,951] Trial 9 finished with value: 0.8289838403653847 and parameters: {'n_estimators': 837, 'learning_rate': 0.021598545625759247, 'max_depth': 10, 'max_bin': 287, 'num_leaves': 699}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:30,292] Trial 10 finished with value: 0.8226678202442604 and parameters: {'n_estimators': 61, 'learning_rate': 0.08873376516687179, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 68}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:32,397] Trial 11 finished with value: 0.8318014728746672 and parameters: {'n_estimators': 426, 'learning_rate': 0.1250299027745909, 'max_depth': 7, 'max_bin': 299, 'num_leaves': 74}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:35,215] Trial 12 finished with value: 0.8370573940820953 and parameters: {'n_estimators': 740, 'learning_rate': 0.10853798363931053, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 470}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:38,446] Trial 13 finished with value: 0.836777976283235 and parameters: {'n_estimators': 891, 'learning_rate': 0.08260998372283532, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 475}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:41,972] Trial 14 finished with value: 0.8342137149361978 and parameters: {'n_estimators': 679, 'learning_rate': 0.06923787086443191, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 534}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:44,641] Trial 15 finished with value: 0.8349442849802564 and parameters: {'n_estimators': 503, 'learning_rate': 0.11397578975827549, 'max_depth': 10, 'max_bin': 239, 'num_leaves': 581}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:48,345] Trial 16 finished with value: 0.8367010526020042 and parameters: {'n_estimators': 352, 'learning_rate': 0.0626690437278449, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 405}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:51,227] Trial 17 finished with value: 0.8362924927965449 and parameters: {'n_estimators': 712, 'learning_rate': 0.09590763818255142, 'max_depth': 9, 'max_bin': 179, 'num_leaves': 740}. Best is trial 4 with value: 0.837144196851335.\n",
      "[I 2023-12-04 09:22:53,561] Trial 18 finished with value: 0.8420546677607827 and parameters: {'n_estimators': 565, 'learning_rate': 0.1469074717142049, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 397}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:22:55,834] Trial 19 finished with value: 0.8360826105838737 and parameters: {'n_estimators': 524, 'learning_rate': 0.1473572149823996, 'max_depth': 10, 'max_bin': 243, 'num_leaves': 375}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:22:58,018] Trial 20 finished with value: 0.8398301822099364 and parameters: {'n_estimators': 168, 'learning_rate': 0.14555530250469148, 'max_depth': 11, 'max_bin': 299, 'num_leaves': 222}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:22:59,353] Trial 21 finished with value: 0.8277555093574241 and parameters: {'n_estimators': 72, 'learning_rate': 0.14762078305251927, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 191}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:01,365] Trial 22 finished with value: 0.8379896258553996 and parameters: {'n_estimators': 158, 'learning_rate': 0.1654338012097769, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 143}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:03,240] Trial 23 finished with value: 0.837496020438922 and parameters: {'n_estimators': 192, 'learning_rate': 0.1716099927566851, 'max_depth': 8, 'max_bin': 277, 'num_leaves': 172}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:05,098] Trial 24 finished with value: 0.8361876156974587 and parameters: {'n_estimators': 140, 'learning_rate': 0.1714662819209877, 'max_depth': 11, 'max_bin': 240, 'num_leaves': 128}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:07,494] Trial 25 finished with value: 0.8374158868156399 and parameters: {'n_estimators': 346, 'learning_rate': 0.1370233930431169, 'max_depth': 11, 'max_bin': 268, 'num_leaves': 346}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:09,058] Trial 26 finished with value: 0.8335037478712811 and parameters: {'n_estimators': 128, 'learning_rate': 0.15480378518743276, 'max_depth': 8, 'max_bin': 294, 'num_leaves': 237}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:10,902] Trial 27 finished with value: 0.8348143062041675 and parameters: {'n_estimators': 371, 'learning_rate': 0.18162248753524313, 'max_depth': 10, 'max_bin': 251, 'num_leaves': 116}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:13,063] Trial 28 finished with value: 0.829321102336175 and parameters: {'n_estimators': 275, 'learning_rate': 0.13973015424322918, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 338}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:14,517] Trial 29 finished with value: 0.8265583051715172 and parameters: {'n_estimators': 178, 'learning_rate': 0.18757135160415295, 'max_depth': 6, 'max_bin': 272, 'num_leaves': 239}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:16,426] Trial 30 finished with value: 0.8340537314066021 and parameters: {'n_estimators': 400, 'learning_rate': 0.16041916101372095, 'max_depth': 9, 'max_bin': 232, 'num_leaves': 128}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:18,376] Trial 31 finished with value: 0.8302370452334611 and parameters: {'n_estimators': 212, 'learning_rate': 0.1744444834585702, 'max_depth': 8, 'max_bin': 279, 'num_leaves': 190}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:20,180] Trial 32 finished with value: 0.8326378173814378 and parameters: {'n_estimators': 295, 'learning_rate': 0.19932371340140603, 'max_depth': 10, 'max_bin': 285, 'num_leaves': 38}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:21,014] Trial 33 finished with value: 0.7908439971975089 and parameters: {'n_estimators': 131, 'learning_rate': 0.1649771650119842, 'max_depth': 3, 'max_bin': 289, 'num_leaves': 422}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:22,690] Trial 34 finished with value: 0.8291763123006122 and parameters: {'n_estimators': 223, 'learning_rate': 0.18123899630566737, 'max_depth': 6, 'max_bin': 273, 'num_leaves': 239}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:24,713] Trial 35 finished with value: 0.8271078080498512 and parameters: {'n_estimators': 188, 'learning_rate': 0.12261397061211803, 'max_depth': 8, 'max_bin': 249, 'num_leaves': 294}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:27,031] Trial 36 finished with value: 0.8375360231945749 and parameters: {'n_estimators': 609, 'learning_rate': 0.15158019981732632, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 160}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:29,526] Trial 37 finished with value: 0.8405997168924333 and parameters: {'n_estimators': 594, 'learning_rate': 0.1374494246583986, 'max_depth': 11, 'max_bin': 264, 'num_leaves': 288}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:31,913] Trial 38 finished with value: 0.8348818297558159 and parameters: {'n_estimators': 484, 'learning_rate': 0.13501827224048324, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 295}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:34,558] Trial 39 finished with value: 0.8368107070888297 and parameters: {'n_estimators': 563, 'learning_rate': 0.12053827647287399, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 263}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:36,990] Trial 40 finished with value: 0.8348705101939131 and parameters: {'n_estimators': 659, 'learning_rate': 0.1307781495075292, 'max_depth': 10, 'max_bin': 291, 'num_leaves': 324}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:39,195] Trial 41 finished with value: 0.8364477006380223 and parameters: {'n_estimators': 604, 'learning_rate': 0.1513413533958781, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 161}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:41,612] Trial 42 finished with value: 0.837291190845424 and parameters: {'n_estimators': 636, 'learning_rate': 0.14448182847227023, 'max_depth': 11, 'max_bin': 264, 'num_leaves': 219}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:43,801] Trial 43 finished with value: 0.8342006869419828 and parameters: {'n_estimators': 564, 'learning_rate': 0.15733073616639284, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 149}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:46,233] Trial 44 finished with value: 0.8363805925810868 and parameters: {'n_estimators': 624, 'learning_rate': 0.14218197555042897, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 76}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:48,475] Trial 45 finished with value: 0.8328853938103846 and parameters: {'n_estimators': 694, 'learning_rate': 0.16203303396382973, 'max_depth': 10, 'max_bin': 295, 'num_leaves': 435}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:50,746] Trial 46 finished with value: 0.8366063743435074 and parameters: {'n_estimators': 757, 'learning_rate': 0.12785980425861304, 'max_depth': 9, 'max_bin': 268, 'num_leaves': 278}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:52,972] Trial 47 finished with value: 0.8329352770152878 and parameters: {'n_estimators': 461, 'learning_rate': 0.14915541127057663, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 376}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:55,407] Trial 48 finished with value: 0.8380305329922558 and parameters: {'n_estimators': 538, 'learning_rate': 0.1327952155763669, 'max_depth': 12, 'max_bin': 166, 'num_leaves': 212}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:23:57,995] Trial 49 finished with value: 0.8410642626765592 and parameters: {'n_estimators': 812, 'learning_rate': 0.11790770060806492, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 217}. Best is trial 18 with value: 0.8420546677607827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8421\n",
      "\tBest params:\n",
      "\t\tn_estimators: 565\n",
      "\t\tlearning_rate: 0.1469074717142049\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 397\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  418.000000\n",
      "1                    TN  335.000000\n",
      "2                    FP   92.000000\n",
      "3                    FN   74.000000\n",
      "4              Accuracy    0.819369\n",
      "5             Precision    0.819608\n",
      "6           Sensitivity    0.849593\n",
      "7           Specificity    0.784500\n",
      "8              F1 score    0.834331\n",
      "9   F1 score (weighted)    0.819047\n",
      "10     F1 score (macro)    0.817883\n",
      "11    Balanced Accuracy    0.817068\n",
      "12                  MCC    0.636404\n",
      "13                  NPV    0.819100\n",
      "14              ROC_AUC    0.817068\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_lgbm_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_lgbm_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_lgbm_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_lgbm_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_lgbm_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_lgbm_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_lgbm_0)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:24:00,829] Trial 50 finished with value: 0.8252737509966049 and parameters: {'n_estimators': 806, 'learning_rate': 0.11468918030689715, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 314}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:03,220] Trial 51 finished with value: 0.8253481612408551 and parameters: {'n_estimators': 521, 'learning_rate': 0.13328607232150835, 'max_depth': 12, 'max_bin': 166, 'num_leaves': 217}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:05,386] Trial 52 finished with value: 0.8256892137575027 and parameters: {'n_estimators': 892, 'learning_rate': 0.13932174695191077, 'max_depth': 12, 'max_bin': 152, 'num_leaves': 100}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:07,874] Trial 53 finished with value: 0.8257796658309344 and parameters: {'n_estimators': 563, 'learning_rate': 0.12759336497594148, 'max_depth': 12, 'max_bin': 184, 'num_leaves': 261}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:10,734] Trial 54 finished with value: 0.8254208937870728 and parameters: {'n_estimators': 805, 'learning_rate': 0.10289490883348687, 'max_depth': 11, 'max_bin': 170, 'num_leaves': 204}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:13,151] Trial 55 finished with value: 0.8259290396862375 and parameters: {'n_estimators': 459, 'learning_rate': 0.11752769273224581, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 188}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:15,779] Trial 56 finished with value: 0.8269878619298865 and parameters: {'n_estimators': 434, 'learning_rate': 0.10846113479749539, 'max_depth': 10, 'max_bin': 150, 'num_leaves': 358}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:17,406] Trial 57 finished with value: 0.8195057172782244 and parameters: {'n_estimators': 93, 'learning_rate': 0.13233365738796282, 'max_depth': 11, 'max_bin': 188, 'num_leaves': 240}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:19,810] Trial 58 finished with value: 0.8235459062872987 and parameters: {'n_estimators': 726, 'learning_rate': 0.14145601804162222, 'max_depth': 12, 'max_bin': 193, 'num_leaves': 501}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:21,780] Trial 59 finished with value: 0.816435341261019 and parameters: {'n_estimators': 498, 'learning_rate': 0.12058205515253086, 'max_depth': 5, 'max_bin': 173, 'num_leaves': 313}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:23,868] Trial 60 finished with value: 0.8254452509859259 and parameters: {'n_estimators': 848, 'learning_rate': 0.1563786016415947, 'max_depth': 10, 'max_bin': 156, 'num_leaves': 142}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:26,185] Trial 61 finished with value: 0.8266790842344574 and parameters: {'n_estimators': 593, 'learning_rate': 0.1483483756195092, 'max_depth': 11, 'max_bin': 243, 'num_leaves': 156}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:28,403] Trial 62 finished with value: 0.8263972459674619 and parameters: {'n_estimators': 664, 'learning_rate': 0.15345108642312394, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 175}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:30,413] Trial 63 finished with value: 0.8210876277381333 and parameters: {'n_estimators': 547, 'learning_rate': 0.16442682363581193, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 112}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:32,761] Trial 64 finished with value: 0.8252595050296685 and parameters: {'n_estimators': 619, 'learning_rate': 0.14535803196267313, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 211}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:34,980] Trial 65 finished with value: 0.8239232159680199 and parameters: {'n_estimators': 264, 'learning_rate': 0.13591839921830653, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 44}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:37,363] Trial 66 finished with value: 0.8161551553201815 and parameters: {'n_estimators': 326, 'learning_rate': 0.12515855744260868, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 265}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:39,479] Trial 67 finished with value: 0.8232720394901636 and parameters: {'n_estimators': 413, 'learning_rate': 0.1518817170642267, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 97}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:41,134] Trial 68 finished with value: 0.817854803727266 and parameters: {'n_estimators': 157, 'learning_rate': 0.11264909444237993, 'max_depth': 7, 'max_bin': 202, 'num_leaves': 178}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:43,056] Trial 69 finished with value: 0.8156876366543886 and parameters: {'n_estimators': 700, 'learning_rate': 0.16887647606040376, 'max_depth': 10, 'max_bin': 285, 'num_leaves': 133}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:45,178] Trial 70 finished with value: 0.8199046755712522 and parameters: {'n_estimators': 522, 'learning_rate': 0.15621514314890106, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 418}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:46,209] Trial 71 finished with value: 0.8092262656788322 and parameters: {'n_estimators': 115, 'learning_rate': 0.1682943267255029, 'max_depth': 5, 'max_bin': 296, 'num_leaves': 160}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:48,195] Trial 72 finished with value: 0.8260044533044495 and parameters: {'n_estimators': 183, 'learning_rate': 0.17673647385210034, 'max_depth': 9, 'max_bin': 274, 'num_leaves': 224}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:49,737] Trial 73 finished with value: 0.8186891999201471 and parameters: {'n_estimators': 88, 'learning_rate': 0.16160235649414653, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 176}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:50,829] Trial 74 finished with value: 0.7910973401765595 and parameters: {'n_estimators': 198, 'learning_rate': 0.13729797061361906, 'max_depth': 3, 'max_bin': 291, 'num_leaves': 249}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:52,793] Trial 75 finished with value: 0.817063161314208 and parameters: {'n_estimators': 240, 'learning_rate': 0.1898593931151888, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 194}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:54,837] Trial 76 finished with value: 0.8167746921598343 and parameters: {'n_estimators': 589, 'learning_rate': 0.14280758485946277, 'max_depth': 8, 'max_bin': 280, 'num_leaves': 288}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:56,763] Trial 77 finished with value: 0.8198995110137414 and parameters: {'n_estimators': 159, 'learning_rate': 0.17402061596406135, 'max_depth': 10, 'max_bin': 262, 'num_leaves': 449}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:24:59,357] Trial 78 finished with value: 0.8268715904487058 and parameters: {'n_estimators': 380, 'learning_rate': 0.12778824017654597, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 399}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:01,623] Trial 79 finished with value: 0.828185742649963 and parameters: {'n_estimators': 539, 'learning_rate': 0.15831373469079604, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 87}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:03,897] Trial 80 finished with value: 0.8201756601785224 and parameters: {'n_estimators': 243, 'learning_rate': 0.149951870406566, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 557}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:06,304] Trial 81 finished with value: 0.8229616863368318 and parameters: {'n_estimators': 306, 'learning_rate': 0.13645164288800246, 'max_depth': 11, 'max_bin': 265, 'num_leaves': 340}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:07,530] Trial 82 finished with value: 0.8088654379603657 and parameters: {'n_estimators': 54, 'learning_rate': 0.14470267027570102, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 360}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:09,856] Trial 83 finished with value: 0.8243973512817451 and parameters: {'n_estimators': 342, 'learning_rate': 0.1293378213839377, 'max_depth': 10, 'max_bin': 277, 'num_leaves': 278}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:11,952] Trial 84 finished with value: 0.8256724262111454 and parameters: {'n_estimators': 484, 'learning_rate': 0.16636139055612545, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 225}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:14,336] Trial 85 finished with value: 0.8252438989661547 and parameters: {'n_estimators': 568, 'learning_rate': 0.14035836054832737, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 323}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:16,591] Trial 86 finished with value: 0.8208263142546384 and parameters: {'n_estimators': 154, 'learning_rate': 0.12289650393411297, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 383}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:18,824] Trial 87 finished with value: 0.8254218945050414 and parameters: {'n_estimators': 216, 'learning_rate': 0.1319292943718831, 'max_depth': 9, 'max_bin': 298, 'num_leaves': 303}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:20,909] Trial 88 finished with value: 0.816900019080097 and parameters: {'n_estimators': 647, 'learning_rate': 0.15255024033110298, 'max_depth': 10, 'max_bin': 156, 'num_leaves': 248}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:22,815] Trial 89 finished with value: 0.8170808484100538 and parameters: {'n_estimators': 117, 'learning_rate': 0.16020562733482074, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 335}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:25,194] Trial 90 finished with value: 0.8249234384438686 and parameters: {'n_estimators': 257, 'learning_rate': 0.146129812662122, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 203}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:27,387] Trial 91 finished with value: 0.8251795592574904 and parameters: {'n_estimators': 624, 'learning_rate': 0.14095673852323834, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 221}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:29,733] Trial 92 finished with value: 0.8241989229419522 and parameters: {'n_estimators': 643, 'learning_rate': 0.13445598606461054, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 124}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:32,047] Trial 93 finished with value: 0.8276515131882738 and parameters: {'n_estimators': 504, 'learning_rate': 0.1460357983117253, 'max_depth': 11, 'max_bin': 236, 'num_leaves': 173}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:34,228] Trial 94 finished with value: 0.8257935520902739 and parameters: {'n_estimators': 863, 'learning_rate': 0.15391596988748535, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 145}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:36,669] Trial 95 finished with value: 0.8245632875116998 and parameters: {'n_estimators': 777, 'learning_rate': 0.12494522159743109, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 267}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:38,968] Trial 96 finished with value: 0.8238158763824108 and parameters: {'n_estimators': 617, 'learning_rate': 0.13732119673964796, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 361}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:41,278] Trial 97 finished with value: 0.8220297248349651 and parameters: {'n_estimators': 678, 'learning_rate': 0.13075650976937653, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 203}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:43,543] Trial 98 finished with value: 0.8245863649868005 and parameters: {'n_estimators': 598, 'learning_rate': 0.14954264534037814, 'max_depth': 10, 'max_bin': 231, 'num_leaves': 231}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:45,279] Trial 99 finished with value: 0.8165247152228969 and parameters: {'n_estimators': 572, 'learning_rate': 0.16474830055018505, 'max_depth': 6, 'max_bin': 248, 'num_leaves': 618}. Best is trial 18 with value: 0.8420546677607827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8421\n",
      "\tBest params:\n",
      "\t\tn_estimators: 565\n",
      "\t\tlearning_rate: 0.1469074717142049\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 397\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  418.000000  416.000000\n",
      "1                    TN  335.000000  359.000000\n",
      "2                    FP   92.000000   76.000000\n",
      "3                    FN   74.000000   68.000000\n",
      "4              Accuracy    0.819369    0.843308\n",
      "5             Precision    0.819608    0.845528\n",
      "6           Sensitivity    0.849593    0.859504\n",
      "7           Specificity    0.784500    0.825300\n",
      "8              F1 score    0.834331    0.852459\n",
      "9   F1 score (weighted)    0.819047    0.843223\n",
      "10     F1 score (macro)    0.817883    0.842703\n",
      "11    Balanced Accuracy    0.817068    0.842396\n",
      "12                  MCC    0.636404    0.685534\n",
      "13                  NPV    0.819100    0.840700\n",
      "14              ROC_AUC    0.817068    0.842396\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_lgbm_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_lgbm_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_lgbm_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_lgbm_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_lgbm_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_lgbm_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_lgbm_1)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:25:47,567] Trial 100 finished with value: 0.8227133816608495 and parameters: {'n_estimators': 555, 'learning_rate': 0.14212038283507245, 'max_depth': 7, 'max_bin': 175, 'num_leaves': 168}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:50,427] Trial 101 finished with value: 0.8269822332779875 and parameters: {'n_estimators': 524, 'learning_rate': 0.11764027009038805, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 722}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:52,157] Trial 102 finished with value: 0.8205767276847462 and parameters: {'n_estimators': 537, 'learning_rate': 0.15734233095860684, 'max_depth': 4, 'max_bin': 260, 'num_leaves': 651}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:54,255] Trial 103 finished with value: 0.8222778387896674 and parameters: {'n_estimators': 498, 'learning_rate': 0.17053912343869299, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 610}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:56,586] Trial 104 finished with value: 0.8302135783300184 and parameters: {'n_estimators': 438, 'learning_rate': 0.1469693544214799, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 670}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:25:58,796] Trial 105 finished with value: 0.8289332552706922 and parameters: {'n_estimators': 581, 'learning_rate': 0.16158613891722307, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 514}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:01,038] Trial 106 finished with value: 0.825300845366095 and parameters: {'n_estimators': 475, 'learning_rate': 0.13361016378031026, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 186}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:03,322] Trial 107 finished with value: 0.8237154516686249 and parameters: {'n_estimators': 203, 'learning_rate': 0.1388305119906467, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 115}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:05,466] Trial 108 finished with value: 0.8265812374326973 and parameters: {'n_estimators': 167, 'learning_rate': 0.15328526222006897, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 213}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:07,431] Trial 109 finished with value: 0.8176850858310875 and parameters: {'n_estimators': 610, 'learning_rate': 0.1255857891644028, 'max_depth': 5, 'max_bin': 213, 'num_leaves': 258}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:10,072] Trial 110 finished with value: 0.8307173105911559 and parameters: {'n_estimators': 637, 'learning_rate': 0.10969094401557482, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 153}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:12,624] Trial 111 finished with value: 0.833465951322005 and parameters: {'n_estimators': 720, 'learning_rate': 0.12093972835082034, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 479}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:15,419] Trial 112 finished with value: 0.824768336518605 and parameters: {'n_estimators': 545, 'learning_rate': 0.10235755614937579, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 415}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:18,378] Trial 113 finished with value: 0.8295968912712912 and parameters: {'n_estimators': 820, 'learning_rate': 0.0957455728583099, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 745}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:20,891] Trial 114 finished with value: 0.8303337884501154 and parameters: {'n_estimators': 741, 'learning_rate': 0.11739157900985231, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 462}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:22,993] Trial 115 finished with value: 0.8296160698289368 and parameters: {'n_estimators': 775, 'learning_rate': 0.14312533263321683, 'max_depth': 11, 'max_bin': 243, 'num_leaves': 57}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:25,325] Trial 116 finished with value: 0.8200524282112909 and parameters: {'n_estimators': 871, 'learning_rate': 0.12940986636669466, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 303}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:27,781] Trial 117 finished with value: 0.8287648566323167 and parameters: {'n_estimators': 795, 'learning_rate': 0.10755356692413737, 'max_depth': 10, 'max_bin': 258, 'num_leaves': 186}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:29,974] Trial 118 finished with value: 0.8295686066490104 and parameters: {'n_estimators': 141, 'learning_rate': 0.11384580971126834, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 136}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:32,212] Trial 119 finished with value: 0.8253194426294792 and parameters: {'n_estimators': 680, 'learning_rate': 0.1496739052826094, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 244}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:34,543] Trial 120 finished with value: 0.8243032814550271 and parameters: {'n_estimators': 508, 'learning_rate': 0.13677324510292665, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 558}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:36,932] Trial 121 finished with value: 0.8237805330990264 and parameters: {'n_estimators': 572, 'learning_rate': 0.12082631345089148, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 277}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:39,384] Trial 122 finished with value: 0.8312254922235015 and parameters: {'n_estimators': 592, 'learning_rate': 0.13147214447734817, 'max_depth': 12, 'max_bin': 198, 'num_leaves': 255}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:42,026] Trial 123 finished with value: 0.8312117308794363 and parameters: {'n_estimators': 746, 'learning_rate': 0.10703694096367176, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 235}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:44,795] Trial 124 finished with value: 0.8261399328469926 and parameters: {'n_estimators': 535, 'learning_rate': 0.11721062854550877, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 434}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:47,357] Trial 125 finished with value: 0.8321199979203244 and parameters: {'n_estimators': 558, 'learning_rate': 0.11122992276720417, 'max_depth': 11, 'max_bin': 184, 'num_leaves': 197}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:49,609] Trial 126 finished with value: 0.8265589913773701 and parameters: {'n_estimators': 601, 'learning_rate': 0.1448265106521857, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 397}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:52,086] Trial 127 finished with value: 0.8303671057041011 and parameters: {'n_estimators': 664, 'learning_rate': 0.12621792815375765, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 215}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:54,182] Trial 128 finished with value: 0.8249331031521582 and parameters: {'n_estimators': 176, 'learning_rate': 0.1566226851441981, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 344}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:55,758] Trial 129 finished with value: 0.8218944062168454 and parameters: {'n_estimators': 95, 'learning_rate': 0.12247675524596977, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 291}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:26:57,922] Trial 130 finished with value: 0.8260438238546985 and parameters: {'n_estimators': 839, 'learning_rate': 0.13447081710381056, 'max_depth': 8, 'max_bin': 168, 'num_leaves': 162}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:01,083] Trial 131 finished with value: 0.8274149575901028 and parameters: {'n_estimators': 624, 'learning_rate': 0.08603173514338505, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 518}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:03,570] Trial 132 finished with value: 0.8281358999466436 and parameters: {'n_estimators': 891, 'learning_rate': 0.13911616792687334, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 487}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:05,540] Trial 133 finished with value: 0.8260297861628052 and parameters: {'n_estimators': 518, 'learning_rate': 0.17408431199806587, 'max_depth': 11, 'max_bin': 268, 'num_leaves': 371}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:07,824] Trial 134 finished with value: 0.8261559947944439 and parameters: {'n_estimators': 819, 'learning_rate': 0.15159310501033738, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 466}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:09,999] Trial 135 finished with value: 0.8260403460933246 and parameters: {'n_estimators': 585, 'learning_rate': 0.17860429317110177, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 677}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:12,033] Trial 136 finished with value: 0.8229618641383043 and parameters: {'n_estimators': 897, 'learning_rate': 0.16354414380237825, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 277}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:14,143] Trial 137 finished with value: 0.8266702817932279 and parameters: {'n_estimators': 869, 'learning_rate': 0.1676013442269158, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 326}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:16,379] Trial 138 finished with value: 0.8228063616862091 and parameters: {'n_estimators': 636, 'learning_rate': 0.14669005329522908, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 221}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:18,739] Trial 139 finished with value: 0.8322573725676545 and parameters: {'n_estimators': 293, 'learning_rate': 0.12836670323967192, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 234}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:21,160] Trial 140 finished with value: 0.8281732544234339 and parameters: {'n_estimators': 553, 'learning_rate': 0.1413616057607847, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 538}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:23,680] Trial 141 finished with value: 0.8227402742527309 and parameters: {'n_estimators': 139, 'learning_rate': 0.06055494523536184, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 393}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:26,410] Trial 142 finished with value: 0.8255472115336092 and parameters: {'n_estimators': 358, 'learning_rate': 0.10494049485707022, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 407}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:29,148] Trial 143 finished with value: 0.8284956199687763 and parameters: {'n_estimators': 315, 'learning_rate': 0.11183437192261045, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 201}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:31,945] Trial 144 finished with value: 0.827181720633756 and parameters: {'n_estimators': 488, 'learning_rate': 0.1003900367967495, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 438}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:34,575] Trial 145 finished with value: 0.8298166846036652 and parameters: {'n_estimators': 403, 'learning_rate': 0.11686211903940535, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 180}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:37,099] Trial 146 finished with value: 0.8214095621458732 and parameters: {'n_estimators': 227, 'learning_rate': 0.09704050981329751, 'max_depth': 9, 'max_bin': 263, 'num_leaves': 370}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:39,090] Trial 147 finished with value: 0.8218067755272462 and parameters: {'n_estimators': 190, 'learning_rate': 0.18239728901035793, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 354}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:41,259] Trial 148 finished with value: 0.8239164470009106 and parameters: {'n_estimators': 470, 'learning_rate': 0.15646326176230319, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 310}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:43,219] Trial 149 finished with value: 0.81979957947256 and parameters: {'n_estimators': 447, 'learning_rate': 0.13421270140716998, 'max_depth': 7, 'max_bin': 227, 'num_leaves': 427}. Best is trial 18 with value: 0.8420546677607827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8421\n",
      "\tBest params:\n",
      "\t\tn_estimators: 565\n",
      "\t\tlearning_rate: 0.1469074717142049\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 397\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  418.000000  416.000000  417.000000\n",
      "1                    TN  335.000000  359.000000  349.000000\n",
      "2                    FP   92.000000   76.000000   73.000000\n",
      "3                    FN   74.000000   68.000000   80.000000\n",
      "4              Accuracy    0.819369    0.843308    0.833515\n",
      "5             Precision    0.819608    0.845528    0.851020\n",
      "6           Sensitivity    0.849593    0.859504    0.839034\n",
      "7           Specificity    0.784500    0.825300    0.827000\n",
      "8              F1 score    0.834331    0.852459    0.844985\n",
      "9   F1 score (weighted)    0.819047    0.843223    0.833609\n",
      "10     F1 score (macro)    0.817883    0.842703    0.832598\n",
      "11    Balanced Accuracy    0.817068    0.842396    0.833024\n",
      "12                  MCC    0.636404    0.685534    0.665294\n",
      "13                  NPV    0.819100    0.840700    0.813500\n",
      "14              ROC_AUC    0.817068    0.842396    0.833024\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_lgbm_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_lgbm_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_lgbm_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_lgbm_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_lgbm_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_lgbm_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_lgbm_2)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:27:45,764] Trial 150 finished with value: 0.8268915521680406 and parameters: {'n_estimators': 116, 'learning_rate': 0.06161451138652288, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 455}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:47,914] Trial 151 finished with value: 0.827743396784047 and parameters: {'n_estimators': 791, 'learning_rate': 0.12520546842979985, 'max_depth': 8, 'max_bin': 269, 'num_leaves': 263}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:50,089] Trial 152 finished with value: 0.8275196356622573 and parameters: {'n_estimators': 746, 'learning_rate': 0.13768089456381652, 'max_depth': 9, 'max_bin': 276, 'num_leaves': 271}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:52,345] Trial 153 finished with value: 0.8338254825752616 and parameters: {'n_estimators': 698, 'learning_rate': 0.13009655326846417, 'max_depth': 10, 'max_bin': 282, 'num_leaves': 242}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:54,347] Trial 154 finished with value: 0.8298559900792183 and parameters: {'n_estimators': 573, 'learning_rate': 0.14296460037812853, 'max_depth': 8, 'max_bin': 263, 'num_leaves': 384}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:56,774] Trial 155 finished with value: 0.8240347293176603 and parameters: {'n_estimators': 274, 'learning_rate': 0.11483360495619134, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 209}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:27:59,093] Trial 156 finished with value: 0.8260171928700991 and parameters: {'n_estimators': 820, 'learning_rate': 0.12091855581173648, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 150}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:01,433] Trial 157 finished with value: 0.8298194233590935 and parameters: {'n_estimators': 753, 'learning_rate': 0.14852251561164714, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 288}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:03,418] Trial 158 finished with value: 0.8239446570191801 and parameters: {'n_estimators': 766, 'learning_rate': 0.16081269132507223, 'max_depth': 8, 'max_bin': 150, 'num_leaves': 587}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:05,732] Trial 159 finished with value: 0.832513666070489 and parameters: {'n_estimators': 531, 'learning_rate': 0.13246074345931916, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 168}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:08,204] Trial 160 finished with value: 0.8224549091090332 and parameters: {'n_estimators': 608, 'learning_rate': 0.12732144846831303, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 250}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:10,334] Trial 161 finished with value: 0.8301646897335087 and parameters: {'n_estimators': 590, 'learning_rate': 0.1506326450262731, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 130}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:12,449] Trial 162 finished with value: 0.8289719777100608 and parameters: {'n_estimators': 654, 'learning_rate': 0.1528320679543546, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 98}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:14,744] Trial 163 finished with value: 0.8281889940580773 and parameters: {'n_estimators': 563, 'learning_rate': 0.14028009337412653, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 188}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:16,952] Trial 164 finished with value: 0.8311270477459736 and parameters: {'n_estimators': 617, 'learning_rate': 0.1456607868610596, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 165}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:19,281] Trial 165 finished with value: 0.8319516308156949 and parameters: {'n_estimators': 711, 'learning_rate': 0.13727358386166422, 'max_depth': 9, 'max_bin': 260, 'num_leaves': 720}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:21,444] Trial 166 finished with value: 0.8295865286848981 and parameters: {'n_estimators': 731, 'learning_rate': 0.1588686375696074, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 116}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:23,490] Trial 167 finished with value: 0.824287244155842 and parameters: {'n_estimators': 678, 'learning_rate': 0.16626691924616252, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 226}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:25,958] Trial 168 finished with value: 0.8349679299769331 and parameters: {'n_estimators': 552, 'learning_rate': 0.10989945783267135, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 147}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:28,331] Trial 169 finished with value: 0.8328161241846358 and parameters: {'n_estimators': 859, 'learning_rate': 0.1550743262591324, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 301}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:30,468] Trial 170 finished with value: 0.8262816084025628 and parameters: {'n_estimators': 631, 'learning_rate': 0.1701432936424272, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 329}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:32,679] Trial 171 finished with value: 0.8360209998777396 and parameters: {'n_estimators': 580, 'learning_rate': 0.14258669476353036, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 59}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:34,954] Trial 172 finished with value: 0.8303012215179235 and parameters: {'n_estimators': 610, 'learning_rate': 0.14954865239137982, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 201}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:37,339] Trial 173 finished with value: 0.8334542500647402 and parameters: {'n_estimators': 596, 'learning_rate': 0.14460109678721259, 'max_depth': 12, 'max_bin': 176, 'num_leaves': 634}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:39,635] Trial 174 finished with value: 0.8320116378671637 and parameters: {'n_estimators': 156, 'learning_rate': 0.13500608891902852, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 222}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:41,864] Trial 175 finished with value: 0.8272073935699534 and parameters: {'n_estimators': 641, 'learning_rate': 0.1387086442029179, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 81}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:45,118] Trial 176 finished with value: 0.8257318527561885 and parameters: {'n_estimators': 512, 'learning_rate': 0.07773554182679254, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 190}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:47,795] Trial 177 finished with value: 0.8327853215347207 and parameters: {'n_estimators': 544, 'learning_rate': 0.118606923599894, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 410}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:50,382] Trial 178 finished with value: 0.8267155105166974 and parameters: {'n_estimators': 572, 'learning_rate': 0.10282389842790053, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 257}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:52,986] Trial 179 finished with value: 0.8321905255486804 and parameters: {'n_estimators': 656, 'learning_rate': 0.1134176561925687, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 177}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:55,164] Trial 180 finished with value: 0.8250835152517716 and parameters: {'n_estimators': 802, 'learning_rate': 0.12351435996100608, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 39}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:28:58,131] Trial 181 finished with value: 0.8240109631039546 and parameters: {'n_estimators': 709, 'learning_rate': 0.09258076215077164, 'max_depth': 9, 'max_bin': 182, 'num_leaves': 346}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:00,601] Trial 182 finished with value: 0.8332523113913055 and parameters: {'n_estimators': 786, 'learning_rate': 0.10512130912895738, 'max_depth': 7, 'max_bin': 173, 'num_leaves': 733}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:03,242] Trial 183 finished with value: 0.8245886387674284 and parameters: {'n_estimators': 726, 'learning_rate': 0.10718748225060043, 'max_depth': 10, 'max_bin': 272, 'num_leaves': 741}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:06,100] Trial 184 finished with value: 0.8280641824491927 and parameters: {'n_estimators': 211, 'learning_rate': 0.09832227529023656, 'max_depth': 11, 'max_bin': 180, 'num_leaves': 707}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:10,523] Trial 185 finished with value: 0.8302889717729831 and parameters: {'n_estimators': 601, 'learning_rate': 0.04638893867260863, 'max_depth': 8, 'max_bin': 167, 'num_leaves': 238}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:13,708] Trial 186 finished with value: 0.831982851896572 and parameters: {'n_estimators': 334, 'learning_rate': 0.08878916802559757, 'max_depth': 9, 'max_bin': 267, 'num_leaves': 704}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:16,347] Trial 187 finished with value: 0.8286866267885415 and parameters: {'n_estimators': 384, 'learning_rate': 0.147353316504019, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 686}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:18,965] Trial 188 finished with value: 0.8228943348176749 and parameters: {'n_estimators': 624, 'learning_rate': 0.10326894841924446, 'max_depth': 6, 'max_bin': 277, 'num_leaves': 750}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:21,299] Trial 189 finished with value: 0.829258011622727 and parameters: {'n_estimators': 691, 'learning_rate': 0.12970455639500558, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 141}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:23,451] Trial 190 finished with value: 0.8273433514067895 and parameters: {'n_estimators': 767, 'learning_rate': 0.14232276121416346, 'max_depth': 10, 'max_bin': 283, 'num_leaves': 215}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:25,380] Trial 191 finished with value: 0.8246321780847594 and parameters: {'n_estimators': 127, 'learning_rate': 0.1700182326597329, 'max_depth': 11, 'max_bin': 236, 'num_leaves': 115}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:26,912] Trial 192 finished with value: 0.8128385422849107 and parameters: {'n_estimators': 71, 'learning_rate': 0.09454124036001674, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 159}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:29,110] Trial 193 finished with value: 0.8259525353482842 and parameters: {'n_estimators': 174, 'learning_rate': 0.17318703228228405, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 134}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:31,078] Trial 194 finished with value: 0.8273446813283482 and parameters: {'n_estimators': 149, 'learning_rate': 0.17982513960332341, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 98}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:32,651] Trial 195 finished with value: 0.8260476783259196 and parameters: {'n_estimators': 90, 'learning_rate': 0.16387556118890442, 'max_depth': 11, 'max_bin': 265, 'num_leaves': 174}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:35,007] Trial 196 finished with value: 0.8300847438860772 and parameters: {'n_estimators': 195, 'learning_rate': 0.15860698363010814, 'max_depth': 12, 'max_bin': 170, 'num_leaves': 283}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:37,560] Trial 197 finished with value: 0.827943928723281 and parameters: {'n_estimators': 584, 'learning_rate': 0.1332397035065232, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 476}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:39,581] Trial 198 finished with value: 0.8273577409423393 and parameters: {'n_estimators': 128, 'learning_rate': 0.15310028155684247, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 154}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:41,667] Trial 199 finished with value: 0.8273044235731962 and parameters: {'n_estimators': 108, 'learning_rate': 0.07833502389274624, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 201}. Best is trial 18 with value: 0.8420546677607827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8421\n",
      "\tBest params:\n",
      "\t\tn_estimators: 565\n",
      "\t\tlearning_rate: 0.1469074717142049\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 397\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  418.000000  416.000000  417.000000  440.000000\n",
      "1                    TN  335.000000  359.000000  349.000000  327.000000\n",
      "2                    FP   92.000000   76.000000   73.000000   77.000000\n",
      "3                    FN   74.000000   68.000000   80.000000   75.000000\n",
      "4              Accuracy    0.819369    0.843308    0.833515    0.834603\n",
      "5             Precision    0.819608    0.845528    0.851020    0.851064\n",
      "6           Sensitivity    0.849593    0.859504    0.839034    0.854369\n",
      "7           Specificity    0.784500    0.825300    0.827000    0.809400\n",
      "8              F1 score    0.834331    0.852459    0.844985    0.852713\n",
      "9   F1 score (weighted)    0.819047    0.843223    0.833609    0.834558\n",
      "10     F1 score (macro)    0.817883    0.842703    0.832598    0.832064\n",
      "11    Balanced Accuracy    0.817068    0.842396    0.833024    0.831887\n",
      "12                  MCC    0.636404    0.685534    0.665294    0.664136\n",
      "13                  NPV    0.819100    0.840700    0.813500    0.813400\n",
      "14              ROC_AUC    0.817068    0.842396    0.833024    0.831887\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_lgbm_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_lgbm_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_lgbm_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_lgbm_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_lgbm_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_lgbm_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_lgbm_3)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:29:44,798] Trial 200 finished with value: 0.8356402469277138 and parameters: {'n_estimators': 426, 'learning_rate': 0.10010975227735432, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 376}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:46,921] Trial 201 finished with value: 0.8291502909103677 and parameters: {'n_estimators': 562, 'learning_rate': 0.14623186302931424, 'max_depth': 9, 'max_bin': 240, 'num_leaves': 317}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:49,217] Trial 202 finished with value: 0.8271223861410826 and parameters: {'n_estimators': 530, 'learning_rate': 0.14043662398614382, 'max_depth': 10, 'max_bin': 252, 'num_leaves': 503}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:51,335] Trial 203 finished with value: 0.8268297271971885 and parameters: {'n_estimators': 501, 'learning_rate': 0.17590028543178246, 'max_depth': 9, 'max_bin': 243, 'num_leaves': 381}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:53,478] Trial 204 finished with value: 0.8293224415700076 and parameters: {'n_estimators': 167, 'learning_rate': 0.15028707508412453, 'max_depth': 10, 'max_bin': 256, 'num_leaves': 395}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:56,171] Trial 205 finished with value: 0.8320378776174092 and parameters: {'n_estimators': 544, 'learning_rate': 0.10907713766449785, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 357}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:29:58,157] Trial 206 finished with value: 0.8277083500190571 and parameters: {'n_estimators': 669, 'learning_rate': 0.15561791003903155, 'max_depth': 8, 'max_bin': 281, 'num_leaves': 270}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:00,543] Trial 207 finished with value: 0.8358900137362211 and parameters: {'n_estimators': 835, 'learning_rate': 0.13743411657533652, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 406}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:02,932] Trial 208 finished with value: 0.825750366089512 and parameters: {'n_estimators': 515, 'learning_rate': 0.14491597391497085, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 442}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:05,657] Trial 209 finished with value: 0.8279165524222172 and parameters: {'n_estimators': 244, 'learning_rate': 0.12021196716287089, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 725}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:08,425] Trial 210 finished with value: 0.8292126958793065 and parameters: {'n_estimators': 604, 'learning_rate': 0.1140300755782268, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 417}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:10,517] Trial 211 finished with value: 0.8259699347505951 and parameters: {'n_estimators': 579, 'learning_rate': 0.14280399671767482, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 47}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:12,705] Trial 212 finished with value: 0.8352792673703959 and parameters: {'n_estimators': 562, 'learning_rate': 0.1488688117732778, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 89}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:14,977] Trial 213 finished with value: 0.8338588248213948 and parameters: {'n_estimators': 577, 'learning_rate': 0.14044993039531045, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 80}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:17,330] Trial 214 finished with value: 0.8317519455491296 and parameters: {'n_estimators': 532, 'learning_rate': 0.1349266942792252, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 68}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:19,648] Trial 215 finished with value: 0.8309844962143025 and parameters: {'n_estimators': 619, 'learning_rate': 0.14426948852234034, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 55}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:21,923] Trial 216 finished with value: 0.830938175475557 and parameters: {'n_estimators': 752, 'learning_rate': 0.1256958146181964, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 62}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:24,008] Trial 217 finished with value: 0.8319758981353708 and parameters: {'n_estimators': 590, 'learning_rate': 0.16259725530310293, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 123}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:26,076] Trial 218 finished with value: 0.82795825915181 and parameters: {'n_estimators': 882, 'learning_rate': 0.1535916046079366, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 185}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:28,226] Trial 219 finished with value: 0.8306227494507785 and parameters: {'n_estimators': 142, 'learning_rate': 0.1297918113057946, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 36}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:36,984] Trial 220 finished with value: 0.8325240748347223 and parameters: {'n_estimators': 550, 'learning_rate': 0.016952501061663905, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 532}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:39,343] Trial 221 finished with value: 0.8314083094799706 and parameters: {'n_estimators': 181, 'learning_rate': 0.13918589657371117, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 411}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:41,656] Trial 222 finished with value: 0.8300775159041335 and parameters: {'n_estimators': 821, 'learning_rate': 0.13589310661438478, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 428}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:43,953] Trial 223 finished with value: 0.8325743943029904 and parameters: {'n_estimators': 847, 'learning_rate': 0.13713524425837012, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 371}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:46,172] Trial 224 finished with value: 0.825953335213295 and parameters: {'n_estimators': 824, 'learning_rate': 0.14169818353705058, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 397}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:48,399] Trial 225 finished with value: 0.828712974443168 and parameters: {'n_estimators': 779, 'learning_rate': 0.14710417981779855, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 250}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:50,373] Trial 226 finished with value: 0.8339963399502057 and parameters: {'n_estimators': 839, 'learning_rate': 0.18440744245233567, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 392}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:52,576] Trial 227 finished with value: 0.8310434331845726 and parameters: {'n_estimators': 632, 'learning_rate': 0.1323631198869682, 'max_depth': 9, 'max_bin': 276, 'num_leaves': 228}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:54,594] Trial 228 finished with value: 0.8307216441446791 and parameters: {'n_estimators': 607, 'learning_rate': 0.17418565476361358, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 106}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:56,491] Trial 229 finished with value: 0.8271969209818408 and parameters: {'n_estimators': 489, 'learning_rate': 0.19370608605941966, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 164}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:30:58,505] Trial 230 finished with value: 0.8274964718171238 and parameters: {'n_estimators': 573, 'learning_rate': 0.1693078604112585, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 141}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:01,305] Trial 231 finished with value: 0.8299012447764804 and parameters: {'n_estimators': 424, 'learning_rate': 0.09936776330029247, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 365}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:04,337] Trial 232 finished with value: 0.8290639851100394 and parameters: {'n_estimators': 860, 'learning_rate': 0.09263799367354855, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 377}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:06,928] Trial 233 finished with value: 0.8299192765282584 and parameters: {'n_estimators': 457, 'learning_rate': 0.1037170022889624, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 378}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:09,623] Trial 234 finished with value: 0.8320499290347559 and parameters: {'n_estimators': 368, 'learning_rate': 0.10022850882941531, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 214}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:12,457] Trial 235 finished with value: 0.8319676141029799 and parameters: {'n_estimators': 349, 'learning_rate': 0.09674691083175926, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 408}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:21,467] Trial 236 finished with value: 0.7815456526838677 and parameters: {'n_estimators': 386, 'learning_rate': 0.0013984616267485317, 'max_depth': 11, 'max_bin': 264, 'num_leaves': 357}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:24,144] Trial 237 finished with value: 0.8318103574805921 and parameters: {'n_estimators': 590, 'learning_rate': 0.10749727450168896, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 342}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:26,715] Trial 238 finished with value: 0.8249175005340726 and parameters: {'n_estimators': 523, 'learning_rate': 0.1122047283333645, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 288}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:28,811] Trial 239 finished with value: 0.8259308266032364 and parameters: {'n_estimators': 420, 'learning_rate': 0.148846775856034, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 391}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:31,039] Trial 240 finished with value: 0.8309476945271866 and parameters: {'n_estimators': 317, 'learning_rate': 0.1443267766869156, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 194}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:33,169] Trial 241 finished with value: 0.8351224640600401 and parameters: {'n_estimators': 560, 'learning_rate': 0.14809391711063627, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 89}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:35,231] Trial 242 finished with value: 0.8310497132641592 and parameters: {'n_estimators': 565, 'learning_rate': 0.1505623828310199, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 87}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:37,426] Trial 243 finished with value: 0.8330576662913032 and parameters: {'n_estimators': 547, 'learning_rate': 0.1390336482603707, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 77}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:40,481] Trial 244 finished with value: 0.8341484595576866 and parameters: {'n_estimators': 729, 'learning_rate': 0.0851036608030403, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 153}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:42,537] Trial 245 finished with value: 0.8331107074866603 and parameters: {'n_estimators': 585, 'learning_rate': 0.15226535276854614, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 132}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:44,590] Trial 246 finished with value: 0.8330870087718093 and parameters: {'n_estimators': 536, 'learning_rate': 0.1423391610591862, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 52}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:46,881] Trial 247 finished with value: 0.8303828168912283 and parameters: {'n_estimators': 164, 'learning_rate': 0.12259133347703653, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 330}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:50,690] Trial 248 finished with value: 0.8350308666402299 and parameters: {'n_estimators': 644, 'learning_rate': 0.06465607148068371, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 584}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:52,796] Trial 249 finished with value: 0.8314782632476213 and parameters: {'n_estimators': 762, 'learning_rate': 0.15861824257612844, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 67}. Best is trial 18 with value: 0.8420546677607827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8421\n",
      "\tBest params:\n",
      "\t\tn_estimators: 565\n",
      "\t\tlearning_rate: 0.1469074717142049\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 397\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  418.000000  416.000000  417.000000  440.000000   \n",
      "1                    TN  335.000000  359.000000  349.000000  327.000000   \n",
      "2                    FP   92.000000   76.000000   73.000000   77.000000   \n",
      "3                    FN   74.000000   68.000000   80.000000   75.000000   \n",
      "4              Accuracy    0.819369    0.843308    0.833515    0.834603   \n",
      "5             Precision    0.819608    0.845528    0.851020    0.851064   \n",
      "6           Sensitivity    0.849593    0.859504    0.839034    0.854369   \n",
      "7           Specificity    0.784500    0.825300    0.827000    0.809400   \n",
      "8              F1 score    0.834331    0.852459    0.844985    0.852713   \n",
      "9   F1 score (weighted)    0.819047    0.843223    0.833609    0.834558   \n",
      "10     F1 score (macro)    0.817883    0.842703    0.832598    0.832064   \n",
      "11    Balanced Accuracy    0.817068    0.842396    0.833024    0.831887   \n",
      "12                  MCC    0.636404    0.685534    0.665294    0.664136   \n",
      "13                  NPV    0.819100    0.840700    0.813500    0.813400   \n",
      "14              ROC_AUC    0.817068    0.842396    0.833024    0.831887   \n",
      "\n",
      "          Set4  \n",
      "0   404.000000  \n",
      "1   350.000000  \n",
      "2    80.000000  \n",
      "3    85.000000  \n",
      "4     0.820457  \n",
      "5     0.834711  \n",
      "6     0.826176  \n",
      "7     0.814000  \n",
      "8     0.830421  \n",
      "9     0.820515  \n",
      "10    0.819835  \n",
      "11    0.820065  \n",
      "12    0.639719  \n",
      "13    0.804600  \n",
      "14    0.820065  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_lgbm_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_lgbm_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_lgbm_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_lgbm_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_lgbm_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_lgbm_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_lgbm_4)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:31:55,314] Trial 250 finished with value: 0.8217145604032355 and parameters: {'n_estimators': 806, 'learning_rate': 0.11656444586326072, 'max_depth': 7, 'max_bin': 270, 'num_leaves': 424}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:31:57,267] Trial 251 finished with value: 0.8244266632398647 and parameters: {'n_estimators': 877, 'learning_rate': 0.16584286481427007, 'max_depth': 11, 'max_bin': 257, 'num_leaves': 110}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:00,990] Trial 252 finished with value: 0.8264875668039042 and parameters: {'n_estimators': 297, 'learning_rate': 0.05281719179701623, 'max_depth': 11, 'max_bin': 154, 'num_leaves': 175}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:03,522] Trial 253 finished with value: 0.8199844861433233 and parameters: {'n_estimators': 558, 'learning_rate': 0.10245419883754107, 'max_depth': 8, 'max_bin': 275, 'num_leaves': 309}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:06,912] Trial 254 finished with value: 0.8292376220573712 and parameters: {'n_estimators': 619, 'learning_rate': 0.07646199758993982, 'max_depth': 12, 'max_bin': 191, 'num_leaves': 237}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:09,127] Trial 255 finished with value: 0.8257964645509863 and parameters: {'n_estimators': 594, 'learning_rate': 0.12740075697035333, 'max_depth': 9, 'max_bin': 284, 'num_leaves': 146}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:11,439] Trial 256 finished with value: 0.8287587590439989 and parameters: {'n_estimators': 575, 'learning_rate': 0.13660621407595497, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 270}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:13,758] Trial 257 finished with value: 0.8301487079527174 and parameters: {'n_estimators': 604, 'learning_rate': 0.14631241397076322, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 202}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:19,127] Trial 258 finished with value: 0.8305609438815766 and parameters: {'n_estimators': 549, 'learning_rate': 0.03997069102349067, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 405}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:21,214] Trial 259 finished with value: 0.8226094709771615 and parameters: {'n_estimators': 516, 'learning_rate': 0.17886456235319592, 'max_depth': 11, 'max_bin': 233, 'num_leaves': 385}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:23,733] Trial 260 finished with value: 0.8254615677755675 and parameters: {'n_estimators': 563, 'learning_rate': 0.13216766440616892, 'max_depth': 10, 'max_bin': 268, 'num_leaves': 566}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:26,068] Trial 261 finished with value: 0.8267227745980079 and parameters: {'n_estimators': 136, 'learning_rate': 0.10613177703427207, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 448}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:28,174] Trial 262 finished with value: 0.8218321591401704 and parameters: {'n_estimators': 894, 'learning_rate': 0.17266733991394503, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 164}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:31,747] Trial 263 finished with value: 0.8302025421617818 and parameters: {'n_estimators': 609, 'learning_rate': 0.07127070819609176, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 127}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:33,979] Trial 264 finished with value: 0.8255929847894139 and parameters: {'n_estimators': 744, 'learning_rate': 0.15481127549845403, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 224}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:36,533] Trial 265 finished with value: 0.8253042417000032 and parameters: {'n_estimators': 155, 'learning_rate': 0.08959399881765093, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 371}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:39,279] Trial 266 finished with value: 0.8312967364249815 and parameters: {'n_estimators': 705, 'learning_rate': 0.09553855063692113, 'max_depth': 9, 'max_bin': 274, 'num_leaves': 262}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:41,607] Trial 267 finished with value: 0.8266705487850448 and parameters: {'n_estimators': 536, 'learning_rate': 0.1415051174790159, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 347}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:44,689] Trial 268 finished with value: 0.8257954267662561 and parameters: {'n_estimators': 787, 'learning_rate': 0.08358064526019358, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 473}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:46,903] Trial 269 finished with value: 0.82466489904704 and parameters: {'n_estimators': 477, 'learning_rate': 0.15038626855833326, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 249}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:49,511] Trial 270 finished with value: 0.8297458482150585 and parameters: {'n_estimators': 192, 'learning_rate': 0.11023057048065472, 'max_depth': 11, 'max_bin': 177, 'num_leaves': 733}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:51,532] Trial 271 finished with value: 0.8245097941983689 and parameters: {'n_estimators': 500, 'learning_rate': 0.14514845372887808, 'max_depth': 8, 'max_bin': 260, 'num_leaves': 75}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:54,524] Trial 272 finished with value: 0.8298076291835355 and parameters: {'n_estimators': 578, 'learning_rate': 0.08956665347238604, 'max_depth': 12, 'max_bin': 186, 'num_leaves': 188}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:57,011] Trial 273 finished with value: 0.8266901728214686 and parameters: {'n_estimators': 834, 'learning_rate': 0.1370435035894222, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 602}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:32:59,408] Trial 274 finished with value: 0.8284377577373248 and parameters: {'n_estimators': 445, 'learning_rate': 0.12007891613925072, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 96}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:02,073] Trial 275 finished with value: 0.8270890731211249 and parameters: {'n_estimators': 636, 'learning_rate': 0.09881992790902971, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 294}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:04,164] Trial 276 finished with value: 0.8221099883128307 and parameters: {'n_estimators': 852, 'learning_rate': 0.14756048874750902, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 30}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:07,306] Trial 277 finished with value: 0.8274763873262252 and parameters: {'n_estimators': 262, 'learning_rate': 0.08079890461772665, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 496}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:09,529] Trial 278 finished with value: 0.831310088408563 and parameters: {'n_estimators': 600, 'learning_rate': 0.16197328916057424, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 213}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:11,426] Trial 279 finished with value: 0.8202921522251507 and parameters: {'n_estimators': 110, 'learning_rate': 0.0918707327573907, 'max_depth': 10, 'max_bin': 299, 'num_leaves': 716}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:13,821] Trial 280 finished with value: 0.8262264540840517 and parameters: {'n_estimators': 690, 'learning_rate': 0.1298782011418367, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 402}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:16,266] Trial 281 finished with value: 0.8269641748446341 and parameters: {'n_estimators': 402, 'learning_rate': 0.14240975661025582, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 434}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:18,663] Trial 282 finished with value: 0.8322905648446863 and parameters: {'n_estimators': 723, 'learning_rate': 0.13380973338135846, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 172}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:20,874] Trial 283 finished with value: 0.8247669329785443 and parameters: {'n_estimators': 801, 'learning_rate': 0.16836851232305508, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 232}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:23,505] Trial 284 finished with value: 0.8258563377867556 and parameters: {'n_estimators': 658, 'learning_rate': 0.12497069160526715, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 634}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:25,784] Trial 285 finished with value: 0.8040869892590321 and parameters: {'n_estimators': 522, 'learning_rate': 0.07353090897803195, 'max_depth': 3, 'max_bin': 287, 'num_leaves': 382}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:29,037] Trial 286 finished with value: 0.8201089814158318 and parameters: {'n_estimators': 558, 'learning_rate': 0.07078222128992626, 'max_depth': 10, 'max_bin': 262, 'num_leaves': 118}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:31,680] Trial 287 finished with value: 0.8290310077335749 and parameters: {'n_estimators': 588, 'learning_rate': 0.10069452573032657, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 154}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:33,956] Trial 288 finished with value: 0.8239899086020286 and parameters: {'n_estimators': 568, 'learning_rate': 0.1523517591585322, 'max_depth': 11, 'max_bin': 268, 'num_leaves': 358}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:36,901] Trial 289 finished with value: 0.8313044817531479 and parameters: {'n_estimators': 628, 'learning_rate': 0.09460835619474049, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 139}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:40,133] Trial 290 finished with value: 0.8274535442500734 and parameters: {'n_estimators': 759, 'learning_rate': 0.08575884163570247, 'max_depth': 11, 'max_bin': 151, 'num_leaves': 750}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:42,156] Trial 291 finished with value: 0.8278471848881498 and parameters: {'n_estimators': 220, 'learning_rate': 0.17633486465291703, 'max_depth': 9, 'max_bin': 265, 'num_leaves': 206}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:44,511] Trial 292 finished with value: 0.824478032783414 and parameters: {'n_estimators': 177, 'learning_rate': 0.13926295784209297, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 321}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:46,662] Trial 293 finished with value: 0.8329048605971596 and parameters: {'n_estimators': 148, 'learning_rate': 0.11694920845206011, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 58}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:48,986] Trial 294 finished with value: 0.8287804725372501 and parameters: {'n_estimators': 547, 'learning_rate': 0.15727678710968387, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 278}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:52,578] Trial 295 finished with value: 0.830807417674721 and parameters: {'n_estimators': 615, 'learning_rate': 0.06726764005999973, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 416}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:54,595] Trial 296 finished with value: 0.8252130719796483 and parameters: {'n_estimators': 529, 'learning_rate': 0.14851153620535198, 'max_depth': 8, 'max_bin': 272, 'num_leaves': 692}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:57,049] Trial 297 finished with value: 0.825540023677636 and parameters: {'n_estimators': 581, 'learning_rate': 0.10585515101056342, 'max_depth': 9, 'max_bin': 279, 'num_leaves': 185}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:33:59,589] Trial 298 finished with value: 0.8306953225770585 and parameters: {'n_estimators': 543, 'learning_rate': 0.11216622194537058, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 105}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:02,138] Trial 299 finished with value: 0.8145982674138651 and parameters: {'n_estimators': 506, 'learning_rate': 0.08147565449173334, 'max_depth': 4, 'max_bin': 277, 'num_leaves': 393}. Best is trial 18 with value: 0.8420546677607827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8421\n",
      "\tBest params:\n",
      "\t\tn_estimators: 565\n",
      "\t\tlearning_rate: 0.1469074717142049\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 397\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  418.000000  416.000000  417.000000  440.000000   \n",
      "1                    TN  335.000000  359.000000  349.000000  327.000000   \n",
      "2                    FP   92.000000   76.000000   73.000000   77.000000   \n",
      "3                    FN   74.000000   68.000000   80.000000   75.000000   \n",
      "4              Accuracy    0.819369    0.843308    0.833515    0.834603   \n",
      "5             Precision    0.819608    0.845528    0.851020    0.851064   \n",
      "6           Sensitivity    0.849593    0.859504    0.839034    0.854369   \n",
      "7           Specificity    0.784500    0.825300    0.827000    0.809400   \n",
      "8              F1 score    0.834331    0.852459    0.844985    0.852713   \n",
      "9   F1 score (weighted)    0.819047    0.843223    0.833609    0.834558   \n",
      "10     F1 score (macro)    0.817883    0.842703    0.832598    0.832064   \n",
      "11    Balanced Accuracy    0.817068    0.842396    0.833024    0.831887   \n",
      "12                  MCC    0.636404    0.685534    0.665294    0.664136   \n",
      "13                  NPV    0.819100    0.840700    0.813500    0.813400   \n",
      "14              ROC_AUC    0.817068    0.842396    0.833024    0.831887   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   404.000000  402.000000  \n",
      "1   350.000000  358.000000  \n",
      "2    80.000000   77.000000  \n",
      "3    85.000000   82.000000  \n",
      "4     0.820457    0.826986  \n",
      "5     0.834711    0.839248  \n",
      "6     0.826176    0.830579  \n",
      "7     0.814000    0.823000  \n",
      "8     0.830421    0.834891  \n",
      "9     0.820515    0.827031  \n",
      "10    0.819835    0.826588  \n",
      "11    0.820065    0.826784  \n",
      "12    0.639719    0.653226  \n",
      "13    0.804600    0.813600  \n",
      "14    0.820065    0.826784  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_lgbm_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_lgbm_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_lgbm_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_lgbm_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_lgbm_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_lgbm_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_lgbm_5)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:34:04,499] Trial 300 finished with value: 0.8356128304870106 and parameters: {'n_estimators': 125, 'learning_rate': 0.1431399584010487, 'max_depth': 11, 'max_bin': 173, 'num_leaves': 661}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:06,744] Trial 301 finished with value: 0.8284463368399224 and parameters: {'n_estimators': 135, 'learning_rate': 0.05751128998871463, 'max_depth': 11, 'max_bin': 173, 'num_leaves': 241}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:08,835] Trial 302 finished with value: 0.8357401948801005 and parameters: {'n_estimators': 125, 'learning_rate': 0.1369779672646758, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 650}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:11,450] Trial 303 finished with value: 0.8378621481208667 and parameters: {'n_estimators': 812, 'learning_rate': 0.1346532466516776, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 526}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:13,962] Trial 304 finished with value: 0.8365884097367544 and parameters: {'n_estimators': 811, 'learning_rate': 0.13056146600391355, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 529}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:16,680] Trial 305 finished with value: 0.8381609927247246 and parameters: {'n_estimators': 831, 'learning_rate': 0.12788246511175055, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 532}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:19,284] Trial 306 finished with value: 0.8351163572028557 and parameters: {'n_estimators': 798, 'learning_rate': 0.1275202132749849, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 526}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:21,832] Trial 307 finished with value: 0.8329707140921603 and parameters: {'n_estimators': 814, 'learning_rate': 0.12354349880267332, 'max_depth': 10, 'max_bin': 226, 'num_leaves': 549}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:24,429] Trial 308 finished with value: 0.8312864712380638 and parameters: {'n_estimators': 784, 'learning_rate': 0.12220303121157586, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 513}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:27,027] Trial 309 finished with value: 0.8373817962394294 and parameters: {'n_estimators': 829, 'learning_rate': 0.13185556545701788, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 499}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:29,694] Trial 310 finished with value: 0.8356770743878933 and parameters: {'n_estimators': 808, 'learning_rate': 0.1302062653834398, 'max_depth': 11, 'max_bin': 232, 'num_leaves': 498}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:32,161] Trial 311 finished with value: 0.8335576614594336 and parameters: {'n_estimators': 824, 'learning_rate': 0.131448894510927, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 483}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:34,723] Trial 312 finished with value: 0.8359767520544364 and parameters: {'n_estimators': 835, 'learning_rate': 0.12840639309292373, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 544}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:37,311] Trial 313 finished with value: 0.8368225726173716 and parameters: {'n_estimators': 855, 'learning_rate': 0.13149692906991603, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 455}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:39,764] Trial 314 finished with value: 0.8354365751874303 and parameters: {'n_estimators': 869, 'learning_rate': 0.13403230842367897, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 463}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:42,305] Trial 315 finished with value: 0.837337400077711 and parameters: {'n_estimators': 863, 'learning_rate': 0.12477153402936712, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 512}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:44,532] Trial 316 finished with value: 0.8365501165724343 and parameters: {'n_estimators': 848, 'learning_rate': 0.128136156083392, 'max_depth': 7, 'max_bin': 221, 'num_leaves': 516}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:46,554] Trial 317 finished with value: 0.8297625993832011 and parameters: {'n_estimators': 861, 'learning_rate': 0.12453029299954797, 'max_depth': 7, 'max_bin': 217, 'num_leaves': 519}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:48,717] Trial 318 finished with value: 0.8307974625036894 and parameters: {'n_estimators': 838, 'learning_rate': 0.1293355461196618, 'max_depth': 7, 'max_bin': 219, 'num_leaves': 509}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:51,207] Trial 319 finished with value: 0.8373674497326775 and parameters: {'n_estimators': 851, 'learning_rate': 0.12627061581550483, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 535}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:53,314] Trial 320 finished with value: 0.8330349596836267 and parameters: {'n_estimators': 854, 'learning_rate': 0.12599608715925223, 'max_depth': 7, 'max_bin': 219, 'num_leaves': 532}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:55,687] Trial 321 finished with value: 0.8339538297510602 and parameters: {'n_estimators': 873, 'learning_rate': 0.11982068778590789, 'max_depth': 6, 'max_bin': 213, 'num_leaves': 490}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:34:58,296] Trial 322 finished with value: 0.8346783208142854 and parameters: {'n_estimators': 846, 'learning_rate': 0.12700381665306795, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 568}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:35:00,599] Trial 323 finished with value: 0.8359645748084024 and parameters: {'n_estimators': 825, 'learning_rate': 0.13225136603094254, 'max_depth': 7, 'max_bin': 215, 'num_leaves': 522}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:35:03,175] Trial 324 finished with value: 0.8316791915459472 and parameters: {'n_estimators': 884, 'learning_rate': 0.12134223502237175, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 508}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:35:05,718] Trial 325 finished with value: 0.8381579538331172 and parameters: {'n_estimators': 899, 'learning_rate': 0.12476838451233417, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 483}. Best is trial 18 with value: 0.8420546677607827.\n",
      "[I 2023-12-04 09:35:08,343] Trial 326 finished with value: 0.8441334757608144 and parameters: {'n_estimators': 896, 'learning_rate': 0.12363183341754728, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 483}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:10,985] Trial 327 finished with value: 0.8389996641580746 and parameters: {'n_estimators': 893, 'learning_rate': 0.11693934770038211, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 475}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:13,527] Trial 328 finished with value: 0.8332639543269874 and parameters: {'n_estimators': 881, 'learning_rate': 0.12109671988393714, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 473}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:16,371] Trial 329 finished with value: 0.8381618566541658 and parameters: {'n_estimators': 900, 'learning_rate': 0.11629545055937669, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 483}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:19,054] Trial 330 finished with value: 0.8294868832402751 and parameters: {'n_estimators': 899, 'learning_rate': 0.11774659682465981, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 458}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:21,841] Trial 331 finished with value: 0.8311064760311666 and parameters: {'n_estimators': 896, 'learning_rate': 0.1165339814366813, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 487}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:24,692] Trial 332 finished with value: 0.8365176932841039 and parameters: {'n_estimators': 900, 'learning_rate': 0.11206883551206794, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 490}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:27,358] Trial 333 finished with value: 0.8303067713802189 and parameters: {'n_estimators': 878, 'learning_rate': 0.115437479359588, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 458}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:29,997] Trial 334 finished with value: 0.8319471770825444 and parameters: {'n_estimators': 867, 'learning_rate': 0.12307929578699776, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 506}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:32,727] Trial 335 finished with value: 0.836816856647508 and parameters: {'n_estimators': 866, 'learning_rate': 0.11800504474382044, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 471}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:35,348] Trial 336 finished with value: 0.8376993937449708 and parameters: {'n_estimators': 887, 'learning_rate': 0.11589167168656109, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 474}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:38,024] Trial 337 finished with value: 0.8333159220271256 and parameters: {'n_estimators': 876, 'learning_rate': 0.11606256818785145, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 473}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:40,670] Trial 338 finished with value: 0.8360534273972295 and parameters: {'n_estimators': 883, 'learning_rate': 0.11921613469515784, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 471}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:43,369] Trial 339 finished with value: 0.8372402150753375 and parameters: {'n_estimators': 863, 'learning_rate': 0.1242477306324639, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 482}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:45,970] Trial 340 finished with value: 0.8332528065844809 and parameters: {'n_estimators': 858, 'learning_rate': 0.12454814812959428, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 485}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:48,492] Trial 341 finished with value: 0.8354883227235289 and parameters: {'n_estimators': 898, 'learning_rate': 0.11508698052202146, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 445}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:51,065] Trial 342 finished with value: 0.8335312644913134 and parameters: {'n_estimators': 871, 'learning_rate': 0.12355538638152275, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 496}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:53,539] Trial 343 finished with value: 0.841191665631786 and parameters: {'n_estimators': 856, 'learning_rate': 0.1204537623860323, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 461}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:56,241] Trial 344 finished with value: 0.8333292267914031 and parameters: {'n_estimators': 854, 'learning_rate': 0.11197041063456739, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 455}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:35:58,847] Trial 345 finished with value: 0.8356427132909727 and parameters: {'n_estimators': 882, 'learning_rate': 0.12562887634005926, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 482}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:01,439] Trial 346 finished with value: 0.8329310203083795 and parameters: {'n_estimators': 844, 'learning_rate': 0.12078053356278937, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 499}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:03,947] Trial 347 finished with value: 0.8359429284884963 and parameters: {'n_estimators': 888, 'learning_rate': 0.13407463319785884, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 458}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:06,784] Trial 348 finished with value: 0.8322777319902684 and parameters: {'n_estimators': 863, 'learning_rate': 0.10898159583043793, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 484}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:09,280] Trial 349 finished with value: 0.8363046367075487 and parameters: {'n_estimators': 897, 'learning_rate': 0.12642357949519087, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 471}. Best is trial 326 with value: 0.8441334757608144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.844133\n",
      "\tBest params:\n",
      "\t\tn_estimators: 896\n",
      "\t\tlearning_rate: 0.12363183341754728\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 221\n",
      "\t\tnum_leaves: 483\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  418.000000  416.000000  417.000000  440.000000   \n",
      "1                    TN  335.000000  359.000000  349.000000  327.000000   \n",
      "2                    FP   92.000000   76.000000   73.000000   77.000000   \n",
      "3                    FN   74.000000   68.000000   80.000000   75.000000   \n",
      "4              Accuracy    0.819369    0.843308    0.833515    0.834603   \n",
      "5             Precision    0.819608    0.845528    0.851020    0.851064   \n",
      "6           Sensitivity    0.849593    0.859504    0.839034    0.854369   \n",
      "7           Specificity    0.784500    0.825300    0.827000    0.809400   \n",
      "8              F1 score    0.834331    0.852459    0.844985    0.852713   \n",
      "9   F1 score (weighted)    0.819047    0.843223    0.833609    0.834558   \n",
      "10     F1 score (macro)    0.817883    0.842703    0.832598    0.832064   \n",
      "11    Balanced Accuracy    0.817068    0.842396    0.833024    0.831887   \n",
      "12                  MCC    0.636404    0.685534    0.665294    0.664136   \n",
      "13                  NPV    0.819100    0.840700    0.813500    0.813400   \n",
      "14              ROC_AUC    0.817068    0.842396    0.833024    0.831887   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   404.000000  402.000000  438.000000  \n",
      "1   350.000000  358.000000  326.000000  \n",
      "2    80.000000   77.000000   78.000000  \n",
      "3    85.000000   82.000000   77.000000  \n",
      "4     0.820457    0.826986    0.831338  \n",
      "5     0.834711    0.839248    0.848837  \n",
      "6     0.826176    0.830579    0.850485  \n",
      "7     0.814000    0.823000    0.806900  \n",
      "8     0.830421    0.834891    0.849661  \n",
      "9     0.820515    0.827031    0.831316  \n",
      "10    0.819835    0.826588    0.828796  \n",
      "11    0.820065    0.826784    0.828708  \n",
      "12    0.639719    0.653226    0.657593  \n",
      "13    0.804600    0.813600    0.808900  \n",
      "14    0.820065    0.826784    0.828708  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_lgbm_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_lgbm_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_lgbm_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_lgbm_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_lgbm_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_lgbm_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_lgbm_6)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:36:12,104] Trial 350 finished with value: 0.8312633490495351 and parameters: {'n_estimators': 838, 'learning_rate': 0.1312058411275479, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 446}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:14,711] Trial 351 finished with value: 0.8298779101108344 and parameters: {'n_estimators': 855, 'learning_rate': 0.12017127223669051, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 501}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:17,417] Trial 352 finished with value: 0.8376660455516095 and parameters: {'n_estimators': 879, 'learning_rate': 0.11300534147510592, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 542}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:20,276] Trial 353 finished with value: 0.8333067832912635 and parameters: {'n_estimators': 882, 'learning_rate': 0.11505008313578166, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 545}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:22,920] Trial 354 finished with value: 0.82997941968141 and parameters: {'n_estimators': 876, 'learning_rate': 0.11403378972167467, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 523}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:25,503] Trial 355 finished with value: 0.8369679514757978 and parameters: {'n_estimators': 899, 'learning_rate': 0.11818675260488701, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 531}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:28,186] Trial 356 finished with value: 0.8337863699355104 and parameters: {'n_estimators': 828, 'learning_rate': 0.1083008910280139, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 504}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:30,772] Trial 357 finished with value: 0.8296449534162148 and parameters: {'n_estimators': 881, 'learning_rate': 0.11176944721055346, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 565}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:33,330] Trial 358 finished with value: 0.8346225736765186 and parameters: {'n_estimators': 873, 'learning_rate': 0.12323874762165693, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 506}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:35,709] Trial 359 finished with value: 0.8274118429722682 and parameters: {'n_estimators': 843, 'learning_rate': 0.1201735348182979, 'max_depth': 5, 'max_bin': 222, 'num_leaves': 488}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:38,367] Trial 360 finished with value: 0.8320637400330579 and parameters: {'n_estimators': 860, 'learning_rate': 0.1261738252057661, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 542}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:40,967] Trial 361 finished with value: 0.836937077629293 and parameters: {'n_estimators': 900, 'learning_rate': 0.1355448733742536, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 519}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:43,698] Trial 362 finished with value: 0.8362515706115964 and parameters: {'n_estimators': 862, 'learning_rate': 0.10901921629222465, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 481}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:46,235] Trial 363 finished with value: 0.8296155006821936 and parameters: {'n_estimators': 833, 'learning_rate': 0.1147959851334814, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 492}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:48,223] Trial 364 finished with value: 0.8275306852619945 and parameters: {'n_estimators': 884, 'learning_rate': 0.18752981552645012, 'max_depth': 10, 'max_bin': 230, 'num_leaves': 467}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:55,005] Trial 365 finished with value: 0.8312034993793352 and parameters: {'n_estimators': 812, 'learning_rate': 0.02907319888731355, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 538}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:36:57,551] Trial 366 finished with value: 0.8337570411857355 and parameters: {'n_estimators': 847, 'learning_rate': 0.12784591769059478, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 555}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:00,127] Trial 367 finished with value: 0.8337811818447778 and parameters: {'n_estimators': 867, 'learning_rate': 0.12229383611536, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 496}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:02,692] Trial 368 finished with value: 0.8357402934317341 and parameters: {'n_estimators': 899, 'learning_rate': 0.11816156408580814, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 477}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:05,090] Trial 369 finished with value: 0.8329984956970786 and parameters: {'n_estimators': 884, 'learning_rate': 0.13416288001468354, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 512}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:07,642] Trial 370 finished with value: 0.8378933656237697 and parameters: {'n_estimators': 867, 'learning_rate': 0.111625355801431, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 527}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:10,561] Trial 371 finished with value: 0.8329914637311069 and parameters: {'n_estimators': 869, 'learning_rate': 0.11281235419737702, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 514}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:12,871] Trial 372 finished with value: 0.8338178282563014 and parameters: {'n_estimators': 846, 'learning_rate': 0.1651947295823752, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 536}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:15,334] Trial 373 finished with value: 0.8340528292541094 and parameters: {'n_estimators': 877, 'learning_rate': 0.12338428417389243, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 525}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:18,109] Trial 374 finished with value: 0.8318616554238881 and parameters: {'n_estimators': 824, 'learning_rate': 0.11830856143550182, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 546}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:20,593] Trial 375 finished with value: 0.8281919006784598 and parameters: {'n_estimators': 855, 'learning_rate': 0.13800577938467543, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 503}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:23,068] Trial 376 finished with value: 0.8334342962892588 and parameters: {'n_estimators': 887, 'learning_rate': 0.13192285914513438, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 578}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:25,280] Trial 377 finished with value: 0.8322174657776703 and parameters: {'n_estimators': 872, 'learning_rate': 0.12862318509139117, 'max_depth': 6, 'max_bin': 224, 'num_leaves': 526}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:27,599] Trial 378 finished with value: 0.829199525945115 and parameters: {'n_estimators': 827, 'learning_rate': 0.17902727856968012, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 491}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:30,607] Trial 379 finished with value: 0.8359731817110708 and parameters: {'n_estimators': 853, 'learning_rate': 0.11013338528728946, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 469}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:33,438] Trial 380 finished with value: 0.834585730437184 and parameters: {'n_estimators': 886, 'learning_rate': 0.10480119009115653, 'max_depth': 10, 'max_bin': 225, 'num_leaves': 513}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:35,519] Trial 381 finished with value: 0.8338332470873814 and parameters: {'n_estimators': 836, 'learning_rate': 0.18381253042513654, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 435}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:37,771] Trial 382 finished with value: 0.8365147285992427 and parameters: {'n_estimators': 898, 'learning_rate': 0.16947394708454894, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 550}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:40,627] Trial 383 finished with value: 0.833764181554429 and parameters: {'n_estimators': 864, 'learning_rate': 0.12511389829716904, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 558}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:43,192] Trial 384 finished with value: 0.8298736237974292 and parameters: {'n_estimators': 202, 'learning_rate': 0.114810344597527, 'max_depth': 11, 'max_bin': 234, 'num_leaves': 501}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:45,296] Trial 385 finished with value: 0.8310182355528661 and parameters: {'n_estimators': 865, 'learning_rate': 0.1985853577679511, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 484}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:47,677] Trial 386 finished with value: 0.8346131421380922 and parameters: {'n_estimators': 884, 'learning_rate': 0.12174135078098718, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 217}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:50,257] Trial 387 finished with value: 0.8282295121210648 and parameters: {'n_estimators': 845, 'learning_rate': 0.1359355069070306, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 535}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:52,442] Trial 388 finished with value: 0.8335959932938899 and parameters: {'n_estimators': 900, 'learning_rate': 0.174648497874233, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 519}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:54,861] Trial 389 finished with value: 0.8363904101478971 and parameters: {'n_estimators': 800, 'learning_rate': 0.13962802600200436, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 463}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:57,193] Trial 390 finished with value: 0.8286592678037387 and parameters: {'n_estimators': 819, 'learning_rate': 0.16102814708156327, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 483}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:37:59,870] Trial 391 finished with value: 0.8337857584676452 and parameters: {'n_estimators': 868, 'learning_rate': 0.12748628119208621, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 447}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:02,096] Trial 392 finished with value: 0.8332939515042677 and parameters: {'n_estimators': 170, 'learning_rate': 0.11994455016400919, 'max_depth': 10, 'max_bin': 225, 'num_leaves': 202}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:04,660] Trial 393 finished with value: 0.8318410194060528 and parameters: {'n_estimators': 840, 'learning_rate': 0.13121001577021657, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 499}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:07,367] Trial 394 finished with value: 0.8370055252679315 and parameters: {'n_estimators': 880, 'learning_rate': 0.11679737558263653, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 599}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:09,564] Trial 395 finished with value: 0.82640945580563 and parameters: {'n_estimators': 856, 'learning_rate': 0.17156652291364316, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 514}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:12,280] Trial 396 finished with value: 0.8321972421612923 and parameters: {'n_estimators': 900, 'learning_rate': 0.11042272151625321, 'max_depth': 11, 'max_bin': 156, 'num_leaves': 532}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:14,497] Trial 397 finished with value: 0.827293581036038 and parameters: {'n_estimators': 873, 'learning_rate': 0.12367461763252682, 'max_depth': 4, 'max_bin': 223, 'num_leaves': 466}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:16,961] Trial 398 finished with value: 0.8371541110012745 and parameters: {'n_estimators': 836, 'learning_rate': 0.1399637526218221, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 494}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:19,446] Trial 399 finished with value: 0.8318833925821443 and parameters: {'n_estimators': 833, 'learning_rate': 0.13987772297234113, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 498}. Best is trial 326 with value: 0.8441334757608144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8441335\n",
      "\tBest params:\n",
      "\t\tn_estimators: 896\n",
      "\t\tlearning_rate: 0.12363183341754728\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 221\n",
      "\t\tnum_leaves: 483\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  418.000000  416.000000  417.000000  440.000000   \n",
      "1                    TN  335.000000  359.000000  349.000000  327.000000   \n",
      "2                    FP   92.000000   76.000000   73.000000   77.000000   \n",
      "3                    FN   74.000000   68.000000   80.000000   75.000000   \n",
      "4              Accuracy    0.819369    0.843308    0.833515    0.834603   \n",
      "5             Precision    0.819608    0.845528    0.851020    0.851064   \n",
      "6           Sensitivity    0.849593    0.859504    0.839034    0.854369   \n",
      "7           Specificity    0.784500    0.825300    0.827000    0.809400   \n",
      "8              F1 score    0.834331    0.852459    0.844985    0.852713   \n",
      "9   F1 score (weighted)    0.819047    0.843223    0.833609    0.834558   \n",
      "10     F1 score (macro)    0.817883    0.842703    0.832598    0.832064   \n",
      "11    Balanced Accuracy    0.817068    0.842396    0.833024    0.831887   \n",
      "12                  MCC    0.636404    0.685534    0.665294    0.664136   \n",
      "13                  NPV    0.819100    0.840700    0.813500    0.813400   \n",
      "14              ROC_AUC    0.817068    0.842396    0.833024    0.831887   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   404.000000  402.000000  438.000000  410.000000  \n",
      "1   350.000000  358.000000  326.000000  355.000000  \n",
      "2    80.000000   77.000000   78.000000   75.000000  \n",
      "3    85.000000   82.000000   77.000000   79.000000  \n",
      "4     0.820457    0.826986    0.831338    0.832427  \n",
      "5     0.834711    0.839248    0.848837    0.845361  \n",
      "6     0.826176    0.830579    0.850485    0.838446  \n",
      "7     0.814000    0.823000    0.806900    0.825600  \n",
      "8     0.830421    0.834891    0.849661    0.841889  \n",
      "9     0.820515    0.827031    0.831316    0.832470  \n",
      "10    0.819835    0.826588    0.828796    0.831824  \n",
      "11    0.820065    0.826784    0.828708    0.832014  \n",
      "12    0.639719    0.653226    0.657593    0.663680  \n",
      "13    0.804600    0.813600    0.808900    0.818000  \n",
      "14    0.820065    0.826784    0.828708    0.832014  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_lgbm_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_lgbm_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_lgbm_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_lgbm_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_lgbm_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_lgbm_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_lgbm_7)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:38:21,879] Trial 400 finished with value: 0.8289287722933046 and parameters: {'n_estimators': 814, 'learning_rate': 0.19253384150238267, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 556}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:24,216] Trial 401 finished with value: 0.8246063383829029 and parameters: {'n_estimators': 854, 'learning_rate': 0.1350832608584511, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 543}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:26,430] Trial 402 finished with value: 0.8175297319129106 and parameters: {'n_estimators': 846, 'learning_rate': 0.14290686423701354, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 481}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:28,925] Trial 403 finished with value: 0.822712076434607 and parameters: {'n_estimators': 798, 'learning_rate': 0.12910494180676854, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 521}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:31,553] Trial 404 finished with value: 0.8279375891091203 and parameters: {'n_estimators': 882, 'learning_rate': 0.13891924945813333, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 573}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:34,001] Trial 405 finished with value: 0.8276543710345091 and parameters: {'n_estimators': 830, 'learning_rate': 0.13361555566536154, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 500}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:36,476] Trial 406 finished with value: 0.8276560524874936 and parameters: {'n_estimators': 865, 'learning_rate': 0.14456656153242264, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 530}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:39,150] Trial 407 finished with value: 0.8278987239796786 and parameters: {'n_estimators': 845, 'learning_rate': 0.12582607369905077, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 508}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:41,857] Trial 408 finished with value: 0.8315456716364601 and parameters: {'n_estimators': 816, 'learning_rate': 0.11989110984374567, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 478}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:43,858] Trial 409 finished with value: 0.8202847906305593 and parameters: {'n_estimators': 886, 'learning_rate': 0.1539896082593528, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 228}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:46,378] Trial 410 finished with value: 0.8305794431783898 and parameters: {'n_estimators': 869, 'learning_rate': 0.12981834031874512, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 490}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:48,794] Trial 411 finished with value: 0.8288159526386558 and parameters: {'n_estimators': 827, 'learning_rate': 0.1385373182547018, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 172}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:38:51,310] Trial 412 finished with value: 0.8312351775980715 and parameters: {'n_estimators': 240, 'learning_rate': 0.13324154288950696, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 515}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:03,490] Trial 413 finished with value: 0.8230782219726691 and parameters: {'n_estimators': 900, 'learning_rate': 0.00938141609757566, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 192}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:06,152] Trial 414 finished with value: 0.8301084753192527 and parameters: {'n_estimators': 786, 'learning_rate': 0.12197366527173999, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 536}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:08,774] Trial 415 finished with value: 0.8216154631870353 and parameters: {'n_estimators': 849, 'learning_rate': 0.11440942611847112, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 448}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:11,004] Trial 416 finished with value: 0.826733920347704 and parameters: {'n_estimators': 183, 'learning_rate': 0.15879980488133077, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 464}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:13,095] Trial 417 finished with value: 0.8205206527118716 and parameters: {'n_estimators': 864, 'learning_rate': 0.16740571337525662, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 554}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:15,448] Trial 418 finished with value: 0.8270436054488064 and parameters: {'n_estimators': 887, 'learning_rate': 0.14620162969540576, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 489}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:17,907] Trial 419 finished with value: 0.8303127731522657 and parameters: {'n_estimators': 873, 'learning_rate': 0.12654799903633734, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 160}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:20,017] Trial 420 finished with value: 0.8292245591373947 and parameters: {'n_estimators': 835, 'learning_rate': 0.1820607960036927, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 509}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:22,717] Trial 421 finished with value: 0.8251776568603553 and parameters: {'n_estimators': 807, 'learning_rate': 0.11772251051407646, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 217}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:25,202] Trial 422 finished with value: 0.8281405877225992 and parameters: {'n_estimators': 277, 'learning_rate': 0.1361520067373318, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 523}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:27,536] Trial 423 finished with value: 0.824388554742821 and parameters: {'n_estimators': 877, 'learning_rate': 0.142093610650001, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 469}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:29,934] Trial 424 finished with value: 0.822984215586619 and parameters: {'n_estimators': 848, 'learning_rate': 0.12397586297191043, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 478}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:32,643] Trial 425 finished with value: 0.8322664573322867 and parameters: {'n_estimators': 900, 'learning_rate': 0.1300955387181997, 'max_depth': 11, 'max_bin': 150, 'num_leaves': 542}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:34,812] Trial 426 finished with value: 0.8254800158291269 and parameters: {'n_estimators': 149, 'learning_rate': 0.15065254287680185, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 493}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:36,922] Trial 427 finished with value: 0.8237925871557883 and parameters: {'n_estimators': 855, 'learning_rate': 0.16504278638964284, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 184}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:38,984] Trial 428 finished with value: 0.8224115508918871 and parameters: {'n_estimators': 883, 'learning_rate': 0.15593888853493443, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 207}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:41,553] Trial 429 finished with value: 0.8277504200923967 and parameters: {'n_estimators': 828, 'learning_rate': 0.11959347848245665, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 254}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:43,638] Trial 430 finished with value: 0.8264697865220606 and parameters: {'n_estimators': 163, 'learning_rate': 0.17738083825293877, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 510}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:45,709] Trial 431 finished with value: 0.8048475977488975 and parameters: {'n_estimators': 859, 'learning_rate': 0.1126453162631133, 'max_depth': 3, 'max_bin': 222, 'num_leaves': 142}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:48,278] Trial 432 finished with value: 0.8275209456210229 and parameters: {'n_estimators': 812, 'learning_rate': 0.13413646450902506, 'max_depth': 11, 'max_bin': 180, 'num_leaves': 453}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:50,545] Trial 433 finished with value: 0.8152167808959128 and parameters: {'n_estimators': 866, 'learning_rate': 0.12674324798510628, 'max_depth': 5, 'max_bin': 216, 'num_leaves': 231}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:53,460] Trial 434 finished with value: 0.8295779003954115 and parameters: {'n_estimators': 884, 'learning_rate': 0.10693653025842444, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 525}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:55,750] Trial 435 finished with value: 0.8308545877330266 and parameters: {'n_estimators': 207, 'learning_rate': 0.14758124983378917, 'max_depth': 10, 'max_bin': 236, 'num_leaves': 495}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:39:58,238] Trial 436 finished with value: 0.8314991102039331 and parameters: {'n_estimators': 842, 'learning_rate': 0.14011247155618664, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 430}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:01,061] Trial 437 finished with value: 0.8274318404194141 and parameters: {'n_estimators': 885, 'learning_rate': 0.12309071768244656, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 481}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:02,436] Trial 438 finished with value: 0.8117881150473725 and parameters: {'n_estimators': 60, 'learning_rate': 0.11667654580218476, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 559}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:04,670] Trial 439 finished with value: 0.8174339611665504 and parameters: {'n_estimators': 861, 'learning_rate': 0.17255693620567855, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 462}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:06,752] Trial 440 finished with value: 0.8301075831904289 and parameters: {'n_estimators': 900, 'learning_rate': 0.19430963663151643, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 509}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:09,326] Trial 441 finished with value: 0.8289699781912245 and parameters: {'n_estimators': 794, 'learning_rate': 0.1314981136894453, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 524}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:11,808] Trial 442 finished with value: 0.8251483346473225 and parameters: {'n_estimators': 819, 'learning_rate': 0.13690199284786864, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 542}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:14,395] Trial 443 finished with value: 0.830349104861631 and parameters: {'n_estimators': 872, 'learning_rate': 0.12718165583456756, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 197}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:16,836] Trial 444 finished with value: 0.8268298745217054 and parameters: {'n_estimators': 836, 'learning_rate': 0.1456618474794318, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 303}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:19,056] Trial 445 finished with value: 0.8231894703627483 and parameters: {'n_estimators': 853, 'learning_rate': 0.16171133599318607, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 495}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:21,593] Trial 446 finished with value: 0.8257997752996188 and parameters: {'n_estimators': 885, 'learning_rate': 0.11366625453453283, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 162}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:24,188] Trial 447 finished with value: 0.8285765250664605 and parameters: {'n_estimators': 463, 'learning_rate': 0.12096798802973238, 'max_depth': 11, 'max_bin': 299, 'num_leaves': 241}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:26,136] Trial 448 finished with value: 0.8293099163914863 and parameters: {'n_estimators': 900, 'learning_rate': 0.18584466650351447, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 176}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:28,550] Trial 449 finished with value: 0.8325265929673696 and parameters: {'n_estimators': 781, 'learning_rate': 0.14210222336433168, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 480}. Best is trial 326 with value: 0.8441334757608144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.84413348\n",
      "\tBest params:\n",
      "\t\tn_estimators: 896\n",
      "\t\tlearning_rate: 0.12363183341754728\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 221\n",
      "\t\tnum_leaves: 483\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  418.000000  416.000000  417.000000  440.000000   \n",
      "1                    TN  335.000000  359.000000  349.000000  327.000000   \n",
      "2                    FP   92.000000   76.000000   73.000000   77.000000   \n",
      "3                    FN   74.000000   68.000000   80.000000   75.000000   \n",
      "4              Accuracy    0.819369    0.843308    0.833515    0.834603   \n",
      "5             Precision    0.819608    0.845528    0.851020    0.851064   \n",
      "6           Sensitivity    0.849593    0.859504    0.839034    0.854369   \n",
      "7           Specificity    0.784500    0.825300    0.827000    0.809400   \n",
      "8              F1 score    0.834331    0.852459    0.844985    0.852713   \n",
      "9   F1 score (weighted)    0.819047    0.843223    0.833609    0.834558   \n",
      "10     F1 score (macro)    0.817883    0.842703    0.832598    0.832064   \n",
      "11    Balanced Accuracy    0.817068    0.842396    0.833024    0.831887   \n",
      "12                  MCC    0.636404    0.685534    0.665294    0.664136   \n",
      "13                  NPV    0.819100    0.840700    0.813500    0.813400   \n",
      "14              ROC_AUC    0.817068    0.842396    0.833024    0.831887   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   404.000000  402.000000  438.000000  410.000000  396.000000  \n",
      "1   350.000000  358.000000  326.000000  355.000000  356.000000  \n",
      "2    80.000000   77.000000   78.000000   75.000000   93.000000  \n",
      "3    85.000000   82.000000   77.000000   79.000000   74.000000  \n",
      "4     0.820457    0.826986    0.831338    0.832427    0.818281  \n",
      "5     0.834711    0.839248    0.848837    0.845361    0.809816  \n",
      "6     0.826176    0.830579    0.850485    0.838446    0.842553  \n",
      "7     0.814000    0.823000    0.806900    0.825600    0.792900  \n",
      "8     0.830421    0.834891    0.849661    0.841889    0.825860  \n",
      "9     0.820515    0.827031    0.831316    0.832470    0.818117  \n",
      "10    0.819835    0.826588    0.828796    0.831824    0.817936  \n",
      "11    0.820065    0.826784    0.828708    0.832014    0.817713  \n",
      "12    0.639719    0.653226    0.657593    0.663680    0.636574  \n",
      "13    0.804600    0.813600    0.808900    0.818000    0.827900  \n",
      "14    0.820065    0.826784    0.828708    0.832014    0.817713  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_lgbm_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_lgbm_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_lgbm_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_lgbm_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_lgbm_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_lgbm_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_lgbm_8)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:40:31,451] Trial 450 finished with value: 0.8301749937555696 and parameters: {'n_estimators': 835, 'learning_rate': 0.130486051037067, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 502}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:33,647] Trial 451 finished with value: 0.8230831266410558 and parameters: {'n_estimators': 192, 'learning_rate': 0.1511590022056645, 'max_depth': 10, 'max_bin': 234, 'num_leaves': 536}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:36,153] Trial 452 finished with value: 0.82614395573226 and parameters: {'n_estimators': 869, 'learning_rate': 0.13627378665598905, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 473}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:38,557] Trial 453 finished with value: 0.8263059184630095 and parameters: {'n_estimators': 854, 'learning_rate': 0.12473435093377731, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 328}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:41,273] Trial 454 finished with value: 0.8258706727296996 and parameters: {'n_estimators': 362, 'learning_rate': 0.11584723418124862, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 459}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:44,091] Trial 455 finished with value: 0.8240851286076604 and parameters: {'n_estimators': 879, 'learning_rate': 0.11086590810861784, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 441}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:46,243] Trial 456 finished with value: 0.8240128503447114 and parameters: {'n_estimators': 814, 'learning_rate': 0.14064274841884172, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 210}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:48,966] Trial 457 finished with value: 0.826521121904995 and parameters: {'n_estimators': 842, 'learning_rate': 0.1185581305966353, 'max_depth': 11, 'max_bin': 184, 'num_leaves': 520}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:51,589] Trial 458 finished with value: 0.8239364896471703 and parameters: {'n_estimators': 173, 'learning_rate': 0.10433244003652667, 'max_depth': 11, 'max_bin': 155, 'num_leaves': 490}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:53,715] Trial 459 finished with value: 0.8215126097069418 and parameters: {'n_estimators': 597, 'learning_rate': 0.13327637248448088, 'max_depth': 6, 'max_bin': 256, 'num_leaves': 509}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:56,084] Trial 460 finished with value: 0.8284808775648512 and parameters: {'n_estimators': 869, 'learning_rate': 0.15496938283410397, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 561}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:40:57,705] Trial 461 finished with value: 0.8072956786440294 and parameters: {'n_estimators': 80, 'learning_rate': 0.0576511437291612, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 148}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:00,070] Trial 462 finished with value: 0.8233723317521207 and parameters: {'n_estimators': 823, 'learning_rate': 0.12796672857250216, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 126}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:02,236] Trial 463 finished with value: 0.8235044383444313 and parameters: {'n_estimators': 900, 'learning_rate': 0.17964749067726227, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 528}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:07,035] Trial 464 finished with value: 0.8205115888287559 and parameters: {'n_estimators': 343, 'learning_rate': 0.03438371053640801, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 219}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:09,554] Trial 465 finished with value: 0.8266801373091701 and parameters: {'n_estimators': 862, 'learning_rate': 0.12321091586680524, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 501}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:11,954] Trial 466 finished with value: 0.8233277104171238 and parameters: {'n_estimators': 617, 'learning_rate': 0.14504170554097562, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 549}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:14,170] Trial 467 finished with value: 0.826236232965225 and parameters: {'n_estimators': 884, 'learning_rate': 0.16561137923858787, 'max_depth': 11, 'max_bin': 170, 'num_leaves': 467}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:16,744] Trial 468 finished with value: 0.8283117014185123 and parameters: {'n_estimators': 226, 'learning_rate': 0.12141102255755368, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 481}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:19,273] Trial 469 finished with value: 0.828117675596457 and parameters: {'n_estimators': 799, 'learning_rate': 0.13763132888107155, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 517}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:21,241] Trial 470 finished with value: 0.8269787064771925 and parameters: {'n_estimators': 853, 'learning_rate': 0.1916191897843, 'max_depth': 10, 'max_bin': 161, 'num_leaves': 193}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:23,731] Trial 471 finished with value: 0.828432793150253 and parameters: {'n_estimators': 832, 'learning_rate': 0.1296946607050081, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 342}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:26,018] Trial 472 finished with value: 0.8279565086469536 and parameters: {'n_estimators': 149, 'learning_rate': 0.11644357480021375, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 233}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:27,752] Trial 473 finished with value: 0.8187213049071831 and parameters: {'n_estimators': 101, 'learning_rate': 0.15104651786684328, 'max_depth': 11, 'max_bin': 233, 'num_leaves': 253}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:30,760] Trial 474 finished with value: 0.8226120036923845 and parameters: {'n_estimators': 883, 'learning_rate': 0.10795217814192919, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 576}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:33,012] Trial 475 finished with value: 0.8268332272818597 and parameters: {'n_estimators': 328, 'learning_rate': 0.16071781747543876, 'max_depth': 11, 'max_bin': 238, 'num_leaves': 455}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:35,032] Trial 476 finished with value: 0.8227025147499984 and parameters: {'n_estimators': 770, 'learning_rate': 0.19686867402046734, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 533}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:37,073] Trial 477 finished with value: 0.8251318472391386 and parameters: {'n_estimators': 864, 'learning_rate': 0.1707718202072981, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 174}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:39,630] Trial 478 finished with value: 0.8336920334509292 and parameters: {'n_estimators': 845, 'learning_rate': 0.13251796845978125, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 487}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:42,206] Trial 479 finished with value: 0.8229335426087596 and parameters: {'n_estimators': 884, 'learning_rate': 0.12502432520644133, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 506}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:44,536] Trial 480 finished with value: 0.8240303667032297 and parameters: {'n_estimators': 305, 'learning_rate': 0.14171680562866965, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 545}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:46,707] Trial 481 finished with value: 0.8270094138873392 and parameters: {'n_estimators': 393, 'learning_rate': 0.17506330459303804, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 478}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:51,423] Trial 482 finished with value: 0.8186312216632515 and parameters: {'n_estimators': 653, 'learning_rate': 0.0442792447567137, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 495}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:53,950] Trial 483 finished with value: 0.8245513356002248 and parameters: {'n_estimators': 815, 'learning_rate': 0.12040321276444217, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 441}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:41:56,571] Trial 484 finished with value: 0.8287786641398542 and parameters: {'n_estimators': 870, 'learning_rate': 0.1131455399612553, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 517}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:04,054] Trial 485 finished with value: 0.824104932450324 and parameters: {'n_estimators': 838, 'learning_rate': 0.02122298651151522, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 468}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:06,327] Trial 486 finished with value: 0.8249509318666896 and parameters: {'n_estimators': 900, 'learning_rate': 0.136146881010272, 'max_depth': 10, 'max_bin': 258, 'num_leaves': 157}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:08,500] Trial 487 finished with value: 0.8214351837628202 and parameters: {'n_estimators': 635, 'learning_rate': 0.1485549189098882, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 314}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:10,902] Trial 488 finished with value: 0.8250853537740304 and parameters: {'n_estimators': 858, 'learning_rate': 0.12699185220258175, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 202}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:13,461] Trial 489 finished with value: 0.823447112913118 and parameters: {'n_estimators': 877, 'learning_rate': 0.1184445134197637, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 138}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:15,847] Trial 490 finished with value: 0.83689190833123 and parameters: {'n_estimators': 850, 'learning_rate': 0.13211120577965071, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 220}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:18,336] Trial 491 finished with value: 0.8267387613265571 and parameters: {'n_estimators': 826, 'learning_rate': 0.1450962941554073, 'max_depth': 11, 'max_bin': 153, 'num_leaves': 425}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:20,461] Trial 492 finished with value: 0.8250234884067773 and parameters: {'n_estimators': 598, 'learning_rate': 0.18979169257148615, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 530}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:23,383] Trial 493 finished with value: 0.8184358651148331 and parameters: {'n_estimators': 889, 'learning_rate': 0.11018964205786563, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 494}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:30,604] Trial 494 finished with value: 0.8246934174570141 and parameters: {'n_estimators': 874, 'learning_rate': 0.02758064539364105, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 510}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:33,489] Trial 495 finished with value: 0.8230840442427703 and parameters: {'n_estimators': 804, 'learning_rate': 0.10228169192366288, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 266}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:36,044] Trial 496 finished with value: 0.8274395160019365 and parameters: {'n_estimators': 488, 'learning_rate': 0.12293864131569786, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 472}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:38,379] Trial 497 finished with value: 0.8207764005249661 and parameters: {'n_estimators': 849, 'learning_rate': 0.14111244631754513, 'max_depth': 10, 'max_bin': 166, 'num_leaves': 567}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:40,692] Trial 498 finished with value: 0.8246664011372398 and parameters: {'n_estimators': 159, 'learning_rate': 0.1553973297480603, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 484}. Best is trial 326 with value: 0.8441334757608144.\n",
      "[I 2023-12-04 09:42:44,832] Trial 499 finished with value: 0.7924211274624722 and parameters: {'n_estimators': 185, 'learning_rate': 0.005986369387390028, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 244}. Best is trial 326 with value: 0.8441334757608144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.844133476\n",
      "\tBest params:\n",
      "\t\tn_estimators: 896\n",
      "\t\tlearning_rate: 0.12363183341754728\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 221\n",
      "\t\tnum_leaves: 483\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  418.000000  416.000000  417.000000  440.000000   \n",
      "1                    TN  335.000000  359.000000  349.000000  327.000000   \n",
      "2                    FP   92.000000   76.000000   73.000000   77.000000   \n",
      "3                    FN   74.000000   68.000000   80.000000   75.000000   \n",
      "4              Accuracy    0.819369    0.843308    0.833515    0.834603   \n",
      "5             Precision    0.819608    0.845528    0.851020    0.851064   \n",
      "6           Sensitivity    0.849593    0.859504    0.839034    0.854369   \n",
      "7           Specificity    0.784500    0.825300    0.827000    0.809400   \n",
      "8              F1 score    0.834331    0.852459    0.844985    0.852713   \n",
      "9   F1 score (weighted)    0.819047    0.843223    0.833609    0.834558   \n",
      "10     F1 score (macro)    0.817883    0.842703    0.832598    0.832064   \n",
      "11    Balanced Accuracy    0.817068    0.842396    0.833024    0.831887   \n",
      "12                  MCC    0.636404    0.685534    0.665294    0.664136   \n",
      "13                  NPV    0.819100    0.840700    0.813500    0.813400   \n",
      "14              ROC_AUC    0.817068    0.842396    0.833024    0.831887   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   404.000000  402.000000  438.000000  410.000000  396.000000  400.000000  \n",
      "1   350.000000  358.000000  326.000000  355.000000  356.000000  375.000000  \n",
      "2    80.000000   77.000000   78.000000   75.000000   93.000000   75.000000  \n",
      "3    85.000000   82.000000   77.000000   79.000000   74.000000   69.000000  \n",
      "4     0.820457    0.826986    0.831338    0.832427    0.818281    0.843308  \n",
      "5     0.834711    0.839248    0.848837    0.845361    0.809816    0.842105  \n",
      "6     0.826176    0.830579    0.850485    0.838446    0.842553    0.852878  \n",
      "7     0.814000    0.823000    0.806900    0.825600    0.792900    0.833300  \n",
      "8     0.830421    0.834891    0.849661    0.841889    0.825860    0.847458  \n",
      "9     0.820515    0.827031    0.831316    0.832470    0.818117    0.843280  \n",
      "10    0.819835    0.826588    0.828796    0.831824    0.817936    0.843192  \n",
      "11    0.820065    0.826784    0.828708    0.832014    0.817713    0.843106  \n",
      "12    0.639719    0.653226    0.657593    0.663680    0.636574    0.686456  \n",
      "13    0.804600    0.813600    0.808900    0.818000    0.827900    0.844600  \n",
      "14    0.820065    0.826784    0.828708    0.832014    0.817713    0.843106  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_lgbm_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_lgbm_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_lgbm_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_lgbm_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_lgbm_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_lgbm_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_lgbm_9)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAv8AAAHJCAYAAAASMFYPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC2gklEQVR4nOydd3gVZfbHvzO3pFdKCC0QkEgHQaWEasGCUgVEFHQFxAq6Rde1/lxdXVfZFQugBhGRFroiqHQBUdCEjiSUhBTSe3LLzO+Py1xumXpb2vk8D4/mTnvnve+dOe95z/kehud5HgRBEARBEARBNHnY+m4AQRAEQRAEQRCBgYx/giAIgiAIgmgmkPFPEARBEARBEM0EMv4JgiAIgiAIoplAxj9BEARBEARBNBPI+CcIgiAIgiCIZgIZ/wRBEARBEATRTCDjnyAIgiAIgiCaCWT8EwRBEARBEEQzgYx/gmjAjBw5EgzD+PUas2bNAsMwuHDhgl+vo5Zly5aBYRgsW7asvpviE5ra/fiTQIx3giCI5g4Z/wQhwq+//oqHH34YiYmJCAkJQWRkJHr37o2//OUvuHz5ss+u09AM70Cwe/duMAyDV199tb6bohrBgJ81a5bkPsJ9jRw50qfXfvXVV8EwDHbv3u3T8wYCYXw7/gsLC0Pv3r3x97//HaWlpX65rj++B4IgiKaCvr4bQBANCZ7n8fzzz+Odd96BXq/Hbbfdhvvuuw8mkwkHDhzAu+++i48++ghffPEFJk+e7Pf2LF++HNXV1X69xltvvYXnn38e7dq18+t11DJhwgQMGjQI8fHx9d0Un9DU7scTxo0bh379+gEA8vLysGXLFrz11ltYt24dDh8+jOjo6HptH0EQRHOCjH+CcOD111/HO++8g06dOmHr1q3o2bOn0/bU1FTMmDED06ZNw44dOzB69Gi/tqdjx45+PT8AxMfHNyjDNCoqClFRUfXdDJ/R1O7HE8aPH++0avLuu+/i5ptvxsmTJ/HBBx/gpZdeqr/GEQRBNDMo7IcgrnL+/Hm88cYbMBgM2Lx5s5vhDwCTJk3C+++/D6vVinnz5oHjOPs2x9jurVu3YsiQIQgLC0NMTAwmT56MP/74w+lcDMPgiy++AAB07tzZHhbRqVMn+z5iMdCOYTO//vor7rjjDkRHRyM6OhqTJk1CVlYWAOCPP/7AlClT0KpVK4SEhGDUqFFIT093uyex0KNOnTq5hWs4/nM05M6ePYvnn38eAwcORKtWrRAUFISEhATMnj0bly5dcrvWqFGjAACvvfaa0zmFsBa5GPlff/0VEydOROvWre3XmTdvHnJycmTva/HixejduzeCg4MRFxeH2bNn+y3kxBWp+/ntt98wdepUJCQkICgoCC1atECfPn3wzDPPwGw2A7B9D6+99hoAYNSoUU795UhOTg4ef/xxdOrUCUajEa1atcKECRPwyy+/yLbnm2++wfDhwxEZGQmGYVBSUoLQ0FB06dIFPM+L3s/YsWPBMAyOHDnicZ+Eh4dj5syZAICff/5ZcX+O4/DRRx/hxhtvRHh4OMLCwjBw4EB89NFHor9BANizZ49TfzWmMDOCIAh/Qp5/grhKSkoKLBYL7rvvPvTu3Vtyv0cffRSvv/46zp49iz179tiNWYH169dj27ZtmDBhAkaOHInff/8dqamp2LVrFw4cOICkpCQAwCuvvIKNGzciLS0NzzzzjD30QW0IxC+//IK3334bI0aMwKOPPopjx45h/fr1OH78ODZs2IDk5GT06NEDDz30EC5duoTU1FTceuutyMzMRHh4uOy558+fL2ocb9myBUePHkVoaKjT/X7yyScYNWoUhgwZAqPRiOPHj+Ozzz7D5s2bceTIEbRv3x6AzQMMAF988QVGjBjhFJftOOkRY9OmTbjvvvvAMAwmT56Mjh074tdff8Unn3yCTZs2Yf/+/UhMTHQ77q9//Su2b9+Oe+65B7fffjt27dqFTz/91P791Qe///47Bg8eDJZlce+996Jz584oLy/HuXPn8PHHH+Of//wnDAYD5s+fj40bN2LPnj2YOXOmaB9lZmYiOTkZubm5uOWWW3D//fcjKysLa9euxTfffIO1a9di3LhxbsetXbsW3333He666y489thjOH/+PGJiYjBt2jSkpKTghx9+wG233eZ0TFZWFrZt24YBAwZgwIABXvWB1ORCjOnTp2P16tXo2LEjHn30UTAMgw0bNuCJJ57A3r17sWrVKgBAv3798Morr+C1115DQkKC0ySVcgAIgiCuwhMEwfM8z48aNYoHwC9ZskRx3/vvv58HwP/f//2f/bOUlBQeAA+A37Jli9P+Cxcu5AHwo0ePdvp85syZPAD+/PnzotcZMWIE7/oz3bVrl/06K1ascNr2yCOP8AD4qKgo/o033nDa9s9//pMHwC9cuFBTGwR27NjB6/V6vmvXrnxBQYH98+zsbL62ttZt/2+//ZZnWZafO3euaPtfeeUV0esI/ZiSkmL/rKKigo+NjeV1Oh3/008/Oe3/5ptv8gD4W2+9VfS+OnbsyF+8eNH+udls5ocNG8YD4A8dOiR7z65t6tu3L//KK6+I/hOuN2LECMX7WbBgAQ+A37Bhg9u1iouLeavVav/7lVde4QHwu3btEm3bbbfdxgPg//Wvfzl9vm/fPp5lWT4mJoYvLy93aw/DMPy2bdvczvfrr7/yAPhJkya5bXvppZdU/0Z4/tp34HjvPM/zVVVVfM+ePXkA/GuvvWb/XGy8f/XVVzwAfuDAgXxlZaX988rKSv6GG24Q/R2IfQ8EQRCEDfL8E8RV8vLyAAAdOnRQ3FfYRyzcZPTo0Rg7dqzTZ08++SQ++OAD7Ny5ExcvXkRCQoLX7R02bBgeeOABp89mzpyJzz//HDExMXj++eedts2YMQMvvvgifv/9d83XOn78OCZPnoyoqCh8++23aNmypX2bVKLwnXfeiR49emDHjh2ar+fKxo0bUVxcjAceeABDhgxx2vbnP/8Zixcvxg8//CDaty+//LJT7oRer8fDDz+Mffv24ZdffsHNN9+suh1paWlIS0vz7mYAe2iK4wqKQExMjOrzZGdn4/vvv0dCQgKee+45p23JycmYNm0aVq5ciQ0bNuChhx5y2n7vvffijjvucDvngAEDcOONN2Lz5s3Iz89HXFwcAMBqteKzzz5DREQEpk+frrqNgO37E8LK8vPzsWXLFly+fBldunTBU089JXvs559/DsCWmB4WFmb/PCwsDP/6179w++2347PPPnP7LRAEQRDiUMw/QVyFvxqGoEZnXNhHbN8RI0a4fabT6ZCcnAzAFuvtC8TCLtq2bQvAFv6g0+lEt2VnZ2u6Tm5uLu6++27U1dVhw4YNuO6665y28zyPFStW4NZbb0WrVq2g1+vtcdbHjx/3iTSq0GeuIVYAYDAY7H0u1rcDBw50+0yYvJWUlGhqx8yZM8HzvOi/Xbt2qT7PtGnToNPpMH78eMycORPLly9HRkaGprYA1+532LBh0OvdfTm33norAODo0aNu2+QmPY8//jjMZrPd8AZsIV85OTmYMWOGkxGuhk2bNuG1117Da6+9hi+++AKRkZH4y1/+gsOHDytOdn777TewLCv6uxo1ahR0Op3o/REEQRDikPFPEFcRFG+EhFk5BANaTCVH8JS60qZNGwBAWVmZp010QkxBRjAA5bYJyaRqqKqqwtixY5GVlYWUlBQMGzbMbZ9nn30WDz74IE6ePIkxY8bgueeewyuvvIJXXnkFCQkJMJlMqq8nhdBnQh+6InwPYn0r1xdWq9XrtnnCjTfeiH379mH06NFYu3YtZs6cia5du6J79+5YvXq16vN40y9SxwDA1KlTERsbi08//dQ+KV68eDEA4LHHHlPdPoGUlBT7JKm6uhonT57EO++8g9jYWMVjy8rKEBsbC4PB4LZNr9ejZcuWKC8v19wmgiCI5gqF/RDEVZKTk7Fr1y788MMPePTRRyX3s1qtdi/v0KFD3bbn5+eLHieEFTUW2UeO43D//ffj6NGj+Oc//4n777/fbZ8rV67gf//7H3r16oUDBw4gIiLCafvXX3/tk7YIfSb0oSu5ublO+zUGBg8ejK1bt6Kurg5HjhzBd999hw8++AD3338/WrVqpUpG1pt+kVvhCgkJwaxZs/Dee+/h+++/R7du3bBjxw4MGjQIffr0UXN7PiMqKgrFxcUwm81uEwCLxYLCwkJERkYGtE0EQRCNGfL8E8RVZs2aBZ1Oh/Xr1+PkyZOS+33++efIyclBUlKSaCiCmIKM1WrF/v37AQD9+/e3fy6E5tSXB1qO+fPnY8uWLXjkkUfw97//XXSfzMxMcByH22+/3c3wz87ORmZmptsxntyz0GdiVW4tFou9b2+44QbV52woBAUFYciQIXj99dfxv//9DzzPY+PGjfbtcv0l9Mv+/fthsVjctguTVE/6Zd68eWAYBosXL8bSpUvBcRzmzp2r+Tze0r9/f3Ach71797pt27t3L6xWq9v9sSzbIH9TBEEQDQEy/gniKomJifj73/8Os9mMe+65R3QCsHHjRjzzzDPQ6XT46KOPwLLuP6GdO3di69atTp8tWrQIGRkZGDVqlFNCaosWLQCoCzUKJAsXLsQHH3yAW265BZ988onkfoL05P79+52MrcrKSsyePVvUIPXknsePH4/Y2Fh8/fXXOHTokFtbMzMzceuttwakKJov2Ldvn2gojrBqFBwcbP9Mrr/at2+P2267DRcuXMDChQudtv38889YuXIlYmJiMGHCBM1t7Nq1K2677TZs3rwZS5YsQXR0NKZOnar5PN7yyCOPAABeeOEFp2rX1dXV9qT2P/3pT07HtGjRosH9pgiCIBoKFPZDEA68+uqrqKqqwnvvvYe+fftizJgx6NmzJ8xmMw4cOICff/4ZISEh+PrrryXDMu69915MmDABEyZMQNeuXZGWloZvv/0WsbGx+Oijj5z2veWWW/Dvf/8bs2fPxqRJkxAeHo7o6Gg8+eSTgbhdUfLy8vDcc8+BYRj07t0b//znP9326devH8aPH482bdpg2rRpWLVqFfr164fbb78dZWVl+P777xEcHIx+/fq5qQslJSWhXbt2WLVqFQwGAzp27AiGYfDggw9KqiCFh4fj888/x3333YcRI0bgvvvuQ8eOHXHkyBHs2LEDbdq0scekNwb+85//YMeOHRg5ciQSExMRHh6OEydOYNu2bYiOjsacOXPs+44aNQosy+KFF17AsWPH7Amy//jHPwAAn3zyCYYOHYq//OUv2LFjBwYOHGjX+WdZFikpKW6rMmqZN28eduzYgcLCQjz99NMICQnx/uY1Mn36dGzatAlr1qxBz549MX78eDAMg40bN+L8+fOYMmWKm9LPLbfcglWrVmHcuHHo378/9Ho9hg8fjuHDhwe8/QRBEA2O+lEYJYiGzc8//8w/9NBDfKdOnfjg4GA+LCyM79mzJ//cc8/xWVlZosc46rlv3bqVHzRoEB8aGspHRUXxEydO5M+cOSN63H/+8x/++uuv541GIw+AT0hIsG+T0/kX08k/f/48D4CfOXOm6LUgon/uqvMvnEPun+P5q6qq+L///e98ly5d+KCgIL59+/b8448/zhcWFoq2n+d5/vDhw/zo0aP5yMhInmEYJx17MV18x+PGjx/Pt2zZkjcYDHyHDh34xx57jL98+bLbvnL1C5RqDbgitEmqXx3PqUbnf/v27fysWbP47t2785GRkXxoaCjfrVs3/qmnnuIvXLjgdu4vv/yS79u3Lx8cHGz/DhzJzs7mH3vsMb5jx468wWDgW7RowY8bN44/fPiw5L2I9a8rFouFb9myJQ+AP3HihOL+rkjp/EshNV6sViv/4Ycf8gMGDOBDQkL4kJAQ/oYbbuAXLVrkVBNBID8/n7///vv51q1b8yzLavquCYIgmjoMz2sos0gQhCTLli3Dww8/jJSUFKfKogTRWMnIyMB1112H5ORk0Zh7giAIovFBMf8EQRCEKP/+97/B83y9hqERBEEQvoVi/gmCIAg7Fy9exJdffok//vgDX375Jfr374/JkyfXd7MIgiAIH0HGP0EQBGHn/PnzeOmllxAWFoYxY8bg448/FlW1IgiCIBonFPNPEARBEARBEM0EcucQBEEQBEEQRDOBjH+CIAiCIAiCaCaQ8U8QBEEQBEEQzQQy/gmCIAiCIAiimUBqPwqUlJTAYrH4/LytWrVCQUGBz89LOEP9HDiorwMD9XNgoH4OHL7ua71ej5iYGJ+djyCaGmT8K2CxWGA2m316ToZh7OcmsSX/Qf0cOKivAwP1c2Cgfg4c1NcEEXgo7IcgCIIgCIIgmglk/BMEQRAEQRBEM4GMf4IgCIIgCIJoJpDxTxAEQRAEQRDNBEr4JQiCIAiC8DE1NTXIz88Hz/OUzEz4FYZhwDAM4uLiEBISorg/Gf8EQRAEQRA+pKamBpcvX0ZERARYloIsCP/DcRwuX76Mdu3aKU4AaEQSBEEQBEH4kPz8fDL8iYDCsiwiIiKQn5+vvG8A2kMQBEEQBNFs4HmeDH8i4LAsqyrEjEYmQRAEQRCED6EYf6K+IOOfIAiCUA0ZLARBEE0fMv4JgiCaMVUmK97fk4WJKScw7vPjmJhyAu/vyUKVyVrfTSMIooEyYMAALF682Ot9vGXVqlXo2rWrX6/hCxpaO8n4JwiCaKZUmayYs+YsUtMKkVdhQmGVBXkVJqSmF2LOmrM0ASCIZsbly5cxf/589O7dG+3atcMNN9yAF198EcXFxZrPtX37djz44IM+a5vYZGLcuHE4ePCgz67hypYtW9CmTRtkZ2eLbh8yZAj+/ve/++36/oKkPgmCIJopSw7m4GJxLbirfwdb6qDjbH8V5tdg2a4/8PjQ9vXXwHqCZxhYy8vBVVYCFArlX/RkhijB8zwYhvH7dS5cuIC77roLXbp0weLFi9GxY0ecOXMGr732Gn788Uds27YNMTExqs/XsmVLP7bWRkhIiCpde0+54447EBsbi9WrV+O5555z2vbzzz/j3LlzWLJkid+u7y/oV0cQBNFM2ZdZbjf8ry++gAH5Z5y2R2TpUJfbOvANq28YoDg8AnWVFQDZ/n6Fbd0aSEys72Y0OKpMVny8Pxt7M0pg4XjoWQbDu8RgXnJ7hBl1frnm888/D6PRiDVr1tgN6vbt26NXr164+eab8eabb+Lf//63ff/Kyko89thj+O677xAREYFnnnkGjz76qH37gAEDMGfOHMydOxcAUF5ejtdeew3btm1DbW0t+vXrh9dffx29evWyH/Pdd9/hP//5D06fPo2wsDAMGjQIy5Ytw/jx45GVlYWXXnoJL730EgDgypUrWLVqFf7xj3/g3LlzOHfuHIYMGYKffvoJ1113nf2cH3/8MT799FP8+uuvYBgGZ86cwauvvoqDBw8iNDQUI0eOxP/93/+hRYsWbn1iMBgwefJkrFq1Cs8++6zTJOzrr79G37590atXL3z88cdYtWoVLl68iOjoaNx+++14+eWXER4eLtrXTz31FMrKyrB8+XL7Z//4xz9w/PhxbNy4EYBt0rdo0SJ88cUXuHLlChITE/Hcc8/hnnvuUf2dSkFhPwRBEM0Qnudh4Tj733HVJbbPGQZWVgcrq4MZLKBjAZ2uWf1jdDowett/67stTf8fmSGuVJmseGTlCaz9LR+55SYUVJqRW27C2t/z8cjKE34JxyspKcGuXbvw8MMPu3nS4+LiMGnSJGzatMlJFODDDz9Ejx498OOPP+KZZ57BSy+9hN27d4uen+d5TJ8+HVeuXMHKlSvxww8/oHfv3pg8eTJKSmzPnu+//x4PP/wwbr31Vvz4449Yt24d+vXrBwBISUlB27Zt8be//Q3Hjh3DsWPH3K7RtWtX9O3bF6mpqU6fr1+/HhMnTgTDMMjPz8f48ePRq1cvfP/991i9ejUKCgowe/Zsyb554IEHcPHiRRw4cMD+WVVVFTZt2oTp06cDsEls/vOf/8SePXvwwQcfYP/+/Xj99delO1wFb731FlatWoV33nkHe/fuxWOPPYbHH3/cqR2eQp5/giCIZgjDMNA76JAHWUwAgP1t++BSZBsAQJsII/70YM96aV99wjAMWsbHw5ybSwpIfiYQ4SyNjY/3Z+NC0bVwPAGOBy4U1+Lj/dn48+gEn14zMzMTPM87ecwdue6661BaWorCwkK0atUKAHDTTTfh6aefBgB06dIFhw8fxuLFizFy5Ei34/fv349Tp07h5MmTCAoKAgD7KsCWLVvw0EMP4f3338f48ePxt7/9zX6csCoQExMDnU6H8PBwxMXFSd7HpEmT8Nlnn+H5558HAGRkZCAtLQ2LFi0CYJtE9O7dGy+++KL9mP/+97/o168fMjIy0KVLF7dzJiUlYcCAAfj6668xdOhQAMDmzZvBcRwmTpwIAPbVDQBISEjA888/j7/+9a945513JNsqR1VVFT755BOkpqbixhtvBAB06tQJP//8M5YvX44hQ4Z4dF4BmnITBEE0U4YlRoK9ansFW80AgFq9EQDAMrbtBEEElr0ZJW6GvwDHA/sySgLaHuCaDLDjZG3gwIFO+wwcOBB//PGH6PFpaWmoqqpCUlISOnXqZP936dIlXLhwAQBw4sQJDB8+3Kt2TpgwAdnZ2fj1118BAOvWrUOvXr2QlJQEAEhPT8dPP/3k1AbBkBbaIcb06dOxdetWVFZWAgBWrlyJu+66C1FRUQBsk5vJkyejT58+6Ny5M5588kkUFxejqqrKo/s4e/Ysamtrcd999zm1dc2aNbLtVAt5/gmCIJopcwa3xa9ZlbhYUosgq83zX6czgmWATjHBmDO4bT23kCCaF7ZwPPnVJjPH+zwJuHPnzmAYBmfPnsVdd93ltv3cuXOIjo4WjYtXA8dxiIuLw4YNG9y2CQZ0cHCwR+d2JC4uDkOHDsX69esxcOBAbNiwAQ899JBTO26//XZ73oDrsVJMmDABL730EjZu3IghQ4bg559/tq9QZGVlYfr06Zg5cyaef/55xMTE4Oeff8b8+fNhsVhEzydW/dlsNju1E7BNMtq0aeO0n7By4g1k/BMEQTRTwow6LJnSDUt/ykZsBgdOp0N0dDhu79YScwa39VtiIUEQ4tjC8eSNej3L+DxcKjY2FiNGjEBKSgrmzp3rFPefn5+P1NRU3HfffU7XPXLkiNM5jhw5Ihk21KdPH1y5cgV6vR4dO3YU3adHjx7Yu3cv7r//ftHtBoMBVqtyvsPkyZPx+uuvY8KECbhw4QImTJjg1I6tW7eiY8eO0GtQmgoPD8e9996Lr7/+GhcvXkRCQoI9BOj333+HxWLBa6+9ZjfqN23aJHu+Fi1a4PTp006fHT9+HAaDAYAt1CgoKAjZ2dleh/iIQWE/BEEQzZgwow7P3NwKU/q1xrQbWmPln/piwYgOZPgTRD0xvEsMpOx/lrFt9wf/+te/YDKZMHXqVBw8eBCXL1/Gzp07MWXKFLRp08ZNz/7w4cP44IMPkJGRgc8++wybN2+WTJwdMWIEBg4ciJkzZ2Lnzp24dOkSDh8+jLfeegu///47AODPf/4zNmzYgLfffhtnz57FyZMn8cEHH9jP0aFDBxw6dAi5ubkoKiqSvI+7774blZWV+Otf/4qhQ4ciPj7evu2RRx5BaWkp5s6di6NHj+LChQvYtWsXnnnmGcWJxfTp0/HLL79g2bJlmD59un0i1KlTJ1gsFnz66ae4cOEC1qxZgy+++EL2XMnJyfj999+xevVqZGZm4u2333aaDISHh+Pxxx/Hyy+/jFWrVuH8+fM4duwYPvvsM6xatUr23Gog47+RQkloBEH4Cr62FgDABAWBEVmOJggicMxLbo9OscFuEwCWATrFhmBesn9qbyQmJmLHjh3o1KkTZs+ejZtuugnPPfcchg4dim+//dZN43/evHlIT0/HLbfcgvfeew+vvfYaRo8eLXpuhmHw9ddfY/DgwZg/fz4GDx6MuXPn4tKlS/YE4qFDh+LTTz/F9u3bMXr0aEyaNAlHjx61n+Nvf/sbLl26hJtuugndu3eXvI+IiAjcfvvtOHHiBCZPnuy0rU2bNti6dSusViumTp2KESNG4B//+AciIyNFQ3EcGTRoELp27YqKigpMnTrV/nnv3r3x+uuv44MPPsCIESOQmprqlFAsxujRo/Hss8/i9ddfx+23347KykpMmTLFaZ/nn38ezz33HP73v/8hOTkZU6dOxY4dO5CQ4H2yN8OTFSlLQUGBUxyWL2AYBvHx8cjVqCRRZbJiycEc7Mssh4XjoGdZDEuMpOV5CTztZ0I71NeBwV/9zOXmwrR9B5joKASNH++z8zZWaDwHDn/0tcFgsBuU9UVmZiYiIiI8Pl7Q+d+XUQIzx8PAMhjmZ51/X9OrVy88//zzmDFjRn03pVlRUVGBRIXaGRTz30ioMlkxZ81Zp2qcAJCaXohfsyqxZEq3RvNAIAiiYXHN8+99wh1BEN4TZtThz6MT8OfRCQGr8OsrqqurcfjwYRQUFNhVdoiGBa3vNhKWHMxxM/wBm+zXxZJaLDmYUy/tIgii8cPX1tn+J9h7FQmCIHxLYzL8AeDLL7/E3LlzMWfOHLtGPdGwIM9/I2FfZrnd8L859wQ6l+c6bY84r0NtlrRMVXOEYYDCiAjUVlSAVu79C/V1YPBbP3O2RDfGB1J7BEE0b+bOnetU9IpoeJDx3wiw6f5e8/knluWA5Z3XABgrwFvMYOAfDwEP3m/n9hc8A/BmC3iLBSCD1K9QXwcGf/cz27q1V8c3tvAEgiCI5ggZ/40Am+6vLUKL4Tm74f9t58EwsTZN2NYRBsycLJ397glVJiu++CUXBy5UwMpx0LEshnSKwMwb4xtHfgHDIDYuDqb8fJA72s9QXwcGf/azTgfGQdtbLSREQBAE0bgg47+RMCwxEqnphWAcKv9VGENhYfVgGWDg9S3BhIf77HpVJivmbhQSjK8Nk4tna3CgIKdRJBgzDANdZCTYqipS7PAz1NeBoaH1MwkREARBND4o4beRMGdwWyTEBMPIXytCYWVYm+5vTDDmDG6r+ZxyxgMlGBMEoQQ9JwiCIBof5PlvJIQZdVgypRtSdmcg4qIOFrBoExmMZI3L62qX6B0TjF3heGB/ZjkWjPDBjREE0Wih5wRBEETjg4z/RkSYUYfHb46DKac1YNDjkQd6ajpe7RK9a4KxGBaOp+Q+gmjG0HOCIAiicUJhP40N61VJPp32eZvaJXrHBGMpdCxDL3SCaMbQc4IgiIbMU089hYceeqi+m9EgaRDG//bt2/HEE0/ggQcewN/+9jecOnVKdv99+/bhL3/5C2bMmIE5c+bgo48+QkVFhei+P/30E6ZMmYJ33nnHH00PPFeNf+htxr+WpD81S/QCwxIjwUq8s1nGtp0giOYNPScIounw1FNPoXXr1vZ/SUlJmDp1Kk6cOOGza7zzzjsYNWqU7D4vvPACbr75ZtFtubm5aNOmDbZu3eqzNjVH6t34P3DgAJYtW4aJEyfi7bffRvfu3fHmm2+isLBQdP/Tp09j0aJFGDVqFN577z08++yzyMjIwCeffOK2b0FBAb788kt07+5bCcx6xWqFycph1/lyTEw5gXGfH8fElBN4f08WqkxWycO0LNED1xKMXV/s3iQYEwTRtKDnBEE0LUaPHo1jx47h2LFjWLduHfR6PWbMmBHQNkyfPh3nz5/HoUOH3LatWrUKsbGxGDNmTEDb1NSod+N/69atGD16NG655Ra0b98es2bNQsuWLbFjxw7R/c+ePYvWrVvjrrvuQuvWrXH99dfj1ltvRWZmptN+HMfhf//7H6ZMmYLWXhauaUhU15qw9UQRjuRUI6/ChMIqC/IqTEhNL8ScNWclJwBal+iFBONJfVoiPsKIVmEGxEcYMalPSywm+T6CIEDPCYJoahiNRsTFxSEuLg69e/fGU089hcuXLzs5ZHNzczF79mxcd911SEpKwkMPPYRLly7Zt//0008YM2YMOnXqhK5du+Luu+9GVlYWVq1ahXfffRcnTpywry6sWrXKrQ29e/dGnz59sHLlSrdtq1atwn333QeWZTF//nwMHDgQHTt2xODBg7FkyRLZexswYAAWL17s9NmoUaOcIkPKy8vx3HPPoUePHkhMTMTEiRNx/Phx1f3XWKjXhF+LxYLMzEyMHz/e6fM+ffrgzJkzosckJSVh1apVOHr0KPr374+ysjIcOnQI/fv3d9pv3bp1iIyMxOjRoxXDiADAbDbDbDbb/2YYBiFXC974OmZVOJ8n511zJA+1NRaYQ5xfqkLc/tKDuVgwsoPoscMSo5CaXgBOJFKIZYDhiVFObQoP0uPZkR3x7MjGWbnTm34mtEF9HRgaYj839ueEGA2xn5sqzaWveZ4HLJbAX1iv97hvKysrsW7dOnTu3BmxsbEAgOrqakyYMAGDBg3Cpk2boNfr8d5772HatGnYvXs3WJbFzJkzMWPGDHzyyScwm804evQoGIbBuHHjcOrUKezatQtr164FAERGiocGTp8+Ha+//jrefPNNhF+tYXTgwAGcP38e06dPB8dxiI+Px9KlSxEbG4tffvkFf/7znxEXF4dx48Z5dL88z2P69OmIiYnBypUrERkZiS+++AKTJ0/GwYMHERMT49F5GyL1avyXl5eD4zhERUU5fR4VFYXS0lLRY5KSkvD0009j4cKFMJvNsFqtGDhwIB555BH7PqdPn8bOnTs1xflv2LAB69ats//duXNnvP3222jVqpW2m9JAmzZtNB9zLLsCXQFwIl58jgcOXKrEO/Hxose+MrEV0vJ+wrkrlU4TAJYBurYOx8sTb0B4UNMTgPKknwnPoL4ODP7s56ZiwPsCGs+Bo8n3tcWC6i+/DPhlQx98EDAYVO///fffo1OnTgBshn5cXBy++uorsFdtjo0bN4JlWbz//vv258T//vc/XHfddfjpp5/Qr18/lJeX4/bbb0fnzp0BAN26dbOfPywsDDqdDnFxcbLtmDRpEl599VVs2bIF999/PwBg5cqVGDhwIJKSkgAAf/vb3+z7JyQk4JdffsGmTZs8Nv7379+PU6dO4eTJkwgKCgIAvPbaa9i2bRu2bNnSpJKHG4SlJ/aikXr5ZGdnIyUlBZMnT0bfvn1RUlKCFStWYOnSpZg3bx5qamrwwQcfYO7cuZIzSjEmTJiAsWPHul2/oKAAFh/P1hmGQZs2bZCXl6cpYZfneVhNttUJCyO+nF5nsiAnJ0ey/z6a2AVLDuRg3/kyWKw89DoGwzpHYc6QtqgoLoB42nTjxNN+JrRDfR0Y/NXPVSYrFh/IwX6H50Jy5yjMHaK+hkhTgsZz4PBHX+v1er867poyQ4cOtTtOS0tLkZKSgmnTpmH79u3o0KED0tLScP78ebthL1BbW4sLFy5g1KhRmDZtGqZOnYoRI0Zg+PDhGDdunKKx70pUVBTuuusurFy5Evfffz8qKyuxdetWvPHGG/Z9li1bhq+++grZ2dmoqamB2WxGr169PL73tLQ0VFVV2ScXrvfWlKhX4z8yMhIsy7p5+cvKytxWAwQ2bNiApKQk3HvvvQBss73g4GC8/PLLmDZtGsrKylBQUIC3337bfozwQJk2bRoWLlwo6mEwGAwwSMyO/fXw53le87kNV/V6rBLx+7qrmXdS5w01sJg/oj3mj2jv5uFrqi85T/qZ8Azq68Dgy36Wrv9RgF+zKuz1P5ojNJ4DR5Pva73e5oWvh+tqITQ0FImJifa/+/btiy5dumDFihV44YUXwHEc+vbti48++sjt2JYtWwKwrQTMnj0bO3fuxMaNG/HWW29h7dq1GDhwoKa2PPDAA5g0aRIyMzNx4MABALCHiW/atAkvv/wyXn31Vdx4440ICwvDhx9+iKNHj0qej2EYtzHm6NzlOA5xcXHYsGGD27FSNmljpV6Nf71ej8TERKSnp+Omm26yf56eno4bb7xR9Ji6ujrodM4vImE5iud5tG3bFu+++67T9lWrVqG2ttaeTNyYuSE+GGXnAauI51+rtB4t7RMEoab+x4IR4nlEBEGog2EYTeE3DQWGYcCyLGpqagDYcjI3bdqEVq1aISIiQvK43r17o3fv3njmmWdw5513Yv369Rg4cCCMRiM4BeVBgeTkZCQkJGDVqlXYv38/xo0bZ4//P3ToEG688UankG8l73zLli2Rn59v/7uiosIpUblPnz64cuUK9Ho9OnbsqKqNjZV6V/sZO3YsfvzxR+zcuRPZ2dlYtmwZCgsLcdtttwGwxXgtWrTIvv/AgQNx+PBh7NixA/n5+Th9+jRSUlLQtWtXxMbGwmg0omPHjk7/wsLCEBwcjI4dO0KvcRbc0BjXPQbRwXq3mH+S1iMIwhO01P8gCKJpYzKZkJ+fj/z8fJw9exYvvPACqqqq7NKakyZNQmxsLB566CEcOnQIFy9exIEDB/Diiy8iJycHFy9exBtvvIFffvkFWVlZ2LVrFzIzM3HdddcBADp06ICLFy/i2LFjKCoqQl1dnWRbGIbB/fffj2XLluHXX3/F9OnT7ds6d+6M33//HTt37kRGRgb+9a9/4ffff5e9t+TkZKxduxaHDh3CqVOn8OSTT9qdxwAwYsQIDBw4EDNnzsTOnTtx6dIlHD58GG+99ZbiuRsb9W4JDxkyBBUVFUhNTUVJSQk6dOiAF154wR6vV1JS4iQxNXLkSNTU1OC7777D8uXLERYWhp49ewZch7a+CGZ4jO3ZAgY+Bhd1Rlg4HnqWQXJiJOYMbp7xuQTRnPEmQVdL/Q9aKSSIps/OnTvRu3dvAEB4eDiuu+46fPrppxg6dCgAW1jQpk2b8H//9394+OGHUVlZiTZt2mD48OGIiIhATU0N/vjjD6xevRolJSWIi4vDI488gpkzZwKwOXy/+eYbTJw4EWVlZfjf//6HadOmSbZn2rRpeOedd9C1a1enwl8zZ87E8ePHMWfOHDAMgwkTJuDhhx/Gjz/+KHmuZ555BhcvXsQDDzyAyMhI/O1vf3Py/DMMg6+//hpvvvkm5s+fj6KiIrRu3RqDBg1qcjkkDN+kg+y8p6CgwEkC1BcwDIP4+Hjk5uZqjnE0//ILrCdOQterJwwDB9pfyv58OTfWF783/Uxog/o6MAj9fO5iNhYfuIx9meWwcBz0LIthHjoAJnx+HPmV0s+4EAOLzX/q1SAcC4F6FtF4Dhz+6GuDwVDvxlpmZqZsWAxB+IuKigqnvA0x6t3zT2jEaivixej1qDJZseRgjk8MAFf8eW6CIDynss6C2avPiCToFuLXrEpNCbpVJiuqzdKVwQGgxsxhzpqz9vMG2hlAzyKCIAjfQsZ/Y+NqZnotx2CuqEKHOgNA7gUurf6h3bggCMK3vPXtKZwvrnX73JME3SUHc1BZp5x8d6G4FvPWnkWlifPKANc6caBnEUEQhO8h47+xcdXzv/FUCS4WR2lS6FDrQSP1D4JomFSZrFj9S5bkdo4H9mWUuf0+xWR9GYbBvsxyqAm04AGcK3KecKg1wL3x3NOziCAIwveQ8d8A0OIN4y024/9Ibg04o7juLMcDW08UOb1c1XrQeJ5Xpf6xYITauyMIwld88tNlWDh5c/1KpRkTU05gUEI4AAaHLlbAwnFgGQaRQTpU1Flh5XnoGAZltZ4XMOR424qAnAHuree+Pp5FjTXHiSAIQi1k/NcTlXUWvLc7C/syyxS9YU4vI6sVPHiYIf9yqrHweHT1GXw6NQlhRp2sB81xSd9staKkRj4GmNQ/CKJ++OmCsuwmByCvwoSNx4vdtl2RSez1BB42Qx6A6LNL6bkjN3EIpBJRZZ0FSw/l2lcnDDoWY3oVY0bfKIQa6l0Rm2iE0PuRqC/UjD0y/uuBKpMVMz/6CefyKyW9YQCclsp1DIPhXaLwqMkMAxhAp/zVXSyps79c5TxoYkv6cuhYhh5sBBFgeJ6HxdrwlGc4XtqTr/TccZ04OBryDMNAL1HJXMCbZ5EQjrQnowxFVWa4du3ygxew53Qw5RV4SXN1FDEMA47jnHTkCcLfcBxHxn9DZfGBHJy7UikZx/rh/myk5VTjQnGtUzzu2rRCmLMu4tHrgtA/IQpHs5WvtT+zHPOHK3vQ1KK1ijBBEL6BYRjodQ3TiBKLwVfjued4YF1aIb47XYxQgw5WnndaBR2WGInU9EKIRTp58yySCkdSuidCHaTQBMTFxeHy5cuIiIigCQAREDiOQ0VFBdq1a6e4Lxn/9cD+82X2l1mYqQbXl1yEnrsWalN8hUVrC4fWIscaaqqx6rcyWEbZyjMrmfTCy1fJg6YGqiJMEPVLcucopKYXiBrD3hCsZxATYoDJyqHGbHtmhBpZGFgWIQYGmcXSVTgFXGPw1XjuAdsKQEUdhwoH1SFhJWHh+C74NasSF0tqne5ZzbNIyuPM87xkOJLSPRHKkEKTjZCQELRr1w75+fngeZ7qRRB+hWFsq6Dt2rVDSEiI4v5k/AcY16X7HsUX0K3kktM+DAPIPSd4ANsu1oAJiVC0/nUsC4ZhZD1oatCzwNgeLfBEcrtm8eAmiIbI3CFt8VtONc4VVPn0vNEhBqQ+3NPJYBbi4LecKFJ9HtcYfE+fO4LXfcWRfCyZ0g1LDuZgf2a5YkVzKY/zjAFxWHEk3/55cbVF0fCXuidCHlJoukZISAg6depU380gCDfI+A8wrkv3RqstAS8nvBUKQqKu7iNv/FcYQ1EWpGz4A9eWxecMbivqQVMLxwMGHUOGfyODjJamRZhRh5sTW/jU+HcMnxHGSpXJirlr/1DlHXdEiMF3jKf3dPRxPLD3qmyp7Z9n9UnWpRVi47EimxHvQTsox0kbpBZHEA0fMv7rAcelez1vC/fJDm+FP2I6gGWAIB2DGov3S4Qdo4Psy+JhRh2WTOmGxQcuY/2xIo88cfTQbhw0tHhbmoD4lj1nC3x2LiF8ZvageKfP1YbFuJ5rWGKkqnh6tRRWmVFZZ0F4kO1VJTeOpNrMAzB7uORJOU7aCKRCE0EQnkPGfz0wd0hbpOXV4NyVSrC87UFpYXX2F3GftmHYeFz9UrsUrs/WMKMOz47siP3nK5BXYdJ8PnpoN3waSrxtQ5uANBV4nofZR4o/DIDE2GBU1FkxfcUpp+9IznsrRcfoIMwYEId5a8+KViD2BCsPLD2Ui/nD28t6/JcczLGFF/nkqjZYBugUSzlOWvC3QhNBEL6BjP96IMyow/rHh+L19UfB59lCaWLDgzCwT0v7i+bHP0qcEuA8Iau0TjS+clhiJNalFWpeAqeHdsOnPuNthYS2hjIBaYpUmzlU1flGq59lgMwi9+/ol0sVMHugDtazTSjmb8zwmeHv2KZd50pFJ5DCWHNVRvMGHQO0Cjfgzt7t8ADp/GvGXwpNBEH4DjL+64nwID0WjOyAuoo4WAt1ePCWJOg6XDPKVjzQHQ9+dRrldfIFt+SQCtUR4v+1vqSHdaaHdkMn0PG2gtd1//lycDgJFhzCDKyoMSZMQBYfuIxnR3b0XSOaAVUmKz7cn42tJ4th8ZF7W2wBgeOBS6V1CNZrN3i/OVWiaj9BWcjC8WDAo6DKImu4czxQWGWrROw6gRQmu94Y/iwDtAg12BOJZw+KR0SwAfHx8cjNzSWVFo1I5ZeRWhxBNBzI+K9neKsVDBgweuevolW4EakP98SH+7NFK3WqRSxUR4j/n7f2rOriXnoWmDOEHtoNmUDH23oS283xwPpjRdh/viIgYUBNIUxN6Gdfe9SlEAw2loHPJUUBd2WhCZ8fR77KysOuK1iehCc5wjLApD4tZcOKCG0I7xe1Ck0EQQQeMv7rG4vNowW9+1cRZtThr6MTADAe5wBIheqEGXX4+L5uNuNNhQLQ2B6x9NBu4AQ63taTpFDAZsDlVZj8FgakJt+gMU0KhH72JTpG3PMvEGJgERdh9FgdTAoxZaHhXaI0yYEKK1hqixcyAPQsAyvPS3qiG8tYaCyEGXWqFJoIgqgfyPivbyxXw3p04sZPlckKgIeeheblfqX4yjCjDovvuw5LD+Vib0YZCkVK3AsvyCeS22u7OBFwqkxWhBuljX9fx9t663X1Rx6CXL7B4UsV6N8uHIcuVjSqJGRv+1mMID2LarP0WQ061s17W2Wyyh6jBrGwD09kiC1Xd1Sa7AqefUHnnzzRgYcMf4JoeJDxX8/w1quefxHj31vJPJYBzFbbS9vxBeeowV1ea4HJysOoYxATokN0iAEVJis4DvSCbEQ4Jj6K4et4WzUhRmrwdR6CfMJzHS6WOFeqbehJyL7qZzGkwnqESaKr97bazHmVXNu1RTA+vs+9n0MN7hONomqz7ERAWMGSSy5lYDP8hYnl/OHtsWAEQ55ogiCaPWT81zdWm+ffNeYfUA6rCNaziAnR4+aEcJitwPYzzsmAFg7YfKIIaTlVduNGSh2j1sKj1mJFcY0VCTHBWHzfdXZtbaLho5T4mBgrbnh5ipoQI7X4Mg9Bq5e8oVcd9WU/OxKsZ6BndW6CAlKTRIZhnGK5tVbt1bNwGn9SoVnLH7geoQYWC/dmq1KMEVYNxCYkEUEsJvZpiff3ZJHkLEEQhAOkYVaP8BwHWK+aKiLGv5IhEx2sQ+rDPfHX0QkINbIQcxA6GjeAspHI8cCF4losPZSr7WaIekVprFSZOJ8bO8MSI8FK2OsMbJ7e+Aij5D4CvspD8NRLLqw+NFTk+tlTyuusqBBREgszsnh/fBfJsSKsBkzq01JTmxxzhgQHRGpaIfIqTCissthzQOasOWtbYRjcFgkxwW7XcJ2chBl1WDi+CyKC3NtbUcfhoa/OYJ3EdWwhlQRBEM0PMv7rEyHZF3AL++F5XlFr28pf01ZXI/GotJ/92rCFQ7y/J4tekI0ALSo/vkTOQOt8daUh9eGemNhb2lBUm4egpu3eeMn90T++QqqfvcHCQdQBUGXisOJIvs/axDK2VSfHnCE1tSiEVYZJfVoiPsKIVmEGxEcYMalPSyy+uoopfF8rjuSjUmQiI1T2lZKcFRwiBEEQzQ2K66hPrA4vLBfjv9rMobTGAjkEj6ka4y+/0oT/7LqkungPxzf8eOjmgJpwmPqqqukk6Xe+HDxYMOCQ3Nk5rGLukLY4kq1d99uTKsFyMeByNOQCdmL9XF5jkky+ZRngnh6xSM+tFu1zlpEWD1CbgyEm58gyQESQTjFnSG0tCmGVYf5w2w0wDCM6JsprLR4pTu3LKGuQoV4EQRD+hoz/eoQXjH+93s3wWHIwxx4RJIWjZJ6S8cfxwIbjRdBi3zT0eOimiq+NXn9W1RQMtGdHMmjTpg3y8vLcPOiCofjh/mzsOFOK2quWZ7CeRZ+2YaLn9bRKsFwMuBSNoeqoaz9nXLqM2avPSE6onhxm87S7aq0P7RyBnefKUFwt7VhQm4MhJ+codbyaFU0Lx6OyzoKlh3KdfgMD2odhb2aZ15XPBa5UmjEx5QTlAHgAJU0TROOGjP/65GrYD6NzN9z3KcQg61k4eUzVeDw5HuJr/TL4oypsQ6ShvMy8NXqljMHZg+L93nal/kvLqUatmbPfV7WZc0tIF1ATGiI2IdVawK4xVh11Tb6Vkq/ked7NOK8yWbFBoWaIJ6sgwgqkcJzU8WpWNBkGmL3mrJsy0zenTJrapAQH/9abaGp44pQgCKJhQsZ/fSJR4EtNGE9UsB6hhmuThhkD4rD9dImbeocYetaWZ6x2HuBLNZaGREN8mXlr9IqGYdRZMX3FKdn78/f3q/W+1IaGiBFm1KHSJP/7YRkgLtzYqKVsxTzvSmNa7Yqi0nhwvN7iA7ZwJOF6yZ0jMHdIO9E+VXP9cCOLzOI6+Z18CK1wKuOpU4IgiIYJGf/1iRD2o3P+GtSE8Rh0rP3lXGWyYv7GDFH1DjGigvUY3TUaezPLUFZrQa1FfhrQkOOhPaWhvsy8NXoFY7CyzoK5a/9AZpH0/QEI2ORHy31pSWCWCi1ROj42RI91s3o0mXEtGOJKY1ppRZEBsDejDLvOlbqNB8eJhclqRbWJQ53FPaF2XXoRNhwvwj09WuCJ5HZOx6amF8peX88CeRVmj/rAGzjedt9k/IvjqVOCIIiGCan91CO8hOcfkJf3c41RVpLvdMWgY7FgZAdseKQXfpzXF/f1lVZjYdDw46E9Qc3LLND4UrVn6aFc2fv7cH+2rNyiL1WetN6XtwnMDMNAp2DU+6Jarb/Rqj60+ID8mF584LLi98ADyK80u42HgkqT03gprraiVsTwF7BywMbjRU7HrktTTsSODNLZc0ICTWGVGZV18iFJzRW1anIEQTQOyPivT4QCXyIx/2p1rgFthY1cJw4Mw9ivJWUu7ckoa3Kynw3xZeZL1R6l+9t+uiRgkx9P7kvL5NcVm2EvP1ZrLDweXX2mwY3pKpMV7+/JwsSUExj3+XFMTDkh+9tz3H/9sULZ7/yn8xWKkyKx4y6W1OK5TRkeVRo/X1yLBRvPqXZOGESehYHCyoPqm4hQX1LCBEH4DzL+6xOLBTx4Uc+/Gp1rQFthIwbiyY1yhXJ42FQxmlJhnIb8MlNj9Mq1q8pkxXu7L+FKpXxyZI2FD+jkR4sxz/O8psmvK0sO5qBShSLMxZK6BqX1rlT8yvW357q/klfdZOUUJ0VicDyQ6YHhL5BZXKfqWJYBhneJQpC+/l5Ljd2D7Y9nVn1JCRME4T8o5r8eqDJZ8ermEzi15wz6XLqCgkwGhuAst1hrOSk9AbWFjVgGMOquJQS6XkuqUI5AU4rtbMgvMynVHga2REipeGxAOo/BE3yd5K2kRjRjQBze35OFPRllKK+1wGTlYWCBIJ3tewg1sjCwrKoE3X2Z5apD4BqSkpWaULRnR3ZU3F+KajOHWk9Dnfw8D3ac1JmtHDYeL/bvBSVojOIGgRAuqC8pYYIg/AN5/gNMlcmK2avPYPnBCyiprEO1iUNRHafoWZd7Gcl5VQU4Hqi18MiX8OKrCR1qSrGd3oSVCGj1sqnZX2zFJy7ccFW1hxONxxa+R63GoBy+nvzIrWS9P74L5m/MwLq0QlypNKPWwoPjgTqrbYWi1sIh3KjD8geux4IRHWQNGi0rYQBg4biAr/BIXU9rKJrWcD8GXtjwfrSFWQZOK5pPJLdHQkyQ/y6o0JZAGP6+GnNaV4s8xZuVOIIgGh7k+Q8wjgaa0WpTtbAyOq8861JeVSlcr6XFYGqMnjExlDzRvqo664lXznXFZ+HebKSmFboZbq7foxZjUA6WAZI7R3h9HtdxIrWS9f6eLNmYcI4HLpXWSf42HBOF1a6ECehYNiBjWWkcaA1F0/KbZRkgIToI5R4mObMMkBgbbAv98fE8iYHN8BeeQ4BtnCyd0g33fnZcUYlMjI7RRtzQPgI/X6xwqzxcViOvblZWa7la+CsKr0xs5eltieIPD32gVHjU1JUgCKLxQMZ/gBEMtBY1ZehbcA4AYGFtD04xKUe1lTbFHsxltRbJl73jtbQYTE0lttOTl5lWeVBfyIkyDKPoEf7mZDH2ZpThSqVvJBJZBtiVUYb957VXP7Xpvl9WNHAYhrFXcU1Nl05UFXD9bVSZrPhwfza2nylFnUPF4Nu6RWNQQgQ2nyhSZagGIlxB7TjQEoqm5jfrWsvgwa9Oy+6vZ2397DoZ7hgdhKTWobig0rmgFuHcZiuPiSkn3MZLdIgBeRXaC3sxDIMnktvhr6N1bs9PQQJXylFSa+GRV2HCurQCpOX9hI8mdnGqp+Ip/pIW9kYaWCtqwlAJgmgckPEfQBy9dW2qrlXZzAuNtf+/VGl7rd5iABj3+XFZT5+jF19NheCmFtup9WWm1cvmC6+cGg9vtZnzqWylhQOKqmySh1qMk8o6C2avPiNr4AC2ftmTUYaiKjOsGoxJYbxWmzk8uvqMWwXYajOHTSeKwTJAmJFFZR0nG+bSKSYoIOEKaseB1rhqpf0n9m7hlCOgtP/d3WNh1LNOk+GbE8Lx2+UqbDtV7NZ+BkCIgUWokUVVnRU1Krz0oQb26kTn2rk3Hy8SHS9aJnGOZDmsErn+psOD9FgypRs+3J+NHWdKJX83PICz+ZV4bM0ZfHyf+NjXYgD7w0PvbT0MbyDDnyAaNxTzH0AcvXVhlloAwIkWnXEpso3DPsDctX94FcOpNvzB0ZOoJPfZ1GM7fSGfqSUmW23+hNYwFl+jRfrz3e3uhr/jORxrC1yp1Gb4A9fG65KDOW6Gv+v1Kuo4hAexaBWuh6OIFcvYDNDxvVoErJCb2nGgNa5aaf+5Q9qp2l84Zv+FcuzLLEdyYiS+mnE9Uh/uCYOORVaJuFoPwwB394jFpkd6YfOjvdE5NlimF2xEBeux8ZGesucWxgvAyz6TpFDz20rLqVaV/HyuqNbpuatVilXA02eBXG5AQxYuIAiiYUPGf4AREk1DzTbjv9IYat/GMrYiN77SX9eS1CqEwUzu2xJx4QYE6xmwjC2UIi7C4CYx2tzwR0y2WjlRNQnd/kTtROWHU/myBs6OM6UeJyQ7jlelKrUCVSYOI7tEY+fj/XDg6f746al++G5Ob9zdIxaHLlZg+opTqo03V9QmbGoZB6EGVpW8r4BYEnWbcOnfquv+LUL1EFQ1hdUewdEwd+0fqDJZVRmtDMMgzKjD4vuuQ7BefqBaHNz4Suf++WIlFt93ndMzSS1yvy2tifHCc1cuuXa2TM0Irc8CLRMMXwgXEATR/KCwnwAjJJqGnbcZ/9V6m6qF4K2rqLP6LIZTa1LrtTCYDvalYortvBZfLoTCSKE1JlutV05rQrc/UAof4HkeZgVXfq2F8zghuWN0EGYPir96HXWGuvB7mT/c9ne1mbPFe3sYd60lYdNx3+Jq+XFTabJi0rKTTudc/sD1CDUoJyOHGXX237HQrn0OKwliEwAh1O293VlYn17odk4tFYEdx0V4kF4xTr/yatJxqIFVPHd+pQnTV5yCnmUxvEsUZg+Kx0Mrz6jKA3D9bTmOXa2J8Y6TXynHzIWSOoz77Dju7hErmt+i9lmgNTfAU+ECgiCaN2T8BxDBIKg2WxFlrQPDANbgUMRFGDA80fZym77ilOw5tMRweqPQ4GjENmeqTFbR+HJXPInJVuuV8yShW7iGryYLShMVhmFg0PlvrFypNNsNwRqz+pvKrzRh3OfHoWdZhBtZXBBRFVITd63FKNNab6HGzKHGfM2gdTyn0gTAm0TS/eflPe/fnipBrUX+DsQqM8vlDtWYOcxZcxZLpnRTNIg5Hih0yD355VIFzCoUjoTflthkLblzhKpzuGLheMVJQ7WZk+x3tc8CrbkBpMJDEIQnkPEfIBxf0gxnBWMygQdQwhjRVs/aH9S+juEkhQZl5PpFKb5cQCom21deOdfvsdrMYWLKccn9fSnPqHaicmv3OCw/eEHSwAnWsx4nJnua1OxoQCrtJ7eqpsUo87beAscD54trMe6z4wg1svbVgNmD4hERbPC4XQJCeImaRHI5xMaFMObPF9dKHie0S43IgIAg9xqsovqvUDRObFK0/lgRPHkEsoxyfwntFOt3tc+CvRllmld+6RlPEIRWKOY/QDi+pEOvJvtaWD1MrN6+ZPz+niwMSojwWwwnvRSuoTauVk18eaiBVR2TLRfDrRYh6bWyTtoYCTfq8J9xXUQTPBkAkUG2XA6hTeN7xSIhJsirIj5/HpMkm4B6e1KMP2tFeY1rnLjj/2tJ2FTyELMMEB9hVJSQrDZz9rjytWmFuHPJMYz//Bhe3XzCPk7Vtst1vE9adhLVJs8VouRCB4UVCymEdsklIEsdJ1xbiq4tgrF4SjesOJIvOSmyarxtlgGGd4lSnXgvliOj5llQWWdBYZW8XK9SnhA94wmCUAN5/gOEo77/HRcOAQCqDUEQ3FDCknGH6CB0iA5CVmkdxXD6CaVQicX3XYfwIL3q+PJQAyNp7PjLK7cvs1xWxjLEwKJVuNEWEnAgB/vPi4cEOLZJCJPwNHwgPEiPpVOTsPjAZdFzAMCPf5SgQmbSopVgPVCr7NhXhY5lUG3mPAoVcTTKlDzEsSF6rJ3ZHeNTTmhazbDyQH6FGcsPXsCe08FYfN91qmLy7dr2Pqr+LFTklRoXoVflP5VkhoUEZ2HMma0cimsssisBIQYWcRFGNw86A6BzbLBdllNpAiZW00DqXh2fu2pXKhzHg/D7UnoWLD2Uq6iAJaz8koefIAhvIOM/ADiqPTAOJpujvj9gexFlldbh3p6xuKljhE9jOOllYYPnedlQifPFtbj3s+OIDjFgWGLkVW+f/ASguMaWsKlUi8FX/a9WPeS93VlXjX6bETuiSyTmDmnnlowIuCez6hjlMSc2ppQMnFCDzqfG/z09WmL24HgsPZiL1GPqDDMxWAYYlBDucaiIYziekodYr2PBsqzHEq5CaMnSQ7mqwgSXHsr1meEPAEYdg9mD4mXHudrwRdfxMmnZSdmEXoOOVYxxV/P7iArWY3TXaKdJ8c0J4QAYe2XgIKMeQzqGY/bgeHti9eFLFarCAMWSuB1/S2LPAjWrjOFGVrQgmnBees4TBKEGMv4DgOPLsDgoApu6DAPHMKjWu+tiCxJ3qQ/39Npb7I9y8o0R16qzxdUWWUNIqPKZml6IMKOygcbxsO/vTbVONQjfqZKCTFmtBetdKueuP1aEI9lVXlUilhpTrprygLuBo8Yo08q+zDIsGNkB80e0x66MUlXx/a4I3l0LB9FYdY4H5JZZXMPx1CZ3aol5F2vT/sxyVddS8oKHGlhEBetVJ5LXWnjMXfuH7DhXaldy5wi3z5WKDQr3ozTBVDP5MOhYLBjZAQtGSj9j27Zti9zcXLsHP8yoQ/924aqMf7kkbqmCYWp+G+eKnMdnanohDl+qQP924Th0saJZP+cJglBPgzD+t2/fjs2bN6O0tBTt27fHrFmz0L17d8n99+3bh82bNyM3NxehoaHo168fHnzwQURE2F4oP//8MzZs2IC8vDxYrVa0adMG99xzD4YPHx6oW3LD/lJjdU7a/mI4Kvp4Y/j7o5x8Y0Oq6qwaOB6orOMQYWRRoSI+2tNqnUoTPGG7FhUZMZEWbysRK42pLc+0gRw2RSAdlFZStJBXacatH6fh9qQY6Dz8rSTGBuM/47pg4rITmo8VJg6zB8UDsH1XapM7vZVwtXA8Zg+Kl73W7EHx2HWuVPY8YUYd1s3qAcD2Hb2/J0txUqI0zoV7E1NX4nlg57ky7D9/ws1I1ZokL/W7GZQQgY3Hi0S3uU7WxM4hdd5DFytEP1dC6dngaTE/23nr3CYkze05TxCENuo94ffAgQNYtmwZJk6ciLfffhvdu3fHm2++icJCd/1pADh9+jQWLVqEUaNG4b333sOzzz6LjIwMfPLJJ/Z9wsPDMXHiRLzxxhv497//jVGjRuGjjz7C77//HqC7ckdLcpsvqjKqMeiaA1JVZ9XCAwg16jC+VyxCDazi96e2IJZSwrHj9ns+O4ZbPk7DHYvTcV7hXhgAcoIo3lQiVhpT/9l+RvaeAf8UHao2c9h4vAhVJqtHxdCqTBy+/DVPcyJoiJ5BYmwwKuosuPez40j+4Dfc8nEaZqw4hb5tQ3Fvzxayid5iSaBKScCO6Fibtr5cIml4kF5xUiQ8b1yrfcuhNM7DjDosHN8FEUEiXm4AxdW2ROZ1ac6Vy+190tvzJPkqkxW/Xa6U3N4xOsij3ClvV66U+syXxfya23OeIAht1Lvnf+vWrRg9ejRuueUWAMCsWbOQlpaGHTt2YPr06W77nz17Fq1bt8Zdd90FAGjdujVuvfVWbN682b5Pz549nY656667sGfPHpw+fRr9+vXz383I4KjH/M3JYslldV9VZVRj0KktFtaYkas6q5aCKjP0LItNf+qFED2D8SknZMNLlGoxKHnQF47vgvkbMzyatBh1gFIOqWP7tFQfVRpT35/Kx5wbbXksUvevJW5aK5UmDpFBOlSarJo86RaOx/7z2j26Zo5HRpGzZ7vWwqO20ozNJ4qREBOsWKxLTMJ1zpqziqsBjs8JuTCYKpMV1WbplRax541Qrffez46j1iLdCKVxvuJIPirr5Fd5eNhCreatPYv/jOuCFUfynULKxPJUlFhyMAdZMuOrX7swj7zhnnrnHZHrsxkD4rD9dAnKFfpMLc3pOU8QhDbq1fNvsViQmZmJvn37On3ep08fnDkj7kVMSkpCUVERjh49Cp7nUVpaikOHDqF///6i+/M8j2PHjiEnJwc9evSQbIvZbEZ1dbX9X01NjX2b4BXz9l94kB7PjuyILbP7oFvrcHFJxNhgzB3SzuNrVJs5vLc7C1cq5atgWrhrShRN9R8AxaqzauB4YP0xm4ey1mqLF5ZDr2PAsqxku5YcFE/AFLx1z23yzPAHgDqrsoKJY/tYllW8Hx1rGydWhRPXma14b3cWJi07gfGfn8CkZSfw/p5sVJs5+72HGXX4bNr1mNCrhc0o1nqDCoQYWUzu0wrxkVe9xpHKkpo6For3JoaFk04FsCflHsyVHQuO/1iWtSsmTe7TCnERBojVTZN7Trhea8nBXEVJWNfzVJs5LD2UB5PCb0dpnMsVEXPlXFEtJi87iXVphcirMNklTtcfK8KcNWedxpDc713NdX++VKn6+eH62bDEKK+881J9Vm3mMH9jBip8ZPgLNJbnvK/bSBCEPPXq+S8vLwfHcYiKinL6PCoqCqWlpaLHJCUl4emnn8bChQthNpthtVoxcOBAPPLII077VVdXY+7cubBYLGBZFn/605/Qp08fybZs2LAB69ats//duXNnvP3222jVqpXnNyjD+ifi8J/tZ/D9qXxYrDz0Oga3dY/Dc2OS7DKTWh9ilXUWzPzoJ5y7UqloABbXWLDk1xL8+er1mioGnXzFZLUIxtxXaWUY06utbDGrO3q1RXx8vOS5Dl46JetBP19S5zNlFlfE2jemV7Hk/QBAeR2HKV+eRo1CtdeCShPWphU4fZaaXoDfcqpxc2IL7DlbALOVh0HH4NbucTj8j4F2ffPBb/0o6yUONbCoMXOy8qYAwDAs3p52Ixjmmhziq5tPSN4fAyAmLBhnr0iHiXgKxwMHLlXiHZmxAIivkryT0B4AUFFrxns7zko+J5Q4eOmUbJ+FhxjQ9eq1APXPEKVxzvM8OJxUbJ8jZpELOv7uXrm3p9v2yjoL3t1+Bj+cyofZykPPAmW1SqsNLNq0aaPq+dqmjXMeyysTWyEtT90zVoxburcR7bNXN5/AxRL3/AhvCTLq0bZt45CHdu1rgiD8R4Ow+sQewlIP5uzsbKSkpGDy5Mno27cvSkpKsGLFCixduhTz5s2z7xccHIx///vfqK2txbFjx7B8+XLExcW5hQQJTJgwAWPHjnW7fkFBASwWHwmJO5y7TZs2mHtTC8y5MdYpofO19Uex/3yZ/UWf3DkKc4eoU214b3cWzuVXqjIcrRx/VS88D0unJjXJpDCGYXxadZbjge+O5+DLB7pjz+lg8aTE2GA80DcKubm5oufgeR51JvnxxHtbklcCqfbN6Bslej8C1SYrqk017htUwPHAuYIqnCuocvrcdezdeX0sUtMLJL+nsT1aYN/5MuSWy69oMeCRl5fn9JnU/dnyIxiczqvwudElUGeyICcnx+15ZlOgypH9rQvPhTk3xtqfEyxrM1zz8vJQIVPsSTheaayZzVan9ql5hqgZ5wDA+mgKK/zuhJAygSqT1aNkfgac2xhx24dhEBcXh/z8fLeiWh9N7IIlB3Kw73wZSqrNsqFRruw/m49zF2Pcnrfbj+d4XYnbFZYBhnQMl/2OfImnynTC+zAvL0+2gJkW9Hq93xx3BNEUqFfjPzIyEizLunn5y8rK3FYDBDZs2ICkpCTce++9AICEhAQEBwfj5ZdfxrRp0xATEwMA9pckAHTq1AmXL1/Gxo0bJY1/g8EAg8Egus1XDySx8wrnrqyzSMSBF+DXrApVqg37MqVLw4sheNUWH7isSZ2mMfHnMUnYczpPVHUkzMDAoMH4BwCLlUeInpHVGg81sLJjRqcUN8BAVlpSCywDtAjV24pVSbTPtdhSSY02g8YTXMfenMHx+DWrQnRClRAdhNmD42GyWrHxeLHseZM7R7r1vev9Cd9XmJF1i9lXA8vY/ikshAC49l07tkk656MAhy+Vy8o2CudxfHaoub7a9ik9QxwLfCmN8+TOnkuZumKx8uA4zsm4XHzgsmbD3yYzGul2LgF7obvz5eBwAiw4JHd2ViQKNbCYPTgePHjsySiDucqsWJxL4FJpndvz1lZM0PdrfeFGna2dfnp/Ab6Vk1Y7pgmC8J56Nf71ej0SExORnp6Om266yf55eno6brzxRtFj6urqoNM5P1TYq0lYcg8OnudhNsuXTq9P1MotSuGpEkVTTwoLD9LjvxO6YsaKU26JdFVmHozGVR2WsXmqvKncq6RlnhgbjMxiz+QfXWkdbkTqrB6K7RPuZ85gK8Z9dhxysw9BFz6/0uRVGx3HnmNC/P7McpisHGquTsrKTVY8+NVphBrk7yEiSCep4iL2fU1MOaFo+IcbGYzsGoMjWZVOkzyzlcfmE0Wqk3Idkf+ty8s2CqE+ao0ktTUHhHMqPUNahBowf3h7VePdWylTR8QU0JTqF7jCwFYka29GGXadK3UzVNXKI8tJ7uoY2y9H6n7FnrcM430isRghBlaypoCa709pP5KTJojGS72H/YwdOxYffPABEhMT0a1bN/zwww8oLCzEbbfdBgBYuXIliouL8eSTTwIABg4ciMWLF2PHjh32sJ8vvvgCXbt2RWysbVl4w4YN6NKlC+Li4mCxWPDbb79h7969ePTRR+vtPpXwVp3HmxeIkmpHY+fLX8VVR5SKN4khJl2opd+qTFaYrRxYxt1AELTM/zPuqtqPl0aTYNypbZ8QRqG0EhJm1GHtzO6KqkdqcBx7jhOQOWvOoqTaZlSoXZkJlTB2XBHyAdRMlqvNPEIMLFIf7un0G6kyWZGWUyW6oiQQbmQxY0Cc2+dajVbBAfDh/mwYdDocvHQKdSYLdCyj6GXVopuv5hmiRYZYbEJXWmNR7SV3bKvrJErN9xesZxATYoCF48EytsJbFXVWlDskQDsaqmodMFL7AYCVt11Xq0qSNwXfpOB45xohajz018LRlD353jqsCIKoP+rd+B8yZAgqKiqQmpqKkpISdOjQAS+88II9Xq+kpMRJ83/kyJGoqanBd999h+XLlyMsLAw9e/bEjBkz7PvU1dXh008/RVFREYxGI9q1a4ennnoKQ4YMCfj9qUGL3KLci9fTF4gv6gr4El9PRPaflw9l0LOAVUa5xRE1ahxS7ZfzGOpZW1z7E8ntJL3gPM+j1sKraqdUUSQ5lhzMwSUVEpw61qZY4gtvpdjYkzOu5HA0dpRQO1l2nHg7ntfxO9p9zlZZ2PV7qTTZFFwcPaDerNBtPVkMjoMmL6vrWHINUXM9RstKgRrUSpkK+RdWnldd3Evp+4sOMSD14Z6orLPg8XV/IL/SfeXX0VBV64BRmrwpqSSJjXlfrpK4XkeNhx4APtyfja0ni93C2aTGGMlJE0Tjpd6NfwAYM2YMxowZI7rtiSeecPvszjvvxJ133il5vmnTpmHatGk+a5+/8ZXHzZMXiK/qCniLL2NHHeF5HhaFl3FUsB6ju0ZjT0Yprih4sqUMTLn2AzaDduvJYnsoi9h5DTrGqQCUWFjRxJQTyKuQTnplGSAu3Chp3IkhtD01vVDR4HYcL956K8XCThiG0ewZF9A6iR2WGIl1aYWKkympibfwHQFAqsh5xDyg3q3QuX+mxsuqJURNa4VdLQirO1KTkRkD4rDiSL6qSUqVyYpwo3Q/CmOrymTF3LV/4HxxreS+HA/syyiDRSGUymzlwHGc4uTNqGNgsvKSv4twI4sqk9XpnqT6pazWoiknyRHht6Xkof9wfzbScqol+0hsjPnKYUUQRP3QIIz/5oxgeJXVShudSga6Y9iE2Avk5oRw/Ha5ClmldT5/ofsCX8SOSr1kGIaBXkws3QGDjsWCkR2wYGQHReNazMCUa//hS7biUVkK8p2unjLH+3END5AzWMMMLJY/cL3qCZPQdrnwFUcSYoIxY0Ac3t+ThT0ZZZI6/ToGaBluwIB24dh3vgwVLlrzwtgTziVMmnQMI/tbkMKTSaxg6MoZhoDypEKrB9TXIR5avKxqcj+0rBR4gtxkRM0kxXHMiuH4XFtyMEdyP0esPBQnZUXVFtz2SbpiMnxksB5hRp3kbyqzuBZz1py1P9dcw94c7//9PVmereYywOxBNklRpfG540yppFPCcb+9GWWaJrENbUWZIIhrkPFfj8iFgQhIGehynmaxF6hdxcJPL3Rv8DR2VO1qQXLnKFkZSUej0ZOwB6UETrWYrLYCbUK8rY5hMLxLlNP9zBncFt+dLnYzpgWqzJymWFuh7WpsixADK1t9WM8yaBmmx6CECPBgcPBCObafKRGN8Q4zsvjn3Z08rmTsiKeTWMHQnbf2LM4VSRuSShNvrR5Qf4R4+NLL6k0yu1akzu1Yp8EVpTGbGBuMj++zGdb7MstVjW0hf0LO0OYB1CgY/iwDjLj6m5UaVxwPXLha1bjSxDk9u2YPikd4kN7rsdIizGCvF6M0PmstyvUzAKCwyozKOos96dzXIWIEQQQOMv7rkcUH5GObQw0s7u4RK5qUpcZT7hqjHKgXulY8iR1V0wfCS2rukLaSMpKuRqMnYQ+ehqm4UlZrQWq6s1d/bVohvjtdjBUPdEercCPCjDqEGnSSxr/WWFu1bWcAjO0RixVH8qWTHTkegxMi8XtOlaJBX2Xi8NK3Fzwy/AVFFcBWp+H2pBh7roRWwow6fHxfN9E4dDWTCjUe0KJqMxbuzbb/jl2968XVZtQpVdNl5aVF/eVlDdRzQktiqtKYrTJxdo+6mvwKwVD1dlLmOF7CjDpUmqSvzQNuE4O1aYVYn16IlmEGp0n/4vuuw9JDuW4ytVKKYMIEBPCtkpCVB5YeyrU7FrwJEWto7yCCaG6Q8V+PKCWiRgXrRT24TUllwdPYUTV98OzIjgC0hTJoDXvgeR5mDxI4xe9T/POKOg4PfnUaqQ/3RKiBhVUhNlnOC+z4uZbkUwbA1hNFsgnHPIBNJ4rA88rJ0xxvC3/wpOcc7eRaC4e0nCrpnVXgbaiLkseY490n5o7KRncsSZc9f6jBNsGRkhZtrF5WV0OfZRjUXlXkcbxNx74LNbCanhdqDF8h/GzJwRxUmaz2mH0141jAsf6BlomHK1YeyK80Y93VSX+oQQcrz7utDNidHyoMbyUPfZCOUVzREHB0LGj93UhN7OYOaae5nwiC8A4y/usJNYmoUkacp55yfyXUeuPB8TR2VE0fPDvyWhu1rHxo2bfabJMv9Balul7ldVb7pE5rf8l992q9ghyUQx4AaX1zURT2dZRqrKyziF7fVxNeb1bGBA+oXN6EY1EzYVIK2ApVKdV3CjPq8PjQtkjLqfJLIm59oCbkUcD1O9Yy/pUmZl1bOEjrehF+5lr/wFuPOw/bpL9CQpZUi+Gt5KHv0zYMG48XqWqX6zsp1MBqytOQWqnd8kwb1X1DEIT3kPFfT6hJRBUzej3xlPu6GIuvJxJaY0fV9IEtfv6SJk10MZSMwCUHcxSNNzWosZmFSZ2W/pL77n+5VIFBCRGKxar8hsKMx1Gq8c6lxyT387WsoNKKiSthRh0Wju+CB7867VZIzrWd648VYf/5CvtY3H++QrE9OpZBeJAeS6Z0w9KDuThwqRJ1JkuDytvRilY5V8fvWMv4lzJ8GQCdr+YGeCot64jjs1rQyvckcV0O10mQWsNbaaIAAD/+USIZSuiIjmVQfTWvSMvzXyrEVbin/2w/gzk3xmrqD4IgPIeM/3pESyKqgFZPOc/zPg0TUjORCDWwmjynMwbEYfvpEjfDyRuN77JaC1LTCmUnO76IO92XWS67XccA7aOD3JSWPEGY1GmJtZX77i+U1CG/woRwow6VJmtAJwBKlYwdx7+aCZY/ZAWVJrmO11txJF9VDQiOB/IqTPbJl5qQsWGdbf0QZtRhwcgOeCc+Hjk5Od7dXD3jSZ6MJ+NfjYfcFzk7gqyolFa+r+B44JuTxZodL2FGHeYPb48FI8QTqb+cfj0e/Oo0KmTyFACgrMaMSSknZEOzhHY4/n6uVJpkV2q/P5VPxj9BBBAy/usRLYmojih5vgYlhDvJJxZXW3xWjEXOmDxfXItxnx1HqJEVVaoRo8pkxfyNGaKGU5iRxfvju4ger7ScL6WJLqWyocV7KrzU9maU4YpI4SBH9CyDGjOnqP2tBmFSp2XJX8mwqbHwqLVYER7EItSoA8fZJiyltRZFSUNPUapk7Dr+1XrHvTH8XQ0iqUmuVCz2nowyTcWiOR64VFqHYL38JFbPAnOGuD8HBDWcxoin8fCejH9APqTL07Y4Ehmkw4wBcZiz5qyibGyogUXbSKPkpFcN1WYO1eZrcsRyK7hq6o8I2ywqGlRj4VFjEa+W7uhI0hLWBQAWK99oxzNBNEbI+K9HPE00lPN8dYwOsmn6K+jKO6LFa6pkTNpeTLY9XJVqxJCT7asycVhxJF90VUKuD1hGOnlWTGVDS/iT1pdanZVXnCCowXUlSE2MupqCRICtT6pMHO64PtYet6xU78ATBO3/4YnXJoVK41+tcSZ4x7UgZxhJTXLFYrHXphXCE4RxyzLSuRJje8Q2upAeJTyJh/dk/Etd25FqM4dqBW+3HBFGFl8+cL1dBUuJqGC9XV1KbW0NJaSKcAkFzrypP6K1HYIjSWsolV7HNOoJLUE0Nsj4r2c8eYnJGU1mK4/Nx4s0PdDVek098ZI5KtWIKeV4WiJeqg+Gdo7ArowyFClU6nW9jtrwJ1/EB3tCQkywvWiPK3LJvcXV6vrhWl+rS5TUgqCEMntQvGYJWjWGopR3XA6l8LVqkzUg33GIgUVchFFy9eOJ5PYBaEXg0TK+lFZCPV3xEcaAXAVdlgHu7WkLR9lxphS1V70KQToGY66PtUvMqg0dsnA8Qq/Wy1DKEdGCUITLbOWw/Uwp6iycrPqUlvojWhAcSVpCqVgGuK17nF/aQxCEOGT8NyC0vMSkjKaJKSc0GS1aZAI9VbBwVKpxNE7NVitKauRffnKrEq59ILRx//kTmtuoJvypymTFNyeLA2746xig2mTF9BWnZMOUtK5KuOLY174sRBUTogfPAw+tPCMbaiWVaKtkKIp5x5Um0rK5EMW1CNIHRoPcoGP9XlG3ISKXiBsRxCLkagiaP/tCGANydIq1TcDCjDr8dXSC03NGQItTRHC0rDiSj0oZw9/AMrDy2sIECyrN2Hi8WP0BfkDH2vpFbX+wjK2PnxuThIriAn82jSAIB8j4bwJ4otsOaJcJrDJZEW70TL5uf2Y55gzWbpwKL0spY04sdCPcyMqGUkghN9GoMlkxe/UZWS+hvxC0vwWkwpS8XZVwXAFyXFnZm1GGkmozPI2OKKu1YH26fPK1I2L672JJya7ecS0qVHKeSR5AnZ/yHVwRfk8NtQCfv1Ab8uXPvlDyTtvamIRQw7Vnnlh71DpFHB0tSteODdVjeJcop74pq7XIPn8C/2RyRrg/Nf3BMkBcuBHJV3X+w4P0UM7sIQjCV5Dx34RQ+9BtEWrQ7FErqDR5tUxt4XjFisauMLAZRxNTTtiNueTOEZg7xLbULuXpZmDzGALaPGdy4U9LDubgkg+XykP0DKKvatizDFBj5lQr7kiFKXmjWiK2AhRm1Nk9tPle5C1IJV+L3YPcd+qYlOw6frXI2aqZJAcq8jizuBZz1pwVrcrt1J4mOClQE/LlL9SMgYhgg5PhL4eaMCbB0aLm2hwPN3We9/dk+SwUz9e4OpKURCkm9m5hr3fR1MY1QTQGyPhvYig9dCf1aelUjEYNVSYrZnx1SpUOtBQ6lsH+8+qNU8GAzyhyTopbl16EDceLcE+PFgAgmZRp4Xh0aRGMWo6xa6KHGVlV0pKuaI1hVQN/9d+ILtcqXLp6QeU8fa5hSt6olsitAKkJjQBscfccr221RSzUSi7R1jUpWaydauRsvS3A5Evk8k3EVzKi8MrEVvXUWv8RaANQVS6JhiRUuTA5PQuM7dECTyTbfucL92Yr5uI4OiKE/wrXUFIU8jdqQrOU5Fipqi9B1C9k/Dcx1GhgazX8564545XhDwBhBgalCqsGtlUJPfQsizAj62b4C1g5YOPxIuhY6aVuHkCVmcPBv99m10SXWr0QM3615iZopdbCI6/ChPXHinAkuwpLpnRzy18Y9/lx2WV+xzAlbwzaxKvFjsRWgNROeqKC9binXztsP54Li9WmNKImRMo11EpdArj7+NWaOO7LhGZvEWuf1FhNTS9AWt5P+GhiF9VeaUIcJUeJliRUOQGCuUPaIdTAotrMqQp7lHJECNcY95n8c0EJQRWOh3Lib+twA0a4hB8NdVh9lcvHWnzfdVh6KLdZ5bIQRGOBjP8mhqfyoWJUmax4dPUZnyhDnC+pg9Kco3W4EamzetilJpXsMsXCT9ZrhmVlnUV1PQFvE2e1IOWZBqCpmBvguUGbWWy7vuv40LKaYNSzePXeXph7YwtwHIdJy046aZGruQdPqld7etycwW3xy6UKXPCx6onrBFatnrtj++RW2jgeOHelEksO5GD+iKapBBQoZB0lHiShuoYxCZVwH/zqNCwchyoThxoFo11qFc6xtojSOZTOP6l3S7s61ry1Z92kjx33HdElCgtGdMCcwbaqxfvPl2N3RplTlWrHZ4ZQ3Xj/eee8m0dvboOIYIPH7SYIwreQ8d8A8Ta+11MNbFeWHMzxmSQcJ8S5SOCYLOaLwjvAtWV7QF09ASGcJNBynlJKQ0qeSVfvoKcKPRwvHhuvKZGxc5TTZ2q+P9d70Fq92vE4ncIYdz0uzKjD0qlJXntRXXGcwNonkSq+D8f2LTmYI7vSxvHAvvNlZPx7iZyjxNskVLVefkeEsEwxg9pXzojW4UYsGHktvEyoOSC3UqwmnwaAZHXjtWmFWJ9eiJZhBgy72reO+TcU808QgYeM/waCFqUSLXjzYN2XWe7xsVKIxYa7ert8EZPNwNkgVQoLSU0vxK5zpdCzLMprpSsi+wsxj7aaEC5HpIwZJZUQ4JrEpWvsuepERgedfbXfn9g9aJ3wALgaYiQdliUXRnF3j1ifhv8MSggXVUz65mSx5Hfg2r69GWWK13Fc1SI8R8pR4rga5QmeOBBahBo05bNoRSqpX2ml+P09WbL5NB/uz0ZaTrVsLoKgWLYuvQjrjxWhU0wQqkwcrDwPg47FmF7FmNE3ikLZCCJAkPHfANCiVBIoeJ6H2erbOHfAFhs+umv01WVh6ZCk5M4RWJde5NW1dmeU4tXNJ/BAn0hV6hqFGgqDyaFjAIOOgcnKw6hjERWiQ0WtfAy8mEfbkxCuMKPOY5UQHrYxB8BulJutnKRsqmMio2tblCYNXVuI5xlonfAANuOoUsZTHn5VtUgMX9YzAIAtJ4sBMPY+EYzLGQPiMHnZSZhFLqJjGMwYEHc1ZOIyCqqUlZXUFuYj1CP0p80Rk4uDl06hzmSBjmU0O2I8EQiQ+k7Vnssxlj+rtE7170dppVjJcaK1tgDHA5nFzivKyw9ewJ7TwfXyriOI5ggZ/w0ALUolgYJhGBh0OgC+nQAYdCwWjOyABSPll3wfHNgGqelFHksu8gCuVJrtLxWlsBAtKNUQaB1uxLqroR9qDHA5pSG1IVxyK0eCgXtBIuzJEWEV5PAlW8BDVkmd27jUs8Dd3WPx5LD2ki9quSJOnWUSjD2Z8OzLLJe9rxADK1tLwGS1IkjPggEQamRRbeI8DgUSktHTcqqcDJkVR/JhkRg0Fo5HyuFcpOVUq/bwDk+MUt6J0IwvHDGehC1KPQPUnIsFEBdhtP9GAHflMLU5X67PF1+FYCpRn+86gmiOkPHfANibUaZJqSRQDEuMxNq0Qp+dTyzGW4wqkxXzN2b4RGtdeKkkxgajoMrsE++uUiSAo5HnKtOn5NGWMvAdP3fdR43BIhjUalYAbH0mnevB8bYEXzlDwpvEcy05K2o10x3PI9VfLGNTN/lsahLmb8zwajXA1ZCRm6DwALafLkGdhVdl+EeHGJzCrAjf4QtHjNawRTmvvJpztQo3IPXhnk6fuSqHCefSGioWSFnc+nzXEURzw2Pj//Llyzh58iQqKiowevRoREdHo7i4GOHh4TAajb5sY5Omss6CQoVlfrnKs/5kzuC2OHypQtIQ1DFAy3ADBidEAGDw88UKmKwcymotooWdwo0sZgxQls9Tqy2vFo4HKuqsSIgJ9kl4h9LhWkN4ZgyIE/XazxgQhxVH8u3e6RozDwZAiJGFwcGzr9ZgEYwWb2Pc1b6kfZF4rnSMJ0nCcv11qbQOK47kO31XcmNaCsc+UjNBqVFZUTgyiMX2BcPBVZV4HI9OSKNVMlYKtcpbIXoGY3u2kJ0QK+XBDO/ivgpUZbLiw/3Z2H6mFLVmDjxsq25BegZRwbbqwWrDmAIpi1tf7zqCaG5oNv45jsPixYuxe/du+2f9+vVDdHQ0lixZgs6dO2Pq1Km+bGOTZumhXFg1KIEEkjCjDp9OTcKH+7Ox40wpaq9aP8F6FrcnxeDxoW0RHmQbQsIDm+d5FFaZRTXKK00c5m/MUFw693VBLcD24nbVnS6q9s1KgCNaQ3ikvNDr0gqx8ViR7WXoch4hJEXw7FebrKoNFqUJnVq0vqT9OX61JgmrM/A6iMo27s8sR36lSbN8py8S2Cf3aYm5Q9shLjIYuVVenY4QwVOpWTHUFuSqs/J2YQUpY1xrHoyURDMPW32R2kqzpjAmX+fFyEG5LAQRGDS/kdavX4/9+/fjwQcfxH/+8x+nbf3798fvv//uq7Y1C9Qo6kgZk4EgzKjDX0cn4Id5fbHvyX7Y92Q//DCvL/46uqM9ln1iygmM+/w4JqacwMK92Ug5nIdKET19R0+0FP6KMdWxDMKD9JgzuC2SEyOhYxkYdd69ZFjG/W+p5XtXXCVIxSramkUMf0cEhZ6yWvlEZcFgAWzfZ/924YrtU6IhvaTnDG6LhJhgVd+HFgNPgGEY+8Qt9eGemNi7pdu1xHDso2GJkaqOkSIuwibRSMmQ/sNTqVkxhCJXwXr5fTkeyKswITW9EHPWnEWVSawOiW3VcFKfloiPMKJVmAHxEUZM6tMSi0WMdzUSzcKzY/GBy6ruRbi+P9V45BwnBEH4Fs2e/927d2PSpEkYO3YsOJeXaOvWrXHlyhWfNa6po8YQ0THA7EHxAWqRPGrjzBlGuvKu0tK5v2JMhyVGqtbLZhnbP6Uwj8TYYFSZOK8KqXm7ysEDMCksHbkaLIcueqpebqOhvaS15Bf4wsCbO6QtjmTLe3Vd+8gb72lD6++mjCdSs1KEB+kRHWJAXoVysTulnAKpEDqx0C+1Es08gHXpRfj2VInbSq7U9fdllqsq3qcVobCaGscJQRDeo9n4Ly4uRrdu3US3GQwG1Nb6Lla7qaPGEGkRZpB8INcncnHTSkHxJisnu3Su9AIO1rOa1Fj0LGRj4wWC9SxiQvRIToyE2cpj43F5qdEqE4fUh3t6HKPqq1UO41VZUTUGi9prJsQEAdAmGVifaMkv8NbAEyYbUkWNxPrIcYKyL6MMVyrNquUbG2J/N1U8kZoVEBt3WuLl1eYUCCFoYspeoQZWs0RztZnDxuNF2HKiCC3DDJL5AN4+r/QsEB2iBwsgMliPCpMVHGcrxnhHr7Z4gHT+CSJgaLYqo6KiJL37OTk5iI2N9bpRzQklQ2SESDJXQ8Abj7VQdEqrTKTwAu7TNgybTxSp9qBGBesRamAV2xwdrLOrZlTWWbDlRJFsPoba+F/HfAjHfX21yhEZrEeYUafKYFFzzRADi0+nJgEAlhzIUazJ0NBQG4/tiYEnIITDPZHcXnUfXZugdMDElBOyHmGWAeLCjY2iv5sSwiRt6cFcHLhUiTqTRfY7VSrOqEVmF1B+psjlCH13uhihBh1KajyTZxYKcUnlA3j7vLJwQLWJQ6iRRaWJw/DEKMweFI+IYAPi4+ORm5tLSewEESA0G//9+/fH+vXr7Um+gO2hUF1djW3btmHAgAG+bmOTRs4QSYgOapAeP289QBYOksvbwsu0ymS1e7SFQlnDE6Ps/ZGWU6U6hMKoZ69eV77NVgdJyPAgPVqGGZBfKa3EJBceItzHnowylNdart6HTWnDscS9t0oawgRRWNlQI6upNOEckxTtZtCM6HKtzY0dLWFCSpO7MKNOVd0KV5S+g4m9W+DZkR013hnhC4Tv9J34eOTk2PKTxL5btTUBFo7vIiqAIIZSyJlcjlBFHYcKmWJ3ahELQRLu/+aECGxSWBGVo9rMuQkWLL3qaCAIInBoNv6nTJmC3377DQsWLEDPnjYv6ddff42srCzodDpMnjzZ541syrgaIiYrh5qrD8dykxUPfnVac3VJf+MLj7XY8rbUy7TOwiHUYHTqA8c+K6kxo1ZGKrGsxoKFe7MVC325vniHd4nyKDxEuA9Xb5+gtLEuvQgbjhfhnh4t8PBNbSSLYelZBlZePJxHaIPgqdYS9iI34ewYHYTfLle5Ffdaf6wIR7KrmkwFTrn+UvLmSqEl9Etp9WHukHYe3RfhOyrrLHh/Tzb2ZZaJjgO1ErsrjuSLCiC4oibkzB9KaGJwvK3+DAC7A6POIi9AoLta/FCtH8PeTwdy8E5Ce6/bTBCEejRbcNHR0XjrrbcwdOhQnD9/HizL4uLFi+jXrx/eeOMNhId7ryTS3BAMkeUPXI/IYD1qr3pHiqosikoQ9YWcegkDKKpcuCqqAPJeLVeVIEf1lc1/6oXOse5qLwLVZg6p6YWoNlkl9xF78WpRkRG7D7mXoFAJdv7GDCwc38VNyWNy35ZYN6uH/fMWoXqE6BnomGsJycF6Fn3ahrmdW8kIlVMP6dcuXLSqrxqlpsaKWCJ7aloh8ipMKPTTb1CrggsRWKpMVkz86CekphVIjgM1krGAOoNdbU5BIKrtChRWmbEurRBXKm3OFSWj/p6esYiL0Fbjh+OBfefLPG8kQRAe4VEmaXR0NObMmePrtjR7fFFdMlAoeS6rTFbUagyZ8bTATniQ3r4S8M3JYtFkYI4HKkwcxMxiqRevp1VqtXjnLpbUYsWRfEkvtPB5ZZ0Fc9f+4TQ+qs0cNp8oQlqOdo+8lOd7YsqJBlltOlD48jeoKmTIyyJohH9YfCAH565USo6DxQcuq5KM5ThOlcEuNZF3xNMV15ahemx8pCf+u++yportSvVnXPn5YqVHYYwWq7sjiCAI/0Kp9Q0ItZ6khoCS53J4lyhNXnZP9Ndd27NgRAdEBsvPZ4Wj9SzQMkyv6G11XGHY+EhPpD7cEwtGuOutC+3S6p1z/V6lDMClh3IVjVJPcZQN9OY7aAp4+xusMlndal+8vydLccWADP+Gxf7zZZIGLMcDP52vUCUZy7KsKoNdmMgrrS55Ui9Cr2PBsuzVlcwgbQdrwMLxmD0oXnS1VL59DadmCEE0FzR7/j/66CPZ7QzDYN68eR43qLniy+qSgULOc6lVUcVb/fUqkxWLD1zGlUp1GtQcD4zsEqUpqVIs4U8sNlwpt8AVR4Na6v48XRXRgi+LHDUWXDXTvfkNqk0AJRo2PM/DouD2tnA8RnSJwvpjyjlBar3halaXtNaLcGyHa8X2GjMnGsrDXD1Oq+dfKKToulpaZbJKSjOzDDCsc8NUtCOIpoxm4//EiRNun1VWVqK2thahoaEIC5NfuiTEaeyGl2u7PAmZ8VR/XW3xLkcE792zI1UeoPKaqemFCDOyYK8mv6mh0mTFpGUnJZNLAzkx9GWRo4aKXEKv0m+wqNqMhXuzRcdwYwrbI6RhGAZ6herfOpaxF3sTS9Z3dHBoMdiVJvJiz1WWAWrMHCpNVkVHS5hRh7+M6oi/jk4Az9sM86WHct2e0XszymSVzlxxnWQ4OoWqzZztWSnlCBrS8BTtCKKpo9n4//DDD0U/P378OD799FM8++yzXjequdLUDC+tMc2e6q8rFe+SwhuDWc7Qq6zjEBGkQ0WdVZXyRY2ZQ41D1UxXT3EgJ4a+0MBvyCh55wclRMjWkOB4aU9+IFZniMCQ3DkKqekFss9iR0N8b0YZyhwkfYUJpl0ZSEOBN6XnkthzVbielKNFbsIr9YxWG7sv9mwQzsUwjMe5UwRB+A+flY7t1asX7rjjDqSkpOCVV17x1WmbFU3Z8FJjmAYiwdYRbwxmuWvysBXKGnN9DPZmlKG0xgwVSn92xDzFgZoYNvUXtZJ3vm/bUCTEBMt6aaV00Btb2B4hzdwhbZGWV2NL+pV5FjsW8iq4atQLkr6uk0S1Bd60PJeE/ZSka9WEo4mFbcoVJ9MxQMtwg1P9lff3ZElK5FJyO0E0HHxm/ANA+/bt8dVXX/nylM2KpmZ4efKQ1/qS8FT+zhuDWc01OR6YP7w9FozoYF/6dqwEyzLAlUqz5IvV1VMcyIlhU35RK3nnf75YieUPXI8lB3NkPZ+u309jD9sjnAkz6rD+8aF4ff3Rqzr/0s9ireFe/p7Iu44xT8LRpFc1rhVcnD0oHuFBNhNCS74L/QYIov7xqfF/8uRJREY2rtCUhkZjN7w8LZAkhti9u/aJJ/J33hrMWg09YenbsRLs+3uysC5dvlKmo6e4viaGjW38yaHWOx9qYDF/eHvsOleKwiqL7L6O47Gphe01d8KD9FgwsgPmj2gv+yzWGu4V6BVeT8PRHFcrhPuX6gdPJhiN8f1GEE0Fzcb/unXr3D4zm824ePEifv/9d9x7770+aRjR+Awvf6mdKE0o5IwuBkCXFsGoMnE+NZi9MfQYhsH+8xWK13D1FDf2iWF9o3XSptWT35TD9po7Ur81T8K9AjmR91U4mqMjQwy1EwzxZ3kUXpnYStX9EAThGzQb/2vXrnU/iV6P1q1bY8qUKWT8N2P8oXaiZkIhZXQxAKJC9KgwWWHleOgY371gvTH01IYqDessP4EgtKNl0qZ1gtfUwvYIZTwN9wrURF5N+1jGu+eJ2gmGWKFCAEhNL0Ba3k/4aGIXhBqo9BBBBALNxv/q1av90Q6iCeAPtRO1Ewop+bvyWouT8eYrzXVvDD01L2Q9C5LA8wNaJm2eTPCkjLqmXBituSJ8v0o6/uFGFlUmq+Qzwd8TeaX2ldVaMDHlhFfhmWomQHKFCs9dqcSSAzmYP6K9pmsTBOEZPo35J5ovvlY7EfZTO6FwNboW7s1GalqhV6sQSm31xnun9EIe2yOWPMV+QMukzVtPfrWZU8x/ofCtxoVY2MqghHB0iA7CpZI60QT+zOJazFlztt6KvCnVGai18MirMHnlGFGzSqb0LN93voyMf4IIEA3C+N++fTs2b96M0tJStG/fHrNmzUL37t0l99+3bx82b96M3NxchIaGol+/fnjwwQcREREBAPjhhx+wd+9eZGVlAQASExNx//33o2vXrgG5n+aIluVvKYOnss6CpYdy7S9WHcOgrFY64RIQn1BomTS44mnCslYDTsmr/EQyvQT9hdikTWpMejrBkwtXO3ypAv3bhePQxQqvk+KJwCH1nW4+UYwO0UHoHBuEzOI6t+Pqu8ib6yS2pMaMWou7le5NO5WeZ7MHxWPXuVLZc1isJIVLEIFClfE/depU1SdkGAarVq1Svf+BAwewbNkyPProo0hKSsIPP/yAN998E++//z5atmzptv/p06exaNEizJw5EwMHDkRxcTGWLl2KTz75BH/5y18A2FSHhg4diqSkJBgMBmzatAlvvPEG3nvvPcTGxqpuG6ENpcTbcCOLiSknnAyeGQPisOJIPvZklKGoyuxRSXnXl4WnqxD+SlgWQ8qrPLRzBOYOaUdGYADQOtHTYpTIh6vV4WKJs5HojzFG+Ba57zSrtA7BemnnR30XeXOcxMrVGfC0nWpWyRRDHXUkhUsQgUKV8T9p0iS//Si3bt2K0aNH45ZbbgEAzJo1C2lpadixYwemT5/utv/Zs2fRunVr3HXXXQCA1q1b49Zbb8XmzZvt+zz99NNOxzz22GP4+eefcezYMYwYQSU2fY1gRMsl3upZBhlFzgVj1qUVYuOxIpsh7sF1pVR1PE3C80fCshzCC3nOYCsWX60BsDujDPvPV5An2M/4e6KntfBcfXuHCWWUVhNrLQ2/yJs/i9EprZIphgZ1jtJ0PYIgPEeV8T9lyhS/XNxisSAzMxPjx493+rxPnz44c+aM6DFJSUlYtWoVjh49iv79+6OsrAyHDh1C//79Ja9TV1cHi8WC8PBwyX3MZjPMZrP9b4ZhEBISYv9/X6Ikm9YYqDIJBmsZLFYeeh2D5M5R+O+Erljxaz72OXweZmDdDH/AVgnXrKZ+vAgsA3SKDcbcIe1E+3FYYhRS0wskXzTDE6Pcjtt/XiFU6Hw5nh3p2+9MyQhdOjWpUUwAGtuYXnJQOvnwYkktlh7MxYKRnhnhPM/D6sG4VjPGGls/N1Zc+9nT79QRvY4Bq7Emia9hGAYGnbIH3tt2io3PuUPaSYYGdW0djrlDxZ/lBEH4nnqN+S8vLwfHcYiKcp7xR0VFobS0VPSYpKQkPP3001i4cCHMZjOsVisGDhyIRx55RPI6X331FWJjY9G7d2/JfTZs2OBUw6Bz5854++230aqV//SH27Rp47dz+5PKOgtmfvQTzl2pdFHSKUBaXg3WPz4U4UF6u/cn+e2dHnn2HQkxsGgRHmSfUNzWPQ7PjUmyV5h05ZWJrZCW595G4UXz8sQbnI7leR4cTsq2gQeLNm3a+PQF9ermE7aXocvnghH6VVoZXrm3p8+u528ay5g+eOmU7ETvwKVKvBMf7/H5g4yngSqz8o4uqB1jjaWfGzuO/az0nYYYdKgxWyUdDnf0aot4L8aUrxjTqxjLD16ol3ZueaYN/rP9DL4/la/6WU4QhO/x+Nd26dIlXL58GSaTe+yg1tAasRed1MsvOzsbKSkpmDx5Mvr27YuSkhKsWLECS5cuxbx589z237RpE3766Se8+uqrMBqNkm2YMGECxo4d63b9goICWCzySadaYRgGbdq0QV5eXqOUAHxvdxbO5VdKSra9vv6o3WvK8zzqTN73X3SIHmsf6u60nFxRXAC5UlkfT+qKFb+X4rvjOfYXzbDOUZgzpK3osaxCoAYDDnl5ed7diAvbj+dIKv5wPPDd8RzMubHh56k0pjGtZkzWmSzIycnxeKI3uGM4UktrJL9bKZTGWGPq58aMWD/LfacsA9zeLRq/51SJJ73GBuOBvlHIzc0N0B1IM6NvFPacDq63ds65MRZzboy1P8sZhkF4kN6nY1qv1/vVcUcQjR3Nxn9dXR3eeecdHD9+XHIftcZ/ZGQkWJZ18/KXlZW5rQYIbNiwAUlJSfZiYgkJCQgODsbLL7+MadOmISYmxr7v5s2bsWHDBrz00ktISEiQbYvBYIDBYBDd5q+XLM/zjfIFvi+zTF6yLdNZsk3HeucpZxkguXOkva/U9lmogcUr9/bEnBtjwXGckyEndo7kzvIxqY5t8AU8z8NsVYi/tfJubW/INJYxrTQmhe2e3sucwfH4NatCUl5RDC1jrLH0c2PHsZ+lvlNB0ebx5HYAIJn0GmpgG8R3FmpgZZNzXdvpzzwF1+s0hP4hiOaAZuM/NTUVV65cwauvvopXX30Vzz33HEJCQvD999/j0qVLmD9/vvqL6/VITExEeno6brrpJvvn6enpuPHGG0WPqaurg07nHAMtxCc6Pjg2b96M1NRUvPjii+jSpYuGOyTk8CRhTEnTXg41FXPVoObl5U3VXk/b5EliMuE9Wqv3akVK/eTmhHD8drkKWaV1ARljhO9QW/chEJV7vUUpOVeLElZDvk+CIMTRbPz/8ssvGDduHJKSkgAALVu2RGJiInr37o3//ve/2LFjB+bMmaP6fGPHjsUHH3yAxMREdOvWDT/88AMKCwtx2223AQBWrlyJ4uJiPPnkkwCAgQMHYvHixdixY4c97OeLL75A165d7TKemzZtwurVq/H000+jdevW9pWF4OBgBAcHa71lwgFPDFalIjOi52CAluEGDE+MCpjqjbdFnTzB30YoIU4gJnpSBpZgWAVqjBG+Q0vdh8ZiEIsZ/kpKWAA010OhSQJBNBw0G/8FBQVo166d3dvuGPM/bNgwfPzxx5qM/yFDhqCiogKpqakoKSlBhw4d8MILL9jj9UpKSlBYWGjff+TIkaipqcF3332H5cuXIywsDD179sSMGTPs++zYsQMWiwXvvfee07UmT57sN+Wi5oRWg9XRqN6bUYZCCT1/R4N/9qD4ekkA86ZqrycEerWBsBHoiZ7jOAr0GCP8Q1P93pQkjz/cn420nGpVMrmeFk0kCMK/aLauwsLCUFdnK1ATFWVLDLr++usB2KQ7hW1aGDNmDMaMGSO67YknnnD77M4778Sdd94peb4PP/xQcxsI9XhisAoGDwCkphW6bQdsL5fhiVENRuc8EC/3+lhtIGw0BCO8qRqQRONFqZ7BjjOlqDVzivVQAlk0kSAIbWg2/jt27IicnBz069cPPXv2xIYNGxAfHw+9Xo/U1FTFxFqi8eOpwcrzvOyLhUf9VsGsLxqCEdrcoT4nCHU5XbUWd8NfwLFCcKCLJhIEoR7Nxv+oUaPsUnT3338/XnrpJbzyyisAbKsCL7zwgm9bSDRI1Bqsjsu+ZqsVJTVW2fMGugpmQzO2G1JbCIJoXqjJ6VJCeIYrrSA0R0cPQTQUVBn/y5Ytw+jRo9GxY0cMGTLE/nnr1q3x3//+F8ePHwfDMEhKSpKtoks0TeQMf7FlXzkCoW5DcagEQailoTgIAtUOpZyuYD2LarP0E12QydWqCkcQROBQZfxv27YN27ZtQ2JiIkaPHo2hQ4ciNDQUgE1BZ+DAgX5tJNE4kVr2lSIQ6jYUh0oQhBJVJite3XwC24/nwGytPwdBfTgqlHK6+rQNw+YTRbKCDyRjTBANG1Xre//9738xbtw4lJaW4tNPP8XcuXOxaNEinDx50t/tIxoxcsu+rjDQpm7jaTEYNXGoBEE0X6pMVsxefQbLD15AbrkJhVUW5FWYkJpeiDlrzqLKJB+66Mt2zFlzFqlphcirCFw7hJyuSX1aIj7CiFZhBsRHGDGpT0ssntINTyS3Q0JMMFzr5LkKPgxLjHTbx3FfkjEmiPqD4TVYURzHIS0tDbt27cKRI0dgsVjQunVrjB49GiNGjLDr7DclCgoKYDabfXpOhmEQHx+P3NzcJlvRkOd5jPv8OAqrLJL7MACC9AxMVh5GHYOoYD2Gd5HW9dfqBRPr54kpJ5BXYXLbVyA+wojUh3tqv+FmTnMY0w0B6mf/8/6eLKSmFYo6LlgGmNSnZUASVRtKO+SKgMkJPthXWSVWEBZfXWX1x5g2GAx2uXCCINzRlPDLsiz69++P/v37o7KyEvv27cPu3buxatUqrFmzBn369MHo0aNx8803+6u9RCNB3bIvYLLw4ADUWnjUVpolw28KKk148KvTKK9z9nZpCdfxpDoxQRDNi4aSqNpQ2iH2LFQj+EAyxgTRcPG4ilJ4eLhdb//ixYvYvn07fvzxR6SlpWHVqlW+bCPRSJFLHAMAi8ibjeOBC8XOMnBVJitmfHUKFXXuB8jJxrl6kSgOlSAIORqKg6ChtEMNctcnGWOCaJh4p+kFIDMzEz/88AMOHToEAIiMpDi+5oqrsT1ncFvJ2FCdzDuAB7A3o8z+95KDOaKGv4DgBQNsE4X392RhYsoJjPvsOJLf3on3dmfZ42MpDpUgCCkaioOgobTDlzSmthJEU8cjz39FRQX27duHXbt24dKlS2BZFn379sXo0aMxYMAAX7eRaMAoxeEvmdINSw7kYP/5a8u+QztHYMvJYlgt0vGdZbUWu6fIcSIghYXjUVlnwdy1fzgn9FaZkVpag1+zKrBkSjePqhMTBNF8UJK6DJSDoKG0gyCIpodq45/nefz222/YvXu3Pdk3Li4O06ZNw8iRIxETE+PPdhINEDnZzMOXKtC/XTgOXaywTwpGdInE3CHtEGbUYf2xItlzm6w2w5/neVhVJIHpWAZLD+WqqihJcagEQUjRUBwEDaUdBEE0PVQZ/ytXrsTevXtRUlICo9GIwYMHY/To0ejRo4e/20c0YORlM+twsaTO6fP1x4pwJLsKi++7DkYdg1oZz79Rx9o9/2oqTg5LjFSdIEdxqARBSBFm1GHp1CR8lVaG747nwGKtHwcBJcwSBOEvVBn/mzZtQmJiIiZOnIjk5GR7gS+ieaNFxx+4lsy79FAuooL1qK2UllCNCtbZjfJhiZFYl1YIqalCZJAOswfFY9e5UtnriyXIkeFPEIQrYUYdXrm3J+bcGAuO4+rtOUGOCoIg/IEq4/+dd95BQkKCv9tCNCLUqFGIHgdbWFBibDAKKs2SGtbDu0TZ/xaWvy8U17pNACKDWHz5wPUID9I3uQQ5giDqn4byzGgo7SAIovGjSu2HDH/CFbXhOGJwPJBRVAsdyyhWiQSuLX9P7nut4mSbcAPu69sSqQ/3QqtwIwBS8iEIovFDxdsIgvA3Huv8E4SSjr8cPGxhOF1aBKPKxCnGs6pZ/qYEOYIgGiNaq5cTBEF4Axn/hMdIGdtq4QFUmTikPtxTUzyr1H5iCXJBRj2GdAzH7MHx9BIlCKLBIaeaprZ6OUEQhBbI+Cc8RkqN4uaEcPx2uQqXSuokk3QFfF2l0nGFAADatm2L3NxcWkonmgQ0jpse8qpp4tXLCYIgvIGMf8IrpMJxhGVspbAgfybhUoIc0RRwDAmxcjyCjKcxuGM45tBqVpNArUQxQRCEr/DY+K+ursbZs2dRUVGB/v37Izw83JftIhohjsa2MCkAQFUqCcJDRENCXKpW0wSg8aJGNc3Xq6MEQRAeybWsW7cOc+fOxVtvvYVFixbhypUrAIDXX38dGzdu9GX7iEbOnMFtkRATrErVhyAIZ9SEhBCNFzWqaSRRTBCEr9Fs/G/fvh3r1q3DqFGj8Pzzzzttu+GGG3D06FGfNY5o/Ah5AZP6XJPpjI8wYlKfllhMXkuCkEVNSAjRuCGJYoIgAo3msJ/vvvsOY8eOxYwZM8C5LFfGx8cjNzfXZ40jmgZUpZIgtEMhIc0DkigmCCLQaDb+r1y5gr59+4puCwkJQXV1tdeNIpouZKQQhDooJKR5IKWaJlXzhCAIwls0G/+hoaEoKysT3XblyhVERtISJUEQhC+QK6RHISFNB1odJQgikGiO+e/Vqxc2bdqE2tpa+2cMw8BqteL777+XXBUgCIIgtEEJ880PMvwJgvA3mj3/U6dOxQsvvIBnn30WN910EwBbHsCFCxdQWFiIBQsW+LyRBEEQzRGqWk0QBEH4Gob3oGRkdnY2vvjiCxw/fhwcx4FlWfTs2ROzZs1C+/bt/dHOeqOgoABms9mn52QYxp4cTRU7/Qf1s2+RC0egvg4cVLXa/9B4Dhz+6GuDwYBWrVr55FwE0RTxqMhX+/bt8eKLL8JsNqOiogLh4eEwGo2+bhtBEPWMY3VZC8dBz7IYRomI9QaFhBAEQRDeojnm/8iRI3aJT4PBgNjYWDL8CaIJIlSXTU0rRF6FCYVVFuRVmJCaXog5a86iymSt7yYSBEEQBKERzcb/O++8g8ceewwrVqxAdna2P9pEEEQDgKrLEgRBEETTQ7Px//zzz6N79+7Ytm0bnnvuObz44ov44YcfUFNT44/2EQRRT1B1WYIgCIJoemiO+e/fvz/69++Pqqoq7N+/H3v27MHSpUvxxRdf4KabbsKoUaPQq1cvf7SVIIgAQdVlCYIgCKJp4lHCLwCEhYVhzJgxGDNmDLKzs7F7927s2bMHP/30E1atWuXLNhIEEWCouixBNBxokk0QhC/x2PgX4HkeRUVFKCwsRHV1NcmiEUQTgarLEkT9QUpbBEH4C4+N/7y8PLu3v7i4GLGxsRg7dixGjRrly/YRBFFPzBncFr9mVeJiSa3TBICqyxKEfxGUtlwT7lPTC/FrViWWTOlGEwCCIDxGs/G/a9cu7N69G6dPn4Zer8fAgQMxatQo9OnTB6xCmABBEI0HseqyepZBMnkfCcKvqFHaWjCiQ720jSCIxo9m4/+TTz5Bp06d8PDDDyM5ORnh4eH+aBdBEA2AMKMOC0Z0wIIRFHdMEIFCjdLWghEBbRJBEE0Izcb/O++8g4SEBH+0hSCIBgwZ/gThf0hpiyAIf6M5TocMf4IgCILwD6S0RRCEv1Hl+V+3bh1Gjx6N2NhYrFu3TnH/yZMne90wgiAIgmiOkNIWQRD+RJXxv3btWvTr1w+xsbFYu3at4v5k/BMEQRCEZ5DSFkEQ/kSV8b969WrR/ycIgiAIwreQ0hZBEP7E6yJfvmD79u3YvHkzSktL0b59e8yaNQvdu3eX3H/fvn3YvHkzcnNzERoain79+uHBBx9EREQEACArKwurV6/G+fPnUVBQgJkzZ+Luu+8O1O0QBEEQhFeQ0hZBEP5Cc8Lv1KlTce7cOdFtmZmZmDp1qqbzHThwAMuWLcPEiRPx9ttvo3v37njzzTdRWFgouv/p06exaNEijBo1Cu+99x6effZZZGRk4JNPPrHvU1dXh7i4OEyfPh3R0dGa2kMQBEEQDQky/AmC8CU+rcrFcZzmh9TWrVsxevRo3HLLLXavf8uWLbFjxw7R/c+ePYvWrVvjrrvuQuvWrXH99dfj1ltvRWZmpn2frl274sEHH8TQoUNhMBi8uieCIAiCIAiCaCr4NOwnMzMToaGhqve3WCzIzMzE+PHjnT7v06cPzpw5I3pMUlISVq1ahaNHj6J///4oKyvDoUOH0L9/f2+aDrPZDLPZbP+bYRiEhITY/9+XCOdrzt6cQCxjUz8HDurrwED9HBionwMH9TVBBB5Vxv+3336Lb7/91v73v//9bzePuslkQllZGQYNGqT64uXl5eA4DlFRUU6fR0VFobS0VPSYpKQkPP3001i4cCHMZjOsVisGDhyIRx55RPV1xdiwYYOTjGnnzp3x9ttvo1WrVl6dV442bdr47dwNkco6C97dfgY/nMqH2crDoGNwa/c4/HlMEsKD/Jd+0tz6uT6hvg4M1M+Bgfo5cFBfE0TgUGVxRUZGon379gCAgoICxMXFuXn4DQYDOnbsiLvuuktzI8Rm/FJegOzsbKSkpGDy5Mno27cvSkpKsGLFCixduhTz5s3TfG2BCRMmYOzYsW7XLygogMVi8fi8YjAMgzZt2iAvLw88LyLk3ASpMlkxe/UZXCyudSpbv/zgBew5nYelU5NEFSy8WSFojv1cX1BfBwbq58BA/Rw4/NHXer3er447gmjsqDL+k5OTkZycDAB47bXX8Oijj6Jdu3ZeXzwyMhIsy7p5+cvKytxWAwQ2bNiApKQk3HvvvQBsFYeDg4Px8ssvY9q0aYiJifGoLQaDQTI/wF8Pf57nm82LZfGBy26GPwBwPHCxpBaLD1zGghEdANgmCksO5mBfZjksHAc9y2KYFxJ3zamf6xvq68BA/RwYqJ8DB/U1QQQOzQm/r7zyik8Mf8A2O09MTER6errT5+np6UhKShI9pq6uzs0TzF4thU4PjobLvsxyN8NfgOOB/ZnlAGyG/5w1Z5GaVoi8ChMKqyzIqzAhNb0Qc9acRZXJGrhGEwRBEARBNDE0G/+7du3CmjVrRLetWbMGe/bs0XS+sWPH4scff8TOnTuRnZ2NZcuWobCwELfddhsAYOXKlVi0aJF9/4EDB+Lw4cPYsWMH8vPzcfr0aaSkpKBr166IjY0FYEskvnDhAi5cuACLxYLi4mJcuHABeXl5Wm+X8AE8z8PCSZn+Niyczeuz5GCO7ArBkoM5/msoQRAEQRBEE0dzluW2bdswcuRI0W2RkZHYtm0bRowYofp8Q4YMQUVFBVJTU1FSUoIOHTrghRdesMfrlZSUOGn+jxw5EjU1Nfjuu++wfPlyhIWFoWfPnpgxY4Z9n+LiYvz1r3+1/71lyxZs2bIFPXr0wKuvvqrthgmvYRgGelZ+nqljGTAMo2qFYIH64UUQBEEQBEE4oNn4z8vLQ4cOHUS3tW/fHrm5uZobMWbMGIwZM0Z02xNPPOH22Z133ok777xT8nytW7eWXJ0g6odhiZFITS8EJxKZxTK27VpWCEgWjiAIgiAIQjseFfmqrq6W/JxTMN6I5smcwW2REBMM1sVmZxmgU0ww5gxuq2mFgCAIgiAIgtCOZuO/Y8eO+Omnn0S37d+/Hx07dvS6UUTTI8yow5Ip3TCpT0vERxjRKsyA+AgjJvVpicVTutlVfIYlRrpNEASEFQKCIAiCIAjCMzSH/dxxxx344IMPsGjRIowZMwYtWrRAUVERduzYgZ9//hlPPvmkP9pJNAHCjDosGNEBC0ZI6/fPGdwWv2ZV4mJJrVOIkOMKAUEQBEEQBOEZmo3/5ORkXL58GRs3bsS+ffvsn7Msi0mTJmHYsGE+bSDRNJEK3RFWCJYczMH+zHJYOB56lkGyFzr/BEEQxDUob4ogmjeajX8AmDp1KkaNGoX09HSUl5cjMjISffv2pYp6hE9Qs0JAEARBqMfXxRMJgmi8eGT8AzZFnVtvvdWXbSEIN8jwJwiC8A6heKJrDZXU9EL8mlWJJQ55VwRBNH08Uvsxm834/vvvsXDhQrzxxht2ec9ffvkF+fn5Pm0gQRAEQRCeQ8UTCYJwRLPxX15ejueffx6ffvopTp06hWPHjqGmpgaAzfjfsmWLzxtJEARBEIRnqCmeSBBE80Gz8b9ixQpUV1fjrbfewkcffeS0rWfPnjh58qTPGkcQBEEQhOdoKZ5IEETzQLPxf/ToUUyZMgWJiYlu8diC7CdBEARBEPUPFU8kCMIVzcZ/TU2NpKqPxWKhCr8EQRAE0YCg4okEQTii2fhv3bo1zp49K7rt3LlzaNuWijARBEEQRENhzuC2SIgJdpsAUPFEgmieaDb+k5OTsWnTJvzyyy/2GEGGYXDu3Dls27aNinwRBEEQRANCKJ44qU9LxEcY0SrMgPgIIyb1aYnFJPNJEM0OzTr/48aNw5kzZ/Duu+8iLCwMAPDPf/4TFRUV6NevH+666y6fN5IgCIIgCM+h4okEQQhoNv71ej1eeOEFHDhwAEePHkVZWRkiIiIwYMAADBkyBKxCYhFBEARBEPUHGf4E0bzxqMIvwzAYOnQohg4d6uv2EARBEARBEAThJ8hNTxAEQRD1DOnsEwQRKFR5/l977TU8+uijaNeuHV577TXZfRmGQXh4OJKSknD77bfDYDD4pKEEQRAE0ZSoMlmx5GAO9mWWw8Jx0LMshiVGYs7gtpSESxCE39Ac9qOUKMTzPPLz8/HLL78gKysLjz32mFcNJAiCIIimRpXJijlrzuJicS0cq+Okphfi16xKLCEVHoIg/IQq4/+VV16x//+rr76q6sQ7d+7EypUrPWoUQRAEQTRllhzMcTP8AYDjgYsltVhyMAcLRnSol7YRBNG08VvMf/fu3XHDDTf46/QEQRCNGorxbt7syyx3M/wFOB7Yn1ke0PYQBNF88Ejth+M4HDhwACdOnEBFRQUiIiLQs2dPDB48GDqdbZkyPj4ejz/+uE8bSxAE0ZihGG8CsE38LJyU6W/DwvGkx08QhF/QbPyXl5fjzTffxPnz58GyLCIiIlBRUYGdO3diy5YtePHFFxEZGemPthIEQTRaKMabEGAYBnqFmjg6liHDnyAIv6A57OeLL75ATk4OnnrqKXz11VdYsmQJvvrqKzz11FPIy8vDF1984Y92EgRBNGrUxHgTzYdhiZFgJWx7lrFtJwiC8Aeajf8jR45g2rRpSE5OtlfzZVkWycnJmDJlCo4cOeLzRhIEQTR2KMabcGTO4LZIiAl2mwCwDNApJhhzBretn4YRBNHk8Ujqs3379qLbOnToQElsBEEQLlCMN+FKmFGHJVO6YcnBHOzPLIeF46FnGSRTDghBEH5Gs/Hfu3dvHDt2DH369HHblp6ejp49e/qkYQRBEE0FivEmxAgz6rBgRAcsGKFcQ4cgCMJXqDL+Kysr7f8/efJkvPvuu+A4DsnJyYiOjkZpaSn27duHw4cP489//rPfGksQBNFYGZYYidT0QnAii6MU402Q4U8QRKBQZfz/6U9/cvts69at2Lp1q9vnf/vb37B69WrvW0YQBNGEmDO4LX7NqsTFklqnCQDFeBMEQRCBRJXxP2nSJPJKEARBeAHFeBMEQRANAVXG/5QpU/zdDoIgiCYPxXgTBEEQ9Y1HFX55nkdFRQUYhkF4eDi9wAiCIDRCz02CIAiiPtBk/J89exYbN27E8ePHUVdXBwAICgpCr169MGHCBFx33XV+aSRBEARBEARBEN6j2vjfvn07li1bBgBITExEq1atAAAFBQX47bff8Ntvv2HWrFkYM2aMXxpKEARBEARBEIR3qDL+z549i5SUFPTv3x+PPvooWrRo4bS9qKgIS5cuxbJly9ClSxd07drVL40lCIIgCIIgCMJz5KvOXGXr1q247rrr8Je//MXN8AeAFi1a4K9//Su6du2KzZs3+7yRBEEQBEEQBEF4jyrj//Tp0xgzZgxYmQqVLMvi9ttvx+nTp33WOIIgCIIgCIIgfIcq47+yshItW7ZU3K9Vq1ZO1YAJgiAIgiAIgmg4qDL+IyIiUFBQoLhfYWEhIiIivG4UQRAEQRAEQRC+R5Xxn5SUhB07doDjOMl9OI7Dd999h+uvv95njSMIgiAIgiAIwneoMv7Hjh2LP/74A++++y5KSkrcthcXF+Pdd99FRkYG7rnnHp83kiAIgiAIgiAI71El9dmtWzfMnDkTX3zxBR5//HF06dIFrVu3BgBcuXIFGRkZ4Hkes2bNIplPgiAIgiAIgmigqC7ydeedd6Jz587YuHEjTpw4gT/++AMAYDQa0bdvX0yYMAFJSUl+ayhBEARBEARBEN6h2vgHgOuvvx7PP/88OI5DRUUFAFsysJwEqBq2b9+OzZs3o7S0FO3bt8esWbPQvXt3yf337duHzZs3Izc3F6GhoejXrx8efPBBp2TjQ4cOYfXq1cjPz0dcXBzuv/9+3HTTTV61kyAIgiAIgiAaMx5Z7SzLIioqClFRUV4b/gcOHMCyZcswceJEvP322+jevTvefPNNFBYWiu5/+vRpLFq0CKNGjcJ7772HZ599FhkZGfjkk0/s+5w9exYLFy7E8OHD8e9//xvDhw/H+++/b1+tIAiCIAiCIIjmiHeWuw/YunUrRo8ejVtuucXu9W/ZsiV27Nghuv/Zs2fRunVr3HXXXWjdujWuv/563HrrrcjMzLTv880336BPnz6YMGEC2rVrhwkTJqBXr1745ptvAnVbBEEQBEEQBNHg0BT242ssFgsyMzMxfvx4p8/79OmDM2fOiB6TlJSEVatW4ejRo+jfvz/Kyspw6NAh9O/f377P2bNncffddzsd17dvX3z77beSbTGbzTCbzfa/GYZBSEiI/f99iXA+X5+XcIb6OXBQXwcG6ufAQP0cOKivCSLw1KvxX15eDo7jEBUV5fR5VFQUSktLRY9JSkrC008/jYULF8JsNsNqtWLgwIF45JFH7PuUlpYiOjra6bjo6GjJcwLAhg0bsG7dOvvfnTt3xttvv41WrVppvi+1tGnTxm/nJq5B/Rw4qK8DA/VzYKB+DhzU1wQROOrV+BcQm/FLeQGys7ORkpKCyZMno2/fvigpKcGKFSuwdOlSzJs3T/IaPM/LehYmTJiAsWPHul2/oKAAFotF7a2ogmEYtGnTBnl5eeB53qfnJq5B/Rw4qK8DA/VzYKB+Dhz+6Gu9Xu9Xxx1BNHbq1fiPjIwEy7JuHvmysjK31QCBDRs2ICkpCffeey8AICEhAcHBwXj55Zcxbdo0xMTEiHr55c4JAAaDAQaDQXSbvx7+PM/TiyUAUD8HDurrwED9HBionwMH9TVBBI56TfjV6/VITExEenq60+fp6emSNQPq6urcPPiC4pDw4OjWrRuOHTvmds5u3br5qukEQRAEQRAE0eiod7WfsWPH4scff8TOnTuRnZ2NZcuWobCwELfddhsAYOXKlVi0aJF9/4EDB+Lw4cPYsWMH8vPzcfr0aaSkpKBr166IjY0FANx1111IS0vDxo0bcfnyZWzcuBHHjh1zSwImCIIgCIIgiOZEvcf8DxkyBBUVFUhNTUVJSQk6dOiAF154wR6vV1JS4qT5P3LkSNTU1OC7777D8uXLERYWhp49e2LGjBn2fZKSkjB//nysWrUKq1evRps2bTB//nxcd911Ab8/giAIgiAIgmgoMDwF2clSUFDgJAHqCxiGQXx8PHJzcynG0Y9QPwcO6uvAQP0cGKifA4c/+tpgMFDCL0HIUO9hPwRBEARBEARBBAYy/gmCIAiCIAiimUDGP0EQBEEQBEE0E8j4JwiCIAiCIIhmAhn/BEEQBEEQBNFMIOOfIAiCIAiCIJoJZPwTBPH/7d19cBT14cfxz14ux0MwCRAhpEmAaBKRp6KUgkJ5Ep1xmKEIUoTOGARhFErRQS2DFepQKGjRKUMfYlFaKgUhpEWoQyYiNAQmSgfNQPyBGBRGEkmaS0ICIbnc/v5gcnpcQLS53Uv2/Zphyn1v7/zux5h+bu+7uwAAwCEo/wAAAIBDUP4BAAAAh6D8AwAAAA5B+QcAAAAcgvIPAAAAOATlHwAAAHAIyj8AAADgEJR/AAAAwCEo/wAAAIBDUP4BAAAAh6D8AwAAAA5B+QcAAAAcgvIPAAAAOATlHwAAAHAIyj8AAADgEJR/AAAAwCEo/wAAAIBDUP4BAAAAh6D828g0TbunAAAAAAdx2z0Bp6lvbFb2kTIdOfuxrjT6FOUyNCYtVvNHJSnGE2X39AAAANCBUf4tVN/YrPlvndLnVQ3yf208p7hSR8/VKXtGBh8AAAAAEDYs+7FQ9pHzIcVfkvym9Lm3QdlHztsyLwAAADgD5d9CBaW1IcW/hd+UDpXWWjofAADsxLlvgPVY9mMR0zTl81+v+l/l85syTVOGYVg0KwAArHX13LfzKiitVbPfVCfP/2lUajfNH9WHpa+ABSj/FjEMQ27Xjb9oiXIZFH8AQIfV6rlv9U3Kqb6so+cucu4bYAGW/VhoTFqsXNfp9i7j6vMAAHRUnPsG2I/yb6H5o5LUt3vnkA8ALkPq172z5o9KsmdiAABYgHPfAPux7MdCMZ4oZc/I0GtHynT4bJ2uNPrkdhkazXX+AQAdHOe+AZGB8m+xGE+UnhqXonV9+uj8eb7eBAA4A+e+AZGBZT824hfcVVzqDQCcgXPfAPtx5N9hIuXr1K9f6s3n98vtcmkMy58AoEObPypJR8/V6XNvg/xfO+7DuW+AdSj/DhBpRbvVS71Jyimu1NFzdVzqDQA6qJZz37KPnNeh0lr5/KY6edy6J7WbHuc6/4AlKP8dXCQW7Zu51NtTY1MsnRMAwBoxnig9NTZFT429+jgpKUllZWUsAQUswpr/Di4Sr6nMpd4AABLnvgF2oPx3cJFWtL/Npd4AAADQtiJi2c++ffu0e/duVVdXKzk5WVlZWRowYECr227cuFEHDx4MGU9OTtb69eslST6fT//4xz908OBBVVVVKSkpSbNnz9b3v//9cO5GxInEaypzqTcAAAD72F7+Dx8+rM2bN2vevHnKzMxUfn6+Vq9erVdeeUUJCQkh28+ZM0ezZ88OPG5ubtYzzzyjkSNHBsa2bdumgoICLViwQN/73vf00Ucf6aWXXtKqVavUv39/S/YrEkRq0R6TFquc4sqgKz204FJvAAAA4WP7sp89e/ZowoQJmjhxYuCof0JCgvLy8lrdvmvXroqPjw/8+fTTT1VfX6/x48cHtikoKNDUqVN11113qXfv3rr//vs1dOhQvf3221btVsSIxGsqzx+VpL7dO4fMi0u9AQAAhJet5d/n86m0tFRDhw4NGh8yZIhOnjx5U++xf/9+DR48WLfeemtgrKmpSR6PJ2g7j8dz0+/ZkURi0W651Nu0IQnqc4tHt8ZEq88tHk0bkqA/cZlPAACAsLF12U9tba38fr/i4uKCxuPi4lRdXf2Nr/d6vfrwww+1ePHioPGhQ4dqz549GjBggHr37q3jx4/r6NGj8t9g/XtTU5OampoCjw3DUJcuXQJ/b0st72fFcptundx67SeZyj58XgVnauRrNuWOMjSmf5zm32PfDbW6dXLr6XGpenpc+G48ZmXOTkfW1iBna5CzdcgasJ7ta/6l1v+jv5lfBAcOHFBMTIxGjBgRND5nzhz98Y9/1JIlS2QYhnr37q1x48bpwIED132v3Nxc7dy5M/C4f//+Wrt2bdA3Cm0tMTExbO99rXV9kyVFzh1+rWRlzk5H1tYgZ2uQs3XIGrCOreU/NjZWLpcr5Ch/TU1NyLcB1zJNU++9957GjBkjtzt4N2JjY/Xss8+qsbFRdXV16t69u95880316tXruu83depUTZ48OfC4pSBXVFTI5/N9yz27McMwlJiYqPLyci5pGUbkbB2ytgY5W4OcrROOrN1ud1gP3AHtna3l3+12Ky0tTcXFxUFH74uLi/WDH/zghq8tKSlReXm5JkyYcN1tPB6PevToIZ/Pp6KiIo0aNeq620ZHRys6OrrV58L1y980uZ69FcjZOmRtDXK2Bjlbh6wB69i+7Gfy5MnasGGD0tLSlJGRofz8fFVWVmrSpEmSpK1bt6qqqkqLFi0Ket3+/fuVnp6u1NTUkPf85JNPVFVVpX79+qmqqko7duyQaZqaMmWKJfsEAAAARCLby/8999yjixcvKicnR16vVykpKVq2bFngKzuv16vKysqg11y6dElFRUXKyspq9T2bmpq0bds2XbhwQZ07d9awYcO0aNEixcTEhHt3AAAAgIhlmHzPdkMVFRVBVwFqC4ZhqE+fPiorK+NrzjAiZ+uQtTXI2RrkbJ1wZB0dHc2af+AGbL/JFwAAAABrUP4BAAAAh6D8AwAAAA5B+QcAAAAcgvIfITipDAAAAOFm+6U+nay+sVl/OvyFCkpr5fP75Xa5NCYtVvNHJSnGE2X39AAAANDBUP5tUnfFp8e3n9TnVQ3yf208p7hSR8/VKXtGBh8AAAAA0KZY9mOTl/eFFn9J8pvS594GZR85b8u8AAAA0HFR/m2S//GXIcW/hd+UDpXWWjofAAAAdHyUfxuYpqmm5huf4Ovzm5wEDAAAgDZF+beBYRiKjjJuuE2Uy5Bh3HgbAAAA4Nug/NvkvgG95bpOt3cZ0pi0WGsnBAAAgA6P8m+TpQ9kqm/3ziEfAFyG1K97Z80flWTPxAAAANBhcalPm3Tr5NZrP8nUnw5/oUOltfL5TbldhkZznX8AAACECeXfRjGeKD01NkVPjb16EjBr/AEAABBOLPuJEBR/AAAAhBvlHwAAAHAIyj8AAADgEJR/AAAAwCEo/wAAAIBDUP4BAAAAh6D8AwAAAA5B+QcAAAAcgvIPAAAAOATlHwAAAHAIyj8AAADgEJR/AAAAwCEo/wAAAIBDUP4BAAAAh6D8AwAAAA5B+QcAAAAcgvIPAAAAOATlHwAAAHAIyj8AAADgEJR/AB2CaZp2TwEAgIjntnsCAPBd1Tc2K/vIeRWU1srn98vtcmlMWqzmj0pSjCfK7ukBABBxKP8A2qX6xmbNf+uUPq9qkP9r4znFlTp6rk7ZMzL4AAAAwDVY9gOgXco+cj6k+EuS35Q+9zYo+8h5W+YFAEAko/wDaJcKSmtDin8LvykdKq21dD4AALQHlH8A7Y5pmvL5r1f9r/L5TU4CBgDgGpR/AO2OYRhyu2786yvKZcgwDItmBABA+0D5B9AujUmLles63d5lXH0eAAAEo/wDaJfmj0pS3+6dQz4AuAypX/fOmj8qyZ6JAQAQwSLiUp/79u3T7t27VV1dreTkZGVlZWnAgAGtbrtx40YdPHgwZDw5OVnr168PPN67d6/y8vJUWVmp2NhY/fCHP9SsWbPk8XjCth8ArBPjiVL2jAxlHzmvQ6W18vlNuV2GRnOdfwAArsv28n/48GFt3rxZ8+bNU2ZmpvLz87V69Wq98sorSkhICNl+zpw5mj17duBxc3OznnnmGY0cOTIwVlBQoK1bt+qJJ55QRkaGysrK9Pvf/16SlJWVFfZ9AmCNGE+UnhqboqfGXj0JmDX+AADcmO3Lfvbs2aMJEyZo4sSJgaP+CQkJysvLa3X7rl27Kj4+PvDn008/VX19vcaPHx/Y5tSpU8rMzNTo0aPVq1cvDR06VPfee69KS0ut2i0AFqP4AwDwzWw98u/z+VRaWqof//jHQeNDhgzRyZMnb+o99u/fr8GDB+vWW28NjN1xxx0qKCjQ6dOndfvtt+vLL7/UsWPHNHbs2Ou+T1NTk5qamgKPDcNQly5dAn9vSy3vR1kJL3K2Dllbg5ytQc7WIWvAeraW/9raWvn9fsXFxQWNx8XFqbq6+htf7/V69eGHH2rx4sVB4/fee69qa2v1y1/+UtLVpUH3339/yIeMr8vNzdXOnTsDj/v376+1a9cGfahoa4mJiWF7b3yFnK1D1tYgZ2uQs3XIGrCO7Wv+pdY/8d/MUYADBw4oJiZGI0aMCBo/ceKEdu3apXnz5ik9PV3l5eV64403FB8fr+nTp7f6XlOnTtXkyZND/vkVFRXy+XzfZne+kWEYSkxMVHl5OTchCiNytg5ZW4OcrUHO1glH1m63O6wH7oD2ztbyHxsbK5fLFXKUv6amJuTbgGuZpqn33ntPY8aMkdsdvBvbt2/Xj370I02cOFGSlJqaqoaGBmVnZ+uhhx6Sq5WbA0VHRys6Ovq6/6xwME3uQGoFcrYOWVuDnK1BztYha8A6tp7w63a7lZaWpuLi4qDx4uJiZWZm3vC1JSUlKi8v14QJE0Keu3LlSsg3By6Xi18sAAAAcDTbl/1MnjxZGzZsUFpamjIyMpSfn6/KykpNmjRJkrR161ZVVVVp0aJFQa/bv3+/0tPTlZqaGvKed999t/bu3av+/fsHlv1s375dw4cPb/WoPwAAAOAEtpf/e+65RxcvXlROTo68Xq9SUlK0bNmywHo9r9erysrKoNdcunRJRUVF171m/7Rp02QYhrZt26aqqirFxsbq7rvv1iOPPBLu3QEAAAAilmGyFuaGKioqgi4B2hYMw1CfPn1UVlbGUqQwImfrkLU1yNka5GydcGQdHR3NCb/ADbAGBgAAAHAI25f9RLprryTUXt4bXyFn65C1NcjZGuRsnbbMmn9vwI2x7AcAAABwCJb92ODy5ct67rnndPnyZbun0qGRs3XI2hrkbA1ytg5ZA9aj/NvANE2dOXOGE8nCjJytQ9bWIGdrkLN1yBqwHuUfAAAAcAjKPwAAAOAQlH8bREdHa/r06YqOjrZ7Kh0aOVuHrK1BztYgZ+uQNWA9rvYDAAAAOARH/gEAAACHoPwDAAAADkH5BwAAAByC8g8AAAA4hNvuCTjNvn37tHv3blVXVys5OVlZWVkaMGCA3dNqN0pKSrR7926dOXNGXq9XS5cu1YgRIwLPm6apHTt26N1331VdXZ3S09M1d+5cpaSkBLZpamrSli1bVFhYqMbGRg0aNEjz5s1Tz5497diliJSbm6v3339fX3zxhTwejzIyMvTTn/5USUlJgW3Ium3k5eUpLy9PFRUVkqTk5GRNnz5dw4YNk0TO4ZKbm6u///3vevDBB5WVlSWJrNvCW2+9pZ07dwaNxcXF6bXXXpNExkAk4Mi/hQ4fPqzNmzfroYce0tq1azVgwACtXr1alZWVdk+t3bhy5Yr69eunxx57rNXn//nPf2rv3r167LHHtGbNGsXHx2vVqlVBt47fvHmz3n//ff385z/Xiy++qIaGBv3mN7+R3++3ajciXklJiR544AH9+te/1vPPPy+/369Vq1apoaEhsA1Zt40ePXpo1qxZWrNmjdasWaNBgwZp3bp1OnfunCRyDofTp08rPz9fffv2DRon67aRkpKi7OzswJ/f/va3gefIGIgAJiyzbNkyMzs7O2hsyZIl5ptvvmnTjNq3hx9+2CwqKgo89vv95uOPP27m5uYGxhobG81HH33UzMvLM03TNOvr682ZM2eahYWFgW3++9//mjNmzDCPHTtm1dTbnZqaGvPhhx82T5w4YZomWYdbVlaW+e6775JzGFy+fNlcvHix+dFHH5krVqww33jjDdM0+ZluK9u3bzeXLl3a6nNkDEQGjvxbxOfzqbS0VEOHDg0aHzJkiE6ePGnTrDqWCxcuqLq6Oijj6Oho3XnnnYGMS0tL1dzcrCFDhgS26dGjh1JTU3Xq1CnL59xeXLp0SZLUrVs3SWQdLn6/X4WFhbpy5YoyMjLIOQz+/Oc/a9iwYUF5SfxMt6Xy8nItWLBACxcu1Kuvvqovv/xSEhkDkYI1/xapra2V3+9XXFxc0HhcXJyqq6vtmVQH05Jjaxm3LK2qrq6W2+0OlNivb8O/h9aZpqm//OUvuuOOO5SamiqJrNva2bNntXz5cjU1Nalz585aunSpkpOTA4WInNtGYWGhzpw5ozVr1oQ8x89020hPT9fChQuVlJSk6upq7dq1S88//7zWr19PxkCEoPxbzDCMmxrDd3dtnuZN3MT6ZrZxqk2bNuns2bN68cUXQ54j67aRlJSkl156SfX19SoqKtLGjRv1q1/9KvA8Of/vKisrtXnzZi1fvlwej+e625H1/6blRHVJSk1NVUZGhn72s5/p4MGDSk9Pl0TGgN1Y9mOR2NhYuVyukCMXNTU1IUdB8N3Ex8dLUkjGtbW1gYzj4+Pl8/lUV1cXsk3L6/GV119/Xf/5z3+0YsWKoCttkHXbcrvdSkxM1G233aZZs2apX79++te//kXObai0tFQ1NTX6xS9+oZkzZ2rmzJkqKSnRO++8o5kzZwbyJOu21blzZ6WmpqqsrIyfZyBCUP4t4na7lZaWpuLi4qDx4uJiZWZm2jSrjqVXr16Kj48Pytjn86mkpCSQcVpamqKiooK28Xq9Onv2rDIyMiyfc6QyTVObNm1SUVGRXnjhBfXq1SvoebIOL9M01dTURM5taPDgwXr55Ze1bt26wJ/bbrtNo0eP1rp169S7d2+yDoOmpiZ98cUX6t69Oz/PQIRg2Y+FJk+erA0bNigtLU0ZGRnKz89XZWWlJk2aZPfU2o2GhgaVl5cHHl+4cEGfffaZunXrpoSEBD344IPKzc1Vnz59lJiYqNzcXHXq1EmjR4+WJHXt2lUTJkzQli1bdMstt6hbt27asmWLUlNTQ04AdLJNmzbp0KFDevbZZ9WlS5fAkbquXbvK4/HIMAyybiNbt27VsGHD1LNnTzU0NKiwsFAnTpzQ8uXLybkNdenSJXDOSotOnTrplltuCYyT9f/ur3/9q4YPH66EhATV1NQoJydHly9f1tixY/l5BiKEYbKQzlItN/nyer1KSUnRo48+qjvvvNPuabUbJ06cCFoL3WLs2LFauHBh4AYy+fn5qq+v1+233665c+cG/Z9+Y2Oj/va3v+nQoUNBN5BJSEiwclci2owZM1odf/LJJzVu3DhJIus28oc//EHHjx+X1+tV165d1bdvX02ZMiVQdMg5fFauXKl+/fqF3OSLrL+7V199VR9//LFqa2sVGxur9PR0zZw5U8nJyZLIGIgElH8AAADAIVjzDwAAADgE5R8AAABwCMo/AAAA4BCUfwAAAMAhKP8AAACAQ1D+AQAAAIeg/AMAAAAOwR1+AbQ717sJ2bVWrFihgQMHhoyvXLky6H+/jf/ltQAA2I3yD6DdWbVqVdDjnJwcnThxQi+88ELQeMtdRa81b968sM0NAIBIRvkH0O5kZGQEPY6NjZVhGCHj17py5Yo6dep03Q8FAAB0dJR/AB3SypUrdfHiRc2dO1dbt27VZ599puHDh2vJkiWtLt3ZsWOHjh07prKyMvn9fiUmJuqBBx7Q+PHjZRiGPTsBAEAbo/wD6LC8Xq82bNigKVOm6JFHHrlhia+oqNB9992nhIQESdInn3yi119/XVVVVZo+fbpVUwYAIKwo/wA6rLq6Oj399NMaNGjQN2775JNPBv7u9/s1cOBAmaapd955R9OmTePoPwCgQ6D8A+iwYmJibqr4S9Lx48eVm5ur06dP6/Lly0HP1dTUKD4+PgwzBADAWpR/AB1W9+7db2q706dPa9WqVRo4cKAWLFignj17yu1264MPPtCuXbvU2NgY5pkCAGANyj+ADutml+oUFhYqKipKzz33nDweT2D8gw8+CNfUAACwBXf4BeB4hmEoKipKLtdXvxIbGxv173//28ZZAQDQ9jjyD8Dx7rrrLu3Zs0e/+93vdN999+nixYt6++23FR0dbffUAABoUxz5B+B4gwYN0hNPPKGzZ89q7dq12rZtm0aOHKkpU6bYPTUAANqUYZqmafckAAAAAIQfR/4BAAAAh6D8AwAAAA5B+QcAAAAcgvIPAAAAOATlHwAAAHAIyj8AAADgEJR/AAAAwCEo/wAAAIBDUP4BAAAAh6D8AwAAAA5B+QcAAAAcgvIPAAAAOMT/AzAtU6PZQXlYAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       205.100000     8.698020\n",
      "1                    TN       181.500000     9.980537\n",
      "2                    FP        37.300000     8.857514\n",
      "3                    FN        35.300000     4.001389\n",
      "4              Accuracy         0.841908     0.019447\n",
      "5             Precision         0.846632     0.032542\n",
      "6           Sensitivity         0.853249     0.014713\n",
      "7           Specificity         0.829940     0.036425\n",
      "8              F1 score         0.849579     0.018318\n",
      "9   F1 score (weighted)         0.841862     0.019607\n",
      "10     F1 score (macro)         0.841309     0.019447\n",
      "11    Balanced Accuracy         0.841602     0.019466\n",
      "12                  MCC         0.683410     0.037836\n",
      "13                  NPV         0.836990     0.018470\n",
      "14              ROC_AUC         0.841602     0.019466\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>414.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>349.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>79.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>76.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.819369</td>\n",
       "      <td>0.843308</td>\n",
       "      <td>0.833515</td>\n",
       "      <td>0.834603</td>\n",
       "      <td>0.820457</td>\n",
       "      <td>0.826986</td>\n",
       "      <td>0.831338</td>\n",
       "      <td>0.832427</td>\n",
       "      <td>0.818281</td>\n",
       "      <td>0.843308</td>\n",
       "      <td>0.830359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.851020</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.834711</td>\n",
       "      <td>0.839248</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.809816</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.838730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.849593</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.839034</td>\n",
       "      <td>0.854369</td>\n",
       "      <td>0.826176</td>\n",
       "      <td>0.830579</td>\n",
       "      <td>0.850485</td>\n",
       "      <td>0.838446</td>\n",
       "      <td>0.842553</td>\n",
       "      <td>0.852878</td>\n",
       "      <td>0.844362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.784500</td>\n",
       "      <td>0.825300</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.809400</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.825600</td>\n",
       "      <td>0.792900</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.814190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.844985</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>0.830421</td>\n",
       "      <td>0.834891</td>\n",
       "      <td>0.849661</td>\n",
       "      <td>0.841889</td>\n",
       "      <td>0.825860</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.841467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.819047</td>\n",
       "      <td>0.843223</td>\n",
       "      <td>0.833609</td>\n",
       "      <td>0.834558</td>\n",
       "      <td>0.820515</td>\n",
       "      <td>0.827031</td>\n",
       "      <td>0.831316</td>\n",
       "      <td>0.832470</td>\n",
       "      <td>0.818117</td>\n",
       "      <td>0.843280</td>\n",
       "      <td>0.830317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.817883</td>\n",
       "      <td>0.842703</td>\n",
       "      <td>0.832598</td>\n",
       "      <td>0.832064</td>\n",
       "      <td>0.819835</td>\n",
       "      <td>0.826588</td>\n",
       "      <td>0.828796</td>\n",
       "      <td>0.831824</td>\n",
       "      <td>0.817936</td>\n",
       "      <td>0.843192</td>\n",
       "      <td>0.829342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.817068</td>\n",
       "      <td>0.842396</td>\n",
       "      <td>0.833024</td>\n",
       "      <td>0.831887</td>\n",
       "      <td>0.820065</td>\n",
       "      <td>0.826784</td>\n",
       "      <td>0.828708</td>\n",
       "      <td>0.832014</td>\n",
       "      <td>0.817713</td>\n",
       "      <td>0.843106</td>\n",
       "      <td>0.829276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.636404</td>\n",
       "      <td>0.685534</td>\n",
       "      <td>0.665294</td>\n",
       "      <td>0.664136</td>\n",
       "      <td>0.639719</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>0.657593</td>\n",
       "      <td>0.663680</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.686456</td>\n",
       "      <td>0.658861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.819100</td>\n",
       "      <td>0.840700</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.813400</td>\n",
       "      <td>0.804600</td>\n",
       "      <td>0.813600</td>\n",
       "      <td>0.808900</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.827900</td>\n",
       "      <td>0.844600</td>\n",
       "      <td>0.820430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.817068</td>\n",
       "      <td>0.842396</td>\n",
       "      <td>0.833024</td>\n",
       "      <td>0.831887</td>\n",
       "      <td>0.820065</td>\n",
       "      <td>0.826784</td>\n",
       "      <td>0.828708</td>\n",
       "      <td>0.832014</td>\n",
       "      <td>0.817713</td>\n",
       "      <td>0.843106</td>\n",
       "      <td>0.829276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  418.000000  416.000000  417.000000  440.000000   \n",
       "1                    TN  335.000000  359.000000  349.000000  327.000000   \n",
       "2                    FP   92.000000   76.000000   73.000000   77.000000   \n",
       "3                    FN   74.000000   68.000000   80.000000   75.000000   \n",
       "4              Accuracy    0.819369    0.843308    0.833515    0.834603   \n",
       "5             Precision    0.819608    0.845528    0.851020    0.851064   \n",
       "6           Sensitivity    0.849593    0.859504    0.839034    0.854369   \n",
       "7           Specificity    0.784500    0.825300    0.827000    0.809400   \n",
       "8              F1 score    0.834331    0.852459    0.844985    0.852713   \n",
       "9   F1 score (weighted)    0.819047    0.843223    0.833609    0.834558   \n",
       "10     F1 score (macro)    0.817883    0.842703    0.832598    0.832064   \n",
       "11    Balanced Accuracy    0.817068    0.842396    0.833024    0.831887   \n",
       "12                  MCC    0.636404    0.685534    0.665294    0.664136   \n",
       "13                  NPV    0.819100    0.840700    0.813500    0.813400   \n",
       "14              ROC_AUC    0.817068    0.842396    0.833024    0.831887   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   404.000000  402.000000  438.000000  410.000000  396.000000  400.000000   \n",
       "1   350.000000  358.000000  326.000000  355.000000  356.000000  375.000000   \n",
       "2    80.000000   77.000000   78.000000   75.000000   93.000000   75.000000   \n",
       "3    85.000000   82.000000   77.000000   79.000000   74.000000   69.000000   \n",
       "4     0.820457    0.826986    0.831338    0.832427    0.818281    0.843308   \n",
       "5     0.834711    0.839248    0.848837    0.845361    0.809816    0.842105   \n",
       "6     0.826176    0.830579    0.850485    0.838446    0.842553    0.852878   \n",
       "7     0.814000    0.823000    0.806900    0.825600    0.792900    0.833300   \n",
       "8     0.830421    0.834891    0.849661    0.841889    0.825860    0.847458   \n",
       "9     0.820515    0.827031    0.831316    0.832470    0.818117    0.843280   \n",
       "10    0.819835    0.826588    0.828796    0.831824    0.817936    0.843192   \n",
       "11    0.820065    0.826784    0.828708    0.832014    0.817713    0.843106   \n",
       "12    0.639719    0.653226    0.657593    0.663680    0.636574    0.686456   \n",
       "13    0.804600    0.813600    0.808900    0.818000    0.827900    0.844600   \n",
       "14    0.820065    0.826784    0.828708    0.832014    0.817713    0.843106   \n",
       "\n",
       "           ave  \n",
       "0   414.100000  \n",
       "1   349.000000  \n",
       "2    79.600000  \n",
       "3    76.300000  \n",
       "4     0.830359  \n",
       "5     0.838730  \n",
       "6     0.844362  \n",
       "7     0.814190  \n",
       "8     0.841467  \n",
       "9     0.830317  \n",
       "10    0.829342  \n",
       "11    0.829276  \n",
       "12    0.658861  \n",
       "13    0.820430  \n",
       "14    0.829276  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluations metrics on the test sets\n",
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.839067</td>\n",
       "      <td>0.015714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.840357</td>\n",
       "      <td>0.023883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.855192</td>\n",
       "      <td>0.019468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.821452</td>\n",
       "      <td>0.026762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.847463</td>\n",
       "      <td>0.016177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.838966</td>\n",
       "      <td>0.015772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.838368</td>\n",
       "      <td>0.015721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.838322</td>\n",
       "      <td>0.015904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.677437</td>\n",
       "      <td>0.031361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.837884</td>\n",
       "      <td>0.019822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.838322</td>\n",
       "      <td>0.015904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.839067     0.015714\n",
       "1             Precision         0.840357     0.023883\n",
       "2           Sensitivity         0.855192     0.019468\n",
       "3           Specificity         0.821452     0.026762\n",
       "4              F1 score         0.847463     0.016177\n",
       "5   F1 score (weighted)         0.838966     0.015772\n",
       "6      F1 score (macro)         0.838368     0.015721\n",
       "7     Balanced Accuracy         0.838322     0.015904\n",
       "8                   MCC         0.677437     0.031361\n",
       "9                   NPV         0.837884     0.019822\n",
       "10              ROC_AUC         0.838322     0.015904"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_lgbm)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "       \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_lgbm))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_lgbm))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_lgbm))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_lgbm))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_lgbm, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_lgbm, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_lgbm))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_lgbm))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_lgbm))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "mat_met_optimized_lgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model f1_score 0.8258 with a standard deviation of 0.0188\n",
      "LightGBM optimized model f1_score 0.8339 with a standard deviation of 0.0195\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#f1_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "f1_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"f1_macro\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)f1_score %0.4f with a standard deviation of %0.4f\" % (f1_cv_lgbm_opt_testSet.mean(), f1_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(f1_cv_lgbm_opt), np.std(f1_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm_clf.joblib']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the modesls, both the one with optimized hyperparameters and the initial one\n",
    "joblib.dump(lgbm_clf, \"OUTPUT/lgbm_clf.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm_clf.joblib\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       206.900000     9.515484\n",
      "1                    TN       176.700000    10.719142\n",
      "2                    FP        42.100000     4.508018\n",
      "3                    FN        33.500000     3.628590\n",
      "4              Accuracy         0.835371     0.012165\n",
      "5             Precision         0.830951     0.016745\n",
      "6           Sensitivity         0.860616     0.014309\n",
      "7           Specificity         0.807270     0.021784\n",
      "8              F1 score         0.845406     0.011637\n",
      "9   F1 score (weighted)         0.835156     0.012214\n",
      "10     F1 score (macro)         0.834373     0.012330\n",
      "11    Balanced Accuracy         0.833940     0.012491\n",
      "12                  MCC         0.669592     0.024575\n",
      "13                  NPV         0.840360     0.017907\n",
      "14              ROC_AUC         0.833940     0.012491\n",
      "CPU times: user 30.4 s, sys: 224 ms, total: 30.7 s\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    n_jobs=16,\n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_clf.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"logloss\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_clf.predict(X_test) \n",
    "    \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=16, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"logloss\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=16, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"logloss\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        \n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:50:25,865] A new study created in memory with name: XGBClassifier\n",
      "[I 2023-12-04 09:50:28,486] Trial 0 finished with value: 0.8079357668927676 and parameters: {'n_estimators': 101, 'eta': 0.05569945587384238, 'max_depth': 9, 'alpha': 0.5312, 'lambda': 34.355158562953406, 'max_bin': 312}. Best is trial 0 with value: 0.8079357668927676.\n",
      "[I 2023-12-04 09:50:39,832] Trial 1 finished with value: 0.8420673543769908 and parameters: {'n_estimators': 782, 'eta': 0.08572933207576598, 'max_depth': 11, 'alpha': 0.1553, 'lambda': 35.276295674096176, 'max_bin': 476}. Best is trial 1 with value: 0.8420673543769908.\n",
      "[I 2023-12-04 09:50:51,119] Trial 2 finished with value: 0.8221718286945652 and parameters: {'n_estimators': 481, 'eta': 0.018788344227590226, 'max_depth': 9, 'alpha': 0.6496000000000001, 'lambda': 19.573973456531142, 'max_bin': 420}. Best is trial 1 with value: 0.8420673543769908.\n",
      "[I 2023-12-04 09:50:54,180] Trial 3 finished with value: 0.8107763688012417 and parameters: {'n_estimators': 151, 'eta': 0.05336722956164393, 'max_depth': 7, 'alpha': 0.13340000000000002, 'lambda': 23.025622020807905, 'max_bin': 477}. Best is trial 1 with value: 0.8420673543769908.\n",
      "[I 2023-12-04 09:50:58,513] Trial 4 finished with value: 0.8133768186462781 and parameters: {'n_estimators': 177, 'eta': 0.041262755370220984, 'max_depth': 9, 'alpha': 0.6631, 'lambda': 34.84512627292017, 'max_bin': 300}. Best is trial 1 with value: 0.8420673543769908.\n",
      "[I 2023-12-04 09:51:06,265] Trial 5 finished with value: 0.8319135756393298 and parameters: {'n_estimators': 339, 'eta': 0.05787707597130525, 'max_depth': 9, 'alpha': 0.41490000000000005, 'lambda': 38.201117843475025, 'max_bin': 479}. Best is trial 1 with value: 0.8420673543769908.\n",
      "[I 2023-12-04 09:51:13,707] Trial 6 finished with value: 0.83757168086618 and parameters: {'n_estimators': 280, 'eta': 0.059342225555596165, 'max_depth': 11, 'alpha': 0.9873000000000001, 'lambda': 15.282803727888073, 'max_bin': 347}. Best is trial 1 with value: 0.8420673543769908.\n",
      "[I 2023-12-04 09:51:19,683] Trial 7 finished with value: 0.7997304146185886 and parameters: {'n_estimators': 193, 'eta': 0.008124134934769243, 'max_depth': 12, 'alpha': 0.5263, 'lambda': 25.0730585817873, 'max_bin': 374}. Best is trial 1 with value: 0.8420673543769908.\n",
      "[I 2023-12-04 09:51:30,298] Trial 8 finished with value: 0.8440092151473625 and parameters: {'n_estimators': 749, 'eta': 0.06822159195443808, 'max_depth': 11, 'alpha': 0.9152, 'lambda': 14.041092750865921, 'max_bin': 268}. Best is trial 8 with value: 0.8440092151473625.\n",
      "[I 2023-12-04 09:51:41,002] Trial 9 finished with value: 0.8477792782514941 and parameters: {'n_estimators': 595, 'eta': 0.05194941010595303, 'max_depth': 12, 'alpha': 0.9504, 'lambda': 3.0462416137246433, 'max_bin': 461}. Best is trial 9 with value: 0.8477792782514941.\n",
      "[I 2023-12-04 09:51:47,817] Trial 10 finished with value: 0.8371451815132668 and parameters: {'n_estimators': 581, 'eta': 0.09793387539638401, 'max_depth': 5, 'alpha': 0.8065, 'lambda': 5.928143501296647, 'max_bin': 422}. Best is trial 9 with value: 0.8477792782514941.\n",
      "[I 2023-12-04 09:51:55,307] Trial 11 finished with value: 0.8513589595154614 and parameters: {'n_estimators': 785, 'eta': 0.07389767489222081, 'max_depth': 12, 'alpha': 0.9650000000000001, 'lambda': 2.5359194636827933, 'max_bin': 259}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:52:02,220] Trial 12 finished with value: 0.8470725839476814 and parameters: {'n_estimators': 636, 'eta': 0.07603382752151329, 'max_depth': 12, 'alpha': 0.8317, 'lambda': 2.680841248306283, 'max_bin': 422}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:52:14,668] Trial 13 finished with value: 0.847490266816456 and parameters: {'n_estimators': 898, 'eta': 0.03692423957621506, 'max_depth': 12, 'alpha': 0.9942000000000001, 'lambda': 1.4061411142343594, 'max_bin': 380}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:52:24,179] Trial 14 finished with value: 0.8437016975084844 and parameters: {'n_estimators': 653, 'eta': 0.07562312558429644, 'max_depth': 7, 'alpha': 0.7755000000000001, 'lambda': 8.135856386457851, 'max_bin': 262}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:52:34,994] Trial 15 finished with value: 0.8400933072089728 and parameters: {'n_estimators': 455, 'eta': 0.03890025991572606, 'max_depth': 10, 'alpha': 0.3299, 'lambda': 8.550990084897213, 'max_bin': 328}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:52:43,667] Trial 16 finished with value: 0.8434980904786986 and parameters: {'n_estimators': 848, 'eta': 0.0684370376574165, 'max_depth': 7, 'alpha': 0.6816, 'lambda': 3.016365654810961, 'max_bin': 437}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:52:51,999] Trial 17 finished with value: 0.8423420277567517 and parameters: {'n_estimators': 728, 'eta': 0.08812188934107429, 'max_depth': 10, 'alpha': 0.8808, 'lambda': 10.149875997996075, 'max_bin': 388}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:52:58,634] Trial 18 finished with value: 0.8264335777896641 and parameters: {'n_estimators': 513, 'eta': 0.04623630384120608, 'max_depth': 5, 'alpha': 0.2881, 'lambda': 5.412090031683119, 'max_bin': 498}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:53:07,356] Trial 19 finished with value: 0.8442294021749254 and parameters: {'n_estimators': 364, 'eta': 0.02922819912273, 'max_depth': 10, 'alpha': 0.7596, 'lambda': 1.0712591672012195, 'max_bin': 287}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:53:19,816] Trial 20 finished with value: 0.8409329001092571 and parameters: {'n_estimators': 629, 'eta': 0.04863672418561443, 'max_depth': 12, 'alpha': 0.008700000000000001, 'lambda': 11.112557249608374, 'max_bin': 342}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:53:33,942] Trial 21 finished with value: 0.8502192456981458 and parameters: {'n_estimators': 852, 'eta': 0.034250894425599636, 'max_depth': 12, 'alpha': 0.9736, 'lambda': 2.2506496280155703, 'max_bin': 377}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:53:52,860] Trial 22 finished with value: 0.8456773307626971 and parameters: {'n_estimators': 812, 'eta': 0.02574937519165755, 'max_depth': 11, 'alpha': 0.9433, 'lambda': 5.826904971311633, 'max_bin': 444}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:54:05,395] Trial 23 finished with value: 0.8436875174612632 and parameters: {'n_estimators': 702, 'eta': 0.04516665890015912, 'max_depth': 12, 'alpha': 0.8704000000000001, 'lambda': 4.992573641451521, 'max_bin': 399}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:54:14,529] Trial 24 finished with value: 0.8439862092028658 and parameters: {'n_estimators': 864, 'eta': 0.06272228516420289, 'max_depth': 11, 'alpha': 0.7341000000000001, 'lambda': 3.6500996688245246, 'max_bin': 360}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:54:25,912] Trial 25 finished with value: 0.843972139408357 and parameters: {'n_estimators': 579, 'eta': 0.04981866906044727, 'max_depth': 12, 'alpha': 0.9079, 'lambda': 7.546176633874975, 'max_bin': 450}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:54:38,814] Trial 26 finished with value: 0.8462392569763562 and parameters: {'n_estimators': 795, 'eta': 0.03193362454148167, 'max_depth': 10, 'alpha': 0.601, 'lambda': 1.3765347029503716, 'max_bin': 402}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:54:53,176] Trial 27 finished with value: 0.8333914877869585 and parameters: {'n_estimators': 694, 'eta': 0.022604334701816366, 'max_depth': 8, 'alpha': 0.9925, 'lambda': 10.574781114058675, 'max_bin': 253}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:55:05,622] Trial 28 finished with value: 0.8431325258647657 and parameters: {'n_estimators': 543, 'eta': 0.037072557273630374, 'max_depth': 11, 'alpha': 0.8133, 'lambda': 5.470967677781696, 'max_bin': 281}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:55:18,558] Trial 29 finished with value: 0.8437208153344009 and parameters: {'n_estimators': 830, 'eta': 0.05487894071175639, 'max_depth': 12, 'alpha': 0.5766, 'lambda': 13.297026832998293, 'max_bin': 318}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:55:26,756] Trial 30 finished with value: 0.8439591409287553 and parameters: {'n_estimators': 426, 'eta': 0.05154826530841075, 'max_depth': 8, 'alpha': 0.4565, 'lambda': 3.6760853464166505, 'max_bin': 362}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:55:39,257] Trial 31 finished with value: 0.8441385588239985 and parameters: {'n_estimators': 893, 'eta': 0.03738673148944439, 'max_depth': 12, 'alpha': 0.9914000000000001, 'lambda': 1.8665829520113837, 'max_bin': 333}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:55:50,231] Trial 32 finished with value: 0.8450493032227742 and parameters: {'n_estimators': 898, 'eta': 0.043107193152489634, 'max_depth': 11, 'alpha': 0.9378000000000001, 'lambda': 1.2528368860262928, 'max_bin': 388}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:56:10,575] Trial 33 finished with value: 0.8412081992479561 and parameters: {'n_estimators': 758, 'eta': 0.01961708035637947, 'max_depth': 12, 'alpha': 0.8534, 'lambda': 7.4382965724908, 'max_bin': 364}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:56:23,823] Trial 34 finished with value: 0.847502692835968 and parameters: {'n_estimators': 788, 'eta': 0.03223806531487855, 'max_depth': 11, 'alpha': 0.9291, 'lambda': 1.0342204526421312, 'max_bin': 464}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:56:37,829] Trial 35 finished with value: 0.8464072384032981 and parameters: {'n_estimators': 668, 'eta': 0.03246797521400972, 'max_depth': 11, 'alpha': 0.7024, 'lambda': 4.327329737395303, 'max_bin': 458}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:56:39,835] Trial 36 finished with value: 0.7986472186794258 and parameters: {'n_estimators': 63, 'eta': 0.014409249933922307, 'max_depth': 10, 'alpha': 0.9316000000000001, 'lambda': 3.8831238344517143, 'max_bin': 497}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:56:57,477] Trial 37 finished with value: 0.8429274228170934 and parameters: {'n_estimators': 777, 'eta': 0.027616342146464384, 'max_depth': 11, 'alpha': 0.8533000000000001, 'lambda': 7.356573528102317, 'max_bin': 461}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:57:11,625] Trial 38 finished with value: 0.7369506128200787 and parameters: {'n_estimators': 729, 'eta': 0.0007727839907816295, 'max_depth': 6, 'alpha': 0.9405, 'lambda': 16.142676698007957, 'max_bin': 475}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:57:23,933] Trial 39 finished with value: 0.8469658585309674 and parameters: {'n_estimators': 809, 'eta': 0.05462936989819213, 'max_depth': 12, 'alpha': 0.6317, 'lambda': 9.35630737972326, 'max_bin': 308}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:57:36,060] Trial 40 finished with value: 0.8418064082098091 and parameters: {'n_estimators': 596, 'eta': 0.04433880876292495, 'max_depth': 9, 'alpha': 0.789, 'lambda': 6.802889702642942, 'max_bin': 405}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:57:50,157] Trial 41 finished with value: 0.8477634642425299 and parameters: {'n_estimators': 853, 'eta': 0.03376929475194005, 'max_depth': 12, 'alpha': 0.9827, 'lambda': 2.645143128426996, 'max_bin': 484}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:58:05,358] Trial 42 finished with value: 0.8455961048585655 and parameters: {'n_estimators': 855, 'eta': 0.031864536648573465, 'max_depth': 11, 'alpha': 0.8927, 'lambda': 3.4850451073143898, 'max_bin': 483}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:58:19,055] Trial 43 finished with value: 0.8458379989453055 and parameters: {'n_estimators': 777, 'eta': 0.041934002045717955, 'max_depth': 12, 'alpha': 0.9462, 'lambda': 5.5764490064838075, 'max_bin': 431}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:58:32,563] Trial 44 finished with value: 0.845522189100166 and parameters: {'n_estimators': 703, 'eta': 0.03317275721749381, 'max_depth': 12, 'alpha': 0.9978, 'lambda': 2.817599619953974, 'max_bin': 468}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:58:51,627] Trial 45 finished with value: 0.845323453951521 and parameters: {'n_estimators': 824, 'eta': 0.022915752785809776, 'max_depth': 11, 'alpha': 0.8312, 'lambda': 4.545550667298043, 'max_bin': 487}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:58:59,589] Trial 46 finished with value: 0.8428311553700025 and parameters: {'n_estimators': 744, 'eta': 0.05846535658447631, 'max_depth': 12, 'alpha': 0.8937, 'lambda': 1.0488848312391381, 'max_bin': 460}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:59:13,621] Trial 47 finished with value: 0.8466964707242459 and parameters: {'n_estimators': 856, 'eta': 0.040723675917488475, 'max_depth': 11, 'alpha': 0.9636, 'lambda': 6.877167599341216, 'max_bin': 430}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:59:19,955] Trial 48 finished with value: 0.8377471110133727 and parameters: {'n_estimators': 219, 'eta': 0.049243738059957326, 'max_depth': 12, 'alpha': 0.7548, 'lambda': 8.783911342769967, 'max_bin': 449}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:59:32,671] Trial 49 finished with value: 0.8425486347045315 and parameters: {'n_estimators': 787, 'eta': 0.03567133414538348, 'max_depth': 11, 'alpha': 0.2243, 'lambda': 2.9547353715784896, 'max_bin': 487}. Best is trial 11 with value: 0.8513589595154614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 785\n",
      "\t\teta: 0.07389767489222081\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9650000000000001\n",
      "\t\tlambda: 2.5359194636827933\n",
      "\t\tmax_bin: 259\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBClassifier\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  424.000000\n",
      "1                    TN  339.000000\n",
      "2                    FP   88.000000\n",
      "3                    FN   68.000000\n",
      "4              Accuracy    0.830250\n",
      "5             Precision    0.828125\n",
      "6           Sensitivity    0.861789\n",
      "7           Specificity    0.793900\n",
      "8              F1 score    0.844622\n",
      "9   F1 score (weighted)    0.829906\n",
      "10     F1 score (macro)    0.828786\n",
      "11    Balanced Accuracy    0.827850\n",
      "12                  MCC    0.658369\n",
      "13                  NPV    0.832900\n",
      "14              ROC_AUC    0.827850\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "   \n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_xgb_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_xgb_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_xgb_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_xgb_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_xgb_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_xgb_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_xgb_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_xgb_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_xgb_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_xgb_0)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 09:59:46,404] Trial 50 finished with value: 0.8288823195097091 and parameters: {'n_estimators': 635, 'eta': 0.06298438399948547, 'max_depth': 10, 'alpha': 0.9017000000000001, 'lambda': 28.495211353530237, 'max_bin': 412}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 09:59:57,455] Trial 51 finished with value: 0.8306388230690818 and parameters: {'n_estimators': 873, 'eta': 0.03891328959194813, 'max_depth': 12, 'alpha': 0.9703, 'lambda': 2.4134828569336735, 'max_bin': 383}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:00:10,483] Trial 52 finished with value: 0.8330826555075902 and parameters: {'n_estimators': 893, 'eta': 0.02828282639562884, 'max_depth': 12, 'alpha': 0.9709000000000001, 'lambda': 1.1656277127851418, 'max_bin': 375}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:00:21,050] Trial 53 finished with value: 0.8309564042207109 and parameters: {'n_estimators': 821, 'eta': 0.04676874180320403, 'max_depth': 12, 'alpha': 0.9988, 'lambda': 4.81329771357837, 'max_bin': 349}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:00:32,878] Trial 54 finished with value: 0.8322740134191353 and parameters: {'n_estimators': 845, 'eta': 0.03542391463177674, 'max_depth': 12, 'alpha': 0.868, 'lambda': 2.2664924061693386, 'max_bin': 472}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:00:45,593] Trial 55 finished with value: 0.8315862195420509 and parameters: {'n_estimators': 680, 'eta': 0.04026563019555841, 'max_depth': 11, 'alpha': 0.9177000000000001, 'lambda': 5.951631290172989, 'max_bin': 416}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:00:54,719] Trial 56 finished with value: 0.8329164203791102 and parameters: {'n_estimators': 760, 'eta': 0.05165803043912204, 'max_depth': 12, 'alpha': 0.8173, 'lambda': 3.338841375457672, 'max_bin': 297}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:01:05,485] Trial 57 finished with value: 0.8309058896722068 and parameters: {'n_estimators': 415, 'eta': 0.03076890310442703, 'max_depth': 12, 'alpha': 0.3759, 'lambda': 4.675063654412899, 'max_bin': 444}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:01:13,834] Trial 58 finished with value: 0.8270232759722818 and parameters: {'n_estimators': 315, 'eta': 0.025941215628747932, 'max_depth': 11, 'alpha': 0.9547, 'lambda': 2.3108666867439, 'max_bin': 372}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:01:24,568] Trial 59 finished with value: 0.8309419029121168 and parameters: {'n_estimators': 502, 'eta': 0.044740255389192615, 'max_depth': 10, 'alpha': 0.7184, 'lambda': 6.413170321193206, 'max_bin': 500}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:01:35,619] Trial 60 finished with value: 0.8298265258327054 and parameters: {'n_estimators': 717, 'eta': 0.0343266144571824, 'max_depth': 12, 'alpha': 0.8457, 'lambda': 1.012659032535359, 'max_bin': 273}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:01:41,816] Trial 61 finished with value: 0.8314402615097123 and parameters: {'n_estimators': 593, 'eta': 0.0771104600910742, 'max_depth': 12, 'alpha': 0.9159, 'lambda': 2.7215520097366195, 'max_bin': 466}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:01:48,351] Trial 62 finished with value: 0.8315411610422144 and parameters: {'n_estimators': 535, 'eta': 0.07710430215917924, 'max_depth': 12, 'alpha': 0.8822000000000001, 'lambda': 4.355570839420655, 'max_bin': 394}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:01:55,376] Trial 63 finished with value: 0.8277220895957879 and parameters: {'n_estimators': 628, 'eta': 0.07204443467451535, 'max_depth': 11, 'alpha': 1.0, 'lambda': 2.342885754840194, 'max_bin': 423}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:02:08,610] Trial 64 finished with value: 0.8307029063827374 and parameters: {'n_estimators': 794, 'eta': 0.037811236572569656, 'max_depth': 12, 'alpha': 0.9569000000000001, 'lambda': 5.508617810822238, 'max_bin': 438}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:02:15,808] Trial 65 finished with value: 0.8252506741366359 and parameters: {'n_estimators': 872, 'eta': 0.08560857710064321, 'max_depth': 11, 'alpha': 0.9227000000000001, 'lambda': 8.42481139092946, 'max_bin': 355}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:02:29,277] Trial 66 finished with value: 0.832382943093686 and parameters: {'n_estimators': 553, 'eta': 0.029223647172951436, 'max_depth': 12, 'alpha': 0.8062, 'lambda': 3.445429168963145, 'max_bin': 454}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:02:40,214] Trial 67 finished with value: 0.8317988573592701 and parameters: {'n_estimators': 841, 'eta': 0.04755163639191831, 'max_depth': 12, 'alpha': 0.8699, 'lambda': 6.468760213096832, 'max_bin': 408}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:02:52,191] Trial 68 finished with value: 0.8318308835203133 and parameters: {'n_estimators': 661, 'eta': 0.04074517023777302, 'max_depth': 11, 'alpha': 0.4994, 'lambda': 4.256139816212822, 'max_bin': 337}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:02:59,997] Trial 69 finished with value: 0.8259333628762906 and parameters: {'n_estimators': 744, 'eta': 0.053581833944145965, 'max_depth': 6, 'alpha': 0.0867, 'lambda': 1.934864226993831, 'max_bin': 325}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:03:08,763] Trial 70 finished with value: 0.8331191762326066 and parameters: {'n_estimators': 473, 'eta': 0.05925927327521732, 'max_depth': 12, 'alpha': 0.78, 'lambda': 5.180879814595391, 'max_bin': 382}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:03:18,829] Trial 71 finished with value: 0.8318370484373074 and parameters: {'n_estimators': 810, 'eta': 0.056389691436192185, 'max_depth': 12, 'alpha': 0.6262, 'lambda': 9.204256886750604, 'max_bin': 308}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:03:29,420] Trial 72 finished with value: 0.829344379934928 and parameters: {'n_estimators': 806, 'eta': 0.04386882182315571, 'max_depth': 12, 'alpha': 0.9695, 'lambda': 3.3747413330353595, 'max_bin': 261}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:03:36,237] Trial 73 finished with value: 0.8307042934448108 and parameters: {'n_estimators': 888, 'eta': 0.05111862488623817, 'max_depth': 12, 'alpha': 0.5365, 'lambda': 1.0398203565404225, 'max_bin': 369}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:03:43,791] Trial 74 finished with value: 0.827157985321058 and parameters: {'n_estimators': 834, 'eta': 0.06290884544274214, 'max_depth': 8, 'alpha': 0.6867000000000001, 'lambda': 1.995571776747342, 'max_bin': 479}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:03:58,323] Trial 75 finished with value: 0.830103662482274 and parameters: {'n_estimators': 870, 'eta': 0.0349812477679316, 'max_depth': 11, 'alpha': 0.9344, 'lambda': 7.43498295256051, 'max_bin': 289}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:04:09,099] Trial 76 finished with value: 0.8299558214229028 and parameters: {'n_estimators': 784, 'eta': 0.05371844780911643, 'max_depth': 12, 'alpha': 0.9012, 'lambda': 9.872097798879427, 'max_bin': 489}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:04:18,721] Trial 77 finished with value: 0.8291051882292265 and parameters: {'n_estimators': 757, 'eta': 0.04736454060637046, 'max_depth': 11, 'alpha': 0.6411, 'lambda': 3.976474627046575, 'max_bin': 394}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:04:34,211] Trial 78 finished with value: 0.8328639054186702 and parameters: {'n_estimators': 714, 'eta': 0.03211572114543676, 'max_depth': 12, 'alpha': 0.9732000000000001, 'lambda': 6.0451987827529035, 'max_bin': 305}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:04:48,493] Trial 79 finished with value: 0.8315467497720329 and parameters: {'n_estimators': 816, 'eta': 0.03737304363038783, 'max_depth': 9, 'alpha': 0.8281000000000001, 'lambda': 7.850873757986735, 'max_bin': 467}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:04:56,801] Trial 80 finished with value: 0.8300965168466149 and parameters: {'n_estimators': 688, 'eta': 0.05651015455422054, 'max_depth': 12, 'alpha': 0.9439000000000001, 'lambda': 2.805405987539724, 'max_bin': 252}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:05:08,901] Trial 81 finished with value: 0.8307062796293044 and parameters: {'n_estimators': 858, 'eta': 0.04187176109311929, 'max_depth': 11, 'alpha': 0.9736, 'lambda': 6.5340328213635885, 'max_bin': 431}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:05:20,454] Trial 82 finished with value: 0.8298835630103774 and parameters: {'n_estimators': 835, 'eta': 0.040698312324151706, 'max_depth': 10, 'alpha': 0.9211, 'lambda': 4.716259688349023, 'max_bin': 425}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:05:30,167] Trial 83 finished with value: 0.8312425209732691 and parameters: {'n_estimators': 872, 'eta': 0.043522238710418736, 'max_depth': 11, 'alpha': 0.8784000000000001, 'lambda': 1.9781811439191541, 'max_bin': 444}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:05:42,522] Trial 84 finished with value: 0.8355636543651178 and parameters: {'n_estimators': 898, 'eta': 0.03895282740099466, 'max_depth': 12, 'alpha': 0.9771000000000001, 'lambda': 4.164093503114756, 'max_bin': 380}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:05:55,470] Trial 85 finished with value: 0.8328478318398206 and parameters: {'n_estimators': 775, 'eta': 0.03379233223895711, 'max_depth': 12, 'alpha': 0.9553, 'lambda': 3.4146054151704726, 'max_bin': 348}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:06:05,533] Trial 86 finished with value: 0.8301928673483457 and parameters: {'n_estimators': 851, 'eta': 0.04953240949875125, 'max_depth': 11, 'alpha': 0.7515000000000001, 'lambda': 5.271023791825405, 'max_bin': 434}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:06:18,176] Trial 87 finished with value: 0.8322398762551263 and parameters: {'n_estimators': 729, 'eta': 0.030093296162871243, 'max_depth': 12, 'alpha': 0.8603000000000001, 'lambda': 1.6931961653421181, 'max_bin': 316}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:06:28,082] Trial 88 finished with value: 0.8291095399380219 and parameters: {'n_estimators': 814, 'eta': 0.046031663345079854, 'max_depth': 11, 'alpha': 0.9055000000000001, 'lambda': 2.8077567014055704, 'max_bin': 453}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:06:42,155] Trial 89 finished with value: 0.8320648220888494 and parameters: {'n_estimators': 772, 'eta': 0.03590806326103605, 'max_depth': 12, 'alpha': 0.9983000000000001, 'lambda': 6.736663216146893, 'max_bin': 491}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:06:52,271] Trial 90 finished with value: 0.8334253929035402 and parameters: {'n_estimators': 618, 'eta': 0.0388417207147412, 'max_depth': 10, 'alpha': 0.9391, 'lambda': 1.0303408520567667, 'max_bin': 274}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:07:05,328] Trial 91 finished with value: 0.8314795533844057 and parameters: {'n_estimators': 668, 'eta': 0.032863675213023004, 'max_depth': 11, 'alpha': 0.6918000000000001, 'lambda': 4.108740035841746, 'max_bin': 460}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:07:21,862] Trial 92 finished with value: 0.8323157827736534 and parameters: {'n_estimators': 851, 'eta': 0.025269698355464318, 'max_depth': 11, 'alpha': 0.5659000000000001, 'lambda': 5.0069727558329244, 'max_bin': 476}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:07:34,503] Trial 93 finished with value: 0.8329069726575483 and parameters: {'n_estimators': 564, 'eta': 0.03324748415299579, 'max_depth': 12, 'alpha': 0.8452000000000001, 'lambda': 2.742032054290139, 'max_bin': 471}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:07:48,978] Trial 94 finished with value: 0.8333615100407702 and parameters: {'n_estimators': 650, 'eta': 0.028691893291853926, 'max_depth': 11, 'alpha': 0.7064, 'lambda': 3.7513393467830403, 'max_bin': 483}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:08:01,097] Trial 95 finished with value: 0.835291692859936 and parameters: {'n_estimators': 792, 'eta': 0.031151098984578816, 'max_depth': 12, 'alpha': 0.891, 'lambda': 2.0514528631744016, 'max_bin': 457}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:08:13,739] Trial 96 finished with value: 0.8318146378356319 and parameters: {'n_estimators': 882, 'eta': 0.036930886121168814, 'max_depth': 12, 'alpha': 0.6093000000000001, 'lambda': 5.727566987148158, 'max_bin': 440}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:08:23,668] Trial 97 finished with value: 0.8293280176554763 and parameters: {'n_estimators': 734, 'eta': 0.04303655979863883, 'max_depth': 12, 'alpha': 0.662, 'lambda': 3.2313026964121034, 'max_bin': 447}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:08:32,613] Trial 98 finished with value: 0.8323361917775405 and parameters: {'n_estimators': 800, 'eta': 0.06585327416543434, 'max_depth': 10, 'alpha': 0.9571000000000001, 'lambda': 6.892790728729857, 'max_bin': 356}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:08:44,199] Trial 99 finished with value: 0.8295479628497006 and parameters: {'n_estimators': 821, 'eta': 0.035226823553477844, 'max_depth': 11, 'alpha': 0.8043, 'lambda': 1.8841043996234386, 'max_bin': 418}. Best is trial 11 with value: 0.8513589595154614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 785\n",
      "\t\teta: 0.07389767489222081\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9650000000000001\n",
      "\t\tlambda: 2.5359194636827933\n",
      "\t\tmax_bin: 259\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  424.000000  421.000000\n",
      "1                    TN  339.000000  364.000000\n",
      "2                    FP   88.000000   71.000000\n",
      "3                    FN   68.000000   63.000000\n",
      "4              Accuracy    0.830250    0.854189\n",
      "5             Precision    0.828125    0.855691\n",
      "6           Sensitivity    0.861789    0.869835\n",
      "7           Specificity    0.793900    0.836800\n",
      "8              F1 score    0.844622    0.862705\n",
      "9   F1 score (weighted)    0.829906    0.854110\n",
      "10     F1 score (macro)    0.828786    0.853626\n",
      "11    Balanced Accuracy    0.827850    0.853308\n",
      "12                  MCC    0.658369    0.707383\n",
      "13                  NPV    0.832900    0.852500\n",
      "14              ROC_AUC    0.827850    0.853308\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_xgb_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_xgb_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_xgb_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_xgb_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_xgb_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_xgb_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_xgb_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_xgb_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_xgb_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_xgb_1)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 10:09:00,783] Trial 100 finished with value: 0.831124565842383 and parameters: {'n_estimators': 604, 'eta': 0.026863514525593366, 'max_depth': 12, 'alpha': 0.9185000000000001, 'lambda': 8.246626659422173, 'max_bin': 398}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:09:12,811] Trial 101 finished with value: 0.8352954321229829 and parameters: {'n_estimators': 766, 'eta': 0.031543636681723504, 'max_depth': 10, 'alpha': 0.6116, 'lambda': 1.436015948181276, 'max_bin': 403}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:09:27,046] Trial 102 finished with value: 0.8338853386814697 and parameters: {'n_estimators': 831, 'eta': 0.030258726312520155, 'max_depth': 9, 'alpha': 0.5896, 'lambda': 4.557019185632569, 'max_bin': 367}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:09:42,870] Trial 103 finished with value: 0.8353243680678535 and parameters: {'n_estimators': 865, 'eta': 0.02429163093130583, 'max_depth': 11, 'alpha': 0.5591, 'lambda': 2.7509604370678087, 'max_bin': 427}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:09:54,229] Trial 104 finished with value: 0.8339552420863205 and parameters: {'n_estimators': 703, 'eta': 0.0398954592504703, 'max_depth': 12, 'alpha': 0.9840000000000001, 'lambda': 3.543037521179863, 'max_bin': 414}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:10:06,744] Trial 105 finished with value: 0.8342038591719888 and parameters: {'n_estimators': 643, 'eta': 0.028574033616772194, 'max_depth': 12, 'alpha': 0.47140000000000004, 'lambda': 1.6470176284178355, 'max_bin': 463}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:10:19,498] Trial 106 finished with value: 0.8338912250324997 and parameters: {'n_estimators': 796, 'eta': 0.03642426283488708, 'max_depth': 10, 'alpha': 0.6564, 'lambda': 5.258862784247595, 'max_bin': 410}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:10:29,488] Trial 107 finished with value: 0.8352664356746187 and parameters: {'n_estimators': 751, 'eta': 0.041825263433045656, 'max_depth': 11, 'alpha': 0.7339, 'lambda': 2.523692849284422, 'max_bin': 393}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:10:39,075] Trial 108 finished with value: 0.8314663330399442 and parameters: {'n_estimators': 843, 'eta': 0.04521527436824307, 'max_depth': 12, 'alpha': 0.9575, 'lambda': 3.8031731005867924, 'max_bin': 387}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:10:52,219] Trial 109 finished with value: 0.8327499520998641 and parameters: {'n_estimators': 889, 'eta': 0.03357039462162448, 'max_depth': 12, 'alpha': 0.9337000000000001, 'lambda': 4.743513640903597, 'max_bin': 376}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:11:04,708] Trial 110 finished with value: 0.8350072198761973 and parameters: {'n_estimators': 863, 'eta': 0.027030750952656106, 'max_depth': 11, 'alpha': 0.5239, 'lambda': 1.036967370669256, 'max_bin': 493}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:11:16,914] Trial 111 finished with value: 0.8312076084294844 and parameters: {'n_estimators': 677, 'eta': 0.03893572490035717, 'max_depth': 12, 'alpha': 0.9795, 'lambda': 5.816162770705047, 'max_bin': 430}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:11:27,357] Trial 112 finished with value: 0.8341583439606375 and parameters: {'n_estimators': 785, 'eta': 0.04236410620270582, 'max_depth': 12, 'alpha': 0.9507000000000001, 'lambda': 2.9702188366853712, 'max_bin': 404}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:11:38,106] Trial 113 finished with value: 0.8355635677531913 and parameters: {'n_estimators': 806, 'eta': 0.04798296275811851, 'max_depth': 12, 'alpha': 0.9019, 'lambda': 7.231239308815104, 'max_bin': 436}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:11:48,724] Trial 114 finished with value: 0.8341893803837255 and parameters: {'n_estimators': 900, 'eta': 0.03797264387341766, 'max_depth': 12, 'alpha': 0.9209, 'lambda': 2.162167768146672, 'max_bin': 452}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:11:58,327] Trial 115 finished with value: 0.8331730474697057 and parameters: {'n_estimators': 514, 'eta': 0.050746368902711364, 'max_depth': 12, 'alpha': 0.9959, 'lambda': 4.285350923555311, 'max_bin': 422}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:12:11,750] Trial 116 finished with value: 0.8350713949620457 and parameters: {'n_estimators': 830, 'eta': 0.04108169250296961, 'max_depth': 11, 'alpha': 0.9642000000000001, 'lambda': 6.060274020374926, 'max_bin': 258}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:12:24,146] Trial 117 finished with value: 0.8369308762547739 and parameters: {'n_estimators': 741, 'eta': 0.03552643982444424, 'max_depth': 12, 'alpha': 0.8737, 'lambda': 3.6141724143084826, 'max_bin': 481}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:12:31,324] Trial 118 finished with value: 0.8283898135826234 and parameters: {'n_estimators': 763, 'eta': 0.06103159790507504, 'max_depth': 12, 'alpha': 0.9404, 'lambda': 1.744622359678237, 'max_bin': 473}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:12:35,693] Trial 119 finished with value: 0.822469311185773 and parameters: {'n_estimators': 151, 'eta': 0.03233046339906775, 'max_depth': 11, 'alpha': 0.8923000000000001, 'lambda': 2.8524666010646422, 'max_bin': 286}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:12:45,259] Trial 120 finished with value: 0.8322112261082767 and parameters: {'n_estimators': 849, 'eta': 0.053611593390591454, 'max_depth': 8, 'alpha': 0.8352, 'lambda': 5.451106821407201, 'max_bin': 443}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:13:05,852] Trial 121 finished with value: 0.8298040492265655 and parameters: {'n_estimators': 817, 'eta': 0.020985357753479306, 'max_depth': 11, 'alpha': 0.9828, 'lambda': 7.879811866303623, 'max_bin': 432}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:13:19,834] Trial 122 finished with value: 0.8308609414920106 and parameters: {'n_estimators': 775, 'eta': 0.030715047549764507, 'max_depth': 12, 'alpha': 0.935, 'lambda': 4.4784587556947635, 'max_bin': 448}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:13:35,307] Trial 123 finished with value: 0.8336205504259528 and parameters: {'n_estimators': 718, 'eta': 0.02504957118975016, 'max_depth': 11, 'alpha': 0.9611000000000001, 'lambda': 2.3192254617274393, 'max_bin': 440}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:13:50,752] Trial 124 finished with value: 0.8358627661866482 and parameters: {'n_estimators': 876, 'eta': 0.034399184051377256, 'max_depth': 10, 'alpha': 0.9146000000000001, 'lambda': 9.286479416185152, 'max_bin': 464}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:13:59,084] Trial 125 finished with value: 0.8341626472262582 and parameters: {'n_estimators': 580, 'eta': 0.07247002845062492, 'max_depth': 12, 'alpha': 0.9815, 'lambda': 6.62587003645812, 'max_bin': 456}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:14:14,070] Trial 126 finished with value: 0.8327865640953949 and parameters: {'n_estimators': 802, 'eta': 0.027467344883740873, 'max_depth': 12, 'alpha': 0.9448000000000001, 'lambda': 3.3970380810076577, 'max_bin': 421}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:14:29,838] Trial 127 finished with value: 0.8328172312459202 and parameters: {'n_estimators': 618, 'eta': 0.029176450516669828, 'max_depth': 11, 'alpha': 0.7654000000000001, 'lambda': 10.936610777023137, 'max_bin': 426}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:14:39,530] Trial 128 finished with value: 0.8328072749281127 and parameters: {'n_estimators': 395, 'eta': 0.04531446233614278, 'max_depth': 12, 'alpha': 0.6319, 'lambda': 5.2321462679629605, 'max_bin': 469}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:14:49,932] Trial 129 finished with value: 0.8342223036069868 and parameters: {'n_estimators': 841, 'eta': 0.03701241883649769, 'max_depth': 11, 'alpha': 0.677, 'lambda': 1.1676668045000893, 'max_bin': 270}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:15:01,076] Trial 130 finished with value: 0.8323133973659909 and parameters: {'n_estimators': 825, 'eta': 0.05508293419682812, 'max_depth': 12, 'alpha': 0.8645, 'lambda': 12.215050739157585, 'max_bin': 360}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:15:13,707] Trial 131 finished with value: 0.8328655823281043 and parameters: {'n_estimators': 857, 'eta': 0.03235669195836176, 'max_depth': 11, 'alpha': 0.8867, 'lambda': 3.2309159049974543, 'max_bin': 298}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:15:26,059] Trial 132 finished with value: 0.8355976478300571 and parameters: {'n_estimators': 880, 'eta': 0.03443751848281518, 'max_depth': 11, 'alpha': 0.4001, 'lambda': 4.336605727317959, 'max_bin': 477}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:15:32,386] Trial 133 finished with value: 0.832865990190134 and parameters: {'n_estimators': 780, 'eta': 0.08098236169921373, 'max_depth': 10, 'alpha': 0.9093, 'lambda': 2.136418669406596, 'max_bin': 496}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:15:45,065] Trial 134 finished with value: 0.8350608440855041 and parameters: {'n_estimators': 812, 'eta': 0.038935410490003035, 'max_depth': 11, 'alpha': 0.9281, 'lambda': 5.8265605938088765, 'max_bin': 485}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:15:58,017] Trial 135 finished with value: 0.8249171391653078 and parameters: {'n_estimators': 857, 'eta': 0.030308373959376293, 'max_depth': 6, 'alpha': 0.9675, 'lambda': 3.9155584588299037, 'max_bin': 459}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:16:14,335] Trial 136 finished with value: 0.8316974500988128 and parameters: {'n_estimators': 793, 'eta': 0.02372040316012118, 'max_depth': 12, 'alpha': 0.9937, 'lambda': 2.821449724866227, 'max_bin': 445}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:16:24,379] Trial 137 finished with value: 0.8325930537136058 and parameters: {'n_estimators': 838, 'eta': 0.048638167723876136, 'max_depth': 11, 'alpha': 1.0, 'lambda': 4.834775924401924, 'max_bin': 451}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:16:42,158] Trial 138 finished with value: 0.8311466604849344 and parameters: {'n_estimators': 753, 'eta': 0.026163927995736823, 'max_depth': 12, 'alpha': 0.9497000000000001, 'lambda': 7.318403117377549, 'max_bin': 436}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:16:54,326] Trial 139 finished with value: 0.834163440018487 and parameters: {'n_estimators': 818, 'eta': 0.03205769803999531, 'max_depth': 9, 'alpha': 0.8901, 'lambda': 1.5446157687690791, 'max_bin': 416}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:17:07,950] Trial 140 finished with value: 0.8296463340864066 and parameters: {'n_estimators': 872, 'eta': 0.036744848618922306, 'max_depth': 7, 'alpha': 0.5945, 'lambda': 6.317431514381731, 'max_bin': 341}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:17:21,718] Trial 141 finished with value: 0.8341838988793976 and parameters: {'n_estimators': 696, 'eta': 0.029257359630098193, 'max_depth': 12, 'alpha': 0.9700000000000001, 'lambda': 2.5570447579666995, 'max_bin': 471}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:17:34,971] Trial 142 finished with value: 0.8393518903036823 and parameters: {'n_estimators': 637, 'eta': 0.03423487250950006, 'max_depth': 12, 'alpha': 0.9551000000000001, 'lambda': 3.637858059452708, 'max_bin': 468}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:17:44,073] Trial 143 finished with value: 0.8323080416199289 and parameters: {'n_estimators': 655, 'eta': 0.04101128369537886, 'max_depth': 12, 'alpha': 0.9315, 'lambda': 1.033652983724651, 'max_bin': 463}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:17:51,347] Trial 144 finished with value: 0.8320375565675512 and parameters: {'n_estimators': 725, 'eta': 0.05694263555745837, 'max_depth': 12, 'alpha': 0.9780000000000001, 'lambda': 2.3263127635101615, 'max_bin': 379}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:18:01,911] Trial 145 finished with value: 0.8336883044166677 and parameters: {'n_estimators': 765, 'eta': 0.0437329299377473, 'max_depth': 12, 'alpha': 0.9132, 'lambda': 4.110456238975324, 'max_bin': 482}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:18:13,855] Trial 146 finished with value: 0.8377213134683702 and parameters: {'n_estimators': 671, 'eta': 0.03399909712362612, 'max_depth': 11, 'alpha': 0.20370000000000002, 'lambda': 3.271295776107512, 'max_bin': 456}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:18:26,079] Trial 147 finished with value: 0.8300900227881078 and parameters: {'n_estimators': 705, 'eta': 0.0321320168234083, 'max_depth': 12, 'alpha': 0.9433, 'lambda': 1.7783520140202551, 'max_bin': 327}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:18:39,419] Trial 148 finished with value: 0.8339662663618135 and parameters: {'n_estimators': 791, 'eta': 0.051905410432532315, 'max_depth': 10, 'alpha': 0.9853000000000001, 'lambda': 15.822642563468673, 'max_bin': 385}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:18:52,101] Trial 149 finished with value: 0.8341426326243561 and parameters: {'n_estimators': 685, 'eta': 0.0362072097902786, 'max_depth': 12, 'alpha': 0.7141000000000001, 'lambda': 4.975701790863148, 'max_bin': 398}. Best is trial 11 with value: 0.8513589595154614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 785\n",
      "\t\teta: 0.07389767489222081\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9650000000000001\n",
      "\t\tlambda: 2.5359194636827933\n",
      "\t\tmax_bin: 259\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  424.000000  421.000000  422.000000\n",
      "1                    TN  339.000000  364.000000  338.000000\n",
      "2                    FP   88.000000   71.000000   84.000000\n",
      "3                    FN   68.000000   63.000000   75.000000\n",
      "4              Accuracy    0.830250    0.854189    0.826986\n",
      "5             Precision    0.828125    0.855691    0.833992\n",
      "6           Sensitivity    0.861789    0.869835    0.849095\n",
      "7           Specificity    0.793900    0.836800    0.800900\n",
      "8              F1 score    0.844622    0.862705    0.841476\n",
      "9   F1 score (weighted)    0.829906    0.854110    0.826830\n",
      "10     F1 score (macro)    0.828786    0.853626    0.825528\n",
      "11    Balanced Accuracy    0.827850    0.853308    0.825021\n",
      "12                  MCC    0.658369    0.707383    0.651217\n",
      "13                  NPV    0.832900    0.852500    0.818400\n",
      "14              ROC_AUC    0.827850    0.853308    0.825021\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_xgb_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_xgb_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_xgb_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_xgb_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_xgb_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_xgb_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_xgb_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_xgb_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_xgb_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_xgb_2)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 10:19:03,279] Trial 150 finished with value: 0.8382611396094607 and parameters: {'n_estimators': 833, 'eta': 0.03820709326136296, 'max_depth': 11, 'alpha': 0.5401, 'lambda': 1.0113842251985508, 'max_bin': 371}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:19:22,490] Trial 151 finished with value: 0.8373382635907729 and parameters: {'n_estimators': 858, 'eta': 0.021833078015239708, 'max_depth': 11, 'alpha': 0.7901, 'lambda': 2.72807118434846, 'max_bin': 476}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:19:42,105] Trial 152 finished with value: 0.8354201800693136 and parameters: {'n_estimators': 810, 'eta': 0.02707609929966719, 'max_depth': 11, 'alpha': 0.8281000000000001, 'lambda': 8.65380409170458, 'max_bin': 491}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:20:03,744] Trial 153 finished with value: 0.8362271941562252 and parameters: {'n_estimators': 886, 'eta': 0.019172697896271698, 'max_depth': 11, 'alpha': 0.7407, 'lambda': 4.349605538078439, 'max_bin': 484}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:20:23,352] Trial 154 finished with value: 0.8357329135321807 and parameters: {'n_estimators': 827, 'eta': 0.023992597886217687, 'max_depth': 12, 'alpha': 0.8455, 'lambda': 5.443252699374503, 'max_bin': 486}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:20:45,964] Trial 155 finished with value: 0.8371044619913098 and parameters: {'n_estimators': 848, 'eta': 0.016949385747342033, 'max_depth': 12, 'alpha': 0.9641000000000001, 'lambda': 3.288115465430424, 'max_bin': 392}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:20:57,514] Trial 156 finished with value: 0.8265670911966698 and parameters: {'n_estimators': 899, 'eta': 0.0304200439108422, 'max_depth': 5, 'alpha': 0.9982000000000001, 'lambda': 2.118720000144961, 'max_bin': 478}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:21:07,323] Trial 157 finished with value: 0.8379298839473851 and parameters: {'n_estimators': 779, 'eta': 0.06733821731156836, 'max_depth': 10, 'alpha': 0.8977, 'lambda': 6.143025390700449, 'max_bin': 466}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:21:19,511] Trial 158 finished with value: 0.8414732887218589 and parameters: {'n_estimators': 743, 'eta': 0.046375500992602854, 'max_depth': 11, 'alpha': 0.8616, 'lambda': 4.637278559437464, 'max_bin': 499}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:21:36,255] Trial 159 finished with value: 0.8381949431259853 and parameters: {'n_estimators': 800, 'eta': 0.0401116040807369, 'max_depth': 12, 'alpha': 0.0041, 'lambda': 17.955514142349863, 'max_bin': 440}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:21:51,861] Trial 160 finished with value: 0.8376059171989008 and parameters: {'n_estimators': 606, 'eta': 0.027621644339344567, 'max_depth': 12, 'alpha': 0.9217000000000001, 'lambda': 3.994228325645194, 'max_bin': 453}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:22:03,102] Trial 161 finished with value: 0.8396462019590935 and parameters: {'n_estimators': 870, 'eta': 0.042843519139790565, 'max_depth': 11, 'alpha': 0.9361, 'lambda': 1.5109635673021773, 'max_bin': 387}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:22:09,606] Trial 162 finished with value: 0.834885056572011 and parameters: {'n_estimators': 887, 'eta': 0.07253625432127385, 'max_depth': 11, 'alpha': 0.9560000000000001, 'lambda': 2.775423346540831, 'max_bin': 429}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:22:16,184] Trial 163 finished with value: 0.8348971411068618 and parameters: {'n_estimators': 246, 'eta': 0.03645429020159445, 'max_depth': 11, 'alpha': 0.9754, 'lambda': 1.9334371709107294, 'max_bin': 375}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:22:31,779] Trial 164 finished with value: 0.8403867556548844 and parameters: {'n_estimators': 849, 'eta': 0.032993396364500584, 'max_depth': 11, 'alpha': 0.9067000000000001, 'lambda': 3.433353420608952, 'max_bin': 409}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:22:42,428] Trial 165 finished with value: 0.8381872896406979 and parameters: {'n_estimators': 827, 'eta': 0.05862514479102501, 'max_depth': 12, 'alpha': 0.9437000000000001, 'lambda': 7.092316036533873, 'max_bin': 367}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:22:48,188] Trial 166 finished with value: 0.8349936029328748 and parameters: {'n_estimators': 867, 'eta': 0.09812912431846613, 'max_depth': 11, 'alpha': 0.9800000000000001, 'lambda': 2.5026217183057757, 'max_bin': 473}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:23:04,236] Trial 167 finished with value: 0.8398248543966236 and parameters: {'n_estimators': 840, 'eta': 0.03926584164037278, 'max_depth': 12, 'alpha': 0.8786, 'lambda': 9.875941712966227, 'max_bin': 379}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:23:13,838] Trial 168 finished with value: 0.8367498700340554 and parameters: {'n_estimators': 899, 'eta': 0.05008784968859437, 'max_depth': 10, 'alpha': 0.7894, 'lambda': 1.8693254573334053, 'max_bin': 391}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:23:30,744] Trial 169 finished with value: 0.8351367745271407 and parameters: {'n_estimators': 810, 'eta': 0.03108718729822263, 'max_depth': 12, 'alpha': 0.9291, 'lambda': 5.152327556398199, 'max_bin': 402}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:23:42,773] Trial 170 finished with value: 0.8417645860186008 and parameters: {'n_estimators': 879, 'eta': 0.04469487946697896, 'max_depth': 11, 'alpha': 0.9565, 'lambda': 3.219443089120822, 'max_bin': 490}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:23:52,969] Trial 171 finished with value: 0.8329543662723464 and parameters: {'n_estimators': 436, 'eta': 0.025391876335580134, 'max_depth': 10, 'alpha': 0.6879000000000001, 'lambda': 1.0069637216273355, 'max_bin': 265}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:24:08,657] Trial 172 finished with value: 0.8366083172106127 and parameters: {'n_estimators': 787, 'eta': 0.028664572176122408, 'max_depth': 9, 'alpha': 0.8149000000000001, 'lambda': 1.7402421983273872, 'max_bin': 276}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:24:18,169] Trial 173 finished with value: 0.8359735940735282 and parameters: {'n_estimators': 377, 'eta': 0.03451802267075378, 'max_depth': 11, 'alpha': 0.7544000000000001, 'lambda': 2.5726553497807876, 'max_bin': 250}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:24:24,302] Trial 174 finished with value: 0.8397465034166439 and parameters: {'n_estimators': 308, 'eta': 0.09254002556770351, 'max_depth': 12, 'alpha': 0.7727, 'lambda': 3.7838875705274466, 'max_bin': 293}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:24:35,291] Trial 175 finished with value: 0.8374225003248998 and parameters: {'n_estimators': 858, 'eta': 0.07989293489529069, 'max_depth': 11, 'alpha': 0.645, 'lambda': 22.46374714185643, 'max_bin': 312}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:24:48,301] Trial 176 finished with value: 0.8389828963966822 and parameters: {'n_estimators': 766, 'eta': 0.032717625559549796, 'max_depth': 12, 'alpha': 0.9953000000000001, 'lambda': 1.6664505599291486, 'max_bin': 282}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:25:03,720] Trial 177 finished with value: 0.8425451026067059 and parameters: {'n_estimators': 625, 'eta': 0.02977678639617972, 'max_depth': 12, 'alpha': 0.7192000000000001, 'lambda': 3.0374289126963325, 'max_bin': 447}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:25:16,069] Trial 178 finished with value: 0.835412057512495 and parameters: {'n_estimators': 472, 'eta': 0.03577849343324698, 'max_depth': 11, 'alpha': 0.9186000000000001, 'lambda': 7.888151338351086, 'max_bin': 302}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:25:24,187] Trial 179 finished with value: 0.8368486653949375 and parameters: {'n_estimators': 650, 'eta': 0.06966435283825001, 'max_depth': 8, 'alpha': 0.6711, 'lambda': 4.079482885755568, 'max_bin': 461}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:25:26,344] Trial 180 finished with value: 0.8095870599116999 and parameters: {'n_estimators': 61, 'eta': 0.02265844757862668, 'max_depth': 12, 'alpha': 0.040100000000000004, 'lambda': 5.784172015690094, 'max_bin': 435}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:25:35,694] Trial 181 finished with value: 0.8376605213885181 and parameters: {'n_estimators': 365, 'eta': 0.04226483102184035, 'max_depth': 12, 'alpha': 0.9686, 'lambda': 1.105179154538336, 'max_bin': 317}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:25:48,696] Trial 182 finished with value: 0.838528262486568 and parameters: {'n_estimators': 874, 'eta': 0.03806129877551345, 'max_depth': 12, 'alpha': 0.9434, 'lambda': 2.2403282031074006, 'max_bin': 280}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:25:59,513] Trial 183 finished with value: 0.836886884434047 and parameters: {'n_estimators': 823, 'eta': 0.04724617999539473, 'max_depth': 12, 'alpha': 0.9962000000000001, 'lambda': 2.424303740269945, 'max_bin': 292}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:26:13,471] Trial 184 finished with value: 0.8386868962022694 and parameters: {'n_estimators': 845, 'eta': 0.04061131594355577, 'max_depth': 12, 'alpha': 0.9682000000000001, 'lambda': 4.714629508055683, 'max_bin': 336}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:26:25,309] Trial 185 finished with value: 0.8368640289017772 and parameters: {'n_estimators': 887, 'eta': 0.061510131451954435, 'max_depth': 12, 'alpha': 0.9517, 'lambda': 11.767407886990469, 'max_bin': 256}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:26:43,133] Trial 186 finished with value: 0.8376609678093487 and parameters: {'n_estimators': 804, 'eta': 0.03477342774244019, 'max_depth': 11, 'alpha': 0.9828, 'lambda': 13.387083182492074, 'max_bin': 421}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:26:59,753] Trial 187 finished with value: 0.8409796044859424 and parameters: {'n_estimators': 861, 'eta': 0.031221926131410505, 'max_depth': 11, 'alpha': 0.9023, 'lambda': 3.166130931368583, 'max_bin': 333}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:27:12,952] Trial 188 finished with value: 0.8412152725065983 and parameters: {'n_estimators': 834, 'eta': 0.03692715034045792, 'max_depth': 10, 'alpha': 0.9994000000000001, 'lambda': 1.7736740465237675, 'max_bin': 306}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:27:21,539] Trial 189 finished with value: 0.839262389689719 and parameters: {'n_estimators': 895, 'eta': 0.05225670261275139, 'max_depth': 12, 'alpha': 0.9298000000000001, 'lambda': 1.009304579499533, 'max_bin': 384}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:27:30,609] Trial 190 finished with value: 0.8378174798180975 and parameters: {'n_estimators': 518, 'eta': 0.06442659922470947, 'max_depth': 12, 'alpha': 0.9674, 'lambda': 4.221632975359541, 'max_bin': 458}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:27:38,994] Trial 191 finished with value: 0.840094255015039 and parameters: {'n_estimators': 729, 'eta': 0.07515546865811036, 'max_depth': 11, 'alpha': 0.6181, 'lambda': 6.52190086977689, 'max_bin': 265}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:27:52,578] Trial 192 finished with value: 0.8414678585302372 and parameters: {'n_estimators': 745, 'eta': 0.032982457674326636, 'max_depth': 11, 'alpha': 0.8539, 'lambda': 2.4949729246493555, 'max_bin': 262}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:28:01,897] Trial 193 finished with value: 0.836562649768973 and parameters: {'n_estimators': 777, 'eta': 0.06982038226895244, 'max_depth': 11, 'alpha': 0.9432, 'lambda': 8.97886169374997, 'max_bin': 466}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:28:17,285] Trial 194 finished with value: 0.8395167785696274 and parameters: {'n_estimators': 692, 'eta': 0.028254155846934144, 'max_depth': 11, 'alpha': 0.8896000000000001, 'lambda': 3.1239126981926004, 'max_bin': 267}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:28:26,863] Trial 195 finished with value: 0.8426316025348944 and parameters: {'n_estimators': 673, 'eta': 0.054173960718452395, 'max_depth': 12, 'alpha': 0.9182, 'lambda': 1.7739441457948573, 'max_bin': 494}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:28:29,413] Trial 196 finished with value: 0.8209933391139547 and parameters: {'n_estimators': 93, 'eta': 0.07494218655453265, 'max_depth': 10, 'alpha': 0.3049, 'lambda': 9.864904014524008, 'max_bin': 355}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:28:42,667] Trial 197 finished with value: 0.8400820845140397 and parameters: {'n_estimators': 794, 'eta': 0.03763072764861248, 'max_depth': 11, 'alpha': 0.9604, 'lambda': 3.7516091386686963, 'max_bin': 259}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:28:53,931] Trial 198 finished with value: 0.8382092676468764 and parameters: {'n_estimators': 758, 'eta': 0.049389095400512985, 'max_depth': 12, 'alpha': 0.877, 'lambda': 5.101512374939913, 'max_bin': 479}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:29:03,822] Trial 199 finished with value: 0.8370756191150184 and parameters: {'n_estimators': 707, 'eta': 0.06565547575849878, 'max_depth': 11, 'alpha': 0.9764, 'lambda': 7.801799825869684, 'max_bin': 372}. Best is trial 11 with value: 0.8513589595154614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 785\n",
      "\t\teta: 0.07389767489222081\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9650000000000001\n",
      "\t\tlambda: 2.5359194636827933\n",
      "\t\tmax_bin: 259\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  424.000000  421.000000  422.000000  449.000000\n",
      "1                    TN  339.000000  364.000000  338.000000  322.000000\n",
      "2                    FP   88.000000   71.000000   84.000000   82.000000\n",
      "3                    FN   68.000000   63.000000   75.000000   66.000000\n",
      "4              Accuracy    0.830250    0.854189    0.826986    0.838955\n",
      "5             Precision    0.828125    0.855691    0.833992    0.845574\n",
      "6           Sensitivity    0.861789    0.869835    0.849095    0.871845\n",
      "7           Specificity    0.793900    0.836800    0.800900    0.797000\n",
      "8              F1 score    0.844622    0.862705    0.841476    0.858509\n",
      "9   F1 score (weighted)    0.829906    0.854110    0.826830    0.838560\n",
      "10     F1 score (macro)    0.828786    0.853626    0.825528    0.835820\n",
      "11    Balanced Accuracy    0.827850    0.853308    0.825021    0.834437\n",
      "12                  MCC    0.658369    0.707383    0.651217    0.672165\n",
      "13                  NPV    0.832900    0.852500    0.818400    0.829900\n",
      "14              ROC_AUC    0.827850    0.853308    0.825021    0.834437\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_xgb_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_xgb_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_xgb_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_xgb_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_xgb_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_xgb_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_xgb_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_xgb_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_xgb_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_xgb_3)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 10:29:16,734] Trial 200 finished with value: 0.8421544898937672 and parameters: {'n_estimators': 820, 'eta': 0.03545524552102297, 'max_depth': 12, 'alpha': 0.7988000000000001, 'lambda': 2.5313830822985137, 'max_bin': 273}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:29:24,865] Trial 201 finished with value: 0.8416881811609918 and parameters: {'n_estimators': 872, 'eta': 0.05911590389944361, 'max_depth': 11, 'alpha': 0.7108, 'lambda': 3.6831271806394144, 'max_bin': 363}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:29:36,263] Trial 202 finished with value: 0.838406259455186 and parameters: {'n_estimators': 848, 'eta': 0.06298678826462267, 'max_depth': 11, 'alpha': 0.7448, 'lambda': 14.0595772806587, 'max_bin': 322}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:29:43,194] Trial 203 finished with value: 0.8383450308588627 and parameters: {'n_estimators': 562, 'eta': 0.06755741750203524, 'max_depth': 11, 'alpha': 0.48400000000000004, 'lambda': 1.691612135255792, 'max_bin': 426}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:29:52,416] Trial 204 finished with value: 0.8429701626352999 and parameters: {'n_estimators': 861, 'eta': 0.056517810041820236, 'max_depth': 11, 'alpha': 0.7248, 'lambda': 4.435561394300425, 'max_bin': 414}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:30:13,491] Trial 205 finished with value: 0.8350261597498289 and parameters: {'n_estimators': 900, 'eta': 0.026585018728659354, 'max_depth': 12, 'alpha': 0.6981, 'lambda': 17.024708975347014, 'max_bin': 347}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:30:29,236] Trial 206 finished with value: 0.8378606766454528 and parameters: {'n_estimators': 812, 'eta': 0.031298740573052194, 'max_depth': 9, 'alpha': 0.9385, 'lambda': 5.7505460434104805, 'max_bin': 432}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:30:43,599] Trial 207 finished with value: 0.8419182973815665 and parameters: {'n_estimators': 878, 'eta': 0.033216734376169677, 'max_depth': 11, 'alpha': 0.7781, 'lambda': 3.2095717418542185, 'max_bin': 382}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:30:52,613] Trial 208 finished with value: 0.8377619206916066 and parameters: {'n_estimators': 785, 'eta': 0.07157557600593094, 'max_depth': 12, 'alpha': 0.7596, 'lambda': 10.696255881394533, 'max_bin': 488}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:31:03,306] Trial 209 finished with value: 0.8402946202539248 and parameters: {'n_estimators': 834, 'eta': 0.055576626873980614, 'max_depth': 12, 'alpha': 0.8255, 'lambda': 7.028792580830873, 'max_bin': 377}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:31:15,597] Trial 210 finished with value: 0.8408832675390695 and parameters: {'n_estimators': 799, 'eta': 0.03934965991819827, 'max_depth': 11, 'alpha': 0.9575, 'lambda': 2.1253284354868853, 'max_bin': 472}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:31:26,045] Trial 211 finished with value: 0.837864131779434 and parameters: {'n_estimators': 572, 'eta': 0.05229595512052524, 'max_depth': 12, 'alpha': 0.9194, 'lambda': 5.765394380310662, 'max_bin': 442}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:31:38,403] Trial 212 finished with value: 0.8390632112617039 and parameters: {'n_estimators': 601, 'eta': 0.04528324839378698, 'max_depth': 12, 'alpha': 0.901, 'lambda': 6.770528309442145, 'max_bin': 454}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:31:50,787] Trial 213 finished with value: 0.8401964412525713 and parameters: {'n_estimators': 533, 'eta': 0.042913582922295405, 'max_depth': 12, 'alpha': 0.9800000000000001, 'lambda': 8.961355290364878, 'max_bin': 450}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:32:04,246] Trial 214 finished with value: 0.8350783964802838 and parameters: {'n_estimators': 636, 'eta': 0.04889101375111167, 'max_depth': 12, 'alpha': 0.9387000000000001, 'lambda': 11.42765084718653, 'max_bin': 460}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:32:14,582] Trial 215 finished with value: 0.8392031192400001 and parameters: {'n_estimators': 596, 'eta': 0.04047977897199824, 'max_depth': 12, 'alpha': 0.9979, 'lambda': 1.1347846654353309, 'max_bin': 463}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:32:22,972] Trial 216 finished with value: 0.8400747911670352 and parameters: {'n_estimators': 855, 'eta': 0.06013273404338962, 'max_depth': 12, 'alpha': 0.9569000000000001, 'lambda': 2.6839583233814333, 'max_bin': 448}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:32:32,011] Trial 217 finished with value: 0.8367854265912232 and parameters: {'n_estimators': 582, 'eta': 0.05747928797173893, 'max_depth': 11, 'alpha': 0.5944, 'lambda': 3.65049491828133, 'max_bin': 389}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:32:42,904] Trial 218 finished with value: 0.8408444501941361 and parameters: {'n_estimators': 661, 'eta': 0.050780682402265576, 'max_depth': 11, 'alpha': 0.9134, 'lambda': 5.054761152232102, 'max_bin': 437}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:32:55,513] Trial 219 finished with value: 0.8385918021192216 and parameters: {'n_estimators': 491, 'eta': 0.029901218640619003, 'max_depth': 12, 'alpha': 0.8652000000000001, 'lambda': 1.9720349467070108, 'max_bin': 468}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:33:09,204] Trial 220 finished with value: 0.8407766614705563 and parameters: {'n_estimators': 773, 'eta': 0.04734115531036357, 'max_depth': 12, 'alpha': 0.9754, 'lambda': 8.476737263767607, 'max_bin': 254}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:33:15,945] Trial 221 finished with value: 0.8305048160732152 and parameters: {'n_estimators': 329, 'eta': 0.055403467983534, 'max_depth': 8, 'alpha': 0.7355, 'lambda': 7.653733408897696, 'max_bin': 364}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:33:22,708] Trial 222 finished with value: 0.8361508081862853 and parameters: {'n_estimators': 349, 'eta': 0.05343364196945228, 'max_depth': 8, 'alpha': 0.4781, 'lambda': 2.8529300191664486, 'max_bin': 354}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:33:32,124] Trial 223 finished with value: 0.837791139005702 and parameters: {'n_estimators': 436, 'eta': 0.05014893067533774, 'max_depth': 11, 'alpha': 0.4177, 'lambda': 4.596648206279447, 'max_bin': 368}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:33:41,077] Trial 224 finished with value: 0.8353697298870261 and parameters: {'n_estimators': 396, 'eta': 0.035250053149941604, 'max_depth': 9, 'alpha': 0.9375, 'lambda': 3.7844510463791634, 'max_bin': 360}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:33:55,688] Trial 225 finished with value: 0.8315668789958757 and parameters: {'n_estimators': 883, 'eta': 0.05373808044782226, 'max_depth': 7, 'alpha': 0.9534, 'lambda': 19.049740416559388, 'max_bin': 311}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:34:01,580] Trial 226 finished with value: 0.8350103376730091 and parameters: {'n_estimators': 839, 'eta': 0.07680650602025195, 'max_depth': 8, 'alpha': 0.45940000000000003, 'lambda': 1.6816239497979752, 'max_bin': 340}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:34:09,433] Trial 227 finished with value: 0.8351310925858957 and parameters: {'n_estimators': 419, 'eta': 0.051779055846310776, 'max_depth': 8, 'alpha': 0.5255000000000001, 'lambda': 2.9465685289922448, 'max_bin': 374}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:34:18,558] Trial 228 finished with value: 0.8367166498969688 and parameters: {'n_estimators': 818, 'eta': 0.06432531309627791, 'max_depth': 11, 'alpha': 0.4485, 'lambda': 6.321610563206978, 'max_bin': 283}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:34:32,000] Trial 229 finished with value: 0.8391451587619502 and parameters: {'n_estimators': 618, 'eta': 0.031934752998055, 'max_depth': 12, 'alpha': 0.5082, 'lambda': 4.167798622877883, 'max_bin': 456}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:34:42,294] Trial 230 finished with value: 0.8400075675189889 and parameters: {'n_estimators': 753, 'eta': 0.06877491732161523, 'max_depth': 12, 'alpha': 0.44420000000000004, 'lambda': 14.805284476256613, 'max_bin': 394}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:35:00,722] Trial 231 finished with value: 0.8371876257390432 and parameters: {'n_estimators': 795, 'eta': 0.04438237360724665, 'max_depth': 12, 'alpha': 0.5776, 'lambda': 38.10657671757136, 'max_bin': 322}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:35:11,931] Trial 232 finished with value: 0.8407246249307031 and parameters: {'n_estimators': 825, 'eta': 0.058321769631129845, 'max_depth': 12, 'alpha': 0.6539, 'lambda': 12.918617515656045, 'max_bin': 381}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:35:24,344] Trial 233 finished with value: 0.83810266765757 and parameters: {'n_estimators': 863, 'eta': 0.05471464955129217, 'max_depth': 12, 'alpha': 0.9999, 'lambda': 10.596456661234882, 'max_bin': 317}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:35:32,388] Trial 234 finished with value: 0.8364086029458742 and parameters: {'n_estimators': 294, 'eta': 0.061443454000756084, 'max_depth': 12, 'alpha': 0.6217, 'lambda': 15.367970294674663, 'max_bin': 328}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:35:45,394] Trial 235 finished with value: 0.841664313374423 and parameters: {'n_estimators': 880, 'eta': 0.048026664130241675, 'max_depth': 11, 'alpha': 0.9784, 'lambda': 9.753272566797113, 'max_bin': 482}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:35:56,022] Trial 236 finished with value: 0.8353564566219308 and parameters: {'n_estimators': 844, 'eta': 0.03341144651600106, 'max_depth': 12, 'alpha': 0.5514, 'lambda': 1.0281040459869617, 'max_bin': 302}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:36:12,258] Trial 237 finished with value: 0.8364358730180271 and parameters: {'n_estimators': 808, 'eta': 0.03788285130092888, 'max_depth': 12, 'alpha': 0.6387, 'lambda': 14.348853579888805, 'max_bin': 291}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:36:23,301] Trial 238 finished with value: 0.83670899936961 and parameters: {'n_estimators': 449, 'eta': 0.05191782640011447, 'max_depth': 11, 'alpha': 0.9613, 'lambda': 12.524261139391488, 'max_bin': 444}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:36:30,188] Trial 239 finished with value: 0.8401102641309548 and parameters: {'n_estimators': 779, 'eta': 0.07364173266075491, 'max_depth': 12, 'alpha': 0.928, 'lambda': 2.2878262838249226, 'max_bin': 347}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:36:49,159] Trial 240 finished with value: 0.8386149990425176 and parameters: {'n_estimators': 864, 'eta': 0.02912911398323756, 'max_depth': 11, 'alpha': 0.5701, 'lambda': 10.812655584644656, 'max_bin': 419}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:36:58,448] Trial 241 finished with value: 0.8375772860981903 and parameters: {'n_estimators': 650, 'eta': 0.07135027260168397, 'max_depth': 12, 'alpha': 0.8043, 'lambda': 7.954668325196708, 'max_bin': 259}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:37:07,199] Trial 242 finished with value: 0.8363930918623229 and parameters: {'n_estimators': 680, 'eta': 0.07563100561700588, 'max_depth': 12, 'alpha': 0.7638, 'lambda': 8.33635768682282, 'max_bin': 263}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:37:16,084] Trial 243 finished with value: 0.8351490576839419 and parameters: {'n_estimators': 645, 'eta': 0.07790602045370636, 'max_depth': 7, 'alpha': 0.7831, 'lambda': 7.139595683201584, 'max_bin': 276}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:37:27,502] Trial 244 finished with value: 0.835147665868649 and parameters: {'n_estimators': 826, 'eta': 0.07057114005288796, 'max_depth': 9, 'alpha': 0.738, 'lambda': 16.423296933933138, 'max_bin': 451}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:37:36,496] Trial 245 finished with value: 0.8370785932153011 and parameters: {'n_estimators': 668, 'eta': 0.07372299057188914, 'max_depth': 6, 'alpha': 0.5979, 'lambda': 7.382176149705672, 'max_bin': 397}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:37:48,328] Trial 246 finished with value: 0.8356555829468999 and parameters: {'n_estimators': 890, 'eta': 0.06783330586825699, 'max_depth': 7, 'alpha': 0.8417, 'lambda': 11.614482819128565, 'max_bin': 408}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:37:58,248] Trial 247 finished with value: 0.8326107279721755 and parameters: {'n_estimators': 900, 'eta': 0.07806772810970224, 'max_depth': 6, 'alpha': 0.9791000000000001, 'lambda': 9.065734891861757, 'max_bin': 430}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:38:09,524] Trial 248 finished with value: 0.8257755007771731 and parameters: {'n_estimators': 628, 'eta': 0.02390498449550594, 'max_depth': 7, 'alpha': 0.9451, 'lambda': 6.297093346485604, 'max_bin': 271}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:38:22,866] Trial 249 finished with value: 0.8364704249929791 and parameters: {'n_estimators': 718, 'eta': 0.05640416417446678, 'max_depth': 10, 'alpha': 0.9083, 'lambda': 13.915431338075429, 'max_bin': 295}. Best is trial 11 with value: 0.8513589595154614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 785\n",
      "\t\teta: 0.07389767489222081\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9650000000000001\n",
      "\t\tlambda: 2.5359194636827933\n",
      "\t\tmax_bin: 259\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  424.000000  421.000000  422.000000  449.000000   \n",
      "1                    TN  339.000000  364.000000  338.000000  322.000000   \n",
      "2                    FP   88.000000   71.000000   84.000000   82.000000   \n",
      "3                    FN   68.000000   63.000000   75.000000   66.000000   \n",
      "4              Accuracy    0.830250    0.854189    0.826986    0.838955   \n",
      "5             Precision    0.828125    0.855691    0.833992    0.845574   \n",
      "6           Sensitivity    0.861789    0.869835    0.849095    0.871845   \n",
      "7           Specificity    0.793900    0.836800    0.800900    0.797000   \n",
      "8              F1 score    0.844622    0.862705    0.841476    0.858509   \n",
      "9   F1 score (weighted)    0.829906    0.854110    0.826830    0.838560   \n",
      "10     F1 score (macro)    0.828786    0.853626    0.825528    0.835820   \n",
      "11    Balanced Accuracy    0.827850    0.853308    0.825021    0.834437   \n",
      "12                  MCC    0.658369    0.707383    0.651217    0.672165   \n",
      "13                  NPV    0.832900    0.852500    0.818400    0.829900   \n",
      "14              ROC_AUC    0.827850    0.853308    0.825021    0.834437   \n",
      "\n",
      "          Set4  \n",
      "0   404.000000  \n",
      "1   351.000000  \n",
      "2    79.000000  \n",
      "3    85.000000  \n",
      "4     0.821545  \n",
      "5     0.836439  \n",
      "6     0.826176  \n",
      "7     0.816300  \n",
      "8     0.831276  \n",
      "9     0.821613  \n",
      "10    0.820950  \n",
      "11    0.821227  \n",
      "12    0.641970  \n",
      "13    0.805000  \n",
      "14    0.821227  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_xgb_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_xgb_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_xgb_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_xgb_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_xgb_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_xgb_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_xgb_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_xgb_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_xgb_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_xgb_4)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 10:38:39,209] Trial 250 finished with value: 0.8355317329762318 and parameters: {'n_estimators': 797, 'eta': 0.03547299352086025, 'max_depth': 11, 'alpha': 0.8913000000000001, 'lambda': 10.09980004171462, 'max_bin': 462}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:38:50,342] Trial 251 finished with value: 0.8358433203300478 and parameters: {'n_estimators': 843, 'eta': 0.04206545899910347, 'max_depth': 12, 'alpha': 0.9699000000000001, 'lambda': 3.2630914903690718, 'max_bin': 250}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:39:08,393] Trial 252 finished with value: 0.8322449264398182 and parameters: {'n_estimators': 872, 'eta': 0.033948387053758106, 'max_depth': 10, 'alpha': 0.9233, 'lambda': 12.809002998335355, 'max_bin': 476}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:39:22,809] Trial 253 finished with value: 0.8358853854562736 and parameters: {'n_estimators': 814, 'eta': 0.03084668339004399, 'max_depth': 11, 'alpha': 0.6815, 'lambda': 2.2000273193436346, 'max_bin': 332}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:39:29,728] Trial 254 finished with value: 0.839793755348105 and parameters: {'n_estimators': 852, 'eta': 0.08011195002542873, 'max_depth': 12, 'alpha': 0.42300000000000004, 'lambda': 5.133462411492502, 'max_bin': 384}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:39:43,502] Trial 255 finished with value: 0.8363790929223743 and parameters: {'n_estimators': 693, 'eta': 0.04646991731367098, 'max_depth': 11, 'alpha': 0.9544, 'lambda': 12.304559722415231, 'max_bin': 266}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:39:49,824] Trial 256 finished with value: 0.835325512131351 and parameters: {'n_estimators': 764, 'eta': 0.07356235275041578, 'max_depth': 12, 'alpha': 0.9848, 'lambda': 1.0280901872938981, 'max_bin': 256}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:39:58,542] Trial 257 finished with value: 0.8363567353649366 and parameters: {'n_estimators': 832, 'eta': 0.05279951631184579, 'max_depth': 12, 'alpha': 0.8130000000000001, 'lambda': 2.0069384550574236, 'max_bin': 287}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:40:08,680] Trial 258 finished with value: 0.8352667640322059 and parameters: {'n_estimators': 397, 'eta': 0.03724410950073215, 'max_depth': 11, 'alpha': 0.7755000000000001, 'lambda': 2.9993939379309054, 'max_bin': 455}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:40:22,952] Trial 259 finished with value: 0.8354242955445889 and parameters: {'n_estimators': 787, 'eta': 0.05071333343516493, 'max_depth': 12, 'alpha': 0.9312, 'lambda': 20.281146156021435, 'max_bin': 377}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:40:29,668] Trial 260 finished with value: 0.8258781325990265 and parameters: {'n_estimators': 247, 'eta': 0.026884642161714545, 'max_depth': 11, 'alpha': 0.7548, 'lambda': 3.9109618487409072, 'max_bin': 469}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:40:50,335] Trial 261 finished with value: 0.8088142622819552 and parameters: {'n_estimators': 661, 'eta': 0.0032077030921634775, 'max_depth': 12, 'alpha': 0.9628000000000001, 'lambda': 5.642381757151572, 'max_bin': 487}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:41:05,951] Trial 262 finished with value: 0.8316867891687899 and parameters: {'n_estimators': 612, 'eta': 0.03222547124584008, 'max_depth': 11, 'alpha': 0.8749, 'lambda': 26.04130348729416, 'max_bin': 370}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:41:15,299] Trial 263 finished with value: 0.833315248133626 and parameters: {'n_estimators': 740, 'eta': 0.06578558362285923, 'max_depth': 12, 'alpha': 0.9992000000000001, 'lambda': 8.410127155980055, 'max_bin': 425}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:41:33,256] Trial 264 finished with value: 0.8345260594472744 and parameters: {'n_estimators': 874, 'eta': 0.02080440955834854, 'max_depth': 12, 'alpha': 0.7177, 'lambda': 1.6837748963543469, 'max_bin': 308}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:41:45,880] Trial 265 finished with value: 0.835015351901698 and parameters: {'n_estimators': 856, 'eta': 0.03972877944948614, 'max_depth': 11, 'alpha': 0.9387000000000001, 'lambda': 4.677797561953517, 'max_bin': 460}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:41:55,208] Trial 266 finished with value: 0.8385906890586886 and parameters: {'n_estimators': 816, 'eta': 0.04403234075628963, 'max_depth': 12, 'alpha': 0.5029, 'lambda': 2.6792746913750687, 'max_bin': 494}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:42:03,744] Trial 267 finished with value: 0.8316657115577538 and parameters: {'n_estimators': 377, 'eta': 0.060363804978341984, 'max_depth': 9, 'alpha': 0.8968, 'lambda': 17.552341862644, 'max_bin': 437}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:42:14,527] Trial 268 finished with value: 0.8357820069231809 and parameters: {'n_estimators': 803, 'eta': 0.05545773013192907, 'max_depth': 11, 'alpha': 0.9515, 'lambda': 9.479535890506503, 'max_bin': 482}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:42:26,029] Trial 269 finished with value: 0.8357586917128839 and parameters: {'n_estimators': 549, 'eta': 0.057808873166622005, 'max_depth': 12, 'alpha': 0.6104, 'lambda': 13.751606211793561, 'max_bin': 466}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:42:38,276] Trial 270 finished with value: 0.8297923884947268 and parameters: {'n_estimators': 465, 'eta': 0.02876482902061286, 'max_depth': 11, 'alpha': 0.9697, 'lambda': 14.932268578791328, 'max_bin': 322}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:42:51,886] Trial 271 finished with value: 0.8341625638974337 and parameters: {'n_estimators': 767, 'eta': 0.03500310116507776, 'max_depth': 10, 'alpha': 0.9836, 'lambda': 3.4820653912561284, 'max_bin': 449}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:43:05,761] Trial 272 finished with value: 0.8365574261328984 and parameters: {'n_estimators': 841, 'eta': 0.049171086171420374, 'max_depth': 12, 'alpha': 0.9143, 'lambda': 15.788671985950723, 'max_bin': 388}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:43:24,971] Trial 273 finished with value: 0.8365954735736087 and parameters: {'n_estimators': 887, 'eta': 0.025676719038988504, 'max_depth': 12, 'alpha': 0.9324, 'lambda': 6.771486233030031, 'max_bin': 300}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:43:31,463] Trial 274 finished with value: 0.8355373308587092 and parameters: {'n_estimators': 637, 'eta': 0.08248867027879847, 'max_depth': 11, 'alpha': 0.8312, 'lambda': 2.3152301600385785, 'max_bin': 268}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:43:37,373] Trial 275 finished with value: 0.838008722635637 and parameters: {'n_estimators': 866, 'eta': 0.07540103322013594, 'max_depth': 12, 'alpha': 0.6973, 'lambda': 1.6934804645212438, 'max_bin': 316}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:43:44,819] Trial 276 finished with value: 0.8357782731269703 and parameters: {'n_estimators': 831, 'eta': 0.07157800246513878, 'max_depth': 12, 'alpha': 0.8549, 'lambda': 4.055218747704356, 'max_bin': 500}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:43:56,293] Trial 277 finished with value: 0.8365698168682172 and parameters: {'n_estimators': 780, 'eta': 0.03889167523555186, 'max_depth': 11, 'alpha': 0.5772, 'lambda': 2.9353340332651263, 'max_bin': 402}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:44:08,050] Trial 278 finished with value: 0.8319906236685386 and parameters: {'n_estimators': 591, 'eta': 0.03052786815608743, 'max_depth': 8, 'alpha': 0.7978000000000001, 'lambda': 7.5400660268965245, 'max_bin': 473}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:44:16,913] Trial 279 finished with value: 0.835808631148278 and parameters: {'n_estimators': 898, 'eta': 0.061896144997275956, 'max_depth': 12, 'alpha': 0.662, 'lambda': 5.24003029588671, 'max_bin': 353}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:44:32,626] Trial 280 finished with value: 0.8365758816321239 and parameters: {'n_estimators': 810, 'eta': 0.03361447767196424, 'max_depth': 11, 'alpha': 0.9523, 'lambda': 6.072506971961612, 'max_bin': 374}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:44:49,816] Trial 281 finished with value: 0.8335372634196494 and parameters: {'n_estimators': 850, 'eta': 0.03678511721729275, 'max_depth': 10, 'alpha': 0.9805, 'lambda': 13.392166177817838, 'max_bin': 359}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:44:58,848] Trial 282 finished with value: 0.8298738809869366 and parameters: {'n_estimators': 880, 'eta': 0.053988709179259305, 'max_depth': 6, 'alpha': 0.7442000000000001, 'lambda': 1.3756485071163722, 'max_bin': 441}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:45:10,428] Trial 283 finished with value: 0.834420408709836 and parameters: {'n_estimators': 709, 'eta': 0.04144759945451218, 'max_depth': 7, 'alpha': 0.902, 'lambda': 4.461055891681088, 'max_bin': 457}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:45:31,033] Trial 284 finished with value: 0.8369332735364472 and parameters: {'n_estimators': 793, 'eta': 0.01764356573212992, 'max_depth': 12, 'alpha': 0.9614, 'lambda': 2.4676567971818795, 'max_bin': 342}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:45:39,666] Trial 285 finished with value: 0.8344504948907152 and parameters: {'n_estimators': 338, 'eta': 0.06893711401925848, 'max_depth': 11, 'alpha': 0.9979, 'lambda': 10.942149601458175, 'max_bin': 262}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:45:52,834] Trial 286 finished with value: 0.8387956114997005 and parameters: {'n_estimators': 828, 'eta': 0.06358893997578499, 'max_depth': 12, 'alpha': 0.9233, 'lambda': 21.672734660197648, 'max_bin': 431}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:46:02,493] Trial 287 finished with value: 0.8381921872629716 and parameters: {'n_estimators': 750, 'eta': 0.06638950420498166, 'max_depth': 12, 'alpha': 0.6326, 'lambda': 11.818708013620922, 'max_bin': 490}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:46:11,855] Trial 288 finished with value: 0.8374423339470735 and parameters: {'n_estimators': 674, 'eta': 0.04568060500898241, 'max_depth': 11, 'alpha': 0.8828, 'lambda': 1.0140135975325426, 'max_bin': 278}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:46:20,405] Trial 289 finished with value: 0.8365944458839805 and parameters: {'n_estimators': 855, 'eta': 0.07734682665630892, 'max_depth': 12, 'alpha': 0.9438000000000001, 'lambda': 8.567992738903197, 'max_bin': 485}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:46:28,819] Trial 290 finished with value: 0.8358276586677811 and parameters: {'n_estimators': 654, 'eta': 0.05980779971061918, 'max_depth': 11, 'alpha': 0.9690000000000001, 'lambda': 3.4682658850076655, 'max_bin': 381}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:46:51,043] Trial 291 finished with value: 0.8330719029430258 and parameters: {'n_estimators': 884, 'eta': 0.022736856999259373, 'max_depth': 11, 'alpha': 0.9785, 'lambda': 16.835933322850053, 'max_bin': 366}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:47:10,297] Trial 292 finished with value: 0.8340912349299832 and parameters: {'n_estimators': 727, 'eta': 0.032582475218913644, 'max_depth': 12, 'alpha': 0.7303000000000001, 'lambda': 30.23181206150059, 'max_bin': 446}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:47:24,082] Trial 293 finished with value: 0.8360239833607379 and parameters: {'n_estimators': 867, 'eta': 0.05193898628224671, 'max_depth': 12, 'alpha': 0.766, 'lambda': 19.300826366230233, 'max_bin': 465}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:47:30,444] Trial 294 finished with value: 0.8410546880551731 and parameters: {'n_estimators': 808, 'eta': 0.07329082790562931, 'max_depth': 12, 'alpha': 0.5518000000000001, 'lambda': 2.1457222022764473, 'max_bin': 479}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:47:44,424] Trial 295 finished with value: 0.8382488965501539 and parameters: {'n_estimators': 785, 'eta': 0.047970494810354, 'max_depth': 11, 'alpha': 0.9255, 'lambda': 14.681728318554002, 'max_bin': 415}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:47:54,550] Trial 296 finished with value: 0.831689502125125 and parameters: {'n_estimators': 488, 'eta': 0.03607098903071589, 'max_depth': 9, 'alpha': 0.3859, 'lambda': 3.408036900666429, 'max_bin': 454}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:48:06,083] Trial 297 finished with value: 0.8371955174993367 and parameters: {'n_estimators': 421, 'eta': 0.040929776948813015, 'max_depth': 12, 'alpha': 0.9052, 'lambda': 6.695193347684391, 'max_bin': 461}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:48:23,119] Trial 298 finished with value: 0.8380122614189067 and parameters: {'n_estimators': 840, 'eta': 0.02909471962053471, 'max_depth': 11, 'alpha': 0.9495, 'lambda': 4.6815826640172835, 'max_bin': 425}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:48:36,738] Trial 299 finished with value: 0.8371851662257097 and parameters: {'n_estimators': 626, 'eta': 0.0502283338283911, 'max_depth': 10, 'alpha': 0.9871000000000001, 'lambda': 18.186465074497264, 'max_bin': 385}. Best is trial 11 with value: 0.8513589595154614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 785\n",
      "\t\teta: 0.07389767489222081\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9650000000000001\n",
      "\t\tlambda: 2.5359194636827933\n",
      "\t\tmax_bin: 259\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  424.000000  421.000000  422.000000  449.000000   \n",
      "1                    TN  339.000000  364.000000  338.000000  322.000000   \n",
      "2                    FP   88.000000   71.000000   84.000000   82.000000   \n",
      "3                    FN   68.000000   63.000000   75.000000   66.000000   \n",
      "4              Accuracy    0.830250    0.854189    0.826986    0.838955   \n",
      "5             Precision    0.828125    0.855691    0.833992    0.845574   \n",
      "6           Sensitivity    0.861789    0.869835    0.849095    0.871845   \n",
      "7           Specificity    0.793900    0.836800    0.800900    0.797000   \n",
      "8              F1 score    0.844622    0.862705    0.841476    0.858509   \n",
      "9   F1 score (weighted)    0.829906    0.854110    0.826830    0.838560   \n",
      "10     F1 score (macro)    0.828786    0.853626    0.825528    0.835820   \n",
      "11    Balanced Accuracy    0.827850    0.853308    0.825021    0.834437   \n",
      "12                  MCC    0.658369    0.707383    0.651217    0.672165   \n",
      "13                  NPV    0.832900    0.852500    0.818400    0.829900   \n",
      "14              ROC_AUC    0.827850    0.853308    0.825021    0.834437   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   404.000000  416.000000  \n",
      "1   351.000000  363.000000  \n",
      "2    79.000000   72.000000  \n",
      "3    85.000000   68.000000  \n",
      "4     0.821545    0.847661  \n",
      "5     0.836439    0.852459  \n",
      "6     0.826176    0.859504  \n",
      "7     0.816300    0.834500  \n",
      "8     0.831276    0.855967  \n",
      "9     0.821613    0.847622  \n",
      "10    0.820950    0.847152  \n",
      "11    0.821227    0.846993  \n",
      "12    0.641970    0.694337  \n",
      "13    0.805000    0.842200  \n",
      "14    0.821227    0.846993  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_xgb_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_xgb_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_xgb_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_xgb_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_xgb_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_xgb_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_xgb_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_xgb_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_xgb_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_xgb_5)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 10:48:51,716] Trial 300 finished with value: 0.8423659016959162 and parameters: {'n_estimators': 691, 'eta': 0.04292888738948939, 'max_depth': 12, 'alpha': 0.9993000000000001, 'lambda': 9.266698971512374, 'max_bin': 436}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:48:59,464] Trial 301 finished with value: 0.8391489715592055 and parameters: {'n_estimators': 899, 'eta': 0.07030281878917594, 'max_depth': 12, 'alpha': 0.17, 'lambda': 5.503376650555056, 'max_bin': 255}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:49:15,816] Trial 302 finished with value: 0.8417973354721793 and parameters: {'n_estimators': 822, 'eta': 0.03831758947337114, 'max_depth': 11, 'alpha': 0.3593, 'lambda': 15.51602568607129, 'max_bin': 272}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:49:28,713] Trial 303 finished with value: 0.8432726780294552 and parameters: {'n_estimators': 764, 'eta': 0.03154832904811084, 'max_depth': 11, 'alpha': 0.8176, 'lambda': 2.6876294750012875, 'max_bin': 474}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:49:36,880] Trial 304 finished with value: 0.843539677443818 and parameters: {'n_estimators': 860, 'eta': 0.05663381841730047, 'max_depth': 8, 'alpha': 0.8677, 'lambda': 1.6118159201746909, 'max_bin': 259}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:49:51,329] Trial 305 finished with value: 0.8394271873474881 and parameters: {'n_estimators': 805, 'eta': 0.034534694910138386, 'max_depth': 7, 'alpha': 0.784, 'lambda': 16.33541188210766, 'max_bin': 351}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:50:00,532] Trial 306 finished with value: 0.8437593092121084 and parameters: {'n_estimators': 877, 'eta': 0.054010675841154286, 'max_depth': 12, 'alpha': 0.9602, 'lambda': 4.00474868514684, 'max_bin': 379}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:50:09,549] Trial 307 finished with value: 0.8465276416957679 and parameters: {'n_estimators': 867, 'eta': 0.05385658737700868, 'max_depth': 12, 'alpha': 0.9434, 'lambda': 4.0192549721248385, 'max_bin': 376}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:50:19,293] Trial 308 finished with value: 0.8462550566120763 and parameters: {'n_estimators': 877, 'eta': 0.053256485872393955, 'max_depth': 12, 'alpha': 0.9347000000000001, 'lambda': 4.237270013978124, 'max_bin': 377}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:50:28,846] Trial 309 finished with value: 0.8404727597879805 and parameters: {'n_estimators': 881, 'eta': 0.05241401965205399, 'max_depth': 12, 'alpha': 0.9254, 'lambda': 4.666416365442295, 'max_bin': 383}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:50:37,496] Trial 310 finished with value: 0.8415376685637345 and parameters: {'n_estimators': 871, 'eta': 0.05784016071872004, 'max_depth': 12, 'alpha': 0.9391, 'lambda': 3.6826083968231216, 'max_bin': 375}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:50:46,981] Trial 311 finished with value: 0.8418670974552642 and parameters: {'n_estimators': 853, 'eta': 0.04953764162421467, 'max_depth': 12, 'alpha': 0.9155000000000001, 'lambda': 3.0285643147620434, 'max_bin': 373}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:50:55,881] Trial 312 finished with value: 0.8423649743100501 and parameters: {'n_estimators': 362, 'eta': 0.05476656729087345, 'max_depth': 12, 'alpha': 0.8951, 'lambda': 5.795530283891509, 'max_bin': 391}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:51:13,527] Trial 313 finished with value: 0.8456642510664631 and parameters: {'n_estimators': 900, 'eta': 0.025183398604217925, 'max_depth': 12, 'alpha': 0.9509000000000001, 'lambda': 3.902542852362126, 'max_bin': 369}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:51:30,488] Trial 314 finished with value: 0.842369297103937 and parameters: {'n_estimators': 880, 'eta': 0.02717815107310756, 'max_depth': 12, 'alpha': 0.9440000000000001, 'lambda': 4.282251122948947, 'max_bin': 389}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:51:51,517] Trial 315 finished with value: 0.8432017739873763 and parameters: {'n_estimators': 892, 'eta': 0.021798220754166618, 'max_depth': 12, 'alpha': 0.9680000000000001, 'lambda': 5.076259463000926, 'max_bin': 369}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:52:07,719] Trial 316 finished with value: 0.8421155082259888 and parameters: {'n_estimators': 900, 'eta': 0.026428947915787246, 'max_depth': 12, 'alpha': 0.9425, 'lambda': 2.6287386078698383, 'max_bin': 377}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:52:22,082] Trial 317 finished with value: 0.8446382686782151 and parameters: {'n_estimators': 866, 'eta': 0.029790981346325594, 'max_depth': 12, 'alpha': 0.9115000000000001, 'lambda': 3.7689101276516808, 'max_bin': 365}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:52:39,736] Trial 318 finished with value: 0.8410592645755697 and parameters: {'n_estimators': 868, 'eta': 0.024131215524428665, 'max_depth': 12, 'alpha': 0.9599000000000001, 'lambda': 3.8042799665053777, 'max_bin': 365}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:52:54,837] Trial 319 finished with value: 0.8410621583378495 and parameters: {'n_estimators': 861, 'eta': 0.029715568078271224, 'max_depth': 12, 'alpha': 0.927, 'lambda': 3.139960050489599, 'max_bin': 370}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:53:09,155] Trial 320 finished with value: 0.843015116107565 and parameters: {'n_estimators': 844, 'eta': 0.02823664771207584, 'max_depth': 12, 'alpha': 0.9111, 'lambda': 1.8804529807013597, 'max_bin': 360}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:53:28,070] Trial 321 finished with value: 0.8426742479446997 and parameters: {'n_estimators': 883, 'eta': 0.02414346964599, 'max_depth': 12, 'alpha': 0.9776, 'lambda': 4.776204176591576, 'max_bin': 371}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:53:42,065] Trial 322 finished with value: 0.8429904131082948 and parameters: {'n_estimators': 900, 'eta': 0.03195644715835559, 'max_depth': 12, 'alpha': 0.9485, 'lambda': 3.823595496886865, 'max_bin': 364}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:54:01,018] Trial 323 finished with value: 0.8413185682896394 and parameters: {'n_estimators': 864, 'eta': 0.01987796318462026, 'max_depth': 11, 'alpha': 0.8854000000000001, 'lambda': 2.0724791209218605, 'max_bin': 375}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:54:18,325] Trial 324 finished with value: 0.8437756410862309 and parameters: {'n_estimators': 845, 'eta': 0.025096163880237363, 'max_depth': 11, 'alpha': 0.9753000000000001, 'lambda': 2.9513415919322776, 'max_bin': 379}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:54:33,801] Trial 325 finished with value: 0.8437338200473634 and parameters: {'n_estimators': 887, 'eta': 0.030050869518803487, 'max_depth': 12, 'alpha': 0.9286000000000001, 'lambda': 4.352109646421912, 'max_bin': 366}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:54:45,949] Trial 326 finished with value: 0.8394034233231664 and parameters: {'n_estimators': 833, 'eta': 0.03415631522920457, 'max_depth': 12, 'alpha': 0.9553, 'lambda': 1.3936620892217686, 'max_bin': 370}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:54:57,021] Trial 327 finished with value: 0.8454469599433383 and parameters: {'n_estimators': 871, 'eta': 0.03634819822633451, 'max_depth': 11, 'alpha': 0.9093, 'lambda': 1.0160978887117624, 'max_bin': 385}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:55:07,527] Trial 328 finished with value: 0.8424032192258245 and parameters: {'n_estimators': 874, 'eta': 0.03754295956460659, 'max_depth': 12, 'alpha': 0.9062, 'lambda': 1.1871804516093847, 'max_bin': 386}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:55:19,890] Trial 329 finished with value: 0.8443186608363231 and parameters: {'n_estimators': 862, 'eta': 0.03626883894607959, 'max_depth': 11, 'alpha': 0.9313, 'lambda': 2.2159783845451266, 'max_bin': 382}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:55:31,850] Trial 330 finished with value: 0.8432364382107632 and parameters: {'n_estimators': 852, 'eta': 0.035638173049078695, 'max_depth': 11, 'alpha': 0.9359000000000001, 'lambda': 1.940472846759945, 'max_bin': 394}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:55:38,087] Trial 331 finished with value: 0.8435682249120555 and parameters: {'n_estimators': 866, 'eta': 0.08285037948087585, 'max_depth': 12, 'alpha': 0.9666, 'lambda': 2.301148577006111, 'max_bin': 380}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:55:47,970] Trial 332 finished with value: 0.8421425616911928 and parameters: {'n_estimators': 887, 'eta': 0.03909748920920433, 'max_depth': 11, 'alpha': 0.8872, 'lambda': 1.1061610779659399, 'max_bin': 383}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:56:01,248] Trial 333 finished with value: 0.8413277480844601 and parameters: {'n_estimators': 851, 'eta': 0.03244706705733166, 'max_depth': 12, 'alpha': 0.9134, 'lambda': 2.6271790957772208, 'max_bin': 399}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:56:13,649] Trial 334 finished with value: 0.8421082029317845 and parameters: {'n_estimators': 831, 'eta': 0.03642801826974405, 'max_depth': 12, 'alpha': 0.9830000000000001, 'lambda': 3.3388772706995624, 'max_bin': 389}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:56:19,487] Trial 335 finished with value: 0.8423537381738375 and parameters: {'n_estimators': 877, 'eta': 0.09237462636517475, 'max_depth': 10, 'alpha': 0.9434, 'lambda': 1.7104686051939866, 'max_bin': 385}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:56:31,782] Trial 336 finished with value: 0.8437208676681095 and parameters: {'n_estimators': 900, 'eta': 0.034099678414306805, 'max_depth': 11, 'alpha': 0.9999, 'lambda': 2.4235403416859214, 'max_bin': 406}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:56:41,471] Trial 337 finished with value: 0.83993673089496 and parameters: {'n_estimators': 862, 'eta': 0.0400138571248444, 'max_depth': 12, 'alpha': 0.9626, 'lambda': 1.110281972966759, 'max_bin': 375}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:56:53,096] Trial 338 finished with value: 0.8435228718759845 and parameters: {'n_estimators': 842, 'eta': 0.037061233196330666, 'max_depth': 11, 'alpha': 0.9277000000000001, 'lambda': 3.2384095122644565, 'max_bin': 421}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:56:59,175] Trial 339 finished with value: 0.8429360773225595 and parameters: {'n_estimators': 879, 'eta': 0.08636134204139197, 'max_depth': 12, 'alpha': 0.9028, 'lambda': 2.1642251327882436, 'max_bin': 411}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:57:15,219] Trial 340 finished with value: 0.8423976924748751 and parameters: {'n_estimators': 824, 'eta': 0.02744611823750007, 'max_depth': 11, 'alpha': 0.8647, 'lambda': 5.359725126435079, 'max_bin': 378}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:57:28,479] Trial 341 finished with value: 0.8442958000767291 and parameters: {'n_estimators': 865, 'eta': 0.03359299898334974, 'max_depth': 12, 'alpha': 0.9524, 'lambda': 3.9002214937729507, 'max_bin': 381}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:57:43,128] Trial 342 finished with value: 0.8421509765153647 and parameters: {'n_estimators': 856, 'eta': 0.03136323991492674, 'max_depth': 12, 'alpha': 0.9382, 'lambda': 4.068529207563868, 'max_bin': 394}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:57:48,322] Trial 343 finished with value: 0.8326138672578599 and parameters: {'n_estimators': 171, 'eta': 0.03407864087226034, 'max_depth': 12, 'alpha': 0.9527, 'lambda': 4.834787285773038, 'max_bin': 382}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:58:03,588] Trial 344 finished with value: 0.8426087431573812 and parameters: {'n_estimators': 838, 'eta': 0.032827754040965385, 'max_depth': 11, 'alpha': 0.9255, 'lambda': 5.800714367000321, 'max_bin': 389}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:58:18,224] Trial 345 finished with value: 0.8424311942394273 and parameters: {'n_estimators': 819, 'eta': 0.030011809192438274, 'max_depth': 12, 'alpha': 0.8851, 'lambda': 3.5146535361835944, 'max_bin': 383}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:58:34,384] Trial 346 finished with value: 0.8424390404296187 and parameters: {'n_estimators': 863, 'eta': 0.0280907601631119, 'max_depth': 10, 'alpha': 0.9138000000000001, 'lambda': 4.202963584081736, 'max_bin': 484}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:58:45,425] Trial 347 finished with value: 0.8443026897305664 and parameters: {'n_estimators': 837, 'eta': 0.04209097453056089, 'max_depth': 11, 'alpha': 0.9626, 'lambda': 2.791819763933772, 'max_bin': 379}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:58:57,164] Trial 348 finished with value: 0.841859008781106 and parameters: {'n_estimators': 835, 'eta': 0.04069159695096103, 'max_depth': 11, 'alpha': 0.9669000000000001, 'lambda': 2.955008187575694, 'max_bin': 378}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:59:09,742] Trial 349 finished with value: 0.8440500122982127 and parameters: {'n_estimators': 801, 'eta': 0.0413741734998207, 'max_depth': 11, 'alpha': 0.9485, 'lambda': 4.951230967189913, 'max_bin': 375}. Best is trial 11 with value: 0.8513589595154614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 785\n",
      "\t\teta: 0.07389767489222081\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9650000000000001\n",
      "\t\tlambda: 2.5359194636827933\n",
      "\t\tmax_bin: 259\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  424.000000  421.000000  422.000000  449.000000   \n",
      "1                    TN  339.000000  364.000000  338.000000  322.000000   \n",
      "2                    FP   88.000000   71.000000   84.000000   82.000000   \n",
      "3                    FN   68.000000   63.000000   75.000000   66.000000   \n",
      "4              Accuracy    0.830250    0.854189    0.826986    0.838955   \n",
      "5             Precision    0.828125    0.855691    0.833992    0.845574   \n",
      "6           Sensitivity    0.861789    0.869835    0.849095    0.871845   \n",
      "7           Specificity    0.793900    0.836800    0.800900    0.797000   \n",
      "8              F1 score    0.844622    0.862705    0.841476    0.858509   \n",
      "9   F1 score (weighted)    0.829906    0.854110    0.826830    0.838560   \n",
      "10     F1 score (macro)    0.828786    0.853626    0.825528    0.835820   \n",
      "11    Balanced Accuracy    0.827850    0.853308    0.825021    0.834437   \n",
      "12                  MCC    0.658369    0.707383    0.651217    0.672165   \n",
      "13                  NPV    0.832900    0.852500    0.818400    0.829900   \n",
      "14              ROC_AUC    0.827850    0.853308    0.825021    0.834437   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   404.000000  416.000000  449.000000  \n",
      "1   351.000000  363.000000  326.000000  \n",
      "2    79.000000   72.000000   78.000000  \n",
      "3    85.000000   68.000000   66.000000  \n",
      "4     0.821545    0.847661    0.843308  \n",
      "5     0.836439    0.852459    0.851992  \n",
      "6     0.826176    0.859504    0.871845  \n",
      "7     0.816300    0.834500    0.806900  \n",
      "8     0.831276    0.855967    0.861804  \n",
      "9     0.821613    0.847622    0.843029  \n",
      "10    0.820950    0.847152    0.840450  \n",
      "11    0.821227    0.846993    0.839388  \n",
      "12    0.641970    0.694337    0.681196  \n",
      "13    0.805000    0.842200    0.831600  \n",
      "14    0.821227    0.846993    0.839388  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_xgb_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_xgb_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_xgb_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_xgb_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_xgb_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_xgb_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_xgb_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_xgb_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_xgb_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_xgb_6)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 10:59:23,778] Trial 350 finished with value: 0.8391072542310349 and parameters: {'n_estimators': 850, 'eta': 0.038524685298318324, 'max_depth': 11, 'alpha': 0.9786, 'lambda': 4.0883796318247505, 'max_bin': 386}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:59:34,485] Trial 351 finished with value: 0.8386108869838182 and parameters: {'n_estimators': 822, 'eta': 0.046096463100990914, 'max_depth': 11, 'alpha': 0.9368000000000001, 'lambda': 3.261653928584838, 'max_bin': 396}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 10:59:50,250] Trial 352 finished with value: 0.8374759073540158 and parameters: {'n_estimators': 872, 'eta': 0.03548907755539253, 'max_depth': 11, 'alpha': 0.9593, 'lambda': 6.165502909713305, 'max_bin': 380}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:00:01,806] Trial 353 finished with value: 0.8421152753911224 and parameters: {'n_estimators': 843, 'eta': 0.04216155860746145, 'max_depth': 12, 'alpha': 0.919, 'lambda': 2.4969447284233977, 'max_bin': 470}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:00:08,175] Trial 354 finished with value: 0.8395887547623181 and parameters: {'n_estimators': 810, 'eta': 0.0998449540871441, 'max_depth': 12, 'alpha': 0.9852000000000001, 'lambda': 5.342956671614889, 'max_bin': 371}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:00:30,424] Trial 355 finished with value: 0.8368967802111449 and parameters: {'n_estimators': 862, 'eta': 0.01551558229994228, 'max_depth': 11, 'alpha': 0.9429000000000001, 'lambda': 3.6140243991507406, 'max_bin': 387}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:00:42,784] Trial 356 finished with value: 0.839937044688121 and parameters: {'n_estimators': 884, 'eta': 0.044038498529822764, 'max_depth': 12, 'alpha': 0.9677, 'lambda': 4.526914132238792, 'max_bin': 433}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:01:00,066] Trial 357 finished with value: 0.8372028251479666 and parameters: {'n_estimators': 851, 'eta': 0.043698454728521796, 'max_depth': 11, 'alpha': 0.9039, 'lambda': 22.95717670073624, 'max_bin': 490}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:01:18,448] Trial 358 finished with value: 0.8404109249810607 and parameters: {'n_estimators': 797, 'eta': 0.021923175997341164, 'max_depth': 12, 'alpha': 0.9255, 'lambda': 2.765092039416562, 'max_bin': 374}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:01:30,666] Trial 359 finished with value: 0.8391141426952672 and parameters: {'n_estimators': 836, 'eta': 0.03748255186965158, 'max_depth': 11, 'alpha': 0.9537, 'lambda': 1.9053274029037899, 'max_bin': 377}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:01:45,161] Trial 360 finished with value: 0.8396569914364334 and parameters: {'n_estimators': 821, 'eta': 0.03289228061674274, 'max_depth': 12, 'alpha': 0.9818, 'lambda': 3.8499300178310567, 'max_bin': 428}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:02:00,939] Trial 361 finished with value: 0.8369497331809767 and parameters: {'n_estimators': 899, 'eta': 0.035146757154059384, 'max_depth': 12, 'alpha': 0.9368000000000001, 'lambda': 6.559441569989936, 'max_bin': 380}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:02:07,970] Trial 362 finished with value: 0.837785322819563 and parameters: {'n_estimators': 872, 'eta': 0.07950458641697339, 'max_depth': 12, 'alpha': 0.9991000000000001, 'lambda': 2.914699095873846, 'max_bin': 392}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:02:28,862] Trial 363 finished with value: 0.8345182074532842 and parameters: {'n_estimators': 860, 'eta': 0.03123111220683279, 'max_depth': 11, 'alpha': 0.8996000000000001, 'lambda': 25.495197780659236, 'max_bin': 478}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:02:40,271] Trial 364 finished with value: 0.8404409963136281 and parameters: {'n_estimators': 772, 'eta': 0.0396563884629441, 'max_depth': 12, 'alpha': 0.9636, 'lambda': 1.762706121289488, 'max_bin': 383}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:02:47,728] Trial 365 finished with value: 0.8396707899249993 and parameters: {'n_estimators': 886, 'eta': 0.08849760328845663, 'max_depth': 11, 'alpha': 0.8472000000000001, 'lambda': 5.036754970043198, 'max_bin': 495}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:02:58,814] Trial 366 finished with value: 0.8408064790849282 and parameters: {'n_estimators': 785, 'eta': 0.04719128823941538, 'max_depth': 12, 'alpha': 0.9232, 'lambda': 3.53852443609303, 'max_bin': 371}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:03:18,644] Trial 367 finished with value: 0.8401974971662678 and parameters: {'n_estimators': 825, 'eta': 0.019996049406132612, 'max_depth': 12, 'alpha': 0.9531000000000001, 'lambda': 2.544266266647887, 'max_bin': 400}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:03:38,538] Trial 368 finished with value: 0.8352596599596623 and parameters: {'n_estimators': 847, 'eta': 0.03619139043046539, 'max_depth': 11, 'alpha': 0.9838, 'lambda': 20.83118055561521, 'max_bin': 443}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:03:52,956] Trial 369 finished with value: 0.8391658843604564 and parameters: {'n_estimators': 878, 'eta': 0.03306111872600052, 'max_depth': 11, 'alpha': 0.8751, 'lambda': 4.113805965920091, 'max_bin': 471}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:04:12,221] Trial 370 finished with value: 0.8370841832678584 and parameters: {'n_estimators': 808, 'eta': 0.024918026063657606, 'max_depth': 12, 'alpha': 0.9716, 'lambda': 5.689162686758572, 'max_bin': 419}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:04:18,904] Trial 371 finished with value: 0.8366666453344866 and parameters: {'n_estimators': 861, 'eta': 0.0757612398019323, 'max_depth': 12, 'alpha': 0.9396, 'lambda': 1.8198293076987273, 'max_bin': 389}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:04:30,378] Trial 372 finished with value: 0.8361193281211262 and parameters: {'n_estimators': 836, 'eta': 0.04196352945426895, 'max_depth': 11, 'alpha': 0.9101, 'lambda': 2.9508330540849466, 'max_bin': 383}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:04:40,721] Trial 373 finished with value: 0.8430103515822264 and parameters: {'n_estimators': 874, 'eta': 0.04512900873190176, 'max_depth': 12, 'alpha': 0.9999, 'lambda': 1.045546185963466, 'max_bin': 480}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:04:54,340] Trial 374 finished with value: 0.8416185436919141 and parameters: {'n_estimators': 798, 'eta': 0.06293087108885015, 'max_depth': 12, 'alpha': 0.9495, 'lambda': 24.737761179860833, 'max_bin': 466}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:05:09,877] Trial 375 finished with value: 0.8375059714243506 and parameters: {'n_estimators': 900, 'eta': 0.030874158726523854, 'max_depth': 11, 'alpha': 0.9662000000000001, 'lambda': 4.56249317071711, 'max_bin': 367}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:05:22,185] Trial 376 finished with value: 0.8396119728862572 and parameters: {'n_estimators': 855, 'eta': 0.0384684190862581, 'max_depth': 12, 'alpha': 0.9249, 'lambda': 2.16861614824865, 'max_bin': 486}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:05:43,268] Trial 377 finished with value: 0.8350188120437358 and parameters: {'n_estimators': 822, 'eta': 0.034563999621923365, 'max_depth': 12, 'alpha': 0.8923000000000001, 'lambda': 32.14452258497141, 'max_bin': 375}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:06:01,009] Trial 378 finished with value: 0.8385690434554223 and parameters: {'n_estimators': 882, 'eta': 0.026150407538843262, 'max_depth': 11, 'alpha': 0.9804, 'lambda': 3.5212413767497264, 'max_bin': 460}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:06:09,332] Trial 379 finished with value: 0.8366765868466282 and parameters: {'n_estimators': 781, 'eta': 0.07836130282469439, 'max_depth': 12, 'alpha': 0.9399000000000001, 'lambda': 6.647427982242725, 'max_bin': 379}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:06:31,187] Trial 380 finished with value: 0.8336618891623109 and parameters: {'n_estimators': 839, 'eta': 0.013346210067955784, 'max_depth': 11, 'alpha': 0.9595, 'lambda': 4.6517742580613035, 'max_bin': 452}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:06:45,476] Trial 381 finished with value: 0.8404488048477182 and parameters: {'n_estimators': 857, 'eta': 0.028515053600797327, 'max_depth': 12, 'alpha': 0.9119, 'lambda': 1.049628049951571, 'max_bin': 424}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:07:05,265] Trial 382 finished with value: 0.8361177875203107 and parameters: {'n_estimators': 873, 'eta': 0.036216041016913884, 'max_depth': 12, 'alpha': 0.9333, 'lambda': 18.925815038244558, 'max_bin': 438}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:07:15,346] Trial 383 finished with value: 0.8419085238684112 and parameters: {'n_estimators': 887, 'eta': 0.050859482884067476, 'max_depth': 11, 'alpha': 0.9802000000000001, 'lambda': 2.7938233438725373, 'max_bin': 476}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:07:27,647] Trial 384 finished with value: 0.8413305336692408 and parameters: {'n_estimators': 807, 'eta': 0.04303310568792678, 'max_depth': 12, 'alpha': 0.9527, 'lambda': 5.354420374279765, 'max_bin': 385}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:07:38,178] Trial 385 finished with value: 0.8402487515216734 and parameters: {'n_estimators': 753, 'eta': 0.06746309098768141, 'max_depth': 11, 'alpha': 0.9009, 'lambda': 10.146099194644767, 'max_bin': 405}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:07:46,598] Trial 386 finished with value: 0.8446207282526235 and parameters: {'n_estimators': 835, 'eta': 0.08323650753457254, 'max_depth': 12, 'alpha': 0.9272, 'lambda': 7.504650892987101, 'max_bin': 392}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:07:54,703] Trial 387 finished with value: 0.8386064777244696 and parameters: {'n_estimators': 821, 'eta': 0.08247564831285191, 'max_depth': 12, 'alpha': 0.8829, 'lambda': 7.060200718240575, 'max_bin': 396}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:08:02,478] Trial 388 finished with value: 0.8374615385143059 and parameters: {'n_estimators': 836, 'eta': 0.08502784595670873, 'max_depth': 11, 'alpha': 0.9188000000000001, 'lambda': 7.442614172220131, 'max_bin': 433}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:08:10,841] Trial 389 finished with value: 0.8407454337948339 and parameters: {'n_estimators': 776, 'eta': 0.08426168495031319, 'max_depth': 11, 'alpha': 0.8557, 'lambda': 6.232756879180442, 'max_bin': 389}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:08:17,074] Trial 390 finished with value: 0.8356047816449454 and parameters: {'n_estimators': 799, 'eta': 0.09434034102368517, 'max_depth': 12, 'alpha': 0.9255, 'lambda': 1.8918023644382274, 'max_bin': 397}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:08:25,629] Trial 391 finished with value: 0.8429240836185908 and parameters: {'n_estimators': 832, 'eta': 0.0730161108438388, 'max_depth': 12, 'alpha': 0.8754000000000001, 'lambda': 8.092561910990812, 'max_bin': 393}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:08:37,422] Trial 392 finished with value: 0.8380998006789889 and parameters: {'n_estimators': 849, 'eta': 0.08021033379439471, 'max_depth': 11, 'alpha': 0.8961, 'lambda': 28.34160750119935, 'max_bin': 389}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:08:43,851] Trial 393 finished with value: 0.8391309885292406 and parameters: {'n_estimators': 813, 'eta': 0.07743460818473996, 'max_depth': 12, 'alpha': 0.9681000000000001, 'lambda': 1.003273451008948, 'max_bin': 412}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:08:54,209] Trial 394 finished with value: 0.8396666667114084 and parameters: {'n_estimators': 792, 'eta': 0.05885206961777521, 'max_depth': 12, 'alpha': 0.9430000000000001, 'lambda': 5.964776963081058, 'max_bin': 467}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:09:04,477] Trial 395 finished with value: 0.8386863359638488 and parameters: {'n_estimators': 852, 'eta': 0.08156834913615893, 'max_depth': 12, 'alpha': 0.9873000000000001, 'lambda': 21.70520491682841, 'max_bin': 492}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:09:14,416] Trial 396 finished with value: 0.8366934676271478 and parameters: {'n_estimators': 900, 'eta': 0.08830939137612415, 'max_depth': 11, 'alpha': 0.9179, 'lambda': 17.770950662564736, 'max_bin': 456}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:09:21,886] Trial 397 finished with value: 0.8415696638481649 and parameters: {'n_estimators': 679, 'eta': 0.07429490447410497, 'max_depth': 11, 'alpha': 0.9670000000000001, 'lambda': 2.37276947243015, 'max_bin': 401}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:09:29,034] Trial 398 finished with value: 0.8401216252891344 and parameters: {'n_estimators': 818, 'eta': 0.08659617861993989, 'max_depth': 12, 'alpha': 0.2735, 'lambda': 7.604862924573949, 'max_bin': 446}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:09:35,121] Trial 399 finished with value: 0.839053314557146 and parameters: {'n_estimators': 735, 'eta': 0.08198607757487787, 'max_depth': 12, 'alpha': 0.6705, 'lambda': 1.9092241457023151, 'max_bin': 361}. Best is trial 11 with value: 0.8513589595154614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 785\n",
      "\t\teta: 0.07389767489222081\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9650000000000001\n",
      "\t\tlambda: 2.5359194636827933\n",
      "\t\tmax_bin: 259\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  424.000000  421.000000  422.000000  449.000000   \n",
      "1                    TN  339.000000  364.000000  338.000000  322.000000   \n",
      "2                    FP   88.000000   71.000000   84.000000   82.000000   \n",
      "3                    FN   68.000000   63.000000   75.000000   66.000000   \n",
      "4              Accuracy    0.830250    0.854189    0.826986    0.838955   \n",
      "5             Precision    0.828125    0.855691    0.833992    0.845574   \n",
      "6           Sensitivity    0.861789    0.869835    0.849095    0.871845   \n",
      "7           Specificity    0.793900    0.836800    0.800900    0.797000   \n",
      "8              F1 score    0.844622    0.862705    0.841476    0.858509   \n",
      "9   F1 score (weighted)    0.829906    0.854110    0.826830    0.838560   \n",
      "10     F1 score (macro)    0.828786    0.853626    0.825528    0.835820   \n",
      "11    Balanced Accuracy    0.827850    0.853308    0.825021    0.834437   \n",
      "12                  MCC    0.658369    0.707383    0.651217    0.672165   \n",
      "13                  NPV    0.832900    0.852500    0.818400    0.829900   \n",
      "14              ROC_AUC    0.827850    0.853308    0.825021    0.834437   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   404.000000  416.000000  449.000000  418.000000  \n",
      "1   351.000000  363.000000  326.000000  349.000000  \n",
      "2    79.000000   72.000000   78.000000   81.000000  \n",
      "3    85.000000   68.000000   66.000000   71.000000  \n",
      "4     0.821545    0.847661    0.843308    0.834603  \n",
      "5     0.836439    0.852459    0.851992    0.837675  \n",
      "6     0.826176    0.859504    0.871845    0.854806  \n",
      "7     0.816300    0.834500    0.806900    0.811600  \n",
      "8     0.831276    0.855967    0.861804    0.846154  \n",
      "9     0.821613    0.847622    0.843029    0.834467  \n",
      "10    0.820950    0.847152    0.840450    0.833665  \n",
      "11    0.821227    0.846993    0.839388    0.833217  \n",
      "12    0.641970    0.694337    0.681196    0.667530  \n",
      "13    0.805000    0.842200    0.831600    0.831000  \n",
      "14    0.821227    0.846993    0.839388    0.833217  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_xgb_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_xgb_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_xgb_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_xgb_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_xgb_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_xgb_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_xgb_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_xgb_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_xgb_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_xgb_7)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:09:47,002] Trial 400 finished with value: 0.835270593350892 and parameters: {'n_estimators': 866, 'eta': 0.06447637401686006, 'max_depth': 11, 'alpha': 0.9367000000000001, 'lambda': 12.329569899621939, 'max_bin': 373}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:09:56,927] Trial 401 finished with value: 0.834962763176344 and parameters: {'n_estimators': 838, 'eta': 0.04867934937913874, 'max_depth': 12, 'alpha': 0.8402000000000001, 'lambda': 3.280945288173592, 'max_bin': 481}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:10:08,134] Trial 402 finished with value: 0.8371052544555209 and parameters: {'n_estimators': 885, 'eta': 0.07111777233791636, 'max_depth': 11, 'alpha': 0.9999, 'lambda': 15.578685419894484, 'max_bin': 386}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:10:17,557] Trial 403 finished with value: 0.8346397447950954 and parameters: {'n_estimators': 785, 'eta': 0.07913953925670099, 'max_depth': 12, 'alpha': 0.9066000000000001, 'lambda': 9.440298654338656, 'max_bin': 464}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:10:32,617] Trial 404 finished with value: 0.8319930898312912 and parameters: {'n_estimators': 703, 'eta': 0.05344559312911659, 'max_depth': 12, 'alpha': 0.08410000000000001, 'lambda': 34.90508245903407, 'max_bin': 391}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:10:39,909] Trial 405 finished with value: 0.8324139544448611 and parameters: {'n_estimators': 639, 'eta': 0.07637310550022865, 'max_depth': 10, 'alpha': 0.9489000000000001, 'lambda': 2.6374764382770444, 'max_bin': 429}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:10:46,960] Trial 406 finished with value: 0.8321873693210889 and parameters: {'n_estimators': 765, 'eta': 0.09048387326341271, 'max_depth': 11, 'alpha': 0.9761000000000001, 'lambda': 5.469243107088056, 'max_bin': 473}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:10:59,785] Trial 407 finished with value: 0.8350962022390626 and parameters: {'n_estimators': 846, 'eta': 0.0563730049618494, 'max_depth': 12, 'alpha': 0.934, 'lambda': 16.691722414505413, 'max_bin': 379}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:11:10,804] Trial 408 finished with value: 0.8234835361059781 and parameters: {'n_estimators': 864, 'eta': 0.03972330689201935, 'max_depth': 5, 'alpha': 0.9571000000000001, 'lambda': 4.363756567230679, 'max_bin': 370}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:11:16,413] Trial 409 finished with value: 0.8353667985956822 and parameters: {'n_estimators': 821, 'eta': 0.09449075954611551, 'max_depth': 11, 'alpha': 0.9178000000000001, 'lambda': 1.780833332085205, 'max_bin': 459}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:11:33,480] Trial 410 finished with value: 0.8357203148778563 and parameters: {'n_estimators': 660, 'eta': 0.02311795485008429, 'max_depth': 12, 'alpha': 0.8885000000000001, 'lambda': 3.5516682508444153, 'max_bin': 384}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:11:45,637] Trial 411 finished with value: 0.8354655488424735 and parameters: {'n_estimators': 884, 'eta': 0.04588988585722415, 'max_depth': 12, 'alpha': 0.9826, 'lambda': 6.697035088991459, 'max_bin': 441}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:11:53,396] Trial 412 finished with value: 0.8370409056196945 and parameters: {'n_estimators': 805, 'eta': 0.06779998200200163, 'max_depth': 11, 'alpha': 0.9345, 'lambda': 2.515931392757417, 'max_bin': 415}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:12:05,662] Trial 413 finished with value: 0.8349368724939623 and parameters: {'n_estimators': 871, 'eta': 0.08584954383083837, 'max_depth': 12, 'alpha': 0.9653, 'lambda': 36.96772963676243, 'max_bin': 375}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:12:18,855] Trial 414 finished with value: 0.8346428548512218 and parameters: {'n_estimators': 830, 'eta': 0.029919502424674164, 'max_depth': 12, 'alpha': 0.9519000000000001, 'lambda': 1.02253092871568, 'max_bin': 366}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:12:28,882] Trial 415 finished with value: 0.8343760515343183 and parameters: {'n_estimators': 851, 'eta': 0.0826010579736269, 'max_depth': 11, 'alpha': 0.6399, 'lambda': 23.694718850467872, 'max_bin': 487}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:12:41,370] Trial 416 finished with value: 0.8329983956953475 and parameters: {'n_estimators': 884, 'eta': 0.04073226655588804, 'max_depth': 12, 'alpha': 0.9154, 'lambda': 4.861488952233282, 'max_bin': 450}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:12:55,804] Trial 417 finished with value: 0.8360408768972768 and parameters: {'n_estimators': 840, 'eta': 0.036783877989695825, 'max_depth': 11, 'alpha': 0.8705, 'lambda': 8.27979092979923, 'max_bin': 496}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:13:02,667] Trial 418 finished with value: 0.8343584214379535 and parameters: {'n_estimators': 607, 'eta': 0.08467391820445541, 'max_depth': 10, 'alpha': 0.9987, 'lambda': 3.214013108818013, 'max_bin': 379}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:13:11,024] Trial 419 finished with value: 0.8355859968455566 and parameters: {'n_estimators': 791, 'eta': 0.08776825580330375, 'max_depth': 12, 'alpha': 0.8968, 'lambda': 11.330455413275091, 'max_bin': 470}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:13:22,431] Trial 420 finished with value: 0.8319328855497776 and parameters: {'n_estimators': 898, 'eta': 0.06150787163120347, 'max_depth': 11, 'alpha': 0.5372, 'lambda': 13.34427757910598, 'max_bin': 423}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:13:30,998] Trial 421 finished with value: 0.8332789517146134 and parameters: {'n_estimators': 868, 'eta': 0.06551998957605462, 'max_depth': 12, 'alpha': 0.9710000000000001, 'lambda': 4.032704933177885, 'max_bin': 388}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:13:37,185] Trial 422 finished with value: 0.8320454721160552 and parameters: {'n_estimators': 814, 'eta': 0.07910955485420708, 'max_depth': 12, 'alpha': 0.9318000000000001, 'lambda': 1.8930566151235309, 'max_bin': 393}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:13:53,647] Trial 423 finished with value: 0.8356614046981654 and parameters: {'n_estimators': 855, 'eta': 0.032010043754228744, 'max_depth': 11, 'alpha': 0.9484, 'lambda': 5.675697352163651, 'max_bin': 406}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:14:03,592] Trial 424 finished with value: 0.8335821310028093 and parameters: {'n_estimators': 831, 'eta': 0.0694902038760415, 'max_depth': 12, 'alpha': 0.5989, 'lambda': 10.222894681075989, 'max_bin': 476}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:14:20,153] Trial 425 finished with value: 0.8359784600996749 and parameters: {'n_estimators': 772, 'eta': 0.026930820671325198, 'max_depth': 11, 'alpha': 0.9722000000000001, 'lambda': 2.858014786321984, 'max_bin': 434}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:14:30,067] Trial 426 finished with value: 0.835746593655174 and parameters: {'n_estimators': 882, 'eta': 0.044104650114823236, 'max_depth': 12, 'alpha': 0.9086000000000001, 'lambda': 1.6775663371520124, 'max_bin': 463}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:14:41,042] Trial 427 finished with value: 0.8354191348496238 and parameters: {'n_estimators': 867, 'eta': 0.05139700569049197, 'max_depth': 12, 'alpha': 0.9316000000000001, 'lambda': 7.353173980911213, 'max_bin': 383}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:14:53,330] Trial 428 finished with value: 0.8360940599997111 and parameters: {'n_estimators': 806, 'eta': 0.05466178159655749, 'max_depth': 11, 'alpha': 0.9593, 'lambda': 13.880654757959325, 'max_bin': 482}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:15:07,733] Trial 429 finished with value: 0.8278082820110649 and parameters: {'n_estimators': 531, 'eta': 0.03457619020311907, 'max_depth': 12, 'alpha': 0.9838, 'lambda': 33.37740527837829, 'max_bin': 375}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:15:25,933] Trial 430 finished with value: 0.8300721843460647 and parameters: {'n_estimators': 844, 'eta': 0.048137933422194225, 'max_depth': 12, 'alpha': 0.9450000000000001, 'lambda': 36.335256299687515, 'max_bin': 369}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:15:38,892] Trial 431 finished with value: 0.8340958326122021 and parameters: {'n_estimators': 755, 'eta': 0.03779943968212126, 'max_depth': 11, 'alpha': 0.865, 'lambda': 3.951166837752928, 'max_bin': 453}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:15:56,452] Trial 432 finished with value: 0.8359459717864299 and parameters: {'n_estimators': 827, 'eta': 0.02873599516654701, 'max_depth': 11, 'alpha': 0.8899, 'lambda': 6.266729062517187, 'max_bin': 398}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:16:03,530] Trial 433 finished with value: 0.832970148826389 and parameters: {'n_estimators': 685, 'eta': 0.08346880503175962, 'max_depth': 12, 'alpha': 0.9248000000000001, 'lambda': 4.8469713837587385, 'max_bin': 490}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:16:12,082] Trial 434 finished with value: 0.8357758482275888 and parameters: {'n_estimators': 722, 'eta': 0.060124357567304054, 'max_depth': 12, 'alpha': 0.9616, 'lambda': 2.3331302795428845, 'max_bin': 379}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:16:33,669] Trial 435 finished with value: 0.8313531299800945 and parameters: {'n_estimators': 898, 'eta': 0.031263919088407144, 'max_depth': 11, 'alpha': 0.9836, 'lambda': 27.115262369168352, 'max_bin': 387}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:16:45,792] Trial 436 finished with value: 0.8330518824228234 and parameters: {'n_estimators': 793, 'eta': 0.04209747359459703, 'max_depth': 12, 'alpha': 0.8349000000000001, 'lambda': 3.3339978739534293, 'max_bin': 427}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:17:04,199] Trial 437 finished with value: 0.837941869999665 and parameters: {'n_estimators': 871, 'eta': 0.022381661523778833, 'max_depth': 12, 'alpha': 0.9988, 'lambda': 1.522591191625718, 'max_bin': 358}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:17:13,127] Trial 438 finished with value: 0.8346217954324373 and parameters: {'n_estimators': 850, 'eta': 0.08128031159437825, 'max_depth': 10, 'alpha': 0.9155000000000001, 'lambda': 8.891634180005616, 'max_bin': 439}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:17:29,958] Trial 439 finished with value: 0.8332965749463659 and parameters: {'n_estimators': 816, 'eta': 0.0250900734046753, 'max_depth': 11, 'alpha': 0.9409000000000001, 'lambda': 2.618813839701902, 'max_bin': 372}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:17:40,771] Trial 440 finished with value: 0.8368697344365648 and parameters: {'n_estimators': 885, 'eta': 0.038872222039790905, 'max_depth': 12, 'alpha': 0.9636, 'lambda': 1.007441332436852, 'max_bin': 363}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:17:44,321] Trial 441 finished with value: 0.8254218157357615 and parameters: {'n_estimators': 123, 'eta': 0.07198225229496122, 'max_depth': 11, 'alpha': 0.8979, 'lambda': 17.273678567390895, 'max_bin': 458}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:17:54,432] Trial 442 finished with value: 0.8346043873125968 and parameters: {'n_estimators': 855, 'eta': 0.07497292182585313, 'max_depth': 12, 'alpha': 0.9377000000000001, 'lambda': 16.185648257921343, 'max_bin': 392}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:18:08,445] Trial 443 finished with value: 0.8376356926447219 and parameters: {'n_estimators': 831, 'eta': 0.033248895692924954, 'max_depth': 11, 'alpha': 0.9825, 'lambda': 4.367299475782156, 'max_bin': 420}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:18:24,898] Trial 444 finished with value: 0.8327177321493504 and parameters: {'n_estimators': 873, 'eta': 0.046766468172642434, 'max_depth': 12, 'alpha': 0.6858000000000001, 'lambda': 20.37069513825416, 'max_bin': 382}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:18:34,065] Trial 445 finished with value: 0.8337591111000048 and parameters: {'n_estimators': 648, 'eta': 0.05231138683612441, 'max_depth': 12, 'alpha': 0.62, 'lambda': 3.4854800466719222, 'max_bin': 468}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:18:48,291] Trial 446 finished with value: 0.8321635672171961 and parameters: {'n_estimators': 778, 'eta': 0.056426757298456955, 'max_depth': 12, 'alpha': 0.9519000000000001, 'lambda': 24.4999699874952, 'max_bin': 447}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:19:01,554] Trial 447 finished with value: 0.8350746060327433 and parameters: {'n_estimators': 620, 'eta': 0.035744160237040495, 'max_depth': 11, 'alpha': 0.6578, 'lambda': 5.421114033213698, 'max_bin': 376}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:19:21,613] Trial 448 finished with value: 0.8330765049099913 and parameters: {'n_estimators': 900, 'eta': 0.01970450561899445, 'max_depth': 12, 'alpha': 0.9165000000000001, 'lambda': 2.0969616052384854, 'max_bin': 486}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:19:28,172] Trial 449 finished with value: 0.8334080927286218 and parameters: {'n_estimators': 843, 'eta': 0.08941724944506188, 'max_depth': 11, 'alpha': 0.9738, 'lambda': 3.2407724125792585, 'max_bin': 477}. Best is trial 11 with value: 0.8513589595154614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 785\n",
      "\t\teta: 0.07389767489222081\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9650000000000001\n",
      "\t\tlambda: 2.5359194636827933\n",
      "\t\tmax_bin: 259\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  424.000000  421.000000  422.000000  449.000000   \n",
      "1                    TN  339.000000  364.000000  338.000000  322.000000   \n",
      "2                    FP   88.000000   71.000000   84.000000   82.000000   \n",
      "3                    FN   68.000000   63.000000   75.000000   66.000000   \n",
      "4              Accuracy    0.830250    0.854189    0.826986    0.838955   \n",
      "5             Precision    0.828125    0.855691    0.833992    0.845574   \n",
      "6           Sensitivity    0.861789    0.869835    0.849095    0.871845   \n",
      "7           Specificity    0.793900    0.836800    0.800900    0.797000   \n",
      "8              F1 score    0.844622    0.862705    0.841476    0.858509   \n",
      "9   F1 score (weighted)    0.829906    0.854110    0.826830    0.838560   \n",
      "10     F1 score (macro)    0.828786    0.853626    0.825528    0.835820   \n",
      "11    Balanced Accuracy    0.827850    0.853308    0.825021    0.834437   \n",
      "12                  MCC    0.658369    0.707383    0.651217    0.672165   \n",
      "13                  NPV    0.832900    0.852500    0.818400    0.829900   \n",
      "14              ROC_AUC    0.827850    0.853308    0.825021    0.834437   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   404.000000  416.000000  449.000000  418.000000  394.000000  \n",
      "1   351.000000  363.000000  326.000000  349.000000  352.000000  \n",
      "2    79.000000   72.000000   78.000000   81.000000   97.000000  \n",
      "3    85.000000   68.000000   66.000000   71.000000   76.000000  \n",
      "4     0.821545    0.847661    0.843308    0.834603    0.811752  \n",
      "5     0.836439    0.852459    0.851992    0.837675    0.802444  \n",
      "6     0.826176    0.859504    0.871845    0.854806    0.838298  \n",
      "7     0.816300    0.834500    0.806900    0.811600    0.784000  \n",
      "8     0.831276    0.855967    0.861804    0.846154    0.819979  \n",
      "9     0.821613    0.847622    0.843029    0.834467    0.811555  \n",
      "10    0.820950    0.847152    0.840450    0.833665    0.811358  \n",
      "11    0.821227    0.846993    0.839388    0.833217    0.811131  \n",
      "12    0.641970    0.694337    0.681196    0.667530    0.623567  \n",
      "13    0.805000    0.842200    0.831600    0.831000    0.822400  \n",
      "14    0.821227    0.846993    0.839388    0.833217    0.811131  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_xgb_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_xgb_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_xgb_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_xgb_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_xgb_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_xgb_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_xgb_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_xgb_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_xgb_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_xgb_8)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:19:36,365] Trial 450 finished with value: 0.8316801735929076 and parameters: {'n_estimators': 813, 'eta': 0.08051340981017342, 'max_depth': 12, 'alpha': 0.5696, 'lambda': 6.278471811890742, 'max_bin': 385}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:19:52,293] Trial 451 finished with value: 0.8344600541523903 and parameters: {'n_estimators': 860, 'eta': 0.029880552297716134, 'max_depth': 11, 'alpha': 0.8762000000000001, 'lambda': 4.311679299535877, 'max_bin': 500}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:20:07,190] Trial 452 finished with value: 0.8323318584498427 and parameters: {'n_estimators': 573, 'eta': 0.04052554953308548, 'max_depth': 12, 'alpha': 0.9285, 'lambda': 18.676133246949803, 'max_bin': 431}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:20:17,407] Trial 453 finished with value: 0.8319076106067553 and parameters: {'n_estimators': 667, 'eta': 0.06267825364300045, 'max_depth': 12, 'alpha': 0.9538000000000001, 'lambda': 11.642171441023809, 'max_bin': 401}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:20:23,975] Trial 454 finished with value: 0.8289865546693805 and parameters: {'n_estimators': 790, 'eta': 0.07719033113339084, 'max_depth': 11, 'alpha': 0.9045000000000001, 'lambda': 2.595504233911095, 'max_bin': 368}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:20:36,055] Trial 455 finished with value: 0.8303958950391135 and parameters: {'n_estimators': 873, 'eta': 0.03435482609385924, 'max_depth': 12, 'alpha': 0.9846, 'lambda': 1.6718857699151122, 'max_bin': 463}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:20:43,126] Trial 456 finished with value: 0.8333662537020107 and parameters: {'n_estimators': 828, 'eta': 0.08474413335132024, 'max_depth': 11, 'alpha': 0.9422, 'lambda': 5.096523422975811, 'max_bin': 381}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:20:52,085] Trial 457 finished with value: 0.8313786994404001 and parameters: {'n_estimators': 848, 'eta': 0.058016859974288376, 'max_depth': 12, 'alpha': 0.9639000000000001, 'lambda': 3.636326734999895, 'max_bin': 372}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:21:05,201] Trial 458 finished with value: 0.8314904579875655 and parameters: {'n_estimators': 884, 'eta': 0.037673383488182735, 'max_depth': 12, 'alpha': 0.8075, 'lambda': 7.81005962110576, 'max_bin': 395}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:21:18,841] Trial 459 finished with value: 0.8309128880222765 and parameters: {'n_estimators': 751, 'eta': 0.031586479467847935, 'max_depth': 10, 'alpha': 0.8499, 'lambda': 2.421193180742447, 'max_bin': 472}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:21:29,914] Trial 460 finished with value: 0.8279961058406269 and parameters: {'n_estimators': 803, 'eta': 0.06664320467084381, 'max_depth': 11, 'alpha': 0.923, 'lambda': 23.52222712972586, 'max_bin': 389}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:21:45,609] Trial 461 finished with value: 0.8311005026028855 and parameters: {'n_estimators': 859, 'eta': 0.04990899761279381, 'max_depth': 12, 'alpha': 0.8921, 'lambda': 26.30680073677827, 'max_bin': 436}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:22:05,457] Trial 462 finished with value: 0.8306474711607791 and parameters: {'n_estimators': 836, 'eta': 0.027583578947178177, 'max_depth': 11, 'alpha': 0.9989, 'lambda': 13.168324228788371, 'max_bin': 444}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:22:20,425] Trial 463 finished with value: 0.8312562928099239 and parameters: {'n_estimators': 879, 'eta': 0.0433849636329461, 'max_depth': 12, 'alpha': 0.9461, 'lambda': 18.462109075575313, 'max_bin': 411}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:22:30,165] Trial 464 finished with value: 0.8306335977779481 and parameters: {'n_estimators': 819, 'eta': 0.05483613336029172, 'max_depth': 11, 'alpha': 0.9650000000000001, 'lambda': 4.294942466621484, 'max_bin': 377}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:22:43,974] Trial 465 finished with value: 0.8321333127072483 and parameters: {'n_estimators': 864, 'eta': 0.025442981424499037, 'max_depth': 12, 'alpha': 0.9244, 'lambda': 1.0068297655465241, 'max_bin': 459}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:22:54,819] Trial 466 finished with value: 0.8319539342988446 and parameters: {'n_estimators': 766, 'eta': 0.053023816969369794, 'max_depth': 12, 'alpha': 0.9049, 'lambda': 6.934580445118418, 'max_bin': 385}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:23:05,276] Trial 467 finished with value: 0.8282633328947249 and parameters: {'n_estimators': 705, 'eta': 0.09500626156794259, 'max_depth': 11, 'alpha': 0.513, 'lambda': 39.02126749408326, 'max_bin': 491}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:23:16,883] Trial 468 finished with value: 0.8303830193294157 and parameters: {'n_estimators': 899, 'eta': 0.03910691047568914, 'max_depth': 12, 'alpha': 0.9708, 'lambda': 2.8535495790594343, 'max_bin': 453}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:23:34,385] Trial 469 finished with value: 0.8290392466001879 and parameters: {'n_estimators': 847, 'eta': 0.035774132053407445, 'max_depth': 9, 'alpha': 0.7004, 'lambda': 18.176603178682925, 'max_bin': 418}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:23:56,756] Trial 470 finished with value: 0.8273558080293121 and parameters: {'n_estimators': 796, 'eta': 0.009903640581726249, 'max_depth': 12, 'alpha': 0.9408000000000001, 'lambda': 1.7963603789554803, 'max_bin': 377}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:24:16,982] Trial 471 finished with value: 0.8320752111442292 and parameters: {'n_estimators': 882, 'eta': 0.021800796944895146, 'max_depth': 11, 'alpha': 0.8783000000000001, 'lambda': 5.75228968734684, 'max_bin': 480}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:24:26,470] Trial 472 finished with value: 0.8297484071466444 and parameters: {'n_estimators': 735, 'eta': 0.08725789747555615, 'max_depth': 12, 'alpha': 0.9544, 'lambda': 22.55630885821479, 'max_bin': 393}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:24:44,098] Trial 473 finished with value: 0.8326660265274892 and parameters: {'n_estimators': 832, 'eta': 0.03326363399981364, 'max_depth': 11, 'alpha': 0.9796, 'lambda': 15.193823963100886, 'max_bin': 365}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:24:54,460] Trial 474 finished with value: 0.8301494925760512 and parameters: {'n_estimators': 593, 'eta': 0.04502482891784564, 'max_depth': 10, 'alpha': 0.9275, 'lambda': 3.452592028780554, 'max_bin': 428}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:25:15,409] Trial 475 finished with value: 0.8282784237546643 and parameters: {'n_estimators': 810, 'eta': 0.03060919799371059, 'max_depth': 12, 'alpha': 0.9093, 'lambda': 31.488533843138736, 'max_bin': 484}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:25:24,230] Trial 476 finished with value: 0.8286536685141123 and parameters: {'n_estimators': 861, 'eta': 0.07222706957170619, 'max_depth': 11, 'alpha': 0.9818, 'lambda': 9.110946377776534, 'max_bin': 465}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:25:40,643] Trial 477 finished with value: 0.8301510068464844 and parameters: {'n_estimators': 777, 'eta': 0.0408461691217874, 'max_depth': 12, 'alpha': 0.998, 'lambda': 17.07208070070616, 'max_bin': 373}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:25:47,288] Trial 478 finished with value: 0.8343349567774994 and parameters: {'n_estimators': 630, 'eta': 0.09006324416244726, 'max_depth': 12, 'alpha': 0.9479000000000001, 'lambda': 4.970968655689909, 'max_bin': 380}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:26:05,523] Trial 479 finished with value: 0.8305118407152536 and parameters: {'n_estimators': 886, 'eta': 0.028337810849388873, 'max_depth': 12, 'alpha': 0.9645, 'lambda': 10.489472662841024, 'max_bin': 387}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:26:13,357] Trial 480 finished with value: 0.8301197096335355 and parameters: {'n_estimators': 840, 'eta': 0.06427575125027625, 'max_depth': 11, 'alpha': 0.9338000000000001, 'lambda': 2.2367060219224695, 'max_bin': 402}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:26:19,116] Trial 481 finished with value: 0.8303489597745614 and parameters: {'n_estimators': 871, 'eta': 0.09154873399482469, 'max_depth': 11, 'alpha': 0.9136000000000001, 'lambda': 3.025455602716165, 'max_bin': 440}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:26:41,381] Trial 482 finished with value: 0.8293504082706848 and parameters: {'n_estimators': 822, 'eta': 0.023420926061181787, 'max_depth': 12, 'alpha': 0.8587, 'lambda': 20.753353056344388, 'max_bin': 345}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:26:54,061] Trial 483 finished with value: 0.8315286594000589 and parameters: {'n_estimators': 852, 'eta': 0.0373337125684416, 'max_depth': 11, 'alpha': 0.8225, 'lambda': 3.97987569435192, 'max_bin': 383}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:27:02,683] Trial 484 finished with value: 0.8331103656963439 and parameters: {'n_estimators': 791, 'eta': 0.0752992788210861, 'max_depth': 12, 'alpha': 0.8904000000000001, 'lambda': 8.122053663109334, 'max_bin': 473}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:27:26,315] Trial 485 finished with value: 0.8279824709155237 and parameters: {'n_estimators': 871, 'eta': 0.017309658924732303, 'max_depth': 12, 'alpha': 0.9549000000000001, 'lambda': 12.761062289546935, 'max_bin': 311}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:27:31,897] Trial 486 finished with value: 0.8289090797854032 and parameters: {'n_estimators': 807, 'eta': 0.08607250503047958, 'max_depth': 11, 'alpha': 0.9819, 'lambda': 1.7004027153686427, 'max_bin': 296}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:27:41,736] Trial 487 finished with value: 0.8284851947037714 and parameters: {'n_estimators': 900, 'eta': 0.09723645235414277, 'max_depth': 12, 'alpha': 0.9327000000000001, 'lambda': 34.074002994339885, 'max_bin': 371}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:27:57,871] Trial 488 finished with value: 0.8298922507867779 and parameters: {'n_estimators': 690, 'eta': 0.050314585371633844, 'max_depth': 12, 'alpha': 0.9624, 'lambda': 29.91420786728868, 'max_bin': 450}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:28:13,969] Trial 489 finished with value: 0.8336908115925343 and parameters: {'n_estimators': 843, 'eta': 0.03368079052763201, 'max_depth': 10, 'alpha': 0.9421, 'lambda': 6.884344000410124, 'max_bin': 288}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:28:24,221] Trial 490 finished with value: 0.8322338100167697 and parameters: {'n_estimators': 827, 'eta': 0.04836026578503902, 'max_depth': 12, 'alpha': 0.9131, 'lambda': 4.454007841272624, 'max_bin': 391}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:28:32,131] Trial 491 finished with value: 0.8290663220053347 and parameters: {'n_estimators': 861, 'eta': 0.0594794865140378, 'max_depth': 11, 'alpha': 0.9788, 'lambda': 2.9472103643779017, 'max_bin': 456}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:28:49,110] Trial 492 finished with value: 0.8287426227485044 and parameters: {'n_estimators': 666, 'eta': 0.03564835590093009, 'max_depth': 12, 'alpha': 0.8964000000000001, 'lambda': 22.04210308511052, 'max_bin': 423}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:29:03,758] Trial 493 finished with value: 0.8298877174829216 and parameters: {'n_estimators': 883, 'eta': 0.032585027758783976, 'max_depth': 11, 'alpha': 0.9993000000000001, 'lambda': 5.54409417357064, 'max_bin': 468}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:29:09,743] Trial 494 finished with value: 0.8332651964730886 and parameters: {'n_estimators': 848, 'eta': 0.07865539559478735, 'max_depth': 12, 'alpha': 0.9595, 'lambda': 1.0114020549228722, 'max_bin': 381}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:29:24,130] Trial 495 finished with value: 0.8273711789076236 and parameters: {'n_estimators': 780, 'eta': 0.029772662742239924, 'max_depth': 11, 'alpha': 0.9204, 'lambda': 2.2036687524142295, 'max_bin': 329}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:29:34,845] Trial 496 finished with value: 0.8328430706045993 and parameters: {'n_estimators': 814, 'eta': 0.042567956213936296, 'max_depth': 12, 'alpha': 0.6444000000000001, 'lambda': 3.80156784391343, 'max_bin': 495}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:29:52,728] Trial 497 finished with value: 0.828255934945679 and parameters: {'n_estimators': 870, 'eta': 0.026270201065214, 'max_depth': 11, 'alpha': 0.8746, 'lambda': 5.110277195148278, 'max_bin': 375}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:30:02,980] Trial 498 finished with value: 0.8323747247153872 and parameters: {'n_estimators': 887, 'eta': 0.053083534259988345, 'max_depth': 12, 'alpha': 0.9374, 'lambda': 6.526738315060269, 'max_bin': 407}. Best is trial 11 with value: 0.8513589595154614.\n",
      "[I 2023-12-04 11:30:13,995] Trial 499 finished with value: 0.8336707533721656 and parameters: {'n_estimators': 796, 'eta': 0.045999057468651273, 'max_depth': 12, 'alpha': 0.9700000000000001, 'lambda': 2.876070582886211, 'max_bin': 434}. Best is trial 11 with value: 0.8513589595154614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 785\n",
      "\t\teta: 0.07389767489222081\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.9650000000000001\n",
      "\t\tlambda: 2.5359194636827933\n",
      "\t\tmax_bin: 259\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  424.000000  421.000000  422.000000  449.000000   \n",
      "1                    TN  339.000000  364.000000  338.000000  322.000000   \n",
      "2                    FP   88.000000   71.000000   84.000000   82.000000   \n",
      "3                    FN   68.000000   63.000000   75.000000   66.000000   \n",
      "4              Accuracy    0.830250    0.854189    0.826986    0.838955   \n",
      "5             Precision    0.828125    0.855691    0.833992    0.845574   \n",
      "6           Sensitivity    0.861789    0.869835    0.849095    0.871845   \n",
      "7           Specificity    0.793900    0.836800    0.800900    0.797000   \n",
      "8              F1 score    0.844622    0.862705    0.841476    0.858509   \n",
      "9   F1 score (weighted)    0.829906    0.854110    0.826830    0.838560   \n",
      "10     F1 score (macro)    0.828786    0.853626    0.825528    0.835820   \n",
      "11    Balanced Accuracy    0.827850    0.853308    0.825021    0.834437   \n",
      "12                  MCC    0.658369    0.707383    0.651217    0.672165   \n",
      "13                  NPV    0.832900    0.852500    0.818400    0.829900   \n",
      "14              ROC_AUC    0.827850    0.853308    0.825021    0.834437   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   404.000000  416.000000  449.000000  418.000000  394.000000  418.000000  \n",
      "1   351.000000  363.000000  326.000000  349.000000  352.000000  375.000000  \n",
      "2    79.000000   72.000000   78.000000   81.000000   97.000000   75.000000  \n",
      "3    85.000000   68.000000   66.000000   71.000000   76.000000   51.000000  \n",
      "4     0.821545    0.847661    0.843308    0.834603    0.811752    0.862894  \n",
      "5     0.836439    0.852459    0.851992    0.837675    0.802444    0.847870  \n",
      "6     0.826176    0.859504    0.871845    0.854806    0.838298    0.891258  \n",
      "7     0.816300    0.834500    0.806900    0.811600    0.784000    0.833300  \n",
      "8     0.831276    0.855967    0.861804    0.846154    0.819979    0.869023  \n",
      "9     0.821613    0.847622    0.843029    0.834467    0.811555    0.862727  \n",
      "10    0.820950    0.847152    0.840450    0.833665    0.811358    0.862594  \n",
      "11    0.821227    0.846993    0.839388    0.833217    0.811131    0.862296  \n",
      "12    0.641970    0.694337    0.681196    0.667530    0.623567    0.726369  \n",
      "13    0.805000    0.842200    0.831600    0.831000    0.822400    0.880300  \n",
      "14    0.821227    0.846993    0.839388    0.833217    0.811131    0.862296  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_xgb_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_xgb_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_xgb_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_xgb_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_xgb_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_xgb_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_xgb_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_xgb_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_xgb_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_xgb_9)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAv8AAAHJCAYAAAASMFYPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACHtUlEQVR4nO3dd3gU5doG8Hu2pHdCEiAkEEpEukSlhBYV1MORXkUBpYgVu+hRhE/xwLGgWCgqYMFQQiCgCCJFelMTIAJC6CQhIb1vme+PsGs225Ntyd6/6/KSzMzOvvvu7uwzb3leQRRFEURERERE1OhJnF0AIiIiIiJyDAb/RERERERugsE/EREREZGbYPBPREREROQmGPwTEREREbkJBv9ERERERG6CwT8RERERkZtg8E9ERERE5CYY/BMRERERuQkG/0QubMCAARAEwa7PMXnyZAiCgIsXL9r1eSy1cuVKCIKAlStXOrsoNtHYXo89OeLzTkTk7hj8Exlw7NgxTJkyBTExMfD29kZAQAA6d+6Ml19+GdeuXbPZ87ha4O0Iu3fvhiAIePvtt51dFItpAvjJkycbPUbzugYMGGDT53777bchCAJ2795t0/M6gubzXfM/X19fdO7cGa+//joKCgrs8rz2eB+IiBoLmbMLQORKRFHEa6+9hoULF0Imk+G+++7D6NGjUVVVhQMHDuD999/H559/jlWrVmHUqFF2L88333yDsrIyuz7He++9h9deew0tWrSw6/NYavjw4ejZsyeaNWvm7KLYRGN7PXUxdOhQdOvWDQCQlZWFzZs347333sP69etx5MgRBAUFObV8RETuhME/UQ3z5s3DwoUL0apVK2zZsgUdO3bU2Z+UlISJEydi3Lhx2L59OxISEuxanqioKLueHwCaNWvmUoFpYGAgAgMDnV0Mm2lsr6cuhg0bptNr8v777+Puu+9Geno6Fi9ejDfffNN5hSMicjMc9kN0y4ULF/DOO+9ALpcjJSVFL/AHgJEjR+Kjjz6CSqXCzJkzoVartftqju3esmULevfuDV9fXwQHB2PUqFH4+++/dc4lCAJWrVoFAGjdurV2WESrVq20xxgaA11z2MyxY8dw//33IygoCEFBQRg5ciSuXLkCAPj7778xZswYNG3aFN7e3hg4cCDS0tL0XpOhoUetWrXSG65R87+agdzZs2fx2muvIS4uDk2bNoWnpyeio6Mxbdo0XL58We+5Bg4cCACYO3euzjk1w1pMjZE/duwYRowYgbCwMO3zzJw5E9evXzf5upYuXYrOnTvDy8sL4eHhmDZtmt2GnNRm7PX88ccfGDt2LKKjo+Hp6YkmTZqgS5cueO6556BQKABUvw9z584FAAwcOFCnvmq6fv06nnzySbRq1QoeHh5o2rQphg8fjqNHj5osz48//oh+/fohICAAgiAgPz8fPj4+aNOmDURRNPh6hgwZAkEQcPz48TrXiZ+fHyZNmgQAOHz4sNnj1Wo1Pv/8c9x5553w8/ODr68v4uLi8Pnnnxv8DgLAnj17dOqrIQ0zIyKyJ7b8E92yYsUKKJVKjB49Gp07dzZ63NSpUzFv3jycPXsWe/bs0QazGhs2bMDWrVsxfPhwDBgwAH/++SeSkpKwa9cuHDhwALGxsQCAOXPmYOPGjUhNTcVzzz2nHfpg6RCIo0ePYsGCBejfvz+mTp2KEydOYMOGDTh58iSSk5MRHx+P22+/HY8++iguX76MpKQk3HvvvcjIyICfn5/Jc8+aNctgcLx582b8/vvv8PHx0Xm9S5YswcCBA9G7d294eHjg5MmT+Oqrr5CSkoLjx48jMjISQHULMACsWrUK/fv31xmXXfOmx5BNmzZh9OjREAQBo0aNQlRUFI4dO4YlS5Zg06ZN2LdvH2JiYvQe98orr2Dbtm3497//jUGDBmHXrl348ssvte+fM/z555/o1asXJBIJHnroIbRu3RpFRUU4d+4cvvjiC7z77ruQy+WYNWsWNm7ciD179mDSpEkG6ygjIwPx8fHIzMzEPffcg/Hjx+PKlStYt24dfvzxR6xbtw5Dhw7Ve9y6devw888/48EHH8QTTzyBCxcuIDg4GOPGjcOKFSuwY8cO3HfffTqPuXLlCrZu3YoePXqgR48e9aoDYzcXhkyYMAFr1qxBVFQUpk6dCkEQkJycjKeeegq//fYbEhMTAQDdunXDnDlzMHfuXERHR+vcpHIOABHRLSIRiaIoigMHDhQBiMuWLTN77Pjx40UA4v/93/9pt61YsUIEIAIQN2/erHP8okWLRABiQkKCzvZJkyaJAMQLFy4YfJ7+/fuLtb+mu3bt0j7Pd999p7PvscceEwGIgYGB4jvvvKOz79133xUBiIsWLbKqDBrbt28XZTKZ2LZtWzEnJ0e7/erVq2JFRYXe8T/99JMokUjEGTNmGCz/nDlzDD6Pph5XrFih3VZcXCyGhISIUqlU3L9/v87x8+fPFwGI9957r8HXFRUVJV66dEm7XaFQiH379hUBiIcOHTL5mmuXqWvXruKcOXMM/qd5vv79+5t9Pc8//7wIQExOTtZ7rry8PFGlUmn/njNnjghA3LVrl8Gy3XfffSIA8b///a/O9r1794oSiUQMDg4Wi4qK9MojCIK4detWvfMdO3ZMBCCOHDlSb9+bb75p8XdEFP95D2q+dlEUxdLSUrFjx44iAHHu3Lna7YY+799//70IQIyLixNLSkq020tKSsQ77rjD4PfA0PtARETV2PJPdEtWVhYAoGXLlmaP1RxjaLhJQkIChgwZorPt6aefxuLFi7Fz505cunQJ0dHR9S5v37598fDDD+tsmzRpEr7++msEBwfjtdde09k3ceJEvPHGG/jzzz+tfq6TJ09i1KhRCAwMxE8//YTQ0FDtPmMThR944AHcfvvt2L59u9XPV9vGjRuRl5eHhx9+GL1799bZ99JLL2Hp0qXYsWOHwbp96623dOZOyGQyTJkyBXv37sXRo0dx9913W1yO1NRUpKam1u/FANqhKTV7UDSCg4MtPs/Vq1fxyy+/IDo6Gi+++KLOvvj4eIwbNw6rV69GcnIyHn30UZ39Dz30EO6//369c/bo0QN33nknUlJSkJ2djfDwcACASqXCV199BX9/f0yYMMHiMgLV759mWFl2djY2b96Ma9euoU2bNnjmmWdMPvbrr78GUD0x3dfXV7vd19cX//3vfzFo0CB89dVXet8FIiIyjGP+iW4Rbw1DsCTPuOYYQ8f2799fb5tUKkV8fDyA6rHetmBo2EXz5s0BVA9/kEqlBvddvXrVqufJzMzEv/71L1RWViI5ORnt2rXT2S+KIr777jvce++9aNq0KWQymXac9cmTJ22SGlVTZ7WHWAGAXC7X1rmhuo2Li9Pbprl5y8/Pt6ockyZNgiiKBv/btWuXxecZN24cpFIphg0bhkmTJuGbb77B+fPnrSoL8M/r7du3L2Qy/bace++9FwDw+++/6+0zddPz5JNPQqFQaANvoHrI1/Xr1zFx4kSdINwSmzZtwty5czF37lysWrUKAQEBePnll3HkyBGzNzt//PEHJBKJwe/VwIEDIZVKDb4+IiIyjME/0S2ajDeaCbOmaAJoQ1lyNC2ltUVERAAACgsL61pEHYYyyGgCQFP7NJNJLVFaWoohQ4bgypUrWLFiBfr27at3zAsvvIBHHnkE6enpGDx4MF588UXMmTMHc+bMQXR0NKqqqix+PmM0daapw9o074OhujVVFyqVqt5lq4s777wTe/fuRUJCAtatW4dJkyahbdu26NChA9asWWPxeepTL8YeAwBjx45FSEgIvvzyS+1N8dKlSwEATzzxhMXl01ixYoX2JqmsrAzp6elYuHAhQkJCzD62sLAQISEhkMvlevtkMhlCQ0NRVFRkdZmIiNwVh/0Q3RIfH49du3Zhx44dmDp1qtHjVCqVtpW3T58+evuzs7MNPk4zrKihpH1Uq9UYP348fv/9d7z77rsYP3683jE3btzAJ598gk6dOuHAgQPw9/fX2f/DDz/YpCyaOtPUYW2ZmZk6xzUEvXr1wpYtW1BZWYnjx4/j559/xuLFizF+/Hg0bdrUojSy9akXUz1c3t7emDx5Mj788EP88ssvaN++PbZv346ePXuiS5culrw8mwkMDEReXh4UCoXeDYBSqURubi4CAgIcWiYiooaMLf9Et0yePBlSqRQbNmxAenq60eO+/vprXL9+HbGxsQaHIhjKIKNSqbBv3z4AQPfu3bXbNUNznNUCbcqsWbOwefNmPPbYY3j99dcNHpORkQG1Wo1BgwbpBf5Xr15FRkaG3mPq8po1dWZolVulUqmt2zvuuMPic7oKT09P9O7dG/PmzcMnn3wCURSxceNG7X5T9aWpl3379kGpVOrt19yk1qVeZs6cCUEQsHTpUixfvhxqtRozZsyw+jz11b17d6jVavz22296+3777TeoVCq91yeRSFzyO0VE5AoY/BPdEhMTg9dffx0KhQL//ve/Dd4AbNy4Ec899xykUik+//xzSCT6X6GdO3diy5YtOts+/fRTnD9/HgMHDtSZkNqkSRMAlg01cqRFixZh8eLFuOeee7BkyRKjx2lST+7bt08n2CopKcG0adMMBqR1ec3Dhg1DSEgIfvjhBxw6dEivrBkZGbj33nsdsiiaLezdu9fgUBxNr5GXl5d2m6n6ioyMxH333YeLFy9i0aJFOvsOHz6M1atXIzg4GMOHD7e6jG3btsV9992HlJQULFu2DEFBQRg7dqzV56mvxx57DAAwe/ZsndWuy8rKtJPaH3/8cZ3HNGnSxOW+U0REroLDfohqePvtt1FaWooPP/wQXbt2xeDBg9GxY0coFAocOHAAhw8fhre3N3744QejwzIeeughDB8+HMOHD0fbtm2RmpqKn376CSEhIfj88891jr3nnnvwv//9D9OmTcPIkSPh5+eHoKAgPP300454uQZlZWXhxRdfhCAI6Ny5M9599129Y7p164Zhw4YhIiIC48aNQ2JiIrp164ZBgwahsLAQv/zyC7y8vNCtWze97EKxsbFo0aIFEhMTIZfLERUVBUEQ8MgjjxjNguTn54evv/4ao0ePRv/+/TF69GhERUXh+PHj2L59OyIiIrRj0huCDz74ANu3b8eAAQMQExMDPz8/nDp1Clu3bkVQUBCmT5+uPXbgwIGQSCSYPXs2Tpw4oZ0g+5///AcAsGTJEvTp0wcvv/wytm/fjri4OG2ef4lEghUrVuj1ylhq5syZ2L59O3Jzc/Hss8/C29u7/i/eShMmTMCmTZuwdu1adOzYEcOGDYMgCNi4cSMuXLiAMWPG6GX6ueeee5CYmIihQ4eie/fukMlk6NevH/r16+fw8hMRuRznZBglcm2HDx8WH330UbFVq1ail5eX6OvrK3bs2FF88cUXxStXrhh8TM187lu2bBF79uwp+vj4iIGBgeKIESPEM2fOGHzcBx98IN52222ih4eHCECMjo7W7jOV599QnvwLFy6IAMRJkyYZfC4YyH9eO8+/5hym/qt5/tLSUvH1118X27RpI3p6eoqRkZHik08+Kebm5hosvyiK4pEjR8SEhAQxICBAFARBJ4+9obz4NR83bNgwMTQ0VJTL5WLLli3FJ554Qrx27ZresabWLzC31kBtmjIZq9ea57Qkz/+2bdvEyZMnix06dBADAgJEHx8fsX379uIzzzwjXrx4Ue/c3377rdi1a1fRy8tL+x7UdPXqVfGJJ54Qo6KiRLlcLjZp0kQcOnSoeOTIEaOvxVD91qZUKsXQ0FARgHjq1Cmzx9dmLM+/McY+LyqVSvzss8/EHj16iN7e3qK3t7d4xx13iJ9++qnOmgga2dnZ4vjx48WwsDBRIpFY9V4TETV2gihascwiERm1cuVKTJkyBStWrNBZWZSooTp//jzatWuH+Ph4g2PuiYio4eGYfyIiMuh///sfRFF06jA0IiKyLY75JyIirUuXLuHbb7/F33//jW+//Rbdu3fHqFGjnF0sIiKyEQb/RESkdeHCBbz55pvw9fXF4MGD8cUXXxjMakVERA0Tx/wTEREREbkJNucQEREREbkJBv9ERERERG6CwT8RERERkZtg8E9ERERE5CaY7ceM/Px8KJVKm5+3adOmyMnJsfl5SRfr2XFY147BenYM1rPj2LquZTIZgoODbXY+osaGwb8ZSqUSCoXCpucUBEF7biZbsh/Ws+Owrh2D9ewYrGfHYV0TOR6H/RARERERuQkG/0REREREboLBPxERERGRm2DwT0RERETkJjjhl4iIiMjGysvLkZ2dDVEUOZmZ7EoQBAiCgPDwcHh7e5s9nsE/ERERkQ2Vl5fj2rVr8Pf3h0TCQRZkf2q1GteuXUOLFi3M3gDwE0lERERkQ9nZ2Qz8yaEkEgn8/f2RnZ1t/lgHlIeIiIjIbYiiyMCfHE4ikVg0xIyfTCIiIiIb4hh/chYG/40ALyBEREREZCsM/l1QaZUKH+25ghErTmHo1ycxYsUpfLTnCkqrVM4uGhEREbm5Hj16YOnSpfU+pr4SExPRtm1buz6HLbhaORn8u5jSKhWmrz2LpNRcZBVXIbdUiaziKiSl5WL62rO8ASAiIiK7uHbtGmbNmoXOnTujRYsWuOOOO/DGG28gLy/P6nNt27YNjzzyiM3KZuhmYujQoTh48KDNnqO2zZs3IyIiAlevXjW4v3fv3nj99dft9vz2wlSfLmbZweu4lFcBNQC5SgkPlUK7Lze7HCt3/Y0n+0TqPEYURQiC4OCSuj5REKAqKoK6pATg8Cm7Yl07BuvZMVjPDiRjGGKOo37jL168iAcffBBt2rTB0qVLERUVhTNnzmDu3Ln49ddfsXXrVgQHB1t8vtDQUDuWtpq3t7dFee3r6v7770dISAjWrFmDF198UWff4cOHce7cOSxbtsxuz28v/Na5mL0ZRVAD8K0qx78v7IdUrdvS739FisrMMFSp1Pj9ajEu5VdCLYqQCAKigz1xR6Q/PKTs0AEACECenz8qS4oB/n7bF+vaMVjPjsF6dhhJWBgQE+PsYric0ioVvth3Fb+dz4dSLUImEdCvTTBmxkfC10Nql+d87bXX4OHhgbVr12oD6sjISHTq1Al333035s+fj//973/a40tKSvDEE0/g559/hr+/P5577jlMnTpVu79Hjx6YPn06ZsyYAQAoKirC3LlzsXXrVlRUVKBbt26YN28eOnXqpH3Mzz//jA8++ACnT5+Gr68vevbsiZUrV2LYsGG4cuUK3nzzTbz55psAgBs3biAxMRH/+c9/cO7cOZw7dw69e/fG/v370a5dO+05v/jiC3z55Zc4duwYBEHAmTNn8Pbbb+PgwYPw8fHBgAED8H//939o0qSJXp3I5XKMGjUKiYmJeOGFF3Ruwn744Qd07doVnTp1whdffIHExERcunQJQUFBGDRoEN566y34+fkZrOtnnnkGhYWF+Oabb7Tb/vOf/+DkyZPYuHEjgOqbvk8//RSrVq3CjRs3EBMTgxdffBH//ve/LX5PjWHw70JEUYRSrQYANKkohFStgigIUAv/BPMKSFAFAZv+KkBBufLW71L1h/HEjUpcKVZhWKcmkPMGAIIACDIpBKmUjXd2xrp2DNazY7CeHYi/VXpKq1R4bPUpXLxZPQpAY92f2Th6uRBfT+ho8xuA/Px87Nq1C6+//rpeS3p4eDhGjhyJTZs2YeHChdoA+LPPPsOsWbPw8ssvY9euXXjzzTfRtm1bDBgwQO/8oihiwoQJCA4OxurVqxEQEIBVq1Zh1KhROHjwIIKDg/HLL79gypQpmDVrFj777DNUVVVhx44dAIAVK1Zg4MCBeOSRRzBx4kSDr6Ft27bo2rUrkpKS8Nprr2m3b9iwASNGjIAgCMjOzsawYcMwceJEzJs3DxUVFZg3bx6mTZuGDRs2GDzvww8/jCVLluDAgQPo06cPAKC0tBSbNm3CW2+9BaA6xea7776Lli1b4vLly3j11Vcxb948LFy40Lo3oob33nsPP/74IxYuXIiYmBgcOnQITz75JJo0aYLevXvX+bwAg3+XIggCZLfyAnupqgAAV/zDsLdFN+0xEf4eKGodgKSiXJ2LgoZEAHJbh+L5/i0dUGLXJggCQps1gyIzk1mT7Ix17RisZ8dgPTsOh6zq+2LfVb3AHwDUInAxrwJf7LuKlxKibfqcGRkZEEVRp8W8pnbt2qGgoAC5ublo2rQpAOCuu+7Cs88+CwBo06YNjhw5gqVLlxoM/vft24e//voL6enp8PT0BABtL8DmzZvx6KOP4qOPPsKwYcPw6quvah+n6RUIDg6GVCqFn58fwsPDjb6OkSNH4quvvtIG/+fPn0dqaio+/fRTANU3EZ07d8Ybb7yhfczHH3+Mbt264fz582jTpo3eOWNjY9GjRw/88MMP2uA/JSUFarUaI0aMAABt7wYAREdH47XXXsMrr7xS5+C/tLQUS5YsQVJSEu68804AQKtWrXD48GF888039Q7+ecvtYvrGBEAiAF7K6uC/Quqh3ScRqvdrhgYZohaBfRlFDigpERER2dpv5/NN/sbvPZ/v0PIA/6Qdr3mzFhcXp3NMXFwc/v77b4OPT01NRWlpKWJjY9GqVSvtf5cvX8bFixcBAKdOnUK/fv3qVc7hw4fj6tWrOHbsGABg/fr16NSpE2JjYwEAaWlp2L9/v04ZNIG0phyGTJgwAVu2bEFJSQkAYPXq1XjwwQcRGBgIoPrmZtSoUejSpQtat26Np59+Gnl5eSgtLa3T6zh79iwqKiowevRonbKuXbvWZDktxZZ/FzO9V3Mcu1IC70zd4F8iAK2CvTCtZzPsOldg8hxKtchJwERERA1M9fBf071NCjv8xrdu3RqCIODs2bN48MEH9fafO3cOQUFBBsfFW0KtViM8PBzJycl6+zQBtJeXV53OXVN4eDj69OmDDRs2IC4uDsnJyXj00Ud1yjFo0CDtvIHajzVm+PDhePPNN7Fx40b07t0bhw8f1vZQXLlyBRMmTMCkSZPw2muvITg4GIcPH8asWbOgVCoNns/Q6s8KxT8JXtS3hoCvXr0aEREROsdpek7qg8G/i/H1kGLZmPb4ZfkpFJVJ4e3vg2b+HoiPCcD0Xs3h6yHVDg0yRioRGPgTERE1MNXDf03/fsvs8BsfEhKC/v37Y8WKFZgxY4bOuP/s7GwkJSVh9OjROs97/PhxnXMcP37c6LChLl264MaNG5DJZIiKijJ4zO23347ffvsN48ePN7hfLpdDpTKf7nzUqFGYN28ehg8fjosXL2L48OE65diyZQuioqIgsyLTlJ+fHx566CH88MMPuHTpEqKjo7VDgP78808olUrMnTtXG9Rv2rTJ5PmaNGmC06dP62w7efIk5HI5gOqhRp6enrh69Wq9h/gYwmE/LsjXQ4oHWvtgTLcwvD+6I5KmdMTz/VtqJ/hohgYZohkaVBcc20pERORc/doEm/yN79fG8nSb1vjvf/+LqqoqjB07FgcPHsS1a9ewc+dOjBkzBhEREXr57I8cOYLFixfj/Pnz+Oqrr5CSkoJp06YZPHf//v0RFxeHSZMmYefOnbh8+TKOHDmC9957D3/++ScA4KWXXkJycjIWLFiAs2fPIj09HYsXL9aeo2XLljh06BAyMzNx8+ZNo6/jX//6F0pKSvDKK6+gT58+aNasmXbfY489hoKCAsyYMQO///47Ll68iF27duG5554ze2MxYcIEHD16FCtXrsSECRO0N0KtWrWCUqnEl19+iYsXL2Lt2rVYtWqVyXPFx8fjzz//xJo1a5CRkYEFCxbo3Az4+fnhySefxFtvvYXExERcuHABJ06cwFdffYXExEST57YEW/5dSGmVCssOXsfejCIM/OsCAqrKUBmcj4dbtISPXKL9oGmGBl3Kr0DN3kHN0KDpvZqbfJ6a3YU1n1OpVkMmkaBvjV4GIiIicpyZ8ZE4erkQF/MM/MaHeGNmfKTxB9dDTEwMtm/fjv/973+YNm0a8vPzERYWhgceeAAvvfSSXo7/mTNnIi0tDR988AF8fX0xd+5cJCQkGDy3IAj44YcfMH/+fMyaNQs3b95EWFgYevbsqZ1A3KdPH3z55Zf48MMPsXjxYvj7+6Nnz57ac7z66qt46aWXcNddd6GyshI3btww+Fz+/v4YNGgQUlJS8PHHH+vsi4iIwJYtWzBv3jyMHTsWVVVViIyMREJCgsGhODX17NkTbdu2RUZGBsaOHavd3rlzZ8ybNw+LFy/Gu+++i549e+KNN97A008/bfRcCQkJeOGFFzBv3jxUVlZi/PjxGDNmDP766y/tMa+99hpCQ0PxySef4NKlSwgMDETnzp0xa9Ysk+W0hCCyudeknJwcnXFYtiAIApo1a4bMGpkkNCv7ahb4Gn12JzxUCmyOiUepty+CvGWQ1wjMgeoFwfZlFGlzAMebCNoNBfk9o/3wx7VSXMmv1JlcJBGA6GAvLBvTvkHfABiqZ7IP1rVjsJ4dg/XsOPaoa7lcrg0onSUjIwP+/v51frwmz//e8/lQqEXIJQL62jnPv6116tQJr732mtHUnGQfxcXFiDGzdgZb/l1EzZV9JWqVdmXfCpkHVGrgZmn1pJGktFwcu1KCZWPa4/n+LfF8f/Or/9W+sdDYeNLwct1qEbiUX4FlB68zZSgREZGD+XpI8VJCNF5KiG5wCTzKyspw5MgR5OTkaLPskGvhmH8XUTN9Z8ebFwBULzFfJdG9P6sZmGuYuyjUvLGwlCZlKFu9iBoGfleJGqeGFPgDwLfffosZM2Zg+vTp2hz15FrY8u8Caq7s66msQpfc8wCAErl39VKTtWgC8+f7Gz9fzYuFqXUBTMkuqcLQr09yHgCRi6o9nE8qCOjXJpDfVSJymhkzZugsekWuh8G/C6i5sq+3slK7fW+LrkYfUzuXv7GJu9N6NkOVBamxDFGLQK6B4UYMKoicTzOc72JeBWq2+a9LzcXPp/Pw3cMd0NTPw+jjiYjIPXHYj4vQruyrql7cq9DTD/lexlN21szlrwkCklJzkVVchdxSJbKKq5CUlosZ6/5GeVVd2v11GRpuRETOs+zgdb3AX6O4Uo1Hvj+N0iqV1cOBOHyIiKhxY8u/i9Ck7xQLdVf2NaZv6wBta/+W9DyUK/QDfE3Abma9EIuZG25ERI6zN6PIYOCvUVSpwtCvTsLHQ2J26B5T/hIRuQ8G/y5Cs7Jv8rpsFF0HKmVyo8fKJMDEuHCDGXxqU4uAmZXCrVJ7uBEROV7NeUKmlCnUKLvVMGBs6J6xbGAc6kdE1Dhx2I8L8fWQYtztgbg93AdVMuMt/0NuD8F3x7OtzuBjC1I7LCtORNYRBAFSK7+HahG4mKc/dM9YNrDGNtSPw5mIiKox+HcxYkUF7oj0R1CQn95wHYkAxIR44an4yDpn8KkPiVA9N4GInK9fm0CrHyOiukX/oz1XUFpVnQjA1LVEM9TP7HkNBNaabc4MukurVPhozxWMWHEKQ78+iRErTum8diIid8RhPy5GrKyEXCrgxfvbIiAvQLuCr1QA+rYJxMQe4Vh64BpulFQ5vGzRwV7a1YWJyLmm92qOn0/nobjSumYAtfjPkJ6lo9uZHT6UX65ASaUSfp7VPxeaYX/GVg0HBBy4WISiCiWqVCI8pAICvWQOT0HK4UxE7u2ZZ55BYWEhvvnmG2cXxeUw+HcRmh9S8ZfzCC65id/zrqNlNz/cHe2PQ5eKoVSrsed8Ibbd+rG3RVual6z6Rzm7RGHR8UtHt+OPJZEL6RcTiJ/+yrf6eqAZ0vP5/msoM5MNrEIpYtras+jewk97LZIIAioUKr1rkaFVwyuUIipKFA4Pui0ZzsQVzIn+8cwzz2DNmjXav4ODg9GtWze89dZb6Nixo02eY+HChdi6dSt27dpl9JjZs2dj586dOHz4sN6+zMxMdO/eHV9++SWGDBlikzK5Iw77cQE1U3VWlJajrEqNq5UCNp7Mw8aTN7XpO2+UKFBko8AfAIK85ejXJhCWjBz2kUu0LX9E5Fw5JVUYueIUfjQQ+Fs6E0AtAptP5WknBJtyKb+y3tciW88hMDecyBbDmYjcTUJCAk6cOIETJ05g/fr1kMlkmDhxokPLMGHCBFy4cAGHDh3S25eYmIiQkBAMHjzYoWVqbBj8u4CaLVSet/L8V5pJ9WkL8a39Mb1Xc7QK8TJ77KDYYLuXhxoPTq60n+yiCoxccRJFlcbHrQd4WtayrnLw21TfoNvSMfyWZENSqNT8nBLV4uHhgfDwcISHh6Nz58545plncO3aNeTm5mqPyczMxLRp09CuXTvExsbi0UcfxeXLl7X79+/fj8GDB6NVq1Zo27Yt/vWvf+HKlStITEzE+++/j1OnTiEsLAxhYWFITEzUK0Pnzp3RpUsXrF69Wm9fYmIiRo8eDYlEglmzZiEuLg5RUVHo1asXli1bZvK19ejRA0uXLtXZNnDgQCxcuFD7d1FREV588UXcfvvtiImJwYgRI3Dy5EmL66+hYFOuC9C0UElENTxV1UNwKkxk+7GVXecKse9CMXpG++H2cB/8fDrPYDDg7yHB5DvD7V4ecpz6pms19PiSSiU+3H0FezMKmSveTkqrVBi1bA9MNdaLADxlAgIgNXmD4Cx1TRdszRj+mqumG5NXrsTIlen8jJJDiKIIKJWOf2KZrM7X+pKSEqxfvx6tW7dGSEgIAKCsrAzDhw9Hz549sWnTJshkMnz44YcYN24cdu/eDYlEgkmTJmHixIlYsmQJFAoFfv/9dwiCgKFDh+Kvv/7Crl27sG7dOgBAQIDhJCITJkzAvHnzMH/+fPj5+QEADhw4gAsXLmDChAlQq9Vo1qwZli9fjpCQEBw9ehQvvfQSwsPDMXTo0Dq9XlEUMWHCBAQHB2P16tUICAjAqlWrMGrUKBw8eBDBwY2nEZTBvxOJoqjTQuVfVQZBFKGQyswu8mULN8uqL0Qpp/IQHeyF7ybehhlr/9YLGIqr1Bi5Mh2hvnKHT9oj26nvQk6mHi8IAiZ9th/nbpRwcqUdaOr+x3TLhuncLFPadH0PW5IIqFMwYu0Y/r4xAUhKyzVaD2oR2pXQ+Rklu1MqUfbttw5/Wp9HHgHkxtcNqu2XX35Bq1atAFQH+uHh4fj+++8huXUzvXHjRkgkEnz00Ufa7/Enn3yCdu3aYf/+/ejWrRuKioowaNAgtG7dGgDQvn177fl9fX0hlUoRHm66QXHkyJF4++23sXnzZowfPx4AsHr1asTFxSE2NhYA8Oqrr2qPj46OxtGjR7Fp06Y6B//79u3DX3/9hfT0dHh6egIA5s6di61bt2Lz5s149NFH63ReV8Tg38Gqf8QzcfDyX6isUkIqEbQT7gIrSwAARR6+gANz6Wvyf7/500WUGGkpVIlAthMm7VHd1WxdrW/mE2OPX5+ai59P50GpFlGu0I+yauaWt+fkysa88JyxujfFVQN/ACisUGLEilNWt7hbMoa/5urjmlXTL+VXmKwPTgAm+kefPn20w2AKCgqwYsUKjBs3Dtu2bUPLli2RmpqKCxcuaAN7jYqKCly8eBEDBw7EuHHjMHbsWPTv3x/9+vXD0KFDzQb7tQUGBuLBBx/E6tWrMX78eJSUlGDLli145513tMesXLkS33//Pa5evYry8nIoFAp06tSpzq89NTUVpaWl2puL2q+tMWHw70DmfsQDqkoB3Ar+HUwEcO5mhdnj+EPpfKYCXWOt8wqVWK/MJ8ZaXUXAbKpJTW55ADbtNapvT0ZDYazuG6oKpWh1i7slY/g1w4nKFGrt56JKpYKntHphwgql2mQvgObmoTHfSJITyWTVrfBOeF5r+Pj4ICYmRvt3165d0aZNG3z33XeYPXs21Go1unbtis8//1zvsaGhoQCqewKmTZuGnTt3YuPGjXjvvfewbt06xMXFWVWWhx9+GCNHjkRGRgYOHDgAABg2bBgAYNOmTXjrrbfw9ttv484774Svry8+++wz/P7770bPJwiC3jwfZY2hWGq1GuHh4UhOTtZ7bGCg9euquDIG/w5k7kc8oKoMAFDohODfGoZa2ci+LAl0TbXuCwKsajUFqoOg0ioVlh/KrB4+UY/y18wtb4teI3fI4a4JQp2xoJ8jWNOQYMkYfkEAyhRqTF97FhfzKmplIhLNZkHKL1dgxIpTUKrVkEslGNwpDxO7BsJHzrwYVH+CIFg1/MZVCIIAiUSC8vJyAECXLl2wadMmNG3aFP7+/kYf17lzZ3Tu3BnPPfccHnjgAWzYsAFxcXHw8PCA2syNvEZ8fDyio6ORmJiIffv2YejQodrx/4cOHcKdd96Jxx57THu8udb50NBQZGdna/8uLi7WmajcpUsX3LhxAzKZDFFRURaVsaFi8O9Amh9xD5UC8dfS4KPUbWkPVlfA11MKaVAQmvl74O5oP1QpRfx0Ot/keT0k1cNyHJm5o66T9sh6lga6psZEm8vJWLPV9LN9V7HtTAEqFLZLK6sph6khQJrPkyWfq8aaw732TZ5UEFBY4YRJgg5iSUOC5vPQNyYA61NzjX4mKxRqfLbvmoHA/9Z5zJRF0yOh8c3Bi9hz2qtR3EgSWaqqqkobIBcWFuKrr75CaWmpNrXmyJEj8dlnn+HRRx/Fq6++imbNmuHatWv48ccf8dRTT0GhUODbb7/F4MGDERERgXPnziEjIwNjxowBALRs2RKXLl3CiRMn0Lx5c/j5+WnH19cmCALGjx+PJUuWoKCgAHPmzNHua926NdauXYudO3ciOjoa69atw59//mkyaI+Pj0diYiIGDx6MwMBA/Pe//9XOZQCA/v37Iy4uDpMmTcKbb76Jtm3bIisrC7/++iseeOABdOvWrb7V6zIY/DtIzW7rsLJ8NCvN1TvG20OKMXdEYOKIuyG5dXcLAMevlphciEspAiO7hOLhO8Lw/KbzuJRfafcxv1KJwMDfQSwJdGf1i6xXC7FUIqBMocbUNWdwKb+yvkU2qvYQIKD69e05X2jRirCa4NhUT0RD7Zmqy9j+xkCTcrPm9aS0SoWlB65j3wXd1YN9PSQoMbIoWUmVCtvPWL/gmTEN/UaSqC527tyJzp07AwD8/PzQrl07fPnll+jTpw+A6mFBmzZtwv/93/9hypQpKCkpQUREBPr16wd/f3+Ul5fj77//xpo1a5Cfn4/w8HA89thjmDRpEgBgyJAh+PHHHzFixAgUFhbik08+wbhx44yWZ9y4cVi4cCHatm2Lu+++W7t90qRJOHnyJKZPnw5BEDB8+HBMmTIFv/76q9FzPffcc7h06RIefvhhBAQE4NVXX9Vp+RcEAT/88APmz5+PWbNm4ebNmwgLC0PPnj3RtGnTetWrqxFEJjo2KScnBwqFZSvgmjNixSlkFVchqigLfa+losDLH0fDbtPub+onx7Ipd0CoEfgDwEd7rpjMWgFUZ9CIDvbComFt8N3xbOzLKIJSXT1sw5LsINaQCNU3G67+gygIApo1a4bMzMwGnc9b87kxRiIAwd5S5Jer6nTTp3k/AWBdqv5NqT0IAKKCq1t7LudXGgzYNJ9pTcurJjg21rJbU6iPDJse79SgblA/2nMFSan1G17VEEkEIMzPA31jAjCxRzhWHMnElvQ8KNX6xwkCoHJwBTXz90DSFNusbkr67HGdlsvlTg/WMjIyTA6LIbKX4uJinXkbhrhEy/+2bduQkpKCgoICREZGYvLkyejQoYPR4/fu3YuUlBRkZmbCx8cH3bp1wyOPPGLwi7Z//358/PHHiIuLwyuvvGLPl2GWJvWccCt0qZTIccO3OneuRAD6dwnVC/xLq1RQqNSQCKYzeGhaqb47no3n+7fUTlwbuTIdZQrjgaMxwq0y1R5KJBGAVsFe2lZbsi9LJjqqReBmmfl87lKhuuW95ueo5vv5yPen61lay4mA2R6G2i2vn+27hgt55ielA3XL4e7sYWyNdWy/OZqUm+tSc7HxxE0ojFzoLBm+VhdSA9e5mjjEkYgaG6fPZDpw4ABWrlyJESNGYMGCBejQoQPmz5+vs5pcTadPn8ann36KgQMH4sMPP8QLL7yA8+fPY8mSJXrH5uTk4NtvvzV5I+FI03s1R3Swl7bSxVs/JsYCak1LZ8pJ/VYwQwytnmkucDREIgCjuoZi6/TOGN01FM38PdDUV45m/h4Y2SUUSzkGtt4sbeGyZKKjpWQSwFMmgY9cglBfmc776SOXQKFyvQWhNJ/p0ioVNqfftOpxmowy09ee1VsBVsPSFWPtzZKbPIkDY09nhbnGAn97MjdXSqjjugRERK7K6S3/W7ZsQUJCAu655x4AwOTJk5Gamort27djwoQJesefPXsWYWFhePDBBwEAYWFhuPfee5GSkqJznFqtxieffIIxY8bgr7/+Qmlpqf1fjBm+HlIsG9Me6zfmQ5InR4lndQAWb6R1si4p/mq2UtUlcKx5I+LrIdXpReAPYP3UNTWlucWKLFWpAqBSQwAQ5ifH8rGxOs8rl0oB2CbolUsEmwVySrWIJfuv1Wm4h6lx266UMciS72qorxy+HlKzeettISbEEx0ifLElPc+i433kEvh6SCERgHKFCiWVar3rlrneS1cV4MmGDiJqXJwa/CuVSmRkZGjztmp06dIFZ86cMfiY2NhYJCYm4vfff0f37t1RWFiIQ4cOoXv37jrHrV+/HgEBAUhISMBff/1ltiwKhUJnbL8gCPD29tb+21b8PGV4JC4CntLWKAsMgMd9xhek2HfB+mEAMqmgM3u9b0wgktJyDP7oCgDaNPFCqUINpUqETCqgb+tATO+tH4w2xMBfU2ZXKLu5QLN2IF7TjN4tcOxKicVDXswRAVzMr8TMdWexZMw/z9s3JhDrUnPqdW6JAIzq0hQP9wjD2G/+QoUlXVZmyKQC9l0sMn+gEWqx+rv0wgDdz8Gyg5kmJ1IvP5iJ5wcYntdiq5vhmucx9V2VCECfVgGAAGQXV9l8Hk9tF/IroRSBVsGeuFxgPoFAoLcMSZM7QhCE6pvcA9exJ6MAheXVk7iBhhn4A0BRpdIlriGNlStdp4nchVOD/6KiIqjVar3FEwIDA1FQUGDwMbGxsXj22WexaNEiKBQKqFQqxMXF6eR6PX36NHbu3Kldpc4SycnJWL9+vfbv1q1bY8GCBXaZNFRRUIBiAEHBwQhs1szgMaIoQo10q84rEYD7OzVHsxrnnDOiKVKz9uPcjRK9sd5tw/yw4ck+8POUNeqW/YiICGcXAW+nnKpusa21XRNofp9aiDkPVU8qNPRebH4uAj3n7zC6AnNdnLtZgSc3nMe3j9+NL3adw/5LxfVunW3q54n/jo2DRCJBqH8GruaX16uMEgFIuC0ca45drdd5VKKAiIgInXo9ePkvkxmDDlwuwcIa36WSSiXe33YGO/7KhkIlQi4VcG+HcLw0OBZ+npZfSo2e519dkJp12OB3NSbUFyduVCAjp9Tq9yci0BN+HjKcy7G891MtAlcKKjHhrijIJAK+OXTJ5POKkKBZs2ba+n0rIgIjPt+PG8UKewzThyAAjprDn1emgl9wKPy9Gl6O9obEFa7TttRYf0/J9Vny2XP6sB/AcEGNFf7q1atYsWIFRo0aha5duyI/Px/fffcdli9fjpkzZ6K8vByLFy/GjBkzEBAQYHEZhg8fjiFDhug9f05Ojs4KcLagys2FJ6pz6JZlZho9TmJFu79EAFqFeOHhroHIrHXOz0e0wbID17H3QqFeC39xXg6K6/pCXJwgVAd8WVlZTs/2s+3kdZOri/504hpKSkuxr8Z7FN86EDNu9cKIoghvmcSmwT8AnM0uQc/5v9osQMsurkTbN7bCSyZBRIAcAuo3RzMqyBOl5WVQ1rPZOK+0EucvX9P2coiiiMoq09/ryiolrl+/rm3NnrbmjF5PQXUu+Cy9nhtjN9PmzvPx8LZYcTgT287ka3tNvGQSqNVKXLhpOCuSKRIB6NcqANN7N8dne6/qnNdbJoEIGO1FUIvAr39lYcOUTth2KhOZRcYTBwhQIysrS/v3h7uv4O/sErsE/oDjAn+getjZ/yX/YbQXyN4ac8MMYJ/rtEwmc3q2H0EQoFardXriiexNrVa7fvAfEBAAiUSi18pfWFhodCnl5ORkxMbG4qGHHgIAREdHw8vLC2+99RbGjRuHwsJC5OTkYMGCBdrHaC4o48aNw6JFiwy2MMjlcsiNrL5n68BRvDV4Wax17toX+fjWpsd6a8bZyiSCdt6Aj1yiV14fuQSz+kdiVv9Iveew5LU19B8fURSdGvyLogiFmQHruSUKJKXm1BoSlINjV4qxaFgbrDiSidxS26Sc1Sufjc+nFqsDyoyblZBJYNFkdWO6tfDFoYvmb08DPCUoqjT+REo1sPTANZ1x/1IzM2g1+0VRxNID10wOEVp64Bqm92pudk6HqfNcyKs+T3p2OSoU/4yZ19SltTTzd6b1agYfuQQvJ0Th5YQone/C0K9PmhxCpFSJUKvVJq9FEqH6WlXzvHszCu0W+DvD3oxCzOof6bDnMzc/qKFfkw1x9nXa1sLDw3Ht2jX4+/vzBoAcQq1Wo7i4GC1atDB7rFODf5lMhpiYGKSlpeGuu+7Sbk9LS8Odd95p8DGVlZWQSnXHRmu+WKIoonnz5nj//fd19icmJqKiogKTJ09GaGiojV9FXdy6wGnGxxq5yE/v1RzHrpToTfDT/KhrsrRY8yNg6bF1nZxK+iyZzGko44hmRdyJ3/2FYiMLG7m6+g7533QyD54y85/ZYhOBv8Zv5wt1gn9TE6klQvV+DVNpONVi9bmPXSkxO3nYXDrPn/6qXs27PiGQRADC/YwnEqh5DTD3udQs5mfuWlQzU5klmYsaGkem+zQ2P2h9ai5+Pp0HH7kUKlHkNdnFeXt7o0WLFsjOzm50NzbkejRJXlq0aKGdr2qK04f9DBkyBIsXL0ZMTAzat2+PHTt2IDc3F/fddx8AYPXq1cjLy8PTTz8NAIiLi8PSpUuxfft27bCfVatWoW3btggJqc6ZX3t5Z19fX4PbnebWRaBSBcwwMQl06eh2WDamPZYdvK5dtKtmK7+9LviulAXF2Wz1g1/XjD0iUK/A35ZZd5xBBFChtKB3yoJz5ZQoUFKp1I7PtzSgtSSYLaxQIqdEYXYVZnPnqe87JRGAEZ2b4IUBll3rLL0B0mQqs+RaJAgCpC7QKh3iLUWZQm3R58cccyua2/LGwFiWNxHVN7k1b3Td8ZrckHh7e6NVq1bOLgaRHqcH/71790ZxcTGSkpKQn5+Pli1bYvbs2drxevn5+To5/wcMGIDy8nL8/PPP+Oabb+Dr64uOHTti4sSJznoJ1rsV/P9yNh+XlEFGhwA89NVJBHnL0TcmAN88fJvVrfx1ZezHx12Wu7dHr4exQNOefOQSfDWuPR79/jTsnBymQVADGPb1Kfzr9hDte2lJQGtJz02VSjTZM7AvowjTeqpQZsceHM1Ny4ze5rt8Naxp0bcm9W+/NoEOWy3aGA+ZFDKpBBUl9R8uV1ShxEd7ruh8LuzVO2rNYm/uck0mItsSRPZFmZSTk6OTAtQWVH+dhld6OuaeLMOWYPPLxksEIDrYy2GtOyNWnEJWsfHJfQ1lufu6LBtvrNfDFu+BJljYl1GE7JIqh9wEmFu91B1p3sulo9vpZOnRfEYEQdALbj/ac8Vkz40l9WyvPPdSAQj1k6NfTGCdAs+an0tb9S6WVqkwYsVJi4Zj2cuwTiE4mVmGczdtkyK35jUAgF2uE6IoYujXJ5Fbal2SiYZyTTakLtdpc+RyudMn/BK5Mqe3/LsnESJEixctcmTrjiVDHBrzcveW9nrU5fVrWk5n9avbD3xdMPDXV7tn7e4oP0AQcPBiEYoqqvPSe0gFBHrJ0K9NoMn5NwKqg29LRpbYI/Bv28QLn49qZ1Wq0drssZifr4cU3z3cAcO+PuW0ib+/Xy2xyZAfjZrXAABmrxOz+kUarcua9Vy7zuuyondjviYTke0x+HcGtRoCBKsyAGiGDjzf347lgmVDHMyNf7UHR/2wmZvY+WN6ntlufnNlrcvKy2R7FUoRWcVV2HRKfxXbCqWIihKFzphqQ0OEfD0EnKtDJh5bKa1S1yvwr82W37FQXzlCfGW46YCbXEMuFxjvvawrzXVYBExeJ5LScrHrXAGkgqC9gQSApQeuY9+FIlSpVCivqk7J5yUXUK4QIQDw9pDUaWiYM67JRNRwMfh3hluNUR3CfbBVYXmLoKNad6zJgmJPdRlTa6h+NF3JhtKcalckPXgdv50vxA0z44PLFGqUKf4JKjTB4aJhbfDd8WyLy1rXCcDkWLV7fJ7v3xLTe6nw6d6r2H62AJnFzp1M4cotvoIgQN4Ib3IrlSqzKyyrRWh79tal5iI5LRciDPXEiSirccmpy8rNjrwmE1HjwODfKap/AQZ1aIKkK14WTwJ1VOuONZMA7cWajEOGbhJ6RlcP5ThyJR15JZXaoRz+nlIEeslQXKmCShQhEQRUKFQorlTXaXiCJh3nI9+fRkmlSi81n7FMHLacAOwnF1CmFHkjYSdqEdhy6qb2cz91zRlcyndea39Nrt7i2xhvcvPLVVZfK2w4+kiHdi2HnoZXiiciMoQTfs2wy4TfEyfgdfZvlDeLQGXc3dqhBPnlCqNjVCUCMLJLqMMyOthjEqA1PtpzBUmpuQa71mvWhbGbBFfStokXvhitfwNQs46rVGrklyktfg01V86t7yq69iC9NblVUy7NelpyiQCJBKhQiC5XZnP8PCRQqUWU2yuSs5Kjrwl1of1+OjDLlbvwlgloEeipbchoqHn/OeGXyPEY/Jthl+A/LQ1ef59DefNmkPXqpd1eUqnEjHV/m1zUyxkXdWcMK7A045CpmwRX0jrkn+wyxoYm/W/XFWw8edNJJdQnEaoD3pIqdb0CN4kARAV54uPhbTFr43mXvlFrKJx9TbBGzWF1uaUKTkK3AQkAX08JSmr1WGq+a8vHxtr1c2HL3wQG/0SOx+DfDLsE/6lp8Dp3DuUtmkPWs6fOPme3uLsCS9LdNfWVY+NjHTFyZbrJmwRXomn9rp1JBqjOMuQKwyMkAuAhlSDQW4p+MYGY2CO8OmA3cEMaFeSJbi38cPhSMZRqEWUKNUqrVEbPGxPihYybDPzrw1smQYifJ/pE+2Far2YN7ppQUqnE8kOZ2uubVAB6tvKHQgVsO5NX7xWhqZqPXKKznoUt2GtdAwb/RI7H4N8MewT/ytRUeJ87j/IWLSDrebfR41x1Ip8jmGv5j/D3QNLk2x2WMtMeJALQMsgTahG4UuD8MeQh3lJsntoZgG7WF0tvSMd8expX88uNnt9eee7dgQBgVNfqIT7Nmze3aaDkLDWvb87uwfOSCdULtTXsKtVhy/Vh6rr+iaW/Ybb+TDP4JzKNE36dQZNH38w1sT6BvyvfOFhSNkszDjXklJnVmWScH/RreMikBt8XS/PAK8yN5zCzmwuSGRcd7InpvZq77He6Lmq+FmtWtbWWueFrEgH4d8cm+O18IbJtsBqwq7B0zQFLWLPqu6keAs0q9TWPUalFeHqcRq8oP0xvgL1ZRA0Rg39n0M7UtO0Pub26ZWsztUBNfcumOZ+pRZX8PCT47Xwhdp0rQGml4WEmjZVMArsMjbA0XaCx91oQBMil5u5mYfIGoImvHAGeUotXZPWWS1CprN98hIaiWwvfRhsUWbKwYF3JJMCQ25tgyl0RRoev1cxgti411y7lcJaaaw7U5/fA3Pon+zKKMKtf9dA/Qz0E61JzseFELoK8ZZAayrBWqkBSQTmOXSl22Er2RO6Mwb9T3LrkCbZrtbYmNWZdz68J3qtUKp1FaeRmflTMlc1YjvxFw9pgxZFMbD9TgAqlGpoe4aJKNYoq3WtwsCaIAVDnScH/6hCM9Oxyu6VwvbdDOL45eNFo62pMiBcy8gxnfZEIQH/Narprz+JCnvkbgMGxQUi9XlanTDKBnhJ4eUihVgMCRFQo1XVO92qKrXozDl8qqf9JHKAuPY7WLHqn+ax2bu6Lzadumn3f1SIglwpo6udhcJG2msPXpvdqjp9P56G4kV1baq45UJffA7VabfbmLLukCkO/PonSKjXKjaxVoFLD5IJvjlzJnsjdccy/GXYZ83/8OLwvXkJ5dBRkcXE2OaelqTHrwpJ0mqbGfn64+wo2pBkumwDA31OqlyNfMx4eAK7kV7r1JNFRnZtgRp8W8PWQorRKVec889HBnvhkeFt8dzzb5hPKBUGAf0hT/PvjPUZvLj4a1sZk66smc01plQqf7buKzafyjAbOrYKrM5oA0Avo7o72w/GrJbhiZIVXqVC9+my/NoGY1rMZ/Dxl2omov50vREG5ArboUAr3k0MQBJtMSNdMcJdIJDafHFlftuhx/GjPFZMT3r1kEgR7y7SfVQAWpxDVZAarydhNSk5JFR75/jSKGnGPoqHfg9r1Ufs9zStTOqyHzdD7ZS2O+ScyjcG/GXYJ/o8dg/elyyhvFQ1Zjx4A6j9G39LUmHVh6WS82vn3NT8eN0qq3GJohj14yyX4dWZXnW2a4Lhmj4hEADxlEqjUaqOBa+0ffXuk6zt36SqWHrhm9ObCmmxWpVUqfHZrJd2KW2OdvGQSDIoNxlPxLfSOr/l6atZRucJwi74AwM9TAh+5VCdPuuaGwNx3CqhOuWjshntE5ybYfb7QJhPSI/w9sGFKR7tkRqmPuk4ENXoeIzeGS26lya39mKUHrmHDCdM9AJobJ0s/66VVKny+7xo2p+dBaeLE3jIB/WKCsPNcPuqwMK9TNfP3wDcP36YT4EsFAf3a1Mjw5aSUvNa+X4Yw+CcyjcG/GXYJ/o8eg/fly8iPjMLyivB6j9G3JjVmXS6olgRBGs38PbBqQixmrPsbF/MqGtxCTq5mWKcQvJIQbXS/5uureV/teRNoiqGg1NzNhaU3HzUvUXX5/JrqeapNE7QuHd0OE777y+R3KthLgiAfD5NpUFMsGJpiidG3Mv24WvBvyx7HuqY5tiQz2AYrP/OanqxXE4/gx7/yjR4XHeyJ/w5pjQnfnm5Q17omPjIEeMkMXqOdvWhgXd6v2hj8E5nGMf9OIaJKpcaHe65iu5e03mP0LRkzK5UIdQqcrJ2Ml1lchUFLT1j9PKSvVbAnnoqPNHlMzffUkvdKqRYdlgnK3HOY2m/Lyev7LlieSUYz7nj5oUyz36nCSjXkMhViQrxQXKWCWg3tsKM/rpUi5eRNs88rFarHpBtb2Vvjt/OFAIAZvVtY+Eocw5KJoM/3t+xclmaVqs3SzGB18VtGocn9l/IrkZSagxBfmcnx7K4mv1yJm2WGy2su8JcIQIi3DHnl9hkKVJ/3i4gs03DzJDZkajUOnMtFVnGVydRp1ugbE6BdRKq2+vwAWjMZj2wj3F+O0V1DrV6l0543gY6kGQKSlFr9HcktVSKruApJabmYvvas0YXEDKlLJhlN0Noz2t/scTdKFMjIq4CPXIrvJ96GpCkdIZdKzM5TEQC0DvbEzzO6IMhbbrZM2SUKJKXlYtqaMyipdI0g05qbTWtZ8xmd3qs5ooO99K5/9Z3I/v62MxZN/t1/oRjyOl4jWwV7wkfu+OtrfYL2Jj5yrJ54m62T1QGoTmxQ38QDRGQeozpnEIGM3FKojST61wQf1rDXDyBQfePg2uGiZfw9Xf/jHuYrQ/KUTni+f8s6TcC1102gI1mSU9xSdb15tSZordlbAJjPWS8RqhfsWnbr5s7Ue2boeT7Ydsaictmbq9xs+npIsWxMe4zsEopm/h5o6itHM38PjOwSqp1EXhe/pGdZdJxSrUZ8a8vew5p85BIsHxuLf90eYvVjnUkqEbD8cBZUdpgQEOglc8rNEJG74bfMCURRDbVahGjiR9HaFjN7/QACwMQe4fDzaPgflYaQwk+EUK+x3Pa8CbS30irVP1lfjBxTlxtjS4PrmqQSAYcvW55eU1MuS1rDm/jIMatfpPY7aew9M/Y8v/yVbXG57M1VbjY1Q4aSpnTExsc6ImlKxzrfQAOaXg3LjpVKJJjRu/o9tOZj5ushhY9con3/G4LqdVYEJKXZZz0EuVTi8j2TRI0Bx/w7gQBAIhFMjq2sS4tZXcfMmlJapcKsjedRXOX6gbMtecsEVKpEh2cpulmmwKLfrtY59abmJrAuEyedSTPUx5JJ4tbOWzC2YJwxEgGIb+2P3edNj/c2VC7A/KrTEkF3WEvN92zv+ULcKFGY7DlQquo2lMYejNWtM282bXHds2jRulv6xgTovIemUpbqPgew6Ler2rVTzE20DfWRwt9LbtEaGPYiAjh30z6rkjeUnkmixqDhN+c2RKKImFBfoz9StrgI2qr1RDMEw934eEgtao2TCbBpl71mRU5rx7bXZOtWUEfQfM4sCWmtvTE21CsW7le9mrCxHpIZvVtYPVxIc+PWM9rf5GeisEKJEStO4aM9V7TvseY92/BYJ4T5e5h8HpnUdeZt2LPH0dnu7RButiW/VbCn9gZH8x6O7BJq9pogAKhQqLXzWvLKVGY/+4IgwdLR7TCsUwhkDfSXWypUD780+L0Lce2eSaLGhKk+zbBLqs/9ByC9dh1zLnthp2cLkwseOZs1aT4bkwh/DywZ1RbDV6Sb/FFu4iNDQtsg7LvwTyu7r4fE6Eq2lqrvwmyOVt8UlJZ+zmxRL5peA3OpJc0tPGWsfNrF6QoqTT7WWC58U88rEYBJvVph+p0hLtP6X5OjMknZW81F64ylw/zX7SF4rsbwLQ1zvVgSAfDzkFi9onTNFJilVSosO3AdP/6Vh7IGtsiAdn2NWytsy6QC7u/UHA93DbTZeH+m+iQyjcG/GfYI/hX79sEn+wby27TF8sJglx2eYcn6Aa5EJgCmMiZqVxOuUpkNykZ2CQUArEs1Pba15g9yzaBy+tqz9e6et1dOfnuoT/Bv6efMnjfGhoJWYwtPmSMRgIc6hkAulWBfRhHyyxVGU3kaupkxueBViBdSnu2P4rwclwz+Gwtji9ZJBSA+JgAzeusvMleT5sbyt/OFKKxQokolwkMqQaC3FP1iAvHb+UJkl1j+u2Lspreun1Fn07yeWf0i7bJqNYN/ItMY/Jthl+B/71743MhBxW2xkHbUDRxdzfCvT1r1I+UMTf1kqFSIKDK2tO0tAZ5SfPvwbfjueDZ+O1+I3FIFVLU+/TUDzEe+P222NVqz+FJtmhVmN53Mq/OCObZY6dJR7N3yrwkWHH1jXLt3QCJU30Ca69mpeeNWl4XXjPVKzOjdAm2jI11mka/Gqi6L1hmjeVzN/1vTqGLuprf2Z+VmmaJB3AhoPvf2WLiOwT+RaZzw6wya61uNMcWuGOCVVqlQpqjbuHNHCvSUIaPEfCu7t1yCpn4etyZFt0RJpRLLD2XqBVgTe4Rj6YFruFFiOvCXCsC0ns0M7vP1kOKVhGgcvFhc55unhpCT31ZMLdQkwHlDoAxNorckeKuZrasuC68Zm7zvLp8HV1TXuq/93lmSIlUiVGeFsqQ3uOZnpaRSiZErTzWIzGYKldplG72IGjsG/06hiXBc+6K37OB1lJj5EbF0KXjJrYmxlqbPs0aGgZzwhqhF3UDKz1OmF2Bpu9EtOGcTXzn8PI1/hURRhKqOLVnulvnCFbPG1GZN8Fbzxq2+ufAZHDU+5lYl1gyJsfa9X34o0+w121XklSsxcmU6+sYEYs4IttITOVIDzRnQwGkCQhf/Td+bUWQysA/3k2NUV/OZLXzkEvz79hAEetvpXtPC+NpUkKXZbmyBqdokAtC/TaDJYywJEmUS/WxBrhTwOkpDyxpjTX57V8mFT66jeu0U/c90ze9+XW76zF2zBQBeMqE63bTg3J8gtYhbK3fnYMTn++uc3YyIrMeWf2fQNvy7bvRvyWJFarF62Iu5HOoVSjXSMssgs9frtaD7wdIgy9zqrJpzWRqcm2vhG3L7PxNDXXHStyPZY50Ke7Gmp6Ih9GqQ42jXTjEwR8nXQ4KPhrWp03ffkmu2CPwz+VyszrpTYmXWIVtTi8C5GyVYduA6ZvWPdGJJiNwHg39n0Lb8u25wY0mrtSBUdzOXVqngIRWMZjRRi8Cl/ArEhHghp9TyyWg+cgl8PaS35h4Y/lGTCEBMiJfZCZiWBFmW/HhKBGBk51BM721ZcG4u8HsqPvJW0Ov6Aa8juXo9WLOYWkNdeI3sw9SaFqVVanx3PLtO81ssuWbXJMJ1Vj1Xi8DeC4UM/okchMG/M4iaC27dAxxHBIrmJmFqFqmxdLx9caUK0cFeZldxrZndwkcuQZlCbTz1YbAXPhjaBrM2njfY+yCTCPh3xxA82cd0aj7Ash/PMD8PPD/A8h9mawI/c+8nbw5cizU9FQ2pV4Psy1TvoloE9mUU4fn+dTu3qWu2q9OsXM3vBpH9Mfh3hjrO99WkdNubUQSlWg2ZRIK+dmw9NNVqXb1IjflVKWtSi8DS0e3w5Pq/ce6m8ew8MSFe+GL0P+O8LQmgDe5vHYA5I3tYlRPd3DCduozPrk/g5+j3nOrGmveUwY37sqR30VD2J0sZu2Y3BK60cjVRY8fg3xk0gagVXbTGstAkpeXi2JUSvVVCbcFU0P3b+UIUWdllLJUI8PWQoqTK9ONKq9R6r8VcAG1ovyAI8POUodiKMtp7fLa1gb+j33N3YU1wxdZIshVrM0VZy9A12155/wM8pfCWS2yyDoxEAPq2Np1AgYhsh8G/U1RfiQUrmv6NZaHRjKdfdvC6XfKgG8tzvutcgdXnKqpQ4qGvTiC/3HRWB3MtX+Z+GOsTqLnS+GxnveeNlTW9KOxxIXupT++iJTeita/Zi367anL4piWrntcW4CnRLphoaphR2yZeKK5Umb1BaBvmh+m9OfGdyFEY/DtDHVJ92nOcqKWsyXNuSJlCjTILGomcvbiVq4zPdoX3vLGwpheFPS5kT9b2LtbnRlQQBLPP99GwNvjueDZ+TM8zmlgBALxkEgR5SdG3TaD2uY2dWwDQ+tbwzWUHr5u9QdjwZB+rhmcSUf0wz78zWJnq05pxoo5iKnc5AHjLBHjJrA+aXS3veX0D/7q+J674njdklvSi1OVYImtZs6aF5kY0KTUXWcVVyC1V3sqNn4vpa89alBvf3PNpVj3f9HgntA7xMrjuSEyIF1Ie74gNj3XC8/1b6s3Hqn3uUV3/eS3TezVHdLD+eQVUn3fJmFiTiyUSke3xG+cMVqb6tPc40bow1ZoUFeQJEcCV/EqrztlY8p7bYsiIK77nDZk1vSjscSF7s7R30VZD/yx5vroOebRkPparDKUkomoM/p1AFK3PrWyPLDT1YeqCrlCJSDl506IUoBIBaOIjg0wiaRQ/BrYcMuJq73lDZW0vij2zsRDVZupzZI8bUXumpDV2vKsMpSSiagz+nUETzFkxbt4VVwk1dkEfseKURYE/UJ03P2ny7Y3mx8CWk3Rd8T1viKztRWGPC7kCe6cFNcden3F+d4icj2P+naEOK/xaM07UGTQXdEt+sDQ0rdeN6cfAkpY6S7n6e96QmJqjUrsXxZpjyX3Ze74Nh/4Rkb2w5d8p6vaj0RC6Ti3NBNQYW6/t0VLXEN7zhsCaXhT2uJAxjk4By6F/RGQPDP6doQ4t/7W5chBobol5H7kE/7o9pMGP76/N3i11rvyeuzprJh1ygiIZ4owUsLwRJSJ7YPDvDFam+mxoTP1gRQd5YtnY2AYVQFnT4s6WOtdlTS8Ke1yoNmcsuscbUSKyBwb/zqDJ9nNrtdzGFlg0hh+sunbvs6WuYbDmO9fYvp9UN85KAcsbUSKyNQb/TlCpUOPomRv49vpZXPe5afdxo87QkH+w6tO93xhufIhIl7Mz72g0pOsoEbkuBv8OVlqlwurjWVDlFyC7pRK5ohKAfceNOltD+8Gqb/d+Q77xISJ9zLxDRI0JU3062LKD15FXqoAoAiL++aGoGViSc9kyXWdjDgbsneqQyJUwBSwRNRZs+XewvRlFuFMz47fWD4k9x42SZVyle99VGZsLMaN3C2cXjciuOJ+HiBoLBv8OpAkshVs/HGLt6B/uHVi6AnbvG2duLsTm5yKcVjYie+N8HiJqLBj8O9A/gWV19G8o+HfXwNKVMF2nYebmQnyw7Qym3xnilLIROQLn8xBRY+ASwf+2bduQkpKCgoICREZGYvLkyejQoYPR4/fu3YuUlBRkZmbCx8cH3bp1wyOPPAJ/f38AwI4dO/Dbb7/hypUrAICYmBiMHz8ebdu2dcjrMaVvTACUfxoe9uPOgaUrYfe+YebmQvzyVzaDf3IbDPyJqKFy+oTfAwcOYOXKlRgxYgQWLFiADh06YP78+cjNzTV4/OnTp/Hpp59i4MCB+PDDD/HCCy/g/PnzWLJkifaY9PR09OnTB3PmzME777yDJk2a4J133kFeXp6jXpZR03s1R4i3DIKg2/Lv7oGlK9F074/sEopm/h5o6itHM38PjOwSiqWNMBuTJSyaC6ESOQmYiIjIxTm95X/Lli1ISEjAPffcAwCYPHkyUlNTsX37dkyYMEHv+LNnzyIsLAwPPvggACAsLAz33nsvUlJStMc8++yzOo954okncPjwYZw4cQL9+zt3Nq2vhxRjujXFiQwpjvl7QOIp57hRF8TufV2WzIWQSauHrPEGgIiIyHU5NfhXKpXIyMjAsGHDdLZ36dIFZ86cMfiY2NhYJCYm4vfff0f37t1RWFiIQ4cOoXv37kafp7KyEkqlEn5+fkaPUSgUUCgU2r8FQYC3t7f237bkKZWgf2wYeifcDgQHuX1gaS+aeq1v/fL9qdY3JhBJaTlG50Lc1yGcdWVntvpMk2msZ8dhXRM5nlOD/6KiIqjVagQGBupsDwwMREFBgcHHxMbG4tlnn8WiRYugUCigUqkQFxeHxx57zOjzfP/99wgJCUHnzp2NHpOcnIz169dr/27dujUWLFiApk2bWveiLHDTzw/qsjKEhYdBFhpq8/OTrogIZqGxhTkjmiI1az/O3SjRmwvRNswPLw6OhZ+n0zsT3QI/047BenYc1jWR47jEL7WhO35jrQBXr17FihUrMGrUKHTt2hX5+fn47rvvsHz5csycOVPv+E2bNmH//v14++234eHhYbQMw4cPx5AhQ/SePycnB0ql0tqXZFJlcRF8pTLcyMmBUKO3gWxLEAREREQgKyuLQ1Fs5PMRbbDswHXsvVAIpUqETCqgb+tAzOjTAn6eMta1nfEz7RisZ8exR13LZDK7NNwRNRZODf4DAgIgkUj0WvkLCwv1egM0kpOTERsbi4ceeggAEB0dDS8vL7z11lsYN24cgoODtcempKQgOTkZb775JqKjo02WRS6XQy6XG9xn64u/qBYB6a2En/xhsTtR5ERUW/GRSzCrfyRm9Y/UmQuh+T/r2jFYz47BenYc1jWR4zg1249MJkNMTAzS0tJ0tqelpSE2NtbgYyorK/V6BSS3JiLWvHCkpKQgKSkJr7/+Otq0aWPjkteTeCtrCsc4UgPGMbpEREQNj9NTfQ4ZMgS//vordu7ciatXr2LlypXIzc3FfffdBwBYvXo1Pv30U+3xcXFxOHLkCLZv347s7GycPn0aK1asQNu2bRESUp1jfNOmTUhMTMTMmTMRFhaGgoICFBQUoKKiwimvUY/mHoXBExERERE5kNPH/Pfu3RvFxcVISkpCfn4+WrZsidmzZ2vH6+Xn5+vk/B8wYADKy8vx888/45tvvoGvry86duyIiRMnao/Zvn07lEolPvzwQ53nGjVqFMaMGeOYF2aKpoeCwT8REREROZAgcpCdSTk5OTopQG2h8vvv4efljar7B0Pw9bXpuekfgiCgWbNmyMzM5FhSO2NdOwbr2TFYz45jj7qWy+Wc8EtkgtOH/bilWxc4tvsTERERkSMx+HcGjvknIiIiIidg8O8MzPZDRERERE7A4N8JRE74JSIiIiInYPDvYDoTmhj8ExEREZED1TnV57Vr15Ceno7i4mIkJCQgKCgIeXl58PPzg4eHhy3L2Lgw+CciIiIiJ7E6+Fer1Vi6dCl2796t3datWzcEBQVh2bJlaN26NcaOHWvLMjYuDP6JiIiIyEmsHvazYcMG7Nu3D4888gg++OADnX3du3fHn3/+aauyuQ3mkSYiIiIiR7C65X/37t0YOXIkhgwZArVarbMvLCwMN27csFnhGqVbgX6VSo1P9l7DnkulUKrVkEkk6BsTgOm9msPXQ+rkQhIRERFRY2R18J+Xl4f27dsb3CeXy1FRUVHvQjVqajUUKhEbj17GhhatoJD8E+gnpeXi2JUSLBvTnjcARERERGRzVg/7CQwMNNq6f/36dYSEhNS7UI3d8SvFyCutgqrWmH+1CFzKr8Cyg9edVDIiIiIiasysDv67d++ODRs2IC8vT7tNEASUlZVh69at6NGjh00L2OiIIi7nV0AU/1notya1COzLKHJ4sYiIiIio8bN62M+YMWPwxx9/4Pnnn0fHjh0BAD/88AOuXLkCqVSKUaNG2byQjYmoVkN9K+oXYTjbj1ItQhRFCMwGREREREQ2ZHXLf1BQEN577z306dMHFy5cgEQiwaVLl9CtWze888478PPzs0c5Gw0BgMRMTC+VCAz8iYiIiMjm6rTIV1BQEKZPn27rsriNqGAvnMouM5jnXyIAfWMCnFAqIiIiImrsrG75p3oSRfRo6Y8QP0+9HgCJALQK9sL0Xs2dUzYiIiIiatSsbvn//PPPTe4XBAEzZ86sc4EaPbUacqmAkT0isVXphYy8iuqZvwIQE+KFD4a2YZpPIiIiIrILq4P/U6dO6W0rKSlBRUUFfHx84Ovra5OCNWYKlYiU368ho0UbaJdJE4GMvArM2nieef6JiIiIyC6sDv4/++wzg9tPnjyJL7/8Ei+88EK9C9WoiSKOXynGzVIF1LV21czz/3z/lk4pHhERERE1XjYb89+pUyfcf//9WLFiha1O2SiJt/L81w78NZjnnxxJFA2tNkFERESNVZ2y/RgTGRmJ77//3panbHQ0ef6N5fgHmOef7Ku0SoVlB69jb0YRlGo1ZBIJ+sYEYHqv5hxuRkRE1MjZNPhPT09HQADTVJoiCALz/JPTlFapMH3tWVzK0+19SkrLxbErJZxvYgRvxomIqLGwOvhfv3693jaFQoFLly7hzz//xEMPPWSTgjVaajWigr1wM1dhcDfz/JM9LTt4XS/wBzjfxBD2kBARUWNkdfC/bt06/ZPIZAgLC8OYMWMY/JtzK8//pYpSSITqoEuDef7J3vZmFJmdb/J8f4cWySWxh4SIiBorq4P/NWvW2KMcbqG0SoVvD16DT1oOyuRe8JJVz7f28ZBALpEgnq2KZEeiKEKpNhb6V+N8k2rsISEiosbKpmP+yThNS2LR9TzcX6lCmVqFMoUaEgEI85Nj+dhYBv1kV4IgQCYxneCL802qsYeEiIgaK5ul+iTTNC2JmtSKmmw/ahG4XFCJZQevO7N45Cb6xgQYnXDO+SbVrOkhISIiamgsavkfO3asxScUBAGJiYl1LlBjpWlJNBR3sSWRHGV6r+Y4dqUEl/IrON/ECPaQEBFRY2ZR8D9y5Ej+0NVDzZZEAbda/mvVJ8dakyP4ekixbEx7LDt4HfsyiqBUi5BJBM43qaVvTACS0nJ1bpA02ENCREQNmUXB/5gxY+xdjkatZktimcwLqU3bQiHRrXq2JJKj+HpI8Xz/lni+P/PXG8MeEiIiaqw45t9BNGOty+ReOBnaBmdCorX72JJIzsLA3zBND8nILqFo5u+Bpr5yNPP3wMguoVjKNJ9ERNSA1Tnbz+XLl3Ht2jVUVVXp7evfn4PXa2NLIlHDwh4SIiJqjKwO/isrK7Fw4UKcPHnS6DEM/vXpjLW+UAQREghQI741x1oTuToG/kRE1FhYHfwnJSXhxo0bePvtt/H222/jxRdfhLe3N3755RdcvnwZs2bNskMxGwdNS+ILAwREREQgKyuL6QKJiIiIyGGsHvN/9OhRDB06FLGxsQCA0NBQdO7cGS+88AJat26N7du327yQjRFbEomIiIjI0awO/nNyctCiRQtIbmWvqTnmv2/fvjh69KjtSkdERERERDZjdfDv6+uLyspKAEBgYCAyMzO1+5RKpXYfERERERG5FquD/6ioKFy/fh0A0LFjRyQnJ+P06dM4d+4ckpKSEB0dbeYMRERERETkDFYH/wMHDkRFRQUAYPz48aisrMScOXPwxhtvICcnB48++qjNC0lERERERPVnUbaflStXIiEhAVFRUejdu7d2e1hYGD7++GOcPHkSgiAgNjYWfn5+dissERERERHVnUXB/9atW7F161bExMQgISEBffr0gY+PDwDAy8sLcXFxdi0kERERERHVn0XDfj7++GMMHToUBQUF+PLLLzFjxgx8+umnSE9Pt3f5iIiIiIjIRixq+Y+IiMCECRMwbtw4pKamYteuXTh48CD27t2LsLAwJCQkoH///ggJCbF3eYmIiIiIqI6sWuFXIpGge/fu6N69O0pKSrB3717s3r0biYmJWLt2Lbp06YKEhATcfffd9iovERERERHVkVXBf01+fn544IEH8MADD+DSpUvYtm0bfv31V6SmpiIxMdGWZSQiIiIiIhuoc/CvkZGRgV27duHQoUMAgICAgHoXioiIiIiIbK9OwX9xcTH27t2LXbt24fLly5BIJOjatSsSEhLQo0cPW5eRiIiIiIhswOLgXxRF/PHHH9i9ezeOHz8OpVKJ8PBwjBs3DgMGDEBwcHCdC7Ft2zakpKSgoKAAkZGRmDx5Mjp06GD0+L179yIlJQWZmZnw8fFBt27d8Mgjj8Df3197zKFDh7BmzRpkZ2cjPDwc48ePx1133VXnMhIRERERNXQWBf+rV6/Gb7/9hvz8fHh4eKBXr15ISEjA7bffXu8CHDhwACtXrsTUqVMRGxuLHTt2YP78+fjoo48QGhqqd/zp06fx6aefYtKkSYiLi0NeXh6WL1+OJUuW4OWXXwYAnD17FosWLcLYsWNx11134ciRI/joo48wb948tGvXrt5ltidRFCEIgrOLQURERESNkEXB/6ZNmxATE4MRI0YgPj5eu8CXLWzZsgUJCQm45557AACTJ09Gamoqtm/fjgkTJugdf/bsWYSFheHBBx8EUL3K8L333ouUlBTtMT/++CO6dOmC4cOHAwCGDx+O9PR0/Pjjj5g1a5bNym4rpVUqLDt4HXsziqBUqyGTSNA3JgDTezWHr4fU2cUjIiIiokbCokW+Fi5ciPfeew+DBg2yaeCvVCqRkZGBrl276mzv0qULzpw5Y/AxsbGxuHnzJn7//XeIooiCggIcOnQI3bt31x5z9uxZdOnSRedxXbt2xdmzZ21WdlsprVJh+tqzSErNRVZxFXJLlcgqrkJSWi6mrz2L0iqVs4tIRERERI2ERS3/0dHRdnnyoqIiqNVqBAYG6mwPDAxEQUGBwcfExsbi2WefxaJFi6BQKKBSqRAXF4fHHntMe0xBQQGCgoJ0HhcUFGT0nACgUCigUCi0fwuCAG9vb+2/bUlzPkEQsOxgJi7lVUBd6xi1CFzKr8Dyg5l4fkBLmz4/4B7Di2rWM9kX69oxWM+OwXp2HNY1kePVO9WnLRj60hu7EFy9ehUrVqzAqFGj0LVrV+Tn5+O7777D8uXLMXPmTKPPYS7YTU5Oxvr167V/t27dGgsWLEDTpk2teCXWiYiIwMHL6XqBv4ZaBA5cLsHCZs1s8nwllUq8v+0MdvyVDYVKhFwq4N4O4XhpcCz8PF3io2AXERERzi6C22BdOwbr2TFYz47DuiZyHKdGfAEBAZBIJHot8oWFhXq9ARrJycmIjY3FQw89BKC6V8LLywtvvfUWxo0bh+DgYIOt/KbOCVTPCxgyZIj2b82NQk5ODpRKZR1enXGCICAiIgKZmZmorDJ97soqJa5fv17vVpHSKhWmrTmj18vwzcGL2HM6C8vHxja6+QWaes7KyoIois4uTqPGunYM1rNjsJ4dxx51LZPJ7NpwR9TQOTX4l8lkiImJQVpamk4azrS0NNx5550GH1NZWQmpVDdIlUiqpy5oLhzt27fHiRMndIL5tLQ0tG/f3mhZ5HI55HK5wX32vPhLJaaDes3++pZh6YFrJocXLT1wDc/3t/3wIlcgiiJ/wB2Ede0YrGfHYD07DuuayHEsmvBrT0OGDMGvv/6KnTt34urVq1i5ciVyc3Nx3333AahOM/rpp59qj4+Li8ORI0ewfft2ZGdn4/Tp01ixYgXatm2LkJAQAMCDDz6I1NRUbNy4EdeuXcPGjRtx4sQJ/Otf/3LKazSlb0wAjMX/EqF6vy3szSgyObxoX0aRTZ6HiIiIiFxXnVv+y8rKcPbsWRQXF6N79+7w8/Or03l69+6N4uJiJCUlIT8/Hy1btsTs2bO1XXb5+fnIzc3VHj9gwACUl5fj559/xjfffANfX1907NgREydO1B4TGxuLWbNmITExEWvWrEFERARmzZrlkjn+p/dqjmNXSnApvwLqGo0eEgFoFeyF6b2a1/s5RFGEUm0s9K+mVItuMQmYiIiIyJ0JYh362davX49NmzahqqoKAPDee+8hJiYG8+bNQ5cuXTBs2DBbl9NpcnJydLIA2YIgCGjWrBkyMzMhiqI2z/++jCIo1SJkEgHxNs7zP2LFKWQVVxndH+HvgQ1TOtrkuVxF7Xom+2FdOwbr2TFYz45jj7qWy+Uc809kgtUt/9u2bcP69esxaNAgdO/eHf/973+1++644w4cOXKkUQX/juDrIcXz/Vvi+f72S8HZNyYASWm5Or0LGrYcXkRERERErsvq4P/nn3/GkCFDMHHiRKhrDSXR3L1T3dlr2I0jhhcRERERkWuzOvi/ceOG3oq8Gt7e3igrK6t3ocj2fD2kWDamvd2HFxERERGR67I6+Pfx8UFhYaHBfTdu3EBAAIePuCpHDC8iIiIiItdldarPTp06YdOmTaioqNBuEwQBKpUKv/zyi9FeAXItDPyJiIiI3I/VLf9jx47F7Nmz8cILL2gX5vr5559x8eJF5Obm4vnnn7d5IYmIiIiIqP6sbvmPiIjA//3f/6FFixbYtm0bAOC3336Dv78/5s6di9DQUJsXkoiIiIiI6q9Oi3xFRkbijTfegEKhQHFxMfz8/ODh4WHrshERERERkQ1Z3fJ//PhxbYpPuVyOkJAQBv5ERERERA2A1S3/CxcuRGBgIPr164cBAwYgMjLSHuUiIiIiIiIbszr4f+2117B7925s3boVmzdvRtu2bTFw4ED06dMH3t7e9igjERERERHZgNXBf/fu3dG9e3eUlpZi37592LNnD5YvX45Vq1bhrrvuwsCBA9GpUyd7lJWIiIiIiOqhThN+AcDX1xeDBw/G4MGDcfXqVezevRt79uzB/v37kZiYaMsyEhERERGRDVg94bc2URRx8+ZN5ObmoqysDKIo2qJcRERERERkY3Vu+c/KytK29ufl5SEkJARDhgzBwIEDbVk+IiIiIiKyEauD/127dmH37t04ffo0ZDIZ4uLiMHDgQHTp0gUSSb07EoiIiIiIyE6sDv6XLFmCVq1aYcqUKYiPj4efn589ykVERERERDZWpzz/0dHR9igLERERERHZkdXjdBj4ExERERE1TBa1/K9fvx4JCQkICQnB+vXrzR4/atSoeheMiIiIiIhsy6Lgf926dejWrRtCQkKwbt06s8cz+CciIiIicj0WBf9r1qwx+G8iIiIiImo4mJuTiIiIiMhNWB38jx07FufOnTO4LyMjA2PHjq13oYiIiIiIyPZs2vKvVqshCIItT0lERERERDZi0+A/IyMDPj4+tjwlERERERHZiEUTfn/66Sf89NNP2r//97//QS6X6xxTVVWFwsJC9OzZ07YlJCIiIiIim7Ao+A8ICEBkZCQAICcnB+Hh4Xot/HK5HFFRUXjwwQdtX0oiIiIiIqo3i4L/+Ph4xMfHAwDmzp2LqVOnokWLFnYtGBERERER2ZZFwX9Nc+bMsUc5iIiIiIjIzqye8Ltr1y6sXbvW4L61a9diz5499S4UERERERHZntXB/9atW+Hn52dwX0BAALZu3VrvQhERERERke1ZHfxnZWWhZcuWBvdFRkYiMzOz3oUiIiIiIiLbq1Oe/7KyMqPb1Wp1vQpERERERET2YXXwHxUVhf379xvct2/fPkRFRdW7UEREREREZHtWB//3338/Dh8+jE8//RR///038vLy8Pfff+Ozzz7D4cOHcf/999ujnEREREREVE9Wp/qMj4/HtWvXsHHjRuzdu1e7XSKRYOTIkejbt69NC0hERERERLZhdfAPAGPHjsXAgQORlpaGoqIiBAQEoGvXrmjatKmty0dERERERDZSp+AfAMLCwnDvvffasixERERERGRHdQr+FQoFdu/ejVOnTqGkpASPP/44mjVrhqNHjyIqKgrh4eG2LicREREREdWT1cF/UVER5s6di6tXryIoKAgFBQUoLy8HABw9ehSpqamYOnWqzQtKRERERET1Y3W2n++++w5lZWV477338Pnnn+vs69ixI9LT021WOCIiIiIish2rg//ff/8dY8aMQUxMDARB0NnXpEkT3Lx502aFIyIiIiIi27E6+C8vLzea1UepVHKFXyIiIiIiF2V18B8WFoazZ88a3Hfu3Dk0b9683oUiIiIiIiLbszr4j4+Px6ZNm3D06FGIoggAEAQB586dw9atW7nIFxERERGRi7I628/QoUNx5swZvP/++/D19QUAvPvuuyguLka3bt3w4IMP2ryQRERERERUf1YH/zKZDLNnz8aBAwfw+++/o7CwEP7+/ujRowd69+4NicTqzgQiIiIiInKAOi3yJQgC+vTpgz59+tikENu2bUNKSgoKCgoQGRmJyZMno0OHDgaP/eyzz7Bnzx697ZGRkfjwww+1f//444/Yvn07cnNzERAQgLvvvhsTJkyAh4eHTcpMRERERNTQ1Cn4t6UDBw5g5cqVmDp1KmJjY7Fjxw7Mnz8fH330EUJDQ/WOnzJlCh5++GHt3yqVCi+//DJ69uyp3bZ3716sXr0aM2fORPv27ZGZmaldk2Dy5Ml2f01ERERERK7IouB/7ty5mDp1Klq0aIG5c+eaPFYQBPj5+SE2NhaDBg2CXC43efyWLVuQkJCAe+65B0B1cJ6amort27djwoQJesf7+PjAx8dH+/eRI0dQWlqKgQMHaredPXsWsbGxiI+PB1CdoahPnz44d+6cJS+XiIiIiKhRsrrlXxRFvcW9au/Pzs7G0aNHceXKFTzxxBNGj1UqlcjIyMCwYcN0tnfp0gVnzpyxqDw7d+5E586dddYeuO2227B3716cO3cObdu2RXZ2Nv744w/079/f6HkUCgUUCoX2b0EQ4O3trf23LWnOZ+vzki7Ws+Owrh2D9ewYrGfHYV0TOZ5Fwf+cOXO0/3777bctOvHOnTuxevVqk8cUFRVBrVYjMDBQZ3tgYCAKCgrMPkd+fj7+/PNPPPvsszrb+/Tpg6KiIrz55psAqocGDRo0SO8mo6bk5GSsX79e+3fr1q2xYMECowua2UJERITdzk3/YD07DuvaMVjPjsF6dhzWNZHj2G3Mf4cOHXDHHXdYdKyhO35LWgF2794NX19f3HXXXTrbT506hQ0bNmDq1Klo164dsrKysGLFCgQFBWHUqFEGzzV8+HAMGTJE7/lzcnKgVCoteh2WEgQBERERyMrK0q6VQLbHenYc1rVjsJ4dg/XsOPaoa5lMZteGO6KGrk7Bv1qtxoEDB3Dq1CkUFxfD398fHTt2RK9evSCVSgEAzZo1w5NPPmnyPAEBAZBIJHqt/IWFhXq9AbWJoohdu3ahb9++kMl0X8aaNWvQr18/7TyCqKgoVFRUYNmyZRgxYoTBdKRyudzo/AR7XfxFUeQPiwOwnh2Hde0YrGfHYD07DuuayHGsDv6Lioowf/58XLhwARKJBP7+/iguLsbOnTuxefNmvPHGGwgICLDsyWUyxMTEIC0tTaf1Pi0tDXfeeafJx6anpyMrKwsJCQl6+yorK/V6DiQSCS8sREREROTWrA7+V61ahevXr+OZZ57RLuql6QlYvnw5Vq1ahWeeecbi8w0ZMgSLFy9GTEwM2rdvjx07diA3Nxf33XcfAGD16tXIy8vD008/rfO4nTt3ol27doiKitI7Z48ePfDjjz+idevW2mE/a9asQVxcHBchIyIiIiK3ZXXwf/z4cYwbN06bRhOoblWPj49HYWEh1q1bZ9X5evfujeLiYiQlJSE/Px8tW7bE7NmzteP18vPzkZubq/OYsrIyHD582GjO/pEjR0IQBCQmJiIvLw8BAQHo0aMHxo8fb92LJSIiIiJqROqU6jMyMtLgvpYtW9ZpaM3gwYMxePBgg/ueeuopvW0+Pj747rvvjJ5PKpVi9OjRGD16tNVlISIiIiJqrKweA9O5c2ecOHHC4L60tDR07Nix3oUiIiIiIiLbs6jlv6SkRPvvUaNG4f3334darUZ8fDyCgoJQUFCAvXv34siRI3jppZfsVlgiIiIiIqo7i4L/xx9/XG/bli1bsGXLFr3tr776KtasWVP/khERERERkU1ZFPxrJtASEREREVHDZVHwP2bMGHuXg4iIiIiI7KxOK/yKooji4mIIggA/Pz/2ChARERERNQBWBf9nz57Fxo0bcfLkSVRWVgIAPD090alTJwwfPhzt2rWzSyGJiIiIiKj+LA7+t23bhpUrVwIAYmJitItw5eTk4I8//sAff/yByZMnG83XT0REREREzmVR8H/27FmsWLEC3bt3x9SpU9GkSROd/Tdv3sTy5cuxcuVKtGnTBm3btrVLYYmIiIiIqO4sWuRry5YtaNeuHV5++WW9wB8AmjRpgldeeQVt27ZFSkqKzQtJRERERET1Z1Hwf/r0aQwePBgSifHDJRIJBg0ahNOnT9uscEREREREZDsWBf8lJSUIDQ01e1zTpk11VgMmIiIiIiLXYVHw7+/vj5ycHLPH5ebmwt/fv96FIiIiIiIi27Mo+I+NjcX27duhVquNHqNWq/Hzzz/jtttus1nhiIiIiIjIdiwK/ocMGYK///4b77//PvLz8/X25+Xl4f3338f58+fx73//2+aFJCIiIiKi+rMo1Wf79u0xadIkrFq1Ck8++STatGmDsLAwAMCNGzdw/vx5iKKIyZMnM80nEREREZGLsniRrwceeACtW7fGxo0bcerUKfz9998AAA8PD3Tt2hXDhw9HbGys3QpKRERERET1Y3HwDwC33XYbXnvtNajVahQXFwOongxsKgUoERERERG5BquCfw2JRILAwEBbl4WIiIiIiOyITfZERERERG6CwT8RERERkZtg8E9ERERE5CYY/BMRERERuQkG/0REREREboLBPxERERGRm2DwT0RERETkJhj8ExERERG5CQb/RERERERugsE/EREREZGbYPBPREREROQmGPwTEREREbkJBv9ERERERG6CwT8RERERkZtg8E9ERERE5CYY/BMRERERuQkG/0REREREboLBPxERERGRm2DwT0RERETkJhj8ExERERG5CQb/RERERERugsE/EREREZGbYPBPREREROQmGPwTEREREbkJBv9ERERERG6CwT8RERERkZtg8E9ERERE5CYY/BMRERERuQkG/0REREREbkLm7AIAwLZt25CSkoKCggJERkZi8uTJ6NChg8FjP/vsM+zZs0dve2RkJD788EPt36Wlpfjhhx9w5MgRlJaWIiwsDI888gjuuOMOu70OIiIiIiJX5vTg/8CBA1i5ciWmTp2K2NhY7NixA/Pnz8dHH32E0NBQveOnTJmChx9+WPu3SqXCyy+/jJ49e2q3KZVKvPPOOwgICMALL7yAJk2a4ObNm/Dy8nLIayIiIiIickVOD/63bNmChIQE3HPPPQCAyZMnIzU1Fdu3b8eECRP0jvfx8YGPj4/2b03L/sCBA7Xbdu7ciZKSEvzf//0fZLLql9i0aVM7vxIiIiIiItfm1OBfqVQiIyMDw4YN09nepUsXnDlzxqJz7Ny5E507d9YJ7o8fP4527drhq6++wrFjxxAQEIA+ffpg2LBhkEgMT3NQKBRQKBTavwVBgLe3t/bftqQ5n63PS7pYz47DunYM1rNjsJ4dh3VN5HhODf6LioqgVqsRGBiosz0wMBAFBQVmH5+fn48///wTzz77rM727Oxs5OTkID4+HrNnz0ZmZia++uorqNVqjBo1yuC5kpOTsX79eu3frVu3xoIFC+zaYxAREWG3c9M/WM+Ow7p2DNazY7CeHYd1TeQ4Th/2Axi+47ekFWD37t3w9fXFXXfdpbNdFEUEBARgxowZkEgkiImJQX5+PlJSUowG/8OHD8eQIUP0nj8nJwdKpdKal2OWIAiIiIhAVlYWRFG06bnpH6xnx2FdOwbr2TFYz45jj7qWyWQc6ktkglOD/4CAAEgkEr1W/sLCQr3egNpEUcSuXbvQt29f7bh+jaCgIMhkMp0hPi1atEBBQQGUSqXe8QAgl8shl8uNPpc9iKLIHxYHYD07DuvaMVjPjsF6dhzWNZHjODXPv0wmQ0xMDNLS0nS2p6WlITY21uRj09PTkZWVhYSEBL19sbGxyMrKglqt1m7LzMxEcHCwwcCfiIiIiMgdOH2RryFDhuDXX3/Fzp07cfXqVaxcuRK5ubm47777AACrV6/Gp59+qve4nTt3ol27doiKitLbN2jQIBQXF2PlypW4fv06fv/9dyQnJ2Pw4MF2fz1ERERERK7K6c3gvXv3RnFxMZKSkpCfn4+WLVti9uzZ2vF6+fn5yM3N1XlMWVkZDh8+jMmTJxs8Z2hoKP7zn/9g1apVePnllxESEoIHHnhAL6sQEREREZE7EUQOsjMpJydHJwWoLQiCgGbNmiEzM5NjHO2I9ew4rGvHYD07BuvZcexR13K5nBN+iUxw+rAfIiIiIiJyDAb/RERERERugsE/EREREZGbYPBPREREROQmGPwTEREREbkJBv9ERERERG6CwT8RERERkZtg8E9ERERE5CYY/BMRERERuQkG/0REREREboLBPxERERGRm2DwT0RERETkJhj8ExERERG5CQb/RERERERugsE/EREREZGbYPBPREREROQmGPwTEREREbkJBv9ERERERG6CwT8RERERkZtg8E9ERERE5CYY/BMRERERuQkG/0REREREboLBPxERERGRm2DwT0RERETkJhj8ExERERG5CQb/RERERERugsE/EREREZGbYPBPREREROQmGPwTEREREbkJBv9ERERERG6CwT8RERERkZtg8E9ERERE5CYY/BMRERERuQkG/0REREREboLBPxERERGRm2DwT0RERETkJhj8ExERERG5CQb/RERERERugsE/EREREZGbYPBPREREROQmGPwTEREREbkJBv9ERERERG6CwT8RERERkZtg8E9ERERE5CYY/BMRERERuQkG/0REREREboLBPxERERGRm2DwT0RERETkJmTOLgAAbNu2DSkpKSgoKEBkZCQmT56MDh06GDz2s88+w549e/S2R0ZG4sMPP9Tbvn//fnz88ceIi4vDK6+8YvOyExERERE1FE4P/g8cOICVK1di6tSpiI2NxY4dOzB//nx89NFHCA0N1Tt+ypQpePjhh7V/q1QqvPzyy+jZs6fesTk5Ofj222+N3kgQEREREbkTpw/72bJlCxISEnDPPfdoW/1DQ0Oxfft2g8f7+PggKChI+9/58+dRWlqKgQMH6hynVqvxySefYMyYMQgLC3PESyEiIiIicmlObflXKpXIyMjAsGHDdLZ36dIFZ86csegcO3fuROfOndG0aVOd7evXr0dAQAASEhLw119/mT2PQqGAQqHQ/i0IAry9vbX/tiXN+Wx9XtLFenYc1rVjsJ4dg/XsOKxrIsdzavBfVFQEtVqNwMBAne2BgYEoKCgw+/j8/Hz8+eefePbZZ3W2nz59Gjt37sTChQstLktycjLWr1+v/bt169ZYsGCB3k2FLUVERNjt3PQP1rPjsK4dg/XsGKxnx2FdEzmO08f8A4bv+C1pBdi9ezd8fX1x1113abeVl5dj8eLFmDFjBgICAiwuw/DhwzFkyBC958/JyYFSqbT4PJYQBAERERHIysqCKIo2PTf9g/XsOKxrx2A9Owbr2XHsUdcymcyuDXdEDZ1Tg/+AgABIJBK9Vv7CwkK93oDaRFHErl270LdvX8hk/7yM7Oxs5OTkYMGCBTrHAsC4ceOwaNEigy0Mcrkccrnc6HPZgyiK/GFxANaz47CuHYP17BisZ8dhXRM5jlODf5lMhpiYGKSlpem03qelpeHOO+80+dj09HRkZWUhISFBZ3vz5s3x/vvv62xLTExERUWFdjIxEREREZE7cvqwnyFDhmDx4sWIiYlB+/btsWPHDuTm5uK+++4DAKxevRp5eXl4+umndR63c+dOtGvXDlFRUTrbPTw89Lb5+voCgN52qm5t4UQrIiIiIvfg9OC/d+/eKC4uRlJSEvLz89GyZUvMnj1bO14vPz8fubm5Oo8pKyvD4cOHMXnyZCeUuOErrVJh2cHr2JtRBKVaDZlEgr4xAZjeqzl8PaTOLh4RERER2YkgcpCdSTk5OTopQG1BEAQ0a9YMmZmZDh/jWFqlwvS1Z3EprwLqGtslAhAd7IVlY9o3mhsAZ9azu2FdOwbr2TFYz45jj7qWy+Wc8EtkgtMX+SLHWnbwul7gDwBqEbiUX4FlB687pVxEREREZH8M/t3M3owivcBfQy0C+zKKHFoeIiIiInIcBv9uRBRFKNXGQv9qSjXTrRERERE1Vgz+3YggCJBJTL/lUonA7D9EREREjRSDfzfTNyYAEiOxvUSo3k9EREREjRODfzczvVdzRAd76d0ASASgVbAXpvdq7pyCEREREZHdOT3PPzmWr4cUy8a0x7KD17EvowhKtQiZREA88/wTERERNXoM/t2Qr4cUz/dvief7c4VfIiIiInfCYT9ujoE/ERERkftg8E9ERERE5CYY/BMRERERuQkG/0REREREboLBPxERERGRm2DwT0RERETkJhj8ExERERG5CQb/RERERERugsE/EREREZGbYPBPREREROQmZM4ugKuTyexXRfY8N/2D9ew4rGvHYD07BuvZcWxZ13zfiEwTRFEUnV0IIiIiIiKyPw77cYLy8nK8+uqrKC8vd3ZRGjXWs+Owrh2D9ewYrGfHYV0TOR6DfycQRREXLlwAO13si/XsOKxrx2A9Owbr2XFY10SOx+CfiIiIiMhNMPgnIiIiInITDP6dQC6XY9SoUZDL5c4uSqPGenYc1rVjsJ4dg/XsOKxrIsdjth8iIiIiIjfBln8iIiIiIjfB4J+IiIiIyE0w+CciIiIichMM/omIiIiI3ITM2QVwN9u2bUNKSgoKCgoQGRmJyZMno0OHDs4uVoORnp6OlJQUXLhwAfn5+XjppZdw1113afeLooh169bh119/RUlJCdq1a4fHH38cLVu21B6jUCjw7bffYv/+/aiqqkKnTp0wdepUNGnSxBkvySUlJyfjyJEjuHbtGjw8PNC+fXtMnDgRzZs31x7DuraN7du3Y/v27cjJyQEAREZGYtSoUejevTsA1rO9JCcn44cffsCDDz6IyZMnA2Bd28LatWuxfv16nW2BgYFYvnw5ANYxkStgy78DHThwACtXrsSIESOwYMECdOjQAfPnz0dubq6zi9ZgVFZWolWrVnjssccM7t+0aRN+/PFHPPbYY3jvvfcQFBSEd955R2fp+JUrV+LIkSN47rnnMG/ePFRUVOC///0v1Gq1o16Gy0tPT8fgwYPx7rvv4j//+Q/UajXeeecdVFRUaI9hXdtGSEgIJkyYgPfeew/vvfceOnXqhIULF+LKlSsAWM/2cO7cOezYsQPR0dE621nXttGyZUssW7ZM+98HH3yg3cc6JnIBIjnM7NmzxWXLlulsmzVrlvj99987qUQN2+jRo8XDhw9r/1ar1eK0adPE5ORk7baqqipx0qRJ4vbt20VRFMXS0lJx3Lhx4v79+7XH3Lx5UxwzZoz4xx9/OKroDU5hYaE4evRo8dSpU6Iosq7tbfLkyeKvv/7KeraD8vJy8dlnnxVTU1PFOXPmiCtWrBBFkZ9pW1mzZo340ksvGdzHOiZyDWz5dxClUomMjAx07dpVZ3uXLl1w5swZJ5Wqcblx4wYKCgp06lgul+P222/X1nFGRgZUKhW6dOmiPSYkJARRUVE4e/asw8vcUJSVlQEA/Pz8ALCu7UWtVmP//v2orKxE+/btWc928OWXX6J79+469QXwM21LWVlZmDFjBp566iksWrQI2dnZAFjHRK6CY/4dpKioCGq1GoGBgTrbAwMDUVBQ4JxCNTKaejRUx5qhVQUFBZDJZNogtuYxfB8ME0URq1atwm233YaoqCgArGtbu3z5Mt544w0oFAp4eXnhpZdeQmRkpDYgYj3bxv79+3HhwgW89957evv4mbaNdu3a4amnnkLz5s1RUFCADRs24D//+Q8+/PBD1jGRi2Dw72CCIFi0jequdn2KFixibckx7uqrr77C5cuXMW/ePL19rGvbaN68Of73v/+htLQUhw8fxmeffYa5c+dq97Oe6y83NxcrV67EG2+8AQ8PD6PHsa7rRzNRHQCioqLQvn17PPPMM9izZw/atWsHgHVM5Gwc9uMgAQEBkEgkei0XhYWFeq0gVDdBQUEAoFfHRUVF2joOCgqCUqlESUmJ3jGax9M/vv76axw/fhxz5szRybTBurYtmUyGiIgItGnTBhMmTECrVq3w008/sZ5tKCMjA4WFhXjttdcwbtw4jBs3Dunp6di6dSvGjRunrU/WtW15eXkhKioKmZmZ/DwTuQgG/w4ik8kQExODtLQ0ne1paWmIjY11Uqkal7CwMAQFBenUsVKpRHp6uraOY2JiIJVKdY7Jz8/H5cuX0b59e4eX2VWJooivvvoKhw8fxltvvYWwsDCd/axr+xJFEQqFgvVsQ507d8b777+PhQsXav9r06YN4uPjsXDhQoSHh7Ou7UChUODatWsIDg7m55nIRXDYjwMNGTIEixcvRkxMDNq3b48dO3YgNzcX9913n7OL1mBUVFQgKytL+/eNGzdw8eJF+Pn5ITQ0FA8++CCSk5PRrFkzREREIDk5GZ6enoiPjwcA+Pj4ICEhAd9++y38/f3h5+eHb7/9FlFRUXoTAN3ZV199hX379uGVV16Bt7e3tqXOx8cHHh4eEASBdW0jq1evRvfu3dGkSRNUVFRg//79OHXqFN544w3Wsw15e3tr56xoeHp6wt/fX7uddV1/33zzDeLi4hAaGorCwkIkJSWhvLwc/fv35+eZyEUIIgfSOZRmka/8/Hy0bNkSkyZNwu233+7sYjUYp06d0hkLrdG/f3889dRT2gVkduzYgdLSUrRt2xaPP/64zo9+VVUVvvvuO+zbt09nAZnQ0FBHvhSXNmbMGIPbn3zySQwYMAAAWNc28sUXX+DkyZPIz8+Hj48PoqOjMXToUG2gw3q2n7fffhutWrXSW+SLdV13ixYtwl9//YWioiIEBASgXbt2GDduHCIjIwGwjolcAYN/IiIiIiI3wTH/RERERERugsE/EREREZGbYPBPREREROQmGPwTEREREbkJBv9ERERERG6CwT8RERERkZtg8E9ERERE5Ca4wi8RNTjGFiGrbc6cOejYsaPe9rffflvn/9aoz2OJiIicjcE/ETU477zzjs7fSUlJOHXqFN566y2d7ZpVRWubOnWq3cpGRETkyhj8E1GD0759e52/AwICIAiC3vbaKisr4enpafSmgIiIqLFj8E9EjdLbb7+N4uJiPP7441i9ejUuXryIuLg4zJo1y+DQnXXr1uGPP/5AZmYm1Go1IiIiMHjwYAwcOBCCIDjnRRAREdkYg38iarTy8/OxePFiDB06FOPHjzcZxOfk5ODee+9FaGgoAODvv//G119/jby8PIwaNcpRRSYiIrIrBv9E1GiVlJTghRdeQKdOncwe++STT2r/rVar0bFjR4iiiK1bt2LkyJFs/SciokaBwT8RNVq+vr4WBf4AcPLkSSQnJ+PcuXMoLy/X2VdYWIigoCA7lJCIiMixGPwTUaMVHBxs0XHnzp3DO++8g44dO2LGjBlo0qQJZDIZjh49ig0bNqCqqsrOJSUiInIMBv9E1GhZOlRn//79kEqlePXVV+Hh4aHdfvToUXsVjYiIyCm4wi8RuT1BECCVSiGR/HNJrKqqwm+//ebEUhEREdkeW/6JyO3dcccd2LJlCz755BPce++9KC4uxubNmyGXy51dNCIiIptiyz8Rub1OnTph5syZuHz5MhYsWIDExET07NkTQ4cOdXbRiIiIbEoQRVF0diGIiIiIiMj+2PJPREREROQmGPwTEREREbkJBv9ERERERG6CwT8RERERkZtg8E9ERERE5CYY/BMRERERuQkG/0REREREboLBPxERERGRm2DwT0RERETkJhj8ExERERG5CQb/RERERERugsE/EREREZGb+H8LfQsqN9ePVQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>210.200000</td>\n",
       "      <td>8.482662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>179.800000</td>\n",
       "      <td>10.282672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>5.944185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>30.200000</td>\n",
       "      <td>3.966527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.849312</td>\n",
       "      <td>0.016548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.843609</td>\n",
       "      <td>0.022347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.874529</td>\n",
       "      <td>0.014709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.821710</td>\n",
       "      <td>0.025943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.858643</td>\n",
       "      <td>0.015047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.849127</td>\n",
       "      <td>0.016662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.848444</td>\n",
       "      <td>0.016695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.848120</td>\n",
       "      <td>0.016931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.697828</td>\n",
       "      <td>0.033039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.855830</td>\n",
       "      <td>0.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.848120</td>\n",
       "      <td>0.016931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       210.200000     8.482662\n",
       "1                    TN       179.800000    10.282672\n",
       "2                    FP        39.000000     5.944185\n",
       "3                    FN        30.200000     3.966527\n",
       "4              Accuracy         0.849312     0.016548\n",
       "5             Precision         0.843609     0.022347\n",
       "6           Sensitivity         0.874529     0.014709\n",
       "7           Specificity         0.821710     0.025943\n",
       "8              F1 score         0.858643     0.015047\n",
       "9   F1 score (weighted)         0.849127     0.016662\n",
       "10     F1 score (macro)         0.848444     0.016695\n",
       "11    Balanced Accuracy         0.848120     0.016931\n",
       "12                  MCC         0.697828     0.033039\n",
       "13                  NPV         0.855830     0.020600\n",
       "14              ROC_AUC         0.848120     0.016931"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>421.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>347.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>80.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>68.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.830250</td>\n",
       "      <td>0.854189</td>\n",
       "      <td>0.826986</td>\n",
       "      <td>0.838955</td>\n",
       "      <td>0.821545</td>\n",
       "      <td>0.847661</td>\n",
       "      <td>0.843308</td>\n",
       "      <td>0.834603</td>\n",
       "      <td>0.811752</td>\n",
       "      <td>0.862894</td>\n",
       "      <td>0.837214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.855691</td>\n",
       "      <td>0.833992</td>\n",
       "      <td>0.845574</td>\n",
       "      <td>0.836439</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.851992</td>\n",
       "      <td>0.837675</td>\n",
       "      <td>0.802444</td>\n",
       "      <td>0.847870</td>\n",
       "      <td>0.839226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>0.869835</td>\n",
       "      <td>0.849095</td>\n",
       "      <td>0.871845</td>\n",
       "      <td>0.826176</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.871845</td>\n",
       "      <td>0.854806</td>\n",
       "      <td>0.838298</td>\n",
       "      <td>0.891258</td>\n",
       "      <td>0.859445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.793900</td>\n",
       "      <td>0.836800</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.816300</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.811600</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.811520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.844622</td>\n",
       "      <td>0.862705</td>\n",
       "      <td>0.841476</td>\n",
       "      <td>0.858509</td>\n",
       "      <td>0.831276</td>\n",
       "      <td>0.855967</td>\n",
       "      <td>0.861804</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.819979</td>\n",
       "      <td>0.869023</td>\n",
       "      <td>0.849151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.829906</td>\n",
       "      <td>0.854110</td>\n",
       "      <td>0.826830</td>\n",
       "      <td>0.838560</td>\n",
       "      <td>0.821613</td>\n",
       "      <td>0.847622</td>\n",
       "      <td>0.843029</td>\n",
       "      <td>0.834467</td>\n",
       "      <td>0.811555</td>\n",
       "      <td>0.862727</td>\n",
       "      <td>0.837042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.828786</td>\n",
       "      <td>0.853626</td>\n",
       "      <td>0.825528</td>\n",
       "      <td>0.835820</td>\n",
       "      <td>0.820950</td>\n",
       "      <td>0.847152</td>\n",
       "      <td>0.840450</td>\n",
       "      <td>0.833665</td>\n",
       "      <td>0.811358</td>\n",
       "      <td>0.862594</td>\n",
       "      <td>0.835993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.827850</td>\n",
       "      <td>0.853308</td>\n",
       "      <td>0.825021</td>\n",
       "      <td>0.834437</td>\n",
       "      <td>0.821227</td>\n",
       "      <td>0.846993</td>\n",
       "      <td>0.839388</td>\n",
       "      <td>0.833217</td>\n",
       "      <td>0.811131</td>\n",
       "      <td>0.862296</td>\n",
       "      <td>0.835487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.658369</td>\n",
       "      <td>0.707383</td>\n",
       "      <td>0.651217</td>\n",
       "      <td>0.672165</td>\n",
       "      <td>0.641970</td>\n",
       "      <td>0.694337</td>\n",
       "      <td>0.681196</td>\n",
       "      <td>0.667530</td>\n",
       "      <td>0.623567</td>\n",
       "      <td>0.726369</td>\n",
       "      <td>0.672410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.832900</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.818400</td>\n",
       "      <td>0.829900</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.842200</td>\n",
       "      <td>0.831600</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.822400</td>\n",
       "      <td>0.880300</td>\n",
       "      <td>0.834620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.827850</td>\n",
       "      <td>0.853308</td>\n",
       "      <td>0.825021</td>\n",
       "      <td>0.834437</td>\n",
       "      <td>0.821227</td>\n",
       "      <td>0.846993</td>\n",
       "      <td>0.839388</td>\n",
       "      <td>0.833217</td>\n",
       "      <td>0.811131</td>\n",
       "      <td>0.862296</td>\n",
       "      <td>0.835487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  424.000000  421.000000  422.000000  449.000000   \n",
       "1                    TN  339.000000  364.000000  338.000000  322.000000   \n",
       "2                    FP   88.000000   71.000000   84.000000   82.000000   \n",
       "3                    FN   68.000000   63.000000   75.000000   66.000000   \n",
       "4              Accuracy    0.830250    0.854189    0.826986    0.838955   \n",
       "5             Precision    0.828125    0.855691    0.833992    0.845574   \n",
       "6           Sensitivity    0.861789    0.869835    0.849095    0.871845   \n",
       "7           Specificity    0.793900    0.836800    0.800900    0.797000   \n",
       "8              F1 score    0.844622    0.862705    0.841476    0.858509   \n",
       "9   F1 score (weighted)    0.829906    0.854110    0.826830    0.838560   \n",
       "10     F1 score (macro)    0.828786    0.853626    0.825528    0.835820   \n",
       "11    Balanced Accuracy    0.827850    0.853308    0.825021    0.834437   \n",
       "12                  MCC    0.658369    0.707383    0.651217    0.672165   \n",
       "13                  NPV    0.832900    0.852500    0.818400    0.829900   \n",
       "14              ROC_AUC    0.827850    0.853308    0.825021    0.834437   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   404.000000  416.000000  449.000000  418.000000  394.000000  418.000000   \n",
       "1   351.000000  363.000000  326.000000  349.000000  352.000000  375.000000   \n",
       "2    79.000000   72.000000   78.000000   81.000000   97.000000   75.000000   \n",
       "3    85.000000   68.000000   66.000000   71.000000   76.000000   51.000000   \n",
       "4     0.821545    0.847661    0.843308    0.834603    0.811752    0.862894   \n",
       "5     0.836439    0.852459    0.851992    0.837675    0.802444    0.847870   \n",
       "6     0.826176    0.859504    0.871845    0.854806    0.838298    0.891258   \n",
       "7     0.816300    0.834500    0.806900    0.811600    0.784000    0.833300   \n",
       "8     0.831276    0.855967    0.861804    0.846154    0.819979    0.869023   \n",
       "9     0.821613    0.847622    0.843029    0.834467    0.811555    0.862727   \n",
       "10    0.820950    0.847152    0.840450    0.833665    0.811358    0.862594   \n",
       "11    0.821227    0.846993    0.839388    0.833217    0.811131    0.862296   \n",
       "12    0.641970    0.694337    0.681196    0.667530    0.623567    0.726369   \n",
       "13    0.805000    0.842200    0.831600    0.831000    0.822400    0.880300   \n",
       "14    0.821227    0.846993    0.839388    0.833217    0.811131    0.862296   \n",
       "\n",
       "           ave  \n",
       "0   421.500000  \n",
       "1   347.900000  \n",
       "2    80.700000  \n",
       "3    68.900000  \n",
       "4     0.837214  \n",
       "5     0.839226  \n",
       "6     0.859445  \n",
       "7     0.811520  \n",
       "8     0.849151  \n",
       "9     0.837042  \n",
       "10    0.835993  \n",
       "11    0.835487  \n",
       "12    0.672410  \n",
       "13    0.834620  \n",
       "14    0.835487  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.848868</td>\n",
       "      <td>0.016375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.842545</td>\n",
       "      <td>0.021326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.874879</td>\n",
       "      <td>0.021653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.820344</td>\n",
       "      <td>0.024352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.858195</td>\n",
       "      <td>0.016546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.848652</td>\n",
       "      <td>0.016409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.848003</td>\n",
       "      <td>0.016434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.847612</td>\n",
       "      <td>0.016508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.697192</td>\n",
       "      <td>0.032881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.856626</td>\n",
       "      <td>0.023207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.847612</td>\n",
       "      <td>0.016508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.848868     0.016375\n",
       "1             Precision         0.842545     0.021326\n",
       "2           Sensitivity         0.874879     0.021653\n",
       "3           Specificity         0.820344     0.024352\n",
       "4              F1 score         0.858195     0.016546\n",
       "5   F1 score (weighted)         0.848652     0.016409\n",
       "6      F1 score (macro)         0.848003     0.016434\n",
       "7     Balanced Accuracy         0.847612     0.016508\n",
       "8                   MCC         0.697192     0.032881\n",
       "9                   NPV         0.856626     0.023207\n",
       "10              ROC_AUC         0.847612     0.016508"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"logloss\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_xgb)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_xgb))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_xgb))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_xgb))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_xgb))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_xgb, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_xgb, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_xgb))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_xgb))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_xgb))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:31:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost baseline model f1_score 0.8202 with a standard deviation of 0.0148\n",
      "XGBoost optimized model f1_score 0.8385 with a standard deviation of 0.0194\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_clf, X, Y, cv=10, scoring=\"f1_macro\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"f1_macro\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"f1_macro\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb_clf.joblib']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the modesls, both the one with optimized hyperparameters and the initial one\n",
    "joblib.dump(xgb_clf, \"OUTPUT/xgb_clf.joblib\")\n",
    "\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb_clf.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       200.400000     9.155448\n",
      "1                    TN       174.200000    11.282238\n",
      "2                    FP        44.600000     8.016649\n",
      "3                    FN        40.000000     5.849976\n",
      "4              Accuracy         0.815770     0.022631\n",
      "5             Precision         0.818346     0.029347\n",
      "6           Sensitivity         0.833753     0.021948\n",
      "7           Specificity         0.796170     0.034913\n",
      "8              F1 score         0.825697     0.020573\n",
      "9   F1 score (weighted)         0.815655     0.022703\n",
      "10     F1 score (macro)         0.814926     0.022673\n",
      "11    Balanced Accuracy         0.814955     0.022707\n",
      "12                  MCC         0.630628     0.045051\n",
      "13                  NPV         0.812990     0.027535\n",
      "14              ROC_AUC         0.814955     0.022707\n",
      "CPU times: user 4.97 s, sys: 1.82 s, total: 6.79 s\n",
      "Wall time: 614 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    \n",
    "    knn_clf.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_clf.predict(X_test) \n",
    "    \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(**param_grid, n_jobs=16)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average='macro')\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "  \n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(**param_grid, n_jobs=16)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:31:50,083] A new study created in memory with name: KNNClassifier\n",
      "[I 2023-12-04 11:31:50,810] Trial 0 finished with value: 0.7883422062423799 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 0 with value: 0.7883422062423799.\n",
      "[I 2023-12-04 11:31:50,939] Trial 1 finished with value: 0.7964452795481077 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 1 with value: 0.7964452795481077.\n",
      "[I 2023-12-04 11:31:51,057] Trial 2 finished with value: 0.8134380644653781 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 66}. Best is trial 2 with value: 0.8134380644653781.\n",
      "[I 2023-12-04 11:31:51,188] Trial 3 finished with value: 0.7945571344027503 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 2 with value: 0.8134380644653781.\n",
      "[I 2023-12-04 11:31:51,315] Trial 4 finished with value: 0.7867839298247075 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 2 with value: 0.8134380644653781.\n",
      "[I 2023-12-04 11:31:51,442] Trial 5 finished with value: 0.7892448557171057 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 2 with value: 0.8134380644653781.\n",
      "[I 2023-12-04 11:31:51,565] Trial 6 finished with value: 0.7944566103557735 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 67}. Best is trial 2 with value: 0.8134380644653781.\n",
      "[I 2023-12-04 11:31:51,686] Trial 7 finished with value: 0.7975591911265925 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 2 with value: 0.8134380644653781.\n",
      "[I 2023-12-04 11:31:52,377] Trial 8 finished with value: 0.7977616941688709 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 2 with value: 0.8134380644653781.\n",
      "[I 2023-12-04 11:31:52,502] Trial 9 finished with value: 0.80117299786701 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 2 with value: 0.8134380644653781.\n",
      "[I 2023-12-04 11:31:52,761] Trial 10 finished with value: 0.8036409622420159 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 2 with value: 0.8134380644653781.\n",
      "[I 2023-12-04 11:31:53,051] Trial 11 finished with value: 0.8170420347726219 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:53,310] Trial 12 finished with value: 0.8170420347726219 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:53,586] Trial 13 finished with value: 0.8170420347726219 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:53,935] Trial 14 finished with value: 0.7953232136293265 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:54,201] Trial 15 finished with value: 0.7799238337986759 and parameters: {'n_neighbors': 14, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:54,461] Trial 16 finished with value: 0.7967770145720717 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:54,724] Trial 17 finished with value: 0.8170420347726219 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 58}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:55,457] Trial 18 finished with value: 0.7883422062423799 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:55,707] Trial 19 finished with value: 0.7967770145720717 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:56,052] Trial 20 finished with value: 0.7800961980142256 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 54}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:56,341] Trial 21 finished with value: 0.8170420347726219 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:56,639] Trial 22 finished with value: 0.8060311846497747 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:56,913] Trial 23 finished with value: 0.7883422062423799 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:57,210] Trial 24 finished with value: 0.7953232136293265 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 47}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:57,948] Trial 25 finished with value: 0.8170420347726219 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:58,202] Trial 26 finished with value: 0.8060311846497747 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:58,466] Trial 27 finished with value: 0.7953232136293265 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 64}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:58,771] Trial 28 finished with value: 0.8060311846497747 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:59,527] Trial 29 finished with value: 0.789444645610229 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:31:59,811] Trial 30 finished with value: 0.7827993710986506 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:32:00,066] Trial 31 finished with value: 0.8170420347726219 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 56}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:32:00,299] Trial 32 finished with value: 0.8170420347726219 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:32:00,572] Trial 33 finished with value: 0.8030562438871071 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 52}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:32:00,841] Trial 34 finished with value: 0.8060311846497747 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:32:01,096] Trial 35 finished with value: 0.7944200962971706 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:32:01,834] Trial 36 finished with value: 0.7767552538890409 and parameters: {'n_neighbors': 21, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:32:02,107] Trial 37 finished with value: 0.8030562438871071 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 58}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:32:02,370] Trial 38 finished with value: 0.8036409622420159 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 11 with value: 0.8170420347726219.\n",
      "[I 2023-12-04 11:32:02,543] Trial 39 finished with value: 0.8172828040339469 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 68}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:02,722] Trial 40 finished with value: 0.8172828040339469 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 79}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:02,874] Trial 41 finished with value: 0.8172828040339469 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:03,029] Trial 42 finished with value: 0.8172828040339469 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:03,187] Trial 43 finished with value: 0.8172828040339469 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:03,339] Trial 44 finished with value: 0.8134380644653781 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 80}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:03,494] Trial 45 finished with value: 0.8075617930306894 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 84}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:03,649] Trial 46 finished with value: 0.8026231020819223 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 86}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:03,802] Trial 47 finished with value: 0.8172828040339469 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 78}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:03,964] Trial 48 finished with value: 0.7944566103557735 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:04,095] Trial 49 finished with value: 0.8075617930306894 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 39 with value: 0.8172828040339469.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8173\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 8\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 68\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNClassifier\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  406.000000\n",
      "1                    TN  341.000000\n",
      "2                    FP   86.000000\n",
      "3                    FN   86.000000\n",
      "4              Accuracy    0.812840\n",
      "5             Precision    0.825203\n",
      "6           Sensitivity    0.825203\n",
      "7           Specificity    0.798600\n",
      "8              F1 score    0.825203\n",
      "9   F1 score (weighted)    0.812840\n",
      "10     F1 score (macro)    0.811899\n",
      "11    Balanced Accuracy    0.811899\n",
      "12                  MCC    0.623798\n",
      "13                  NPV    0.798600\n",
      "14              ROC_AUC    0.811899\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_knn_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_knn_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_knn_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_knn_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_knn_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_knn_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_knn_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_knn_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_knn_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_knn_0)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:32:04,268] Trial 50 finished with value: 0.7992499926131635 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:04,422] Trial 51 finished with value: 0.8043319501618618 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 77}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:04,574] Trial 52 finished with value: 0.8043319501618618 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 90}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:04,738] Trial 53 finished with value: 0.7765297989218899 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 79}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:04,896] Trial 54 finished with value: 0.7945458273745797 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 73}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:05,049] Trial 55 finished with value: 0.8001440794756863 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:05,206] Trial 56 finished with value: 0.8111578202784457 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 68}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:05,361] Trial 57 finished with value: 0.7990626004873284 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:05,518] Trial 58 finished with value: 0.8059405535434682 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:05,667] Trial 59 finished with value: 0.8043319501618618 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 78}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:05,821] Trial 60 finished with value: 0.8111578202784457 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 88}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:05,975] Trial 61 finished with value: 0.8001440794756863 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 83}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:06,130] Trial 62 finished with value: 0.8111578202784457 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 65}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:06,282] Trial 63 finished with value: 0.8059405535434682 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 71}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:06,439] Trial 64 finished with value: 0.7990626004873284 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 76}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:06,592] Trial 65 finished with value: 0.8043319501618618 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 87}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:06,748] Trial 66 finished with value: 0.7984236089726717 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 61}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:07,463] Trial 67 finished with value: 0.8108684321452199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:07,594] Trial 68 finished with value: 0.8003614818437512 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:07,755] Trial 69 finished with value: 0.8043319501618618 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 72}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:07,915] Trial 70 finished with value: 0.8001440794756863 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 79}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:08,632] Trial 71 finished with value: 0.808074893701267 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:08,761] Trial 72 finished with value: 0.8059405535434682 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 49}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:08,914] Trial 73 finished with value: 0.8062675585058123 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:09,073] Trial 74 finished with value: 0.8059405535434682 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 77}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:09,230] Trial 75 finished with value: 0.8111578202784457 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 53}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:09,389] Trial 76 finished with value: 0.7990626004873284 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:09,539] Trial 77 finished with value: 0.8043319501618618 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 84}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:10,301] Trial 78 finished with value: 0.8004066870155366 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:10,433] Trial 79 finished with value: 0.8111578202784457 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 74}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:10,725] Trial 80 finished with value: 0.7851326084170125 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 90}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:11,048] Trial 81 finished with value: 0.7563403100058381 and parameters: {'n_neighbors': 27, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:11,307] Trial 82 finished with value: 0.7959729725269729 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:11,606] Trial 83 finished with value: 0.8004066870155366 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:11,871] Trial 84 finished with value: 0.7917984122559095 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:12,105] Trial 85 finished with value: 0.7959729725269729 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:12,365] Trial 86 finished with value: 0.7895576512290675 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:12,530] Trial 87 finished with value: 0.7902281822357399 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 51}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:12,830] Trial 88 finished with value: 0.7837901661073603 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:12,977] Trial 89 finished with value: 0.8003614818437512 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 56}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:13,132] Trial 90 finished with value: 0.8062675585058123 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:13,425] Trial 91 finished with value: 0.7917984122559095 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:13,722] Trial 92 finished with value: 0.8004066870155366 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 76}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:13,971] Trial 93 finished with value: 0.7959729725269729 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:14,248] Trial 94 finished with value: 0.7895576512290675 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 86}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:14,586] Trial 95 finished with value: 0.7709746895962872 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 60}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:14,862] Trial 96 finished with value: 0.7851326084170125 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 80}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:15,002] Trial 97 finished with value: 0.8111578202784457 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 83}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:15,726] Trial 98 finished with value: 0.808074893701267 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:15,993] Trial 99 finished with value: 0.7895576512290675 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 70}. Best is trial 39 with value: 0.8172828040339469.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8173\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 8\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 68\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  406.000000  403.000000\n",
      "1                    TN  341.000000  352.000000\n",
      "2                    FP   86.000000   83.000000\n",
      "3                    FN   86.000000   81.000000\n",
      "4              Accuracy    0.812840    0.821545\n",
      "5             Precision    0.825203    0.829218\n",
      "6           Sensitivity    0.825203    0.832645\n",
      "7           Specificity    0.798600    0.809200\n",
      "8              F1 score    0.825203    0.830928\n",
      "9   F1 score (weighted)    0.812840    0.821524\n",
      "10     F1 score (macro)    0.811899    0.820994\n",
      "11    Balanced Accuracy    0.811899    0.820920\n",
      "12                  MCC    0.623798    0.641996\n",
      "13                  NPV    0.798600    0.812900\n",
      "14              ROC_AUC    0.811899    0.820920\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_knn_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_knn_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_knn_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_knn_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_knn_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_knn_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_knn_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_knn_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_knn_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_knn_1)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:32:16,203] Trial 100 finished with value: 0.8121682832385856 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:16,515] Trial 101 finished with value: 0.8110826217225547 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:16,788] Trial 102 finished with value: 0.797618732396495 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:17,032] Trial 103 finished with value: 0.797618732396495 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:17,326] Trial 104 finished with value: 0.8110826217225547 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 78}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:17,477] Trial 105 finished with value: 0.8121682832385856 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 46}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:17,768] Trial 106 finished with value: 0.7912021302308936 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:17,932] Trial 107 finished with value: 0.7883071626921709 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:18,218] Trial 108 finished with value: 0.7834267851872716 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 73}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:18,357] Trial 109 finished with value: 0.8142407323026923 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 37}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:18,516] Trial 110 finished with value: 0.8072641859217855 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:19,272] Trial 111 finished with value: 0.8110826217225547 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:20,001] Trial 112 finished with value: 0.8110826217225547 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:20,731] Trial 113 finished with value: 0.797618732396495 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:21,462] Trial 114 finished with value: 0.800776745764136 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:22,192] Trial 115 finished with value: 0.797618732396495 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:22,326] Trial 116 finished with value: 0.7977377124474515 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:22,609] Trial 117 finished with value: 0.800776745764136 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 39 with value: 0.8172828040339469.\n",
      "[I 2023-12-04 11:32:22,774] Trial 118 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 84}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:22,932] Trial 119 finished with value: 0.8161957596240965 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:23,108] Trial 120 finished with value: 0.8142407323026923 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 84}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:23,266] Trial 121 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 88}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:23,427] Trial 122 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:23,584] Trial 123 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:23,734] Trial 124 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:23,891] Trial 125 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:24,049] Trial 126 finished with value: 0.8161957596240965 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:24,207] Trial 127 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:24,368] Trial 128 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:24,525] Trial 129 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:24,684] Trial 130 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:24,857] Trial 131 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:25,024] Trial 132 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:25,174] Trial 133 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:25,324] Trial 134 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:25,479] Trial 135 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:25,636] Trial 136 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:25,794] Trial 137 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:25,945] Trial 138 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:26,101] Trial 139 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:26,257] Trial 140 finished with value: 0.8161957596240965 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:26,414] Trial 141 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:26,567] Trial 142 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 92}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:26,724] Trial 143 finished with value: 0.8161957596240965 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:26,881] Trial 144 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:27,036] Trial 145 finished with value: 0.8161957596240965 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 90}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:27,191] Trial 146 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:27,344] Trial 147 finished with value: 0.8161957596240965 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:27,501] Trial 148 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:27,655] Trial 149 finished with value: 0.8200738378520033 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 118 with value: 0.8200738378520033.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8201\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 84\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  406.000000  403.000000  415.000000\n",
      "1                    TN  341.000000  352.000000  333.000000\n",
      "2                    FP   86.000000   83.000000   89.000000\n",
      "3                    FN   86.000000   81.000000   82.000000\n",
      "4              Accuracy    0.812840    0.821545    0.813928\n",
      "5             Precision    0.825203    0.829218    0.823413\n",
      "6           Sensitivity    0.825203    0.832645    0.835010\n",
      "7           Specificity    0.798600    0.809200    0.789100\n",
      "8              F1 score    0.825203    0.830928    0.829171\n",
      "9   F1 score (weighted)    0.812840    0.821524    0.813801\n",
      "10     F1 score (macro)    0.811899    0.820994    0.812435\n",
      "11    Balanced Accuracy    0.811899    0.820920    0.812055\n",
      "12                  MCC    0.623798    0.641996    0.624965\n",
      "13                  NPV    0.798600    0.812900    0.802400\n",
      "14              ROC_AUC    0.811899    0.820920    0.812055\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "#predict\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_knn_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_knn_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_knn_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_knn_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_knn_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_knn_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_knn_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_knn_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_knn_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_knn_2)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:32:27,871] Trial 150 finished with value: 0.8128436833632282 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:28,041] Trial 151 finished with value: 0.8128436833632282 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 118 with value: 0.8200738378520033.\n",
      "[I 2023-12-04 11:32:28,196] Trial 152 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:28,351] Trial 153 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:28,505] Trial 154 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:28,663] Trial 155 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:28,822] Trial 156 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:28,982] Trial 157 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:29,142] Trial 158 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:29,298] Trial 159 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:29,458] Trial 160 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:29,617] Trial 161 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:29,774] Trial 162 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:29,933] Trial 163 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:30,092] Trial 164 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:30,247] Trial 165 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:30,403] Trial 166 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:30,548] Trial 167 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:30,705] Trial 168 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:30,861] Trial 169 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:31,021] Trial 170 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:31,174] Trial 171 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:31,329] Trial 172 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:31,492] Trial 173 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:31,643] Trial 174 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:31,801] Trial 175 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:31,959] Trial 176 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:32,113] Trial 177 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:32,271] Trial 178 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:32,428] Trial 179 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:32,587] Trial 180 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:32,748] Trial 181 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:32,922] Trial 182 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:33,113] Trial 183 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:33,279] Trial 184 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:33,435] Trial 185 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:33,595] Trial 186 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:33,751] Trial 187 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:33,909] Trial 188 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:34,064] Trial 189 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:34,227] Trial 190 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:34,386] Trial 191 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:34,538] Trial 192 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:34,696] Trial 193 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:34,853] Trial 194 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:35,010] Trial 195 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:35,170] Trial 196 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:35,328] Trial 197 finished with value: 0.8206156842155841 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:35,506] Trial 198 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:35,665] Trial 199 finished with value: 0.8159613372152299 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8206\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  406.000000  403.000000  415.000000  432.000000\n",
      "1                    TN  341.000000  352.000000  333.000000  326.000000\n",
      "2                    FP   86.000000   83.000000   89.000000   78.000000\n",
      "3                    FN   86.000000   81.000000   82.000000   83.000000\n",
      "4              Accuracy    0.812840    0.821545    0.813928    0.824810\n",
      "5             Precision    0.825203    0.829218    0.823413    0.847059\n",
      "6           Sensitivity    0.825203    0.832645    0.835010    0.838835\n",
      "7           Specificity    0.798600    0.809200    0.789100    0.806900\n",
      "8              F1 score    0.825203    0.830928    0.829171    0.842927\n",
      "9   F1 score (weighted)    0.812840    0.821524    0.813801    0.824921\n",
      "10     F1 score (macro)    0.811899    0.820994    0.812435    0.822447\n",
      "11    Balanced Accuracy    0.811899    0.820920    0.812055    0.822883\n",
      "12                  MCC    0.623798    0.641996    0.624965    0.644945\n",
      "13                  NPV    0.798600    0.812900    0.802400    0.797100\n",
      "14              ROC_AUC    0.811899    0.820920    0.812055    0.822883\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_knn_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_knn_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_knn_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_knn_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_knn_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_knn_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_knn_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_knn_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_knn_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_knn_3)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:32:35,871] Trial 200 finished with value: 0.7926805586447515 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:36,001] Trial 201 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:36,162] Trial 202 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:36,314] Trial 203 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:36,483] Trial 204 finished with value: 0.7836206845997948 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:36,639] Trial 205 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:36,794] Trial 206 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:36,953] Trial 207 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:37,110] Trial 208 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:37,271] Trial 209 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:37,432] Trial 210 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:37,591] Trial 211 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:37,751] Trial 212 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:37,912] Trial 213 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:38,073] Trial 214 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:38,233] Trial 215 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:38,394] Trial 216 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:38,578] Trial 217 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:38,735] Trial 218 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:38,893] Trial 219 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:39,084] Trial 220 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:39,272] Trial 221 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:39,465] Trial 222 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:39,632] Trial 223 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:39,800] Trial 224 finished with value: 0.8019088705816161 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:39,963] Trial 225 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:40,128] Trial 226 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:40,284] Trial 227 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:40,442] Trial 228 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:40,604] Trial 229 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:40,766] Trial 230 finished with value: 0.8190050164078759 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:40,922] Trial 231 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:41,085] Trial 232 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:41,241] Trial 233 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:41,402] Trial 234 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:41,571] Trial 235 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:41,738] Trial 236 finished with value: 0.7942184304747881 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:41,901] Trial 237 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:42,066] Trial 238 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:42,226] Trial 239 finished with value: 0.8160365046716832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:42,394] Trial 240 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:42,577] Trial 241 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:42,742] Trial 242 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:42,905] Trial 243 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:43,061] Trial 244 finished with value: 0.8160365046716832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:43,220] Trial 245 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:43,378] Trial 246 finished with value: 0.8160365046716832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:43,536] Trial 247 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:43,695] Trial 248 finished with value: 0.8154912820875365 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:43,856] Trial 249 finished with value: 0.8195857730281404 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8206\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  406.000000  403.000000  415.000000  432.000000   \n",
      "1                    TN  341.000000  352.000000  333.000000  326.000000   \n",
      "2                    FP   86.000000   83.000000   89.000000   78.000000   \n",
      "3                    FN   86.000000   81.000000   82.000000   83.000000   \n",
      "4              Accuracy    0.812840    0.821545    0.813928    0.824810   \n",
      "5             Precision    0.825203    0.829218    0.823413    0.847059   \n",
      "6           Sensitivity    0.825203    0.832645    0.835010    0.838835   \n",
      "7           Specificity    0.798600    0.809200    0.789100    0.806900   \n",
      "8              F1 score    0.825203    0.830928    0.829171    0.842927   \n",
      "9   F1 score (weighted)    0.812840    0.821524    0.813801    0.824921   \n",
      "10     F1 score (macro)    0.811899    0.820994    0.812435    0.822447   \n",
      "11    Balanced Accuracy    0.811899    0.820920    0.812055    0.822883   \n",
      "12                  MCC    0.623798    0.641996    0.624965    0.644945   \n",
      "13                  NPV    0.798600    0.812900    0.802400    0.797100   \n",
      "14              ROC_AUC    0.811899    0.820920    0.812055    0.822883   \n",
      "\n",
      "          Set4  \n",
      "0   390.000000  \n",
      "1   357.000000  \n",
      "2    73.000000  \n",
      "3    99.000000  \n",
      "4     0.812840  \n",
      "5     0.842333  \n",
      "6     0.797546  \n",
      "7     0.830200  \n",
      "8     0.819328  \n",
      "9     0.813030  \n",
      "10    0.812598  \n",
      "11    0.813889  \n",
      "12    0.626502  \n",
      "13    0.782900  \n",
      "14    0.813889  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_knn_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_knn_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_knn_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_knn_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_knn_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_knn_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_knn_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_knn_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_knn_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_knn_4)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:32:44,102] Trial 250 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:44,258] Trial 251 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:44,404] Trial 252 finished with value: 0.8104240317052966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:44,558] Trial 253 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:44,720] Trial 254 finished with value: 0.8104240317052966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:44,880] Trial 255 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:45,040] Trial 256 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:45,199] Trial 257 finished with value: 0.8104240317052966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:45,355] Trial 258 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:45,515] Trial 259 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:45,677] Trial 260 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:45,840] Trial 261 finished with value: 0.7973194110521854 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:46,561] Trial 262 finished with value: 0.8181902307645998 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:46,692] Trial 263 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:46,852] Trial 264 finished with value: 0.8106671737117749 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:47,015] Trial 265 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:47,176] Trial 266 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:47,336] Trial 267 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:47,499] Trial 268 finished with value: 0.8104240317052966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:47,667] Trial 269 finished with value: 0.7894045193089012 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:47,829] Trial 270 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:47,984] Trial 271 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:48,143] Trial 272 finished with value: 0.8104240317052966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:48,303] Trial 273 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:48,475] Trial 274 finished with value: 0.7915782108002963 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:48,635] Trial 275 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:48,789] Trial 276 finished with value: 0.8104240317052966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:48,952] Trial 277 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:49,096] Trial 278 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:49,255] Trial 279 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:49,419] Trial 280 finished with value: 0.8106671737117749 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:49,572] Trial 281 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:49,731] Trial 282 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:50,454] Trial 283 finished with value: 0.8181902307645998 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:50,584] Trial 284 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:50,746] Trial 285 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:50,906] Trial 286 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:51,059] Trial 287 finished with value: 0.8104240317052966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:51,221] Trial 288 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:51,377] Trial 289 finished with value: 0.8104240317052966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:51,535] Trial 290 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:51,694] Trial 291 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:51,853] Trial 292 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:52,020] Trial 293 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:52,177] Trial 294 finished with value: 0.8104240317052966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:52,336] Trial 295 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:52,500] Trial 296 finished with value: 0.8039455342153502 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:52,664] Trial 297 finished with value: 0.8104240317052966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:52,806] Trial 298 finished with value: 0.8147861668129532 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:52,967] Trial 299 finished with value: 0.8106671737117749 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 152 with value: 0.8206156842155841.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8206\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  406.000000  403.000000  415.000000  432.000000   \n",
      "1                    TN  341.000000  352.000000  333.000000  326.000000   \n",
      "2                    FP   86.000000   83.000000   89.000000   78.000000   \n",
      "3                    FN   86.000000   81.000000   82.000000   83.000000   \n",
      "4              Accuracy    0.812840    0.821545    0.813928    0.824810   \n",
      "5             Precision    0.825203    0.829218    0.823413    0.847059   \n",
      "6           Sensitivity    0.825203    0.832645    0.835010    0.838835   \n",
      "7           Specificity    0.798600    0.809200    0.789100    0.806900   \n",
      "8              F1 score    0.825203    0.830928    0.829171    0.842927   \n",
      "9   F1 score (weighted)    0.812840    0.821524    0.813801    0.824921   \n",
      "10     F1 score (macro)    0.811899    0.820994    0.812435    0.822447   \n",
      "11    Balanced Accuracy    0.811899    0.820920    0.812055    0.822883   \n",
      "12                  MCC    0.623798    0.641996    0.624965    0.644945   \n",
      "13                  NPV    0.798600    0.812900    0.802400    0.797100   \n",
      "14              ROC_AUC    0.811899    0.820920    0.812055    0.822883   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   390.000000  397.000000  \n",
      "1   357.000000  351.000000  \n",
      "2    73.000000   84.000000  \n",
      "3    99.000000   87.000000  \n",
      "4     0.812840    0.813928  \n",
      "5     0.842333    0.825364  \n",
      "6     0.797546    0.820248  \n",
      "7     0.830200    0.806900  \n",
      "8     0.819328    0.822798  \n",
      "9     0.813030    0.813959  \n",
      "10    0.812598    0.813461  \n",
      "11    0.813889    0.813572  \n",
      "12    0.626502    0.626939  \n",
      "13    0.782900    0.801400  \n",
      "14    0.813889    0.813572  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_knn_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_knn_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_knn_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_knn_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_knn_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_knn_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_knn_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_knn_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_knn_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_knn_5)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:32:53,208] Trial 300 finished with value: 0.8157419979709128 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:53,366] Trial 301 finished with value: 0.8205462631421276 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:53,528] Trial 302 finished with value: 0.8193741396166171 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:53,701] Trial 303 finished with value: 0.7850215541675081 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:53,864] Trial 304 finished with value: 0.8193741396166171 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 152 with value: 0.8206156842155841.\n",
      "[I 2023-12-04 11:32:54,582] Trial 305 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:32:55,273] Trial 306 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:32:55,967] Trial 307 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:32:56,659] Trial 308 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:32:57,351] Trial 309 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:32:58,044] Trial 310 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:32:58,738] Trial 311 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:32:59,433] Trial 312 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:00,126] Trial 313 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:00,819] Trial 314 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:01,512] Trial 315 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:02,205] Trial 316 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:02,898] Trial 317 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:03,592] Trial 318 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:04,286] Trial 319 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:04,980] Trial 320 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:05,674] Trial 321 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:06,367] Trial 322 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:07,060] Trial 323 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:07,753] Trial 324 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:08,447] Trial 325 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:09,140] Trial 326 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:09,835] Trial 327 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:10,529] Trial 328 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:11,222] Trial 329 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:11,916] Trial 330 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:12,609] Trial 331 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:13,302] Trial 332 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:13,997] Trial 333 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:14,694] Trial 334 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:15,387] Trial 335 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:16,085] Trial 336 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:16,779] Trial 337 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:17,473] Trial 338 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:18,167] Trial 339 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:18,861] Trial 340 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:19,560] Trial 341 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:20,249] Trial 342 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:20,938] Trial 343 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:21,627] Trial 344 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:22,315] Trial 345 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:23,004] Trial 346 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:23,694] Trial 347 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:24,384] Trial 348 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 305 with value: 0.8218374043594718.\n",
      "[I 2023-12-04 11:33:25,073] Trial 349 finished with value: 0.8218374043594718 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 305 with value: 0.8218374043594718.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8218\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 94\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  406.000000  403.000000  415.000000  432.000000   \n",
      "1                    TN  341.000000  352.000000  333.000000  326.000000   \n",
      "2                    FP   86.000000   83.000000   89.000000   78.000000   \n",
      "3                    FN   86.000000   81.000000   82.000000   83.000000   \n",
      "4              Accuracy    0.812840    0.821545    0.813928    0.824810   \n",
      "5             Precision    0.825203    0.829218    0.823413    0.847059   \n",
      "6           Sensitivity    0.825203    0.832645    0.835010    0.838835   \n",
      "7           Specificity    0.798600    0.809200    0.789100    0.806900   \n",
      "8              F1 score    0.825203    0.830928    0.829171    0.842927   \n",
      "9   F1 score (weighted)    0.812840    0.821524    0.813801    0.824921   \n",
      "10     F1 score (macro)    0.811899    0.820994    0.812435    0.822447   \n",
      "11    Balanced Accuracy    0.811899    0.820920    0.812055    0.822883   \n",
      "12                  MCC    0.623798    0.641996    0.624965    0.644945   \n",
      "13                  NPV    0.798600    0.812900    0.802400    0.797100   \n",
      "14              ROC_AUC    0.811899    0.820920    0.812055    0.822883   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   390.000000  397.000000  431.000000  \n",
      "1   357.000000  351.000000  322.000000  \n",
      "2    73.000000   84.000000   82.000000  \n",
      "3    99.000000   87.000000   84.000000  \n",
      "4     0.812840    0.813928    0.819369  \n",
      "5     0.842333    0.825364    0.840156  \n",
      "6     0.797546    0.820248    0.836893  \n",
      "7     0.830200    0.806900    0.797000  \n",
      "8     0.819328    0.822798    0.838521  \n",
      "9     0.813030    0.813959    0.819416  \n",
      "10    0.812598    0.813461    0.816792  \n",
      "11    0.813889    0.813572    0.816961  \n",
      "12    0.626502    0.626939    0.633591  \n",
      "13    0.782900    0.801400    0.793100  \n",
      "14    0.813889    0.813572    0.816961  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_knn_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_knn_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_knn_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_knn_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_knn_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_knn_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_knn_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_knn_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_knn_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_knn_6)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:33:25,964] Trial 350 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:26,653] Trial 351 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:27,343] Trial 352 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:28,033] Trial 353 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:28,723] Trial 354 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:29,414] Trial 355 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:30,105] Trial 356 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:30,796] Trial 357 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:31,486] Trial 358 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:32,175] Trial 359 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:32,864] Trial 360 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:33,554] Trial 361 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:34,245] Trial 362 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:34,950] Trial 363 finished with value: 0.7985714516591205 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:35,641] Trial 364 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:36,332] Trial 365 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:37,029] Trial 366 finished with value: 0.8104707626263684 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:37,719] Trial 367 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:38,411] Trial 368 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:39,101] Trial 369 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:39,792] Trial 370 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:40,495] Trial 371 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:41,186] Trial 372 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:41,879] Trial 373 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:42,578] Trial 374 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:43,274] Trial 375 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:43,970] Trial 376 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:44,668] Trial 377 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:45,368] Trial 378 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:46,068] Trial 379 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:46,766] Trial 380 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:47,458] Trial 381 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:48,153] Trial 382 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:48,845] Trial 383 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:49,540] Trial 384 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:50,235] Trial 385 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:50,929] Trial 386 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:51,628] Trial 387 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:52,333] Trial 388 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:53,037] Trial 389 finished with value: 0.8145418369006018 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:53,733] Trial 390 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:54,431] Trial 391 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:55,131] Trial 392 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:55,827] Trial 393 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:56,542] Trial 394 finished with value: 0.811652728880151 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:57,239] Trial 395 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:57,934] Trial 396 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:58,636] Trial 397 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:33:59,335] Trial 398 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:00,040] Trial 399 finished with value: 0.826424049487634 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8264\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 90\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  406.000000  403.000000  415.000000  432.000000   \n",
      "1                    TN  341.000000  352.000000  333.000000  326.000000   \n",
      "2                    FP   86.000000   83.000000   89.000000   78.000000   \n",
      "3                    FN   86.000000   81.000000   82.000000   83.000000   \n",
      "4              Accuracy    0.812840    0.821545    0.813928    0.824810   \n",
      "5             Precision    0.825203    0.829218    0.823413    0.847059   \n",
      "6           Sensitivity    0.825203    0.832645    0.835010    0.838835   \n",
      "7           Specificity    0.798600    0.809200    0.789100    0.806900   \n",
      "8              F1 score    0.825203    0.830928    0.829171    0.842927   \n",
      "9   F1 score (weighted)    0.812840    0.821524    0.813801    0.824921   \n",
      "10     F1 score (macro)    0.811899    0.820994    0.812435    0.822447   \n",
      "11    Balanced Accuracy    0.811899    0.820920    0.812055    0.822883   \n",
      "12                  MCC    0.623798    0.641996    0.624965    0.644945   \n",
      "13                  NPV    0.798600    0.812900    0.802400    0.797100   \n",
      "14              ROC_AUC    0.811899    0.820920    0.812055    0.822883   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   390.000000  397.000000  431.000000  419.000000  \n",
      "1   357.000000  351.000000  322.000000  346.000000  \n",
      "2    73.000000   84.000000   82.000000   84.000000  \n",
      "3    99.000000   87.000000   84.000000   70.000000  \n",
      "4     0.812840    0.813928    0.819369    0.832427  \n",
      "5     0.842333    0.825364    0.840156    0.833002  \n",
      "6     0.797546    0.820248    0.836893    0.856851  \n",
      "7     0.830200    0.806900    0.797000    0.804700  \n",
      "8     0.819328    0.822798    0.838521    0.844758  \n",
      "9     0.813030    0.813959    0.819416    0.832222  \n",
      "10    0.812598    0.813461    0.816792    0.831362  \n",
      "11    0.813889    0.813572    0.816961    0.830751  \n",
      "12    0.626502    0.626939    0.633591    0.663115  \n",
      "13    0.782900    0.801400    0.793100    0.831700  \n",
      "14    0.813889    0.813572    0.816961    0.830751  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_knn_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_knn_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_knn_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_knn_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_knn_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_knn_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_knn_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_knn_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_knn_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_knn_7)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:34:00,936] Trial 400 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:01,643] Trial 401 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:02,385] Trial 402 finished with value: 0.803562171289753 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:03,087] Trial 403 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:03,787] Trial 404 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:04,485] Trial 405 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:05,182] Trial 406 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:05,887] Trial 407 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:06,598] Trial 408 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:07,296] Trial 409 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:07,994] Trial 410 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:08,701] Trial 411 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:09,409] Trial 412 finished with value: 0.8023263181005869 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:10,120] Trial 413 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:10,822] Trial 414 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:11,527] Trial 415 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:12,225] Trial 416 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:12,926] Trial 417 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:13,626] Trial 418 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:14,332] Trial 419 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:15,031] Trial 420 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:15,779] Trial 421 finished with value: 0.803562171289753 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:16,494] Trial 422 finished with value: 0.799234838114449 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:17,198] Trial 423 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:17,905] Trial 424 finished with value: 0.8129949037597006 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:18,614] Trial 425 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:19,311] Trial 426 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:20,018] Trial 427 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:20,718] Trial 428 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:21,416] Trial 429 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:22,115] Trial 430 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:22,815] Trial 431 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:23,515] Trial 432 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:24,213] Trial 433 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:24,913] Trial 434 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:25,619] Trial 435 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:26,323] Trial 436 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:27,022] Trial 437 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:27,724] Trial 438 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:28,426] Trial 439 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:29,177] Trial 440 finished with value: 0.7959694471457813 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:29,882] Trial 441 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:30,590] Trial 442 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:31,296] Trial 443 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:32,003] Trial 444 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:32,712] Trial 445 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:33,414] Trial 446 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:34,114] Trial 447 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:34,822] Trial 448 finished with value: 0.8214278139809167 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:35,521] Trial 449 finished with value: 0.8167923849810611 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8264\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 90\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  406.000000  403.000000  415.000000  432.000000   \n",
      "1                    TN  341.000000  352.000000  333.000000  326.000000   \n",
      "2                    FP   86.000000   83.000000   89.000000   78.000000   \n",
      "3                    FN   86.000000   81.000000   82.000000   83.000000   \n",
      "4              Accuracy    0.812840    0.821545    0.813928    0.824810   \n",
      "5             Precision    0.825203    0.829218    0.823413    0.847059   \n",
      "6           Sensitivity    0.825203    0.832645    0.835010    0.838835   \n",
      "7           Specificity    0.798600    0.809200    0.789100    0.806900   \n",
      "8              F1 score    0.825203    0.830928    0.829171    0.842927   \n",
      "9   F1 score (weighted)    0.812840    0.821524    0.813801    0.824921   \n",
      "10     F1 score (macro)    0.811899    0.820994    0.812435    0.822447   \n",
      "11    Balanced Accuracy    0.811899    0.820920    0.812055    0.822883   \n",
      "12                  MCC    0.623798    0.641996    0.624965    0.644945   \n",
      "13                  NPV    0.798600    0.812900    0.802400    0.797100   \n",
      "14              ROC_AUC    0.811899    0.820920    0.812055    0.822883   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   390.000000  397.000000  431.000000  419.000000  383.000000  \n",
      "1   357.000000  351.000000  322.000000  346.000000  346.000000  \n",
      "2    73.000000   84.000000   82.000000   84.000000  103.000000  \n",
      "3    99.000000   87.000000   84.000000   70.000000   87.000000  \n",
      "4     0.812840    0.813928    0.819369    0.832427    0.793254  \n",
      "5     0.842333    0.825364    0.840156    0.833002    0.788066  \n",
      "6     0.797546    0.820248    0.836893    0.856851    0.814894  \n",
      "7     0.830200    0.806900    0.797000    0.804700    0.770600  \n",
      "8     0.819328    0.822798    0.838521    0.844758    0.801255  \n",
      "9     0.813030    0.813959    0.819416    0.832222    0.793108  \n",
      "10    0.812598    0.813461    0.816792    0.831362    0.792918  \n",
      "11    0.813889    0.813572    0.816961    0.830751    0.792747  \n",
      "12    0.626502    0.626939    0.633591    0.663115    0.586318  \n",
      "13    0.782900    0.801400    0.793100    0.831700    0.799100  \n",
      "14    0.813889    0.813572    0.816961    0.830751    0.792747  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_knn_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_knn_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_knn_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_knn_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_knn_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_knn_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_knn_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_knn_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_knn_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_knn_8)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:34:36,427] Trial 450 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:37,128] Trial 451 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:37,828] Trial 452 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:38,538] Trial 453 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:39,247] Trial 454 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:39,955] Trial 455 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:40,667] Trial 456 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:41,380] Trial 457 finished with value: 0.8039401969262089 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:42,086] Trial 458 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:42,833] Trial 459 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:43,534] Trial 460 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:44,233] Trial 461 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:44,980] Trial 462 finished with value: 0.7932617053995994 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:45,683] Trial 463 finished with value: 0.808548113173757 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:46,383] Trial 464 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:47,090] Trial 465 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:47,795] Trial 466 finished with value: 0.8061367224693896 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:48,496] Trial 467 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:49,195] Trial 468 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:49,896] Trial 469 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:50,598] Trial 470 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:51,299] Trial 471 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:51,999] Trial 472 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:52,701] Trial 473 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:53,400] Trial 474 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:54,100] Trial 475 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:54,801] Trial 476 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:55,502] Trial 477 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:56,204] Trial 478 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:56,911] Trial 479 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:57,655] Trial 480 finished with value: 0.7932617053995994 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:58,357] Trial 481 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:59,061] Trial 482 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:34:59,768] Trial 483 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:00,471] Trial 484 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:01,175] Trial 485 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:01,883] Trial 486 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:02,584] Trial 487 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:03,284] Trial 488 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:03,991] Trial 489 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:04,700] Trial 490 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:05,409] Trial 491 finished with value: 0.8014780413921967 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:06,112] Trial 492 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:06,817] Trial 493 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:07,517] Trial 494 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:08,221] Trial 495 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:08,922] Trial 496 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:09,681] Trial 497 finished with value: 0.7932617053995994 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:10,387] Trial 498 finished with value: 0.8112154653893351 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 350 with value: 0.826424049487634.\n",
      "[I 2023-12-04 11:35:11,094] Trial 499 finished with value: 0.8103711005393921 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 350 with value: 0.826424049487634.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8264\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 90\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  406.000000  403.000000  415.000000  432.000000   \n",
      "1                    TN  341.000000  352.000000  333.000000  326.000000   \n",
      "2                    FP   86.000000   83.000000   89.000000   78.000000   \n",
      "3                    FN   86.000000   81.000000   82.000000   83.000000   \n",
      "4              Accuracy    0.812840    0.821545    0.813928    0.824810   \n",
      "5             Precision    0.825203    0.829218    0.823413    0.847059   \n",
      "6           Sensitivity    0.825203    0.832645    0.835010    0.838835   \n",
      "7           Specificity    0.798600    0.809200    0.789100    0.806900   \n",
      "8              F1 score    0.825203    0.830928    0.829171    0.842927   \n",
      "9   F1 score (weighted)    0.812840    0.821524    0.813801    0.824921   \n",
      "10     F1 score (macro)    0.811899    0.820994    0.812435    0.822447   \n",
      "11    Balanced Accuracy    0.811899    0.820920    0.812055    0.822883   \n",
      "12                  MCC    0.623798    0.641996    0.624965    0.644945   \n",
      "13                  NPV    0.798600    0.812900    0.802400    0.797100   \n",
      "14              ROC_AUC    0.811899    0.820920    0.812055    0.822883   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   390.000000  397.000000  431.000000  419.000000  383.000000  406.000000  \n",
      "1   357.000000  351.000000  322.000000  346.000000  346.000000  368.000000  \n",
      "2    73.000000   84.000000   82.000000   84.000000  103.000000   82.000000  \n",
      "3    99.000000   87.000000   84.000000   70.000000   87.000000   63.000000  \n",
      "4     0.812840    0.813928    0.819369    0.832427    0.793254    0.842220  \n",
      "5     0.842333    0.825364    0.840156    0.833002    0.788066    0.831967  \n",
      "6     0.797546    0.820248    0.836893    0.856851    0.814894    0.865672  \n",
      "7     0.830200    0.806900    0.797000    0.804700    0.770600    0.817800  \n",
      "8     0.819328    0.822798    0.838521    0.844758    0.801255    0.848485  \n",
      "9     0.813030    0.813959    0.819416    0.832222    0.793108    0.842085  \n",
      "10    0.812598    0.813461    0.816792    0.831362    0.792918    0.841950  \n",
      "11    0.813889    0.813572    0.816961    0.830751    0.792747    0.841725  \n",
      "12    0.626502    0.626939    0.633591    0.663115    0.586318    0.684621  \n",
      "13    0.782900    0.801400    0.793100    0.831700    0.799100    0.853800  \n",
      "14    0.813889    0.813572    0.816961    0.830751    0.792747    0.841725  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_knn_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_knn_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_knn_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_knn_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_knn_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_knn_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_knn_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_knn_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_knn_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_knn_9)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>202.200000</td>\n",
       "      <td>8.256984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>175.300000</td>\n",
       "      <td>10.832564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>7.633988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>38.200000</td>\n",
       "      <td>5.202563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.822089</td>\n",
       "      <td>0.019463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.823362</td>\n",
       "      <td>0.027589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.841320</td>\n",
       "      <td>0.018998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.801230</td>\n",
       "      <td>0.032615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.831956</td>\n",
       "      <td>0.017398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.821965</td>\n",
       "      <td>0.019559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.821251</td>\n",
       "      <td>0.019517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.821272</td>\n",
       "      <td>0.019642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.643351</td>\n",
       "      <td>0.038741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.025049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.821272</td>\n",
       "      <td>0.019642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       202.200000     8.256984\n",
       "1                    TN       175.300000    10.832564\n",
       "2                    FP        43.500000     7.633988\n",
       "3                    FN        38.200000     5.202563\n",
       "4              Accuracy         0.822089     0.019463\n",
       "5             Precision         0.823362     0.027589\n",
       "6           Sensitivity         0.841320     0.018998\n",
       "7           Specificity         0.801230     0.032615\n",
       "8              F1 score         0.831956     0.017398\n",
       "9   F1 score (weighted)         0.821965     0.019559\n",
       "10     F1 score (macro)         0.821251     0.019517\n",
       "11    Balanced Accuracy         0.821272     0.019642\n",
       "12                  MCC         0.643351     0.038741\n",
       "13                  NPV         0.820790     0.025049\n",
       "14              ROC_AUC         0.821272     0.019642"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>432.000000</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>419.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>408.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>368.000000</td>\n",
       "      <td>344.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>84.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>82.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.812840</td>\n",
       "      <td>0.821545</td>\n",
       "      <td>0.813928</td>\n",
       "      <td>0.824810</td>\n",
       "      <td>0.812840</td>\n",
       "      <td>0.813928</td>\n",
       "      <td>0.819369</td>\n",
       "      <td>0.832427</td>\n",
       "      <td>0.793254</td>\n",
       "      <td>0.842220</td>\n",
       "      <td>0.818716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.825203</td>\n",
       "      <td>0.829218</td>\n",
       "      <td>0.823413</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.842333</td>\n",
       "      <td>0.825364</td>\n",
       "      <td>0.840156</td>\n",
       "      <td>0.833002</td>\n",
       "      <td>0.788066</td>\n",
       "      <td>0.831967</td>\n",
       "      <td>0.828578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.825203</td>\n",
       "      <td>0.832645</td>\n",
       "      <td>0.835010</td>\n",
       "      <td>0.838835</td>\n",
       "      <td>0.797546</td>\n",
       "      <td>0.820248</td>\n",
       "      <td>0.836893</td>\n",
       "      <td>0.856851</td>\n",
       "      <td>0.814894</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.832380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.809200</td>\n",
       "      <td>0.789100</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.830200</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.770600</td>\n",
       "      <td>0.817800</td>\n",
       "      <td>0.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.825203</td>\n",
       "      <td>0.830928</td>\n",
       "      <td>0.829171</td>\n",
       "      <td>0.842927</td>\n",
       "      <td>0.819328</td>\n",
       "      <td>0.822798</td>\n",
       "      <td>0.838521</td>\n",
       "      <td>0.844758</td>\n",
       "      <td>0.801255</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.830337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.812840</td>\n",
       "      <td>0.821524</td>\n",
       "      <td>0.813801</td>\n",
       "      <td>0.824921</td>\n",
       "      <td>0.813030</td>\n",
       "      <td>0.813959</td>\n",
       "      <td>0.819416</td>\n",
       "      <td>0.832222</td>\n",
       "      <td>0.793108</td>\n",
       "      <td>0.842085</td>\n",
       "      <td>0.818691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.811899</td>\n",
       "      <td>0.820994</td>\n",
       "      <td>0.812435</td>\n",
       "      <td>0.822447</td>\n",
       "      <td>0.812598</td>\n",
       "      <td>0.813461</td>\n",
       "      <td>0.816792</td>\n",
       "      <td>0.831362</td>\n",
       "      <td>0.792918</td>\n",
       "      <td>0.841950</td>\n",
       "      <td>0.817686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.811899</td>\n",
       "      <td>0.820920</td>\n",
       "      <td>0.812055</td>\n",
       "      <td>0.822883</td>\n",
       "      <td>0.813889</td>\n",
       "      <td>0.813572</td>\n",
       "      <td>0.816961</td>\n",
       "      <td>0.830751</td>\n",
       "      <td>0.792747</td>\n",
       "      <td>0.841725</td>\n",
       "      <td>0.817740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.623798</td>\n",
       "      <td>0.641996</td>\n",
       "      <td>0.624965</td>\n",
       "      <td>0.644945</td>\n",
       "      <td>0.626502</td>\n",
       "      <td>0.626939</td>\n",
       "      <td>0.633591</td>\n",
       "      <td>0.663115</td>\n",
       "      <td>0.586318</td>\n",
       "      <td>0.684621</td>\n",
       "      <td>0.635679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.812900</td>\n",
       "      <td>0.802400</td>\n",
       "      <td>0.797100</td>\n",
       "      <td>0.782900</td>\n",
       "      <td>0.801400</td>\n",
       "      <td>0.793100</td>\n",
       "      <td>0.831700</td>\n",
       "      <td>0.799100</td>\n",
       "      <td>0.853800</td>\n",
       "      <td>0.807300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.811899</td>\n",
       "      <td>0.820920</td>\n",
       "      <td>0.812055</td>\n",
       "      <td>0.822883</td>\n",
       "      <td>0.813889</td>\n",
       "      <td>0.813572</td>\n",
       "      <td>0.816961</td>\n",
       "      <td>0.830751</td>\n",
       "      <td>0.792747</td>\n",
       "      <td>0.841725</td>\n",
       "      <td>0.817740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  406.000000  403.000000  415.000000  432.000000   \n",
       "1                    TN  341.000000  352.000000  333.000000  326.000000   \n",
       "2                    FP   86.000000   83.000000   89.000000   78.000000   \n",
       "3                    FN   86.000000   81.000000   82.000000   83.000000   \n",
       "4              Accuracy    0.812840    0.821545    0.813928    0.824810   \n",
       "5             Precision    0.825203    0.829218    0.823413    0.847059   \n",
       "6           Sensitivity    0.825203    0.832645    0.835010    0.838835   \n",
       "7           Specificity    0.798600    0.809200    0.789100    0.806900   \n",
       "8              F1 score    0.825203    0.830928    0.829171    0.842927   \n",
       "9   F1 score (weighted)    0.812840    0.821524    0.813801    0.824921   \n",
       "10     F1 score (macro)    0.811899    0.820994    0.812435    0.822447   \n",
       "11    Balanced Accuracy    0.811899    0.820920    0.812055    0.822883   \n",
       "12                  MCC    0.623798    0.641996    0.624965    0.644945   \n",
       "13                  NPV    0.798600    0.812900    0.802400    0.797100   \n",
       "14              ROC_AUC    0.811899    0.820920    0.812055    0.822883   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   390.000000  397.000000  431.000000  419.000000  383.000000  406.000000   \n",
       "1   357.000000  351.000000  322.000000  346.000000  346.000000  368.000000   \n",
       "2    73.000000   84.000000   82.000000   84.000000  103.000000   82.000000   \n",
       "3    99.000000   87.000000   84.000000   70.000000   87.000000   63.000000   \n",
       "4     0.812840    0.813928    0.819369    0.832427    0.793254    0.842220   \n",
       "5     0.842333    0.825364    0.840156    0.833002    0.788066    0.831967   \n",
       "6     0.797546    0.820248    0.836893    0.856851    0.814894    0.865672   \n",
       "7     0.830200    0.806900    0.797000    0.804700    0.770600    0.817800   \n",
       "8     0.819328    0.822798    0.838521    0.844758    0.801255    0.848485   \n",
       "9     0.813030    0.813959    0.819416    0.832222    0.793108    0.842085   \n",
       "10    0.812598    0.813461    0.816792    0.831362    0.792918    0.841950   \n",
       "11    0.813889    0.813572    0.816961    0.830751    0.792747    0.841725   \n",
       "12    0.626502    0.626939    0.633591    0.663115    0.586318    0.684621   \n",
       "13    0.782900    0.801400    0.793100    0.831700    0.799100    0.853800   \n",
       "14    0.813889    0.813572    0.816961    0.830751    0.792747    0.841725   \n",
       "\n",
       "           ave  \n",
       "0   408.200000  \n",
       "1   344.200000  \n",
       "2    84.400000  \n",
       "3    82.200000  \n",
       "4     0.818716  \n",
       "5     0.828578  \n",
       "6     0.832380  \n",
       "7     0.803100  \n",
       "8     0.830337  \n",
       "9     0.818691  \n",
       "10    0.817686  \n",
       "11    0.817740  \n",
       "12    0.635679  \n",
       "13    0.807300  \n",
       "14    0.817740  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.826613</td>\n",
       "      <td>0.020595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.824938</td>\n",
       "      <td>0.026495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.849186</td>\n",
       "      <td>0.023316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.802014</td>\n",
       "      <td>0.029996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.836635</td>\n",
       "      <td>0.020220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.826435</td>\n",
       "      <td>0.020668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.825748</td>\n",
       "      <td>0.020705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.825596</td>\n",
       "      <td>0.020805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.652464</td>\n",
       "      <td>0.041309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.828804</td>\n",
       "      <td>0.025823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.825596</td>\n",
       "      <td>0.020805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.826613     0.020595\n",
       "1             Precision         0.824938     0.026495\n",
       "2           Sensitivity         0.849186     0.023316\n",
       "3           Specificity         0.802014     0.029996\n",
       "4              F1 score         0.836635     0.020220\n",
       "5   F1 score (weighted)         0.826435     0.020668\n",
       "6      F1 score (macro)         0.825748     0.020705\n",
       "7     Balanced Accuracy         0.825596     0.020805\n",
       "8                   MCC         0.652464     0.041309\n",
       "9                   NPV         0.828804     0.025823\n",
       "10              ROC_AUC         0.825596     0.020805"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_knn)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_knn))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_knn))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_knn))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_knn))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_knn, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_knn, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_knn))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_knn))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_knn))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model f1_score 0.8066 with a standard deviation of 0.0176\n",
      "KNN optimized model f1_score 0.8132 with a standard deviation of 0.0157\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"KNN baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn_clf.joblib']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the modesls, both the one with optimized hyperparameters and the initial one\n",
    "joblib.dump(knn_clf, \"OUTPUT/knn_clf.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn_clf.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       210.300000    10.583530\n",
      "1                    TN       170.000000     9.165151\n",
      "2                    FP        48.800000     6.321041\n",
      "3                    FN        30.100000     4.840799\n",
      "4              Accuracy         0.828186     0.015372\n",
      "5             Precision         0.811652     0.022795\n",
      "6           Sensitivity         0.874698     0.019862\n",
      "7           Specificity         0.777070     0.025271\n",
      "8              F1 score         0.841762     0.016029\n",
      "9   F1 score (weighted)         0.827555     0.015451\n",
      "10     F1 score (macro)         0.826630     0.015060\n",
      "11    Balanced Accuracy         0.825888     0.014532\n",
      "12                  MCC         0.656586     0.029139\n",
      "13                  NPV         0.849780     0.020790\n",
      "14              ROC_AUC         0.825888     0.014532\n",
      "CPU times: user 22.5 s, sys: 52 ms, total: 22.5 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_clf = SVC()\n",
    "    \n",
    "    svm_clf.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_clf.predict(X_test) \n",
    "   \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggestegorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVC(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average='macro')\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggestegorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "  \n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVC(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:35:47,733] A new study created in memory with name: SVM_classifier\n",
      "[I 2023-12-04 11:36:01,849] Trial 0 finished with value: 0.8297864960854554 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:36:19,147] Trial 1 finished with value: 0.5004429785789356 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:36:35,274] Trial 2 finished with value: 0.34218158720348 and parameters: {'C': 0.125, 'gamma': 0.25}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:36:51,274] Trial 3 finished with value: 0.34218158720348 and parameters: {'C': 0.015625, 'gamma': 0.000244140625}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:37:07,364] Trial 4 finished with value: 0.34218158720348 and parameters: {'C': 0.0078125, 'gamma': 0.00390625}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:37:16,354] Trial 5 finished with value: 0.7977750254895243 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:37:28,490] Trial 6 finished with value: 0.7830431291121915 and parameters: {'C': 0.5, 'gamma': 0.0078125}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:37:44,540] Trial 7 finished with value: 0.6734617780379961 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:37:57,121] Trial 8 finished with value: 0.7569849616125123 and parameters: {'C': 1.0, 'gamma': 0.001953125}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:38:09,497] Trial 9 finished with value: 0.8067121050738661 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:38:28,913] Trial 10 finished with value: 0.4030678296724319 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:38:45,872] Trial 11 finished with value: 0.34218158720348 and parameters: {'C': 0.5, 'gamma': 6.103515625e-05}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:39:02,696] Trial 12 finished with value: 0.7913664180112807 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 0 with value: 0.8297864960854554.\n",
      "[I 2023-12-04 11:39:19,415] Trial 13 finished with value: 0.8396060403543674 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 13 with value: 0.8396060403543674.\n",
      "[I 2023-12-04 11:39:35,900] Trial 14 finished with value: 0.4613603625086696 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 13 with value: 0.8396060403543674.\n",
      "[I 2023-12-04 11:39:52,106] Trial 15 finished with value: 0.8393235437286666 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 13 with value: 0.8396060403543674.\n",
      "[I 2023-12-04 11:40:08,689] Trial 16 finished with value: 0.8393235437286666 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 13 with value: 0.8396060403543674.\n",
      "[I 2023-12-04 11:40:25,288] Trial 17 finished with value: 0.8393235437286666 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 13 with value: 0.8396060403543674.\n",
      "[I 2023-12-04 11:40:41,892] Trial 18 finished with value: 0.43140152270486964 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 13 with value: 0.8396060403543674.\n",
      "[I 2023-12-04 11:40:58,315] Trial 19 finished with value: 0.34218158720348 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 13 with value: 0.8396060403543674.\n",
      "[I 2023-12-04 11:41:15,769] Trial 20 finished with value: 0.34218158720348 and parameters: {'C': 0.25, 'gamma': 4.0}. Best is trial 13 with value: 0.8396060403543674.\n",
      "[I 2023-12-04 11:41:32,383] Trial 21 finished with value: 0.8393235437286666 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 13 with value: 0.8396060403543674.\n",
      "[I 2023-12-04 11:41:49,052] Trial 22 finished with value: 0.8393235437286666 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 13 with value: 0.8396060403543674.\n",
      "[I 2023-12-04 11:42:04,219] Trial 23 finished with value: 0.8496816348376213 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:42:20,174] Trial 24 finished with value: 0.720230509526817 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:42:35,390] Trial 25 finished with value: 0.8496816348376213 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:42:50,499] Trial 26 finished with value: 0.8496816348376213 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:43:05,625] Trial 27 finished with value: 0.8496816348376213 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:43:20,766] Trial 28 finished with value: 0.8496816348376213 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:43:35,623] Trial 29 finished with value: 0.8471823432416808 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:43:51,729] Trial 30 finished with value: 0.34218158720348 and parameters: {'C': 0.0625, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:44:06,978] Trial 31 finished with value: 0.8496816348376213 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:44:22,119] Trial 32 finished with value: 0.8496816348376213 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:44:39,059] Trial 33 finished with value: 0.4154224661467373 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:44:54,121] Trial 34 finished with value: 0.8496816348376213 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:45:10,735] Trial 35 finished with value: 0.3871243694079567 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:45:26,498] Trial 36 finished with value: 0.7273717061816722 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:45:43,012] Trial 37 finished with value: 0.34218158720348 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:45:59,589] Trial 38 finished with value: 0.34218158720348 and parameters: {'C': 0.0078125, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:46:16,043] Trial 39 finished with value: 0.563912749475072 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:46:28,569] Trial 40 finished with value: 0.8269012285767452 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:46:43,481] Trial 41 finished with value: 0.8496816348376213 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:46:58,406] Trial 42 finished with value: 0.8496816348376213 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:47:10,744] Trial 43 finished with value: 0.7551365135191017 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:47:21,403] Trial 44 finished with value: 0.7946112773635067 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:47:37,861] Trial 45 finished with value: 0.34218158720348 and parameters: {'C': 0.25, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:47:57,090] Trial 46 finished with value: 0.4030678296724319 and parameters: {'C': 2.0, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:48:12,830] Trial 47 finished with value: 0.6734617780379961 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:48:26,304] Trial 48 finished with value: 0.8411250901724564 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:48:42,914] Trial 49 finished with value: 0.34218158720348 and parameters: {'C': 0.0625, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8497\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_classifier\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  425.000000\n",
      "1                    TN  338.000000\n",
      "2                    FP   89.000000\n",
      "3                    FN   67.000000\n",
      "4              Accuracy    0.830250\n",
      "5             Precision    0.826848\n",
      "6           Sensitivity    0.863821\n",
      "7           Specificity    0.791600\n",
      "8              F1 score    0.844930\n",
      "9   F1 score (weighted)    0.829862\n",
      "10     F1 score (macro)    0.828715\n",
      "11    Balanced Accuracy    0.827695\n",
      "12                  MCC    0.658396\n",
      "13                  NPV    0.834600\n",
      "14              ROC_AUC    0.827695\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_svm_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_svm_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_svm_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_svm_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_svm_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_svm_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_svm_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_svm_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_svm_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_svm_0)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 11:49:00,693] Trial 50 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:49:15,930] Trial 51 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:49:30,832] Trial 52 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:49:46,652] Trial 53 finished with value: 0.34312060587687826 and parameters: {'C': 0.03125, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:50:02,391] Trial 54 finished with value: 0.45772262052317664 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:50:18,513] Trial 55 finished with value: 0.414351860200331 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:50:34,608] Trial 56 finished with value: 0.34312060587687826 and parameters: {'C': 0.125, 'gamma': 1.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:50:48,283] Trial 57 finished with value: 0.7987152399780145 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:51:04,225] Trial 58 finished with value: 0.34312060587687826 and parameters: {'C': 0.015625, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:51:20,509] Trial 59 finished with value: 0.34312060587687826 and parameters: {'C': 0.0078125, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:51:29,805] Trial 60 finished with value: 0.790105170028281 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:51:44,783] Trial 61 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:52:00,213] Trial 62 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:52:15,825] Trial 63 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:52:31,548] Trial 64 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:52:43,681] Trial 65 finished with value: 0.757340475517047 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:52:59,569] Trial 66 finished with value: 0.5342869631040316 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:53:15,414] Trial 67 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:53:25,939] Trial 68 finished with value: 0.8028713198333015 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:53:41,187] Trial 69 finished with value: 0.8248300759397704 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:53:51,096] Trial 70 finished with value: 0.7838707882601986 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:54:07,692] Trial 71 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:54:23,434] Trial 72 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:54:39,022] Trial 73 finished with value: 0.7088318066371473 and parameters: {'C': 0.25, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:54:49,917] Trial 74 finished with value: 0.8096941244380368 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:55:07,332] Trial 75 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:55:24,343] Trial 76 finished with value: 0.34312060587687826 and parameters: {'C': 0.0625, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:55:43,817] Trial 77 finished with value: 0.414351860200331 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:55:59,299] Trial 78 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:56:17,035] Trial 79 finished with value: 0.34312060587687826 and parameters: {'C': 0.03125, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:56:33,915] Trial 80 finished with value: 0.361516456643982 and parameters: {'C': 0.5, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:56:46,261] Trial 81 finished with value: 0.8306864850012318 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:57:02,480] Trial 82 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:57:18,071] Trial 83 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:57:35,367] Trial 84 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:57:50,836] Trial 85 finished with value: 0.7162388056024751 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:58:07,270] Trial 86 finished with value: 0.34312060587687826 and parameters: {'C': 0.015625, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:58:23,537] Trial 87 finished with value: 0.34312060587687826 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:58:40,271] Trial 88 finished with value: 0.4376598999421855 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:58:56,653] Trial 89 finished with value: 0.423566008460403 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:59:13,311] Trial 90 finished with value: 0.414351860200331 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:59:28,756] Trial 91 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 11:59:45,845] Trial 92 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:00:03,859] Trial 93 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:00:19,624] Trial 94 finished with value: 0.8328906451119721 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:00:30,302] Trial 95 finished with value: 0.7808738084821817 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:00:45,943] Trial 96 finished with value: 0.8301487995134025 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:01:02,578] Trial 97 finished with value: 0.5342869631040316 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:01:18,010] Trial 98 finished with value: 0.7137507869027798 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:01:28,189] Trial 99 finished with value: 0.8028713198333015 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8497\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  425.000000  427.000000\n",
      "1                    TN  338.000000  365.000000\n",
      "2                    FP   89.000000   70.000000\n",
      "3                    FN   67.000000   57.000000\n",
      "4              Accuracy    0.830250    0.861806\n",
      "5             Precision    0.826848    0.859155\n",
      "6           Sensitivity    0.863821    0.882231\n",
      "7           Specificity    0.791600    0.839100\n",
      "8              F1 score    0.844930    0.870540\n",
      "9   F1 score (weighted)    0.829862    0.861674\n",
      "10     F1 score (macro)    0.828715    0.861174\n",
      "11    Balanced Accuracy    0.827695    0.860656\n",
      "12                  MCC    0.658396    0.722697\n",
      "13                  NPV    0.834600    0.864900\n",
      "14              ROC_AUC    0.827695    0.860656\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_svm_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_svm_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_svm_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_svm_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_svm_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_svm_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_svm_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_svm_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_svm_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_svm_1)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 12:01:45,391] Trial 100 finished with value: 0.7808350719337488 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:02:01,178] Trial 101 finished with value: 0.8380128009742748 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:02:18,468] Trial 102 finished with value: 0.8380128009742748 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:02:36,808] Trial 103 finished with value: 0.8345810530138881 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:02:53,616] Trial 104 finished with value: 0.8380128009742748 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:03:10,670] Trial 105 finished with value: 0.7433789748883953 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:03:23,033] Trial 106 finished with value: 0.8258795455828858 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:03:44,052] Trial 107 finished with value: 0.3605798226601591 and parameters: {'C': 0.0625, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:04:02,736] Trial 108 finished with value: 0.8341361363587543 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:04:22,085] Trial 109 finished with value: 0.34158679165476 and parameters: {'C': 0.03125, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:04:42,677] Trial 110 finished with value: 0.4034776282091572 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:04:56,831] Trial 111 finished with value: 0.831293492473901 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:05:14,058] Trial 112 finished with value: 0.8380128009742748 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:05:28,916] Trial 113 finished with value: 0.8349406413669722 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:05:41,777] Trial 114 finished with value: 0.8009617246489629 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:05:58,156] Trial 115 finished with value: 0.8380128009742748 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:06:14,665] Trial 116 finished with value: 0.790234683559183 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:06:30,538] Trial 117 finished with value: 0.34158679165476 and parameters: {'C': 0.125, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:06:47,422] Trial 118 finished with value: 0.34158679165476 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:07:04,264] Trial 119 finished with value: 0.39689954344121 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:07:20,079] Trial 120 finished with value: 0.34158679165476 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:07:35,344] Trial 121 finished with value: 0.8341361363587543 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:07:50,496] Trial 122 finished with value: 0.8341361363587543 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:08:06,715] Trial 123 finished with value: 0.4034776282091572 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:08:21,960] Trial 124 finished with value: 0.8380128009742748 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:08:37,788] Trial 125 finished with value: 0.43309143718358784 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:08:51,737] Trial 126 finished with value: 0.8355250711765798 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:09:05,626] Trial 127 finished with value: 0.7172866449636928 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:09:20,955] Trial 128 finished with value: 0.8376636944325192 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:09:33,014] Trial 129 finished with value: 0.7806826200578671 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:09:48,945] Trial 130 finished with value: 0.42041423458321825 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:10:04,448] Trial 131 finished with value: 0.8344002926373536 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:10:19,953] Trial 132 finished with value: 0.8376636944325192 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:10:35,818] Trial 133 finished with value: 0.8344002926373536 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:10:53,137] Trial 134 finished with value: 0.8296556637919894 and parameters: {'C': 1.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:11:08,422] Trial 135 finished with value: 0.8384711662493229 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:11:18,913] Trial 136 finished with value: 0.8094050521525414 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:11:33,167] Trial 137 finished with value: 0.7808350719337488 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:11:49,019] Trial 138 finished with value: 0.5606386535111019 and parameters: {'C': 2.0, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:12:04,252] Trial 139 finished with value: 0.8380128009742748 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:12:19,871] Trial 140 finished with value: 0.347015144769515 and parameters: {'C': 0.0625, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:12:30,167] Trial 141 finished with value: 0.8047963478349741 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:12:45,701] Trial 142 finished with value: 0.8344002926373536 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:12:58,165] Trial 143 finished with value: 0.7659806401813434 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:13:13,665] Trial 144 finished with value: 0.8344002926373536 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:13:28,866] Trial 145 finished with value: 0.8379992245939538 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:13:39,304] Trial 146 finished with value: 0.8258795455828858 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:13:54,559] Trial 147 finished with value: 0.34158679165476 and parameters: {'C': 0.03125, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:14:07,537] Trial 148 finished with value: 0.7433789748883953 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:14:21,061] Trial 149 finished with value: 0.812644268958928 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8497\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  425.000000  427.000000  414.000000\n",
      "1                    TN  338.000000  365.000000  344.000000\n",
      "2                    FP   89.000000   70.000000   78.000000\n",
      "3                    FN   67.000000   57.000000   83.000000\n",
      "4              Accuracy    0.830250    0.861806    0.824810\n",
      "5             Precision    0.826848    0.859155    0.841463\n",
      "6           Sensitivity    0.863821    0.882231    0.832998\n",
      "7           Specificity    0.791600    0.839100    0.815200\n",
      "8              F1 score    0.844930    0.870540    0.837209\n",
      "9   F1 score (weighted)    0.829862    0.861674    0.824883\n",
      "10     F1 score (macro)    0.828715    0.861174    0.823787\n",
      "11    Balanced Accuracy    0.827695    0.860656    0.824082\n",
      "12                  MCC    0.658396    0.722697    0.647624\n",
      "13                  NPV    0.834600    0.864900    0.805600\n",
      "14              ROC_AUC    0.827695    0.860656    0.824082\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_svm_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_svm_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_svm_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_svm_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_svm_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_svm_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_svm_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_svm_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_svm_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_svm_2)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 12:14:42,517] Trial 150 finished with value: 0.40320515585997746 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:14:58,946] Trial 151 finished with value: 0.8356025116809269 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:15:15,337] Trial 152 finished with value: 0.8356025116809269 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:15:31,611] Trial 153 finished with value: 0.4528699604027821 and parameters: {'C': 0.125, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:15:46,496] Trial 154 finished with value: 0.8474603908981269 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:16:01,589] Trial 155 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:16:16,790] Trial 156 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:16:31,984] Trial 157 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:16:47,112] Trial 158 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:17:02,170] Trial 159 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:17:17,271] Trial 160 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:17:32,266] Trial 161 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:17:48,047] Trial 162 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:18:04,177] Trial 163 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:18:19,674] Trial 164 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:18:35,332] Trial 165 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:18:51,740] Trial 166 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:19:08,084] Trial 167 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:19:24,966] Trial 168 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:19:40,932] Trial 169 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:19:57,044] Trial 170 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:20:14,809] Trial 171 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:20:31,205] Trial 172 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:20:47,166] Trial 173 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:21:02,798] Trial 174 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:21:19,367] Trial 175 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:21:35,745] Trial 176 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:21:51,850] Trial 177 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:22:08,664] Trial 178 finished with value: 0.3395075279250065 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:22:25,713] Trial 179 finished with value: 0.4547427874950727 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:22:42,822] Trial 180 finished with value: 0.7975950955255324 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:22:58,171] Trial 181 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:23:14,953] Trial 182 finished with value: 0.3395075279250065 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:23:31,192] Trial 183 finished with value: 0.846368899507444 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:23:46,752] Trial 184 finished with value: 0.8475004617720504 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:23:58,722] Trial 185 finished with value: 0.7794110836099595 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:24:14,892] Trial 186 finished with value: 0.8475004617720504 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:24:30,614] Trial 187 finished with value: 0.8286258116246925 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:24:49,400] Trial 188 finished with value: 0.40534675724544017 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:25:00,148] Trial 189 finished with value: 0.7901208897559767 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:25:16,050] Trial 190 finished with value: 0.8475004617720504 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:25:31,517] Trial 191 finished with value: 0.8475004617720504 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:25:46,963] Trial 192 finished with value: 0.8475004617720504 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:26:02,637] Trial 193 finished with value: 0.8475004617720504 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:26:19,736] Trial 194 finished with value: 0.41687959676443465 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:26:34,549] Trial 195 finished with value: 0.8475004617720504 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:26:44,460] Trial 196 finished with value: 0.8055242038077794 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:27:01,881] Trial 197 finished with value: 0.43063439145755344 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:27:17,213] Trial 198 finished with value: 0.8475004617720504 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:27:31,819] Trial 199 finished with value: 0.8475004617720504 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8497\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  425.000000  427.000000  414.000000  452.000000\n",
      "1                    TN  338.000000  365.000000  344.000000  328.000000\n",
      "2                    FP   89.000000   70.000000   78.000000   76.000000\n",
      "3                    FN   67.000000   57.000000   83.000000   63.000000\n",
      "4              Accuracy    0.830250    0.861806    0.824810    0.848749\n",
      "5             Precision    0.826848    0.859155    0.841463    0.856061\n",
      "6           Sensitivity    0.863821    0.882231    0.832998    0.877670\n",
      "7           Specificity    0.791600    0.839100    0.815200    0.811900\n",
      "8              F1 score    0.844930    0.870540    0.837209    0.866731\n",
      "9   F1 score (weighted)    0.829862    0.861674    0.824883    0.848455\n",
      "10     F1 score (macro)    0.828715    0.861174    0.823787    0.845944\n",
      "11    Balanced Accuracy    0.827695    0.860656    0.824082    0.844776\n",
      "12                  MCC    0.658396    0.722697    0.647624    0.692238\n",
      "13                  NPV    0.834600    0.864900    0.805600    0.838900\n",
      "14              ROC_AUC    0.827695    0.860656    0.824082    0.844776\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_svm_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_svm_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_svm_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_svm_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_svm_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_svm_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_svm_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_svm_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_svm_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_svm_3)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 12:27:49,694] Trial 200 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:28:05,122] Trial 201 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:28:20,657] Trial 202 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:28:35,877] Trial 203 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:28:51,171] Trial 204 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:29:06,599] Trial 205 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:29:16,616] Trial 206 finished with value: 0.8088128512719303 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:29:33,183] Trial 207 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:29:49,471] Trial 208 finished with value: 0.5574299646147788 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:30:05,026] Trial 209 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:30:15,061] Trial 210 finished with value: 0.7965004709902174 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:30:32,006] Trial 211 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:30:48,628] Trial 212 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:31:04,647] Trial 213 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:31:14,387] Trial 214 finished with value: 0.8058422064104341 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:31:29,851] Trial 215 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:31:45,294] Trial 216 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:31:56,841] Trial 217 finished with value: 0.8126478118538294 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:32:13,317] Trial 218 finished with value: 0.34263730938402887 and parameters: {'C': 0.25, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:32:29,498] Trial 219 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:32:46,353] Trial 220 finished with value: 0.6062363705467708 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:33:02,844] Trial 221 finished with value: 0.8370227739646241 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:33:19,418] Trial 222 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:33:34,813] Trial 223 finished with value: 0.34263730938402887 and parameters: {'C': 0.03125, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:33:54,764] Trial 224 finished with value: 0.3539097973022896 and parameters: {'C': 0.5, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:34:11,217] Trial 225 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:34:27,175] Trial 226 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:34:42,469] Trial 227 finished with value: 0.7232305623206778 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:34:57,187] Trial 228 finished with value: 0.34263730938402887 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:35:12,332] Trial 229 finished with value: 0.34263730938402887 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:35:27,883] Trial 230 finished with value: 0.7916021282601348 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:35:43,136] Trial 231 finished with value: 0.83842677783504 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:35:58,323] Trial 232 finished with value: 0.83842677783504 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:36:13,463] Trial 233 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:36:28,704] Trial 234 finished with value: 0.83842677783504 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:36:43,755] Trial 235 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:36:59,323] Trial 236 finished with value: 0.45294765533810616 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:37:14,482] Trial 237 finished with value: 0.83842677783504 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:37:27,966] Trial 238 finished with value: 0.8257999023900616 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:37:43,017] Trial 239 finished with value: 0.38129506786354816 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:37:58,791] Trial 240 finished with value: 0.8372910114110063 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:38:14,103] Trial 241 finished with value: 0.83842677783504 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:38:29,220] Trial 242 finished with value: 0.83842677783504 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:38:44,525] Trial 243 finished with value: 0.83842677783504 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:39:00,093] Trial 244 finished with value: 0.83842677783504 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:39:15,759] Trial 245 finished with value: 0.83842677783504 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:39:32,241] Trial 246 finished with value: 0.3966495577348344 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:39:46,669] Trial 247 finished with value: 0.7195525774596624 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:40:00,101] Trial 248 finished with value: 0.7734523047349671 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:40:16,282] Trial 249 finished with value: 0.42341998946272363 and parameters: {'C': 2.0, 'gamma': 1.0}. Best is trial 23 with value: 0.8496816348376213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8497\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  425.000000  427.000000  414.000000  452.000000   \n",
      "1                    TN  338.000000  365.000000  344.000000  328.000000   \n",
      "2                    FP   89.000000   70.000000   78.000000   76.000000   \n",
      "3                    FN   67.000000   57.000000   83.000000   63.000000   \n",
      "4              Accuracy    0.830250    0.861806    0.824810    0.848749   \n",
      "5             Precision    0.826848    0.859155    0.841463    0.856061   \n",
      "6           Sensitivity    0.863821    0.882231    0.832998    0.877670   \n",
      "7           Specificity    0.791600    0.839100    0.815200    0.811900   \n",
      "8              F1 score    0.844930    0.870540    0.837209    0.866731   \n",
      "9   F1 score (weighted)    0.829862    0.861674    0.824883    0.848455   \n",
      "10     F1 score (macro)    0.828715    0.861174    0.823787    0.845944   \n",
      "11    Balanced Accuracy    0.827695    0.860656    0.824082    0.844776   \n",
      "12                  MCC    0.658396    0.722697    0.647624    0.692238   \n",
      "13                  NPV    0.834600    0.864900    0.805600    0.838900   \n",
      "14              ROC_AUC    0.827695    0.860656    0.824082    0.844776   \n",
      "\n",
      "          Set4  \n",
      "0   404.000000  \n",
      "1   357.000000  \n",
      "2    73.000000  \n",
      "3    85.000000  \n",
      "4     0.828074  \n",
      "5     0.846960  \n",
      "6     0.826176  \n",
      "7     0.830200  \n",
      "8     0.836439  \n",
      "9     0.828189  \n",
      "10    0.827623  \n",
      "11    0.828204  \n",
      "12    0.655530  \n",
      "13    0.807700  \n",
      "14    0.828204  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_svm_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_svm_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_svm_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_svm_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_svm_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_svm_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_svm_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_svm_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_svm_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_svm_4)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 12:40:29,668] Trial 250 finished with value: 0.7754037494936765 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:40:44,261] Trial 251 finished with value: 0.8425381162061057 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:40:59,491] Trial 252 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:41:15,364] Trial 253 finished with value: 0.6039130859659942 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:41:30,113] Trial 254 finished with value: 0.8425381162061057 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:41:44,847] Trial 255 finished with value: 0.8425381162061057 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:42:01,327] Trial 256 finished with value: 0.40930851778221433 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:42:17,145] Trial 257 finished with value: 0.34320078807447085 and parameters: {'C': 0.03125, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:42:31,528] Trial 258 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:42:46,000] Trial 259 finished with value: 0.8425381162061057 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:42:54,909] Trial 260 finished with value: 0.8020640334600113 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:43:09,377] Trial 261 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:43:25,766] Trial 262 finished with value: 0.3870276965309317 and parameters: {'C': 0.5, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:43:41,216] Trial 263 finished with value: 0.7236103747612163 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:43:50,567] Trial 264 finished with value: 0.802382378958662 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:44:00,554] Trial 265 finished with value: 0.8335218654479117 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:44:15,430] Trial 266 finished with value: 0.8444225903978448 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:44:31,535] Trial 267 finished with value: 0.34320078807447085 and parameters: {'C': 0.0078125, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:44:46,220] Trial 268 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:44:59,569] Trial 269 finished with value: 0.8361508622374736 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:45:14,223] Trial 270 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:45:30,151] Trial 271 finished with value: 0.34320078807447085 and parameters: {'C': 0.015625, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:45:49,025] Trial 272 finished with value: 0.3989300126526964 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:46:03,666] Trial 273 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:46:18,398] Trial 274 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:46:32,830] Trial 275 finished with value: 0.8425381162061057 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:46:47,233] Trial 276 finished with value: 0.8425381162061057 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:47:01,678] Trial 277 finished with value: 0.8430654829895877 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:47:17,494] Trial 278 finished with value: 0.78500155385031 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:47:31,977] Trial 279 finished with value: 0.8425381162061057 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:47:48,818] Trial 280 finished with value: 0.43636337518616297 and parameters: {'C': 1.0, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:48:03,432] Trial 281 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:48:18,181] Trial 282 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:48:32,970] Trial 283 finished with value: 0.8425381162061057 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:48:48,818] Trial 284 finished with value: 0.386467978286282 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:49:03,597] Trial 285 finished with value: 0.8444225903978448 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:49:20,301] Trial 286 finished with value: 0.3989300126526964 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:49:35,016] Trial 287 finished with value: 0.7704896945448958 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:49:49,998] Trial 288 finished with value: 0.8424285002590409 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:50:04,552] Trial 289 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:50:20,731] Trial 290 finished with value: 0.4239435334262187 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:50:36,998] Trial 291 finished with value: 0.34320078807447085 and parameters: {'C': 0.0625, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:50:46,541] Trial 292 finished with value: 0.7958104106861207 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:51:01,219] Trial 293 finished with value: 0.8425381162061057 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:51:15,934] Trial 294 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:51:25,170] Trial 295 finished with value: 0.8048712302180977 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:51:41,434] Trial 296 finished with value: 0.3909917605429923 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:51:56,050] Trial 297 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:52:10,699] Trial 298 finished with value: 0.8438098432194326 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:52:20,147] Trial 299 finished with value: 0.8101903756293203 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8497\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  425.000000  427.000000  414.000000  452.000000   \n",
      "1                    TN  338.000000  365.000000  344.000000  328.000000   \n",
      "2                    FP   89.000000   70.000000   78.000000   76.000000   \n",
      "3                    FN   67.000000   57.000000   83.000000   63.000000   \n",
      "4              Accuracy    0.830250    0.861806    0.824810    0.848749   \n",
      "5             Precision    0.826848    0.859155    0.841463    0.856061   \n",
      "6           Sensitivity    0.863821    0.882231    0.832998    0.877670   \n",
      "7           Specificity    0.791600    0.839100    0.815200    0.811900   \n",
      "8              F1 score    0.844930    0.870540    0.837209    0.866731   \n",
      "9   F1 score (weighted)    0.829862    0.861674    0.824883    0.848455   \n",
      "10     F1 score (macro)    0.828715    0.861174    0.823787    0.845944   \n",
      "11    Balanced Accuracy    0.827695    0.860656    0.824082    0.844776   \n",
      "12                  MCC    0.658396    0.722697    0.647624    0.692238   \n",
      "13                  NPV    0.834600    0.864900    0.805600    0.838900   \n",
      "14              ROC_AUC    0.827695    0.860656    0.824082    0.844776   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   404.000000  422.000000  \n",
      "1   357.000000  363.000000  \n",
      "2    73.000000   72.000000  \n",
      "3    85.000000   62.000000  \n",
      "4     0.828074    0.854189  \n",
      "5     0.846960    0.854251  \n",
      "6     0.826176    0.871901  \n",
      "7     0.830200    0.834500  \n",
      "8     0.836439    0.862986  \n",
      "9     0.828189    0.854087  \n",
      "10    0.827623    0.853586  \n",
      "11    0.828204    0.853192  \n",
      "12    0.655530    0.707375  \n",
      "13    0.807700    0.854100  \n",
      "14    0.828204    0.853192  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_svm_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_svm_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_svm_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_svm_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_svm_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_svm_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_svm_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_svm_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_svm_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_svm_5)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 12:52:38,060] Trial 300 finished with value: 0.41951782404671817 and parameters: {'C': 0.5, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:52:53,365] Trial 301 finished with value: 0.8420369139371979 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:53:08,393] Trial 302 finished with value: 0.7315361041870216 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:53:21,077] Trial 303 finished with value: 0.7602576589426617 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:53:36,470] Trial 304 finished with value: 0.8420369139371979 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:53:51,873] Trial 305 finished with value: 0.8456145724378171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:54:07,387] Trial 306 finished with value: 0.3394969141151665 and parameters: {'C': 0.015625, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:54:23,436] Trial 307 finished with value: 0.8428199927298505 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:54:34,007] Trial 308 finished with value: 0.8317560811741526 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:54:48,956] Trial 309 finished with value: 0.3394969141151665 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:54:58,648] Trial 310 finished with value: 0.7945151565829934 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:55:14,149] Trial 311 finished with value: 0.8414917088098168 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:55:29,538] Trial 312 finished with value: 0.8456145724378171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:55:40,655] Trial 313 finished with value: 0.7933721114908653 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:55:58,840] Trial 314 finished with value: 0.4098226072283836 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:56:14,188] Trial 315 finished with value: 0.8456145724378171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:56:29,384] Trial 316 finished with value: 0.8420369139371979 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:56:43,274] Trial 317 finished with value: 0.8363024692586457 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:56:58,244] Trial 318 finished with value: 0.8456145724378171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:57:13,506] Trial 319 finished with value: 0.8420369139371979 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:57:27,024] Trial 320 finished with value: 0.8362345935726152 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:57:42,143] Trial 321 finished with value: 0.8456145724378171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:57:57,262] Trial 322 finished with value: 0.8456145724378171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:58:12,317] Trial 323 finished with value: 0.7936882083534015 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:58:26,643] Trial 324 finished with value: 0.6838515173345805 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:58:41,753] Trial 325 finished with value: 0.46564656272424215 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:58:56,981] Trial 326 finished with value: 0.8420369139371979 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:59:10,536] Trial 327 finished with value: 0.7745309445936525 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:59:26,217] Trial 328 finished with value: 0.4103579175498043 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:59:41,247] Trial 329 finished with value: 0.8456145724378171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 12:59:56,801] Trial 330 finished with value: 0.3394969141151665 and parameters: {'C': 0.0625, 'gamma': 1.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:00:12,042] Trial 331 finished with value: 0.8415016899626604 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:00:26,109] Trial 332 finished with value: 0.7214915045388849 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:00:41,387] Trial 333 finished with value: 0.8420369139371979 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:00:56,978] Trial 334 finished with value: 0.427476675537424 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:01:12,299] Trial 335 finished with value: 0.3394969141151665 and parameters: {'C': 0.03125, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:01:27,758] Trial 336 finished with value: 0.8420369139371979 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:01:41,002] Trial 337 finished with value: 0.8123996433906374 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:01:56,289] Trial 338 finished with value: 0.8420369139371979 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:02:11,512] Trial 339 finished with value: 0.8456145724378171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:02:26,864] Trial 340 finished with value: 0.8456145724378171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:02:41,801] Trial 341 finished with value: 0.7315361041870216 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:02:51,359] Trial 342 finished with value: 0.7929638455408845 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:03:01,809] Trial 343 finished with value: 0.8153554299795974 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:03:17,396] Trial 344 finished with value: 0.5891822284458905 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:03:32,331] Trial 345 finished with value: 0.3394969141151665 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:03:47,218] Trial 346 finished with value: 0.3394969141151665 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:04:02,954] Trial 347 finished with value: 0.8428199927298505 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:04:13,963] Trial 348 finished with value: 0.7996547633116003 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:04:29,298] Trial 349 finished with value: 0.8456145724378171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8497\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  425.000000  427.000000  414.000000  452.000000   \n",
      "1                    TN  338.000000  365.000000  344.000000  328.000000   \n",
      "2                    FP   89.000000   70.000000   78.000000   76.000000   \n",
      "3                    FN   67.000000   57.000000   83.000000   63.000000   \n",
      "4              Accuracy    0.830250    0.861806    0.824810    0.848749   \n",
      "5             Precision    0.826848    0.859155    0.841463    0.856061   \n",
      "6           Sensitivity    0.863821    0.882231    0.832998    0.877670   \n",
      "7           Specificity    0.791600    0.839100    0.815200    0.811900   \n",
      "8              F1 score    0.844930    0.870540    0.837209    0.866731   \n",
      "9   F1 score (weighted)    0.829862    0.861674    0.824883    0.848455   \n",
      "10     F1 score (macro)    0.828715    0.861174    0.823787    0.845944   \n",
      "11    Balanced Accuracy    0.827695    0.860656    0.824082    0.844776   \n",
      "12                  MCC    0.658396    0.722697    0.647624    0.692238   \n",
      "13                  NPV    0.834600    0.864900    0.805600    0.838900   \n",
      "14              ROC_AUC    0.827695    0.860656    0.824082    0.844776   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   404.000000  422.000000  447.000000  \n",
      "1   357.000000  363.000000  326.000000  \n",
      "2    73.000000   72.000000   78.000000  \n",
      "3    85.000000   62.000000   68.000000  \n",
      "4     0.828074    0.854189    0.841132  \n",
      "5     0.846960    0.854251    0.851429  \n",
      "6     0.826176    0.871901    0.867961  \n",
      "7     0.830200    0.834500    0.806900  \n",
      "8     0.836439    0.862986    0.859615  \n",
      "9     0.828189    0.854087    0.840900  \n",
      "10    0.827623    0.853586    0.838329  \n",
      "11    0.828204    0.853192    0.837446  \n",
      "12    0.655530    0.707375    0.676863  \n",
      "13    0.807700    0.854100    0.827400  \n",
      "14    0.828204    0.853192    0.837446  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_svm_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_svm_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_svm_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_svm_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_svm_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_svm_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_svm_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_svm_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_svm_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_svm_6)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 13:04:46,539] Trial 350 finished with value: 0.8409610558277668 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:05:01,250] Trial 351 finished with value: 0.8423156388268176 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:05:14,868] Trial 352 finished with value: 0.7419669500345631 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:05:34,134] Trial 353 finished with value: 0.4043427389781852 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:05:48,779] Trial 354 finished with value: 0.8436228753498357 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:06:03,409] Trial 355 finished with value: 0.8436228753498357 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:06:18,269] Trial 356 finished with value: 0.8423156388268176 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:06:29,573] Trial 357 finished with value: 0.8177536146096424 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:06:43,911] Trial 358 finished with value: 0.8423156388268176 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:06:58,399] Trial 359 finished with value: 0.8436228753498357 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:07:10,742] Trial 360 finished with value: 0.8220167212841749 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:07:26,200] Trial 361 finished with value: 0.6632887614615754 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:07:40,468] Trial 362 finished with value: 0.8436228753498357 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:07:54,621] Trial 363 finished with value: 0.7727560710920613 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:08:10,690] Trial 364 finished with value: 0.4556094962910236 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:08:25,071] Trial 365 finished with value: 0.8423156388268176 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:08:40,751] Trial 366 finished with value: 0.782502866028253 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:08:55,067] Trial 367 finished with value: 0.8436228753498357 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:09:09,474] Trial 368 finished with value: 0.8423156388268176 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:09:24,867] Trial 369 finished with value: 0.38573450406354587 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:09:41,193] Trial 370 finished with value: 0.34254860434542905 and parameters: {'C': 0.0625, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:09:55,460] Trial 371 finished with value: 0.8436228753498357 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:10:11,662] Trial 372 finished with value: 0.4304315724948646 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:10:26,927] Trial 373 finished with value: 0.3898387540756242 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:10:41,278] Trial 374 finished with value: 0.8423156388268176 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:10:55,416] Trial 375 finished with value: 0.8065500023384529 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:11:10,186] Trial 376 finished with value: 0.7184927690008517 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:11:26,386] Trial 377 finished with value: 0.4144431658506377 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:11:35,345] Trial 378 finished with value: 0.8061663570654346 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:11:50,039] Trial 379 finished with value: 0.7272419100935149 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:12:04,221] Trial 380 finished with value: 0.8436228753498357 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:12:18,507] Trial 381 finished with value: 0.8436228753498357 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:12:32,891] Trial 382 finished with value: 0.8423156388268176 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:12:42,578] Trial 383 finished with value: 0.806973592973028 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:12:58,554] Trial 384 finished with value: 0.5563366155322276 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:13:12,949] Trial 385 finished with value: 0.8423156388268176 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:13:28,315] Trial 386 finished with value: 0.34254860434542905 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:13:42,616] Trial 387 finished with value: 0.8409610558277668 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:13:54,321] Trial 388 finished with value: 0.760077985364314 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:14:09,750] Trial 389 finished with value: 0.34254860434542905 and parameters: {'C': 0.015625, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:14:24,053] Trial 390 finished with value: 0.8436228753498357 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:14:38,470] Trial 391 finished with value: 0.8423156388268176 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:14:48,373] Trial 392 finished with value: 0.8313282790790135 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:15:02,807] Trial 393 finished with value: 0.8423156388268176 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:15:16,007] Trial 394 finished with value: 0.7419669500345631 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:15:30,308] Trial 395 finished with value: 0.8366189409839982 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:15:43,394] Trial 396 finished with value: 0.8337086733760642 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:16:02,231] Trial 397 finished with value: 0.4043427389781852 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:16:12,271] Trial 398 finished with value: 0.7798064066360402 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:16:26,651] Trial 399 finished with value: 0.8436228753498357 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8497\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  425.000000  427.000000  414.000000  452.000000   \n",
      "1                    TN  338.000000  365.000000  344.000000  328.000000   \n",
      "2                    FP   89.000000   70.000000   78.000000   76.000000   \n",
      "3                    FN   67.000000   57.000000   83.000000   63.000000   \n",
      "4              Accuracy    0.830250    0.861806    0.824810    0.848749   \n",
      "5             Precision    0.826848    0.859155    0.841463    0.856061   \n",
      "6           Sensitivity    0.863821    0.882231    0.832998    0.877670   \n",
      "7           Specificity    0.791600    0.839100    0.815200    0.811900   \n",
      "8              F1 score    0.844930    0.870540    0.837209    0.866731   \n",
      "9   F1 score (weighted)    0.829862    0.861674    0.824883    0.848455   \n",
      "10     F1 score (macro)    0.828715    0.861174    0.823787    0.845944   \n",
      "11    Balanced Accuracy    0.827695    0.860656    0.824082    0.844776   \n",
      "12                  MCC    0.658396    0.722697    0.647624    0.692238   \n",
      "13                  NPV    0.834600    0.864900    0.805600    0.838900   \n",
      "14              ROC_AUC    0.827695    0.860656    0.824082    0.844776   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   404.000000  422.000000  447.000000  430.000000  \n",
      "1   357.000000  363.000000  326.000000  343.000000  \n",
      "2    73.000000   72.000000   78.000000   87.000000  \n",
      "3    85.000000   62.000000   68.000000   59.000000  \n",
      "4     0.828074    0.854189    0.841132    0.841132  \n",
      "5     0.846960    0.854251    0.851429    0.831721  \n",
      "6     0.826176    0.871901    0.867961    0.879346  \n",
      "7     0.830200    0.834500    0.806900    0.797700  \n",
      "8     0.836439    0.862986    0.859615    0.854871  \n",
      "9     0.828189    0.854087    0.840900    0.840669  \n",
      "10    0.827623    0.853586    0.838329    0.839695  \n",
      "11    0.828204    0.853192    0.837446    0.838510  \n",
      "12    0.655530    0.707375    0.676863    0.680976  \n",
      "13    0.807700    0.854100    0.827400    0.853200  \n",
      "14    0.828204    0.853192    0.837446    0.838510  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_svm_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_svm_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_svm_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_svm_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_svm_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_svm_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_svm_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_svm_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_svm_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_svm_7)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 13:16:43,912] Trial 400 finished with value: 0.8374394925251213 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:16:59,034] Trial 401 finished with value: 0.8423618519883341 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:17:13,204] Trial 402 finished with value: 0.8379932641465635 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:17:26,772] Trial 403 finished with value: 0.7775958109221816 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:17:41,835] Trial 404 finished with value: 0.7815168259339846 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:17:56,922] Trial 405 finished with value: 0.8423618519883341 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:18:11,743] Trial 406 finished with value: 0.8374394925251213 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:18:26,670] Trial 407 finished with value: 0.45290374037157577 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:18:41,501] Trial 408 finished with value: 0.8379822299714004 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:18:52,864] Trial 409 finished with value: 0.776732723880977 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:19:07,927] Trial 410 finished with value: 0.8423618519883341 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:19:22,761] Trial 411 finished with value: 0.38450477222459933 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:19:37,334] Trial 412 finished with value: 0.6025407272673 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:19:52,395] Trial 413 finished with value: 0.8423618519883341 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:20:02,036] Trial 414 finished with value: 0.7946163669862641 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:20:17,258] Trial 415 finished with value: 0.4070966673517236 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:20:32,692] Trial 416 finished with value: 0.366690284026799 and parameters: {'C': 0.5, 'gamma': 1.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:20:47,471] Trial 417 finished with value: 0.8374394925251213 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:21:03,018] Trial 418 finished with value: 0.3448153721536368 and parameters: {'C': 0.125, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:21:12,255] Trial 419 finished with value: 0.8012080276652744 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:21:27,429] Trial 420 finished with value: 0.8423618519883341 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:21:42,535] Trial 421 finished with value: 0.8423618519883341 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:21:57,665] Trial 422 finished with value: 0.8294268822748544 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:22:12,671] Trial 423 finished with value: 0.3448153721536368 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:22:27,535] Trial 424 finished with value: 0.840975196651627 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:22:42,740] Trial 425 finished with value: 0.8423618519883341 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:22:57,820] Trial 426 finished with value: 0.3448153721536368 and parameters: {'C': 0.0078125, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:23:12,714] Trial 427 finished with value: 0.8374394925251213 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:23:27,675] Trial 428 finished with value: 0.5319656494081515 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:23:39,852] Trial 429 finished with value: 0.7601817238883525 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:23:55,112] Trial 430 finished with value: 0.8390546539047004 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:24:10,294] Trial 431 finished with value: 0.8374394925251213 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:24:21,043] Trial 432 finished with value: 0.7941952731512749 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:24:36,212] Trial 433 finished with value: 0.8374394925251213 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:24:50,079] Trial 434 finished with value: 0.831486192911194 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:25:00,295] Trial 435 finished with value: 0.826423267093201 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:25:15,594] Trial 436 finished with value: 0.8423618519883341 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:25:30,652] Trial 437 finished with value: 0.8374394925251213 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:25:40,044] Trial 438 finished with value: 0.7918392720947827 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:25:54,930] Trial 439 finished with value: 0.6551640354699753 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:26:09,330] Trial 440 finished with value: 0.8379932641465635 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:26:24,820] Trial 441 finished with value: 0.8374394925251213 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:26:42,635] Trial 442 finished with value: 0.4070966673517236 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:26:55,855] Trial 443 finished with value: 0.7757709753806301 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:27:11,420] Trial 444 finished with value: 0.8423618519883341 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:27:26,617] Trial 445 finished with value: 0.840975196651627 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:27:41,404] Trial 446 finished with value: 0.6025407272673 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:27:56,546] Trial 447 finished with value: 0.8374394925251213 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:28:11,939] Trial 448 finished with value: 0.7812422842631291 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:28:27,117] Trial 449 finished with value: 0.8374394925251213 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8497\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  425.000000  427.000000  414.000000  452.000000   \n",
      "1                    TN  338.000000  365.000000  344.000000  328.000000   \n",
      "2                    FP   89.000000   70.000000   78.000000   76.000000   \n",
      "3                    FN   67.000000   57.000000   83.000000   63.000000   \n",
      "4              Accuracy    0.830250    0.861806    0.824810    0.848749   \n",
      "5             Precision    0.826848    0.859155    0.841463    0.856061   \n",
      "6           Sensitivity    0.863821    0.882231    0.832998    0.877670   \n",
      "7           Specificity    0.791600    0.839100    0.815200    0.811900   \n",
      "8              F1 score    0.844930    0.870540    0.837209    0.866731   \n",
      "9   F1 score (weighted)    0.829862    0.861674    0.824883    0.848455   \n",
      "10     F1 score (macro)    0.828715    0.861174    0.823787    0.845944   \n",
      "11    Balanced Accuracy    0.827695    0.860656    0.824082    0.844776   \n",
      "12                  MCC    0.658396    0.722697    0.647624    0.692238   \n",
      "13                  NPV    0.834600    0.864900    0.805600    0.838900   \n",
      "14              ROC_AUC    0.827695    0.860656    0.824082    0.844776   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   404.000000  422.000000  447.000000  430.000000  395.000000  \n",
      "1   357.000000  363.000000  326.000000  343.000000  359.000000  \n",
      "2    73.000000   72.000000   78.000000   87.000000   90.000000  \n",
      "3    85.000000   62.000000   68.000000   59.000000   75.000000  \n",
      "4     0.828074    0.854189    0.841132    0.841132    0.820457  \n",
      "5     0.846960    0.854251    0.851429    0.831721    0.814433  \n",
      "6     0.826176    0.871901    0.867961    0.879346    0.840426  \n",
      "7     0.830200    0.834500    0.806900    0.797700    0.799600  \n",
      "8     0.836439    0.862986    0.859615    0.854871    0.827225  \n",
      "9     0.828189    0.854087    0.840900    0.840669    0.820342  \n",
      "10    0.827623    0.853586    0.838329    0.839695    0.820181  \n",
      "11    0.828204    0.853192    0.837446    0.838510    0.819990  \n",
      "12    0.655530    0.707375    0.676863    0.680976    0.640800  \n",
      "13    0.807700    0.854100    0.827400    0.853200    0.827200  \n",
      "14    0.828204    0.853192    0.837446    0.838510    0.819990  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_svm_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_svm_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_svm_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_svm_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_svm_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_svm_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_svm_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_svm_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_svm_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_svm_8)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 13:28:45,537] Trial 450 finished with value: 0.3754283295075426 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:29:00,426] Trial 451 finished with value: 0.8338920222875223 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:29:16,471] Trial 452 finished with value: 0.4559213552294496 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:29:31,741] Trial 453 finished with value: 0.3448473704973857 and parameters: {'C': 0.5, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:29:46,462] Trial 454 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:30:02,881] Trial 455 finished with value: 0.4099674669212707 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:30:18,409] Trial 456 finished with value: 0.71957579172175 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:30:28,980] Trial 457 finished with value: 0.7780739134094647 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:30:43,701] Trial 458 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:30:58,856] Trial 459 finished with value: 0.8338920222875223 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:31:13,671] Trial 460 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:31:30,184] Trial 461 finished with value: 0.3448473704973857 and parameters: {'C': 0.0078125, 'gamma': 1.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:31:46,036] Trial 462 finished with value: 0.3448473704973857 and parameters: {'C': 0.015625, 'gamma': 2.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:32:00,818] Trial 463 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:32:15,818] Trial 464 finished with value: 0.8338920222875223 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:32:30,697] Trial 465 finished with value: 0.8330768833829977 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:32:41,259] Trial 466 finished with value: 0.7826842620089083 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:32:55,983] Trial 467 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:33:11,976] Trial 468 finished with value: 0.534539848657625 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:33:26,781] Trial 469 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:33:43,234] Trial 470 finished with value: 0.8296234816224899 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:33:58,171] Trial 471 finished with value: 0.8338920222875223 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:34:10,327] Trial 472 finished with value: 0.7494865826394057 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:34:24,937] Trial 473 finished with value: 0.8246246818017928 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:34:34,573] Trial 474 finished with value: 0.7964342235281847 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:34:49,327] Trial 475 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:35:04,290] Trial 476 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:35:17,081] Trial 477 finished with value: 0.8142506203538511 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:35:27,247] Trial 478 finished with value: 0.7991691323865686 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:35:40,887] Trial 479 finished with value: 0.7318836264669779 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:35:55,617] Trial 480 finished with value: 0.767025960243015 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:36:10,789] Trial 481 finished with value: 0.835152159376279 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:36:25,871] Trial 482 finished with value: 0.8338920222875223 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:36:41,479] Trial 483 finished with value: 0.6439760345879703 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:36:56,267] Trial 484 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:37:10,005] Trial 485 finished with value: 0.8356996535953876 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:37:28,768] Trial 486 finished with value: 0.4099674669212707 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:37:44,429] Trial 487 finished with value: 0.5860648702368427 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:37:59,283] Trial 488 finished with value: 0.8338920222875223 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:38:14,053] Trial 489 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:38:30,163] Trial 490 finished with value: 0.3448473704973857 and parameters: {'C': 0.03125, 'gamma': 0.125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:38:45,200] Trial 491 finished with value: 0.8338920222875223 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:39:01,220] Trial 492 finished with value: 0.4559213552294496 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:39:15,408] Trial 493 finished with value: 0.8015903162331949 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:39:30,301] Trial 494 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:39:41,466] Trial 495 finished with value: 0.7737896952709635 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:39:56,321] Trial 496 finished with value: 0.8338920222875223 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:40:11,131] Trial 497 finished with value: 0.8333842597092256 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:40:28,082] Trial 498 finished with value: 0.3448473704973857 and parameters: {'C': 0.125, 'gamma': 4.0}. Best is trial 23 with value: 0.8496816348376213.\n",
      "[I 2023-12-04 13:40:43,165] Trial 499 finished with value: 0.8338920222875223 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 23 with value: 0.8496816348376213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8497\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  425.000000  427.000000  414.000000  452.000000   \n",
      "1                    TN  338.000000  365.000000  344.000000  328.000000   \n",
      "2                    FP   89.000000   70.000000   78.000000   76.000000   \n",
      "3                    FN   67.000000   57.000000   83.000000   63.000000   \n",
      "4              Accuracy    0.830250    0.861806    0.824810    0.848749   \n",
      "5             Precision    0.826848    0.859155    0.841463    0.856061   \n",
      "6           Sensitivity    0.863821    0.882231    0.832998    0.877670   \n",
      "7           Specificity    0.791600    0.839100    0.815200    0.811900   \n",
      "8              F1 score    0.844930    0.870540    0.837209    0.866731   \n",
      "9   F1 score (weighted)    0.829862    0.861674    0.824883    0.848455   \n",
      "10     F1 score (macro)    0.828715    0.861174    0.823787    0.845944   \n",
      "11    Balanced Accuracy    0.827695    0.860656    0.824082    0.844776   \n",
      "12                  MCC    0.658396    0.722697    0.647624    0.692238   \n",
      "13                  NPV    0.834600    0.864900    0.805600    0.838900   \n",
      "14              ROC_AUC    0.827695    0.860656    0.824082    0.844776   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   404.000000  422.000000  447.000000  430.000000  395.000000  422.000000  \n",
      "1   357.000000  363.000000  326.000000  343.000000  359.000000  373.000000  \n",
      "2    73.000000   72.000000   78.000000   87.000000   90.000000   77.000000  \n",
      "3    85.000000   62.000000   68.000000   59.000000   75.000000   47.000000  \n",
      "4     0.828074    0.854189    0.841132    0.841132    0.820457    0.865071  \n",
      "5     0.846960    0.854251    0.851429    0.831721    0.814433    0.845691  \n",
      "6     0.826176    0.871901    0.867961    0.879346    0.840426    0.899787  \n",
      "7     0.830200    0.834500    0.806900    0.797700    0.799600    0.828900  \n",
      "8     0.836439    0.862986    0.859615    0.854871    0.827225    0.871901  \n",
      "9     0.828189    0.854087    0.840900    0.840669    0.820342    0.864835  \n",
      "10    0.827623    0.853586    0.838329    0.839695    0.820181    0.864686  \n",
      "11    0.828204    0.853192    0.837446    0.838510    0.819990    0.864338  \n",
      "12    0.655530    0.707375    0.676863    0.680976    0.640800    0.731227  \n",
      "13    0.807700    0.854100    0.827400    0.853200    0.827200    0.888100  \n",
      "14    0.828204    0.853192    0.837446    0.838510    0.819990    0.864338  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_svm_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_svm_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_svm_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_svm_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_svm_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_svm_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_svm_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_svm_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_svm_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_svm_9)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAHJCAYAAACovxwqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABV10lEQVR4nO3dd1gU5/428HuBpUlTinQVERugghWN2HtEY+9iibFEkxgLJioaS/QYNTnRn0ajYEeJ3Sj2bhTFBgYbRUEQEGnSYd4/fNnjyqLsMgir9+e6uHRnnnnmO8+O7O20lQiCIICIiIiIRKFR0QUQERERfUwYroiIiIhExHBFREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcFWBJBIJJBLJO9vUrFkTEokEUVFRH6YoqnTatWv33v3kQxk9ejQkEgn8/PwqupRyV5nGnYjUC8MVERERkYgYroiIiIhExHClZl6+fAl9fX3Url0bgiAobNOrVy9IJBLcuHEDABAVFQWJRILRo0cjPDwcffr0QbVq1VClShW0adMGx48fL3F9O3fuRPv27VG1alXo6uqifv36WLRoEXJycoq1lUgkaNeuHZ49ewZvb29YWVlBU1NTdgqp6JRSREQEVq5ciXr16kFXVxe2trb49ttvkZaWVqzPM2fO4Msvv0SDBg1gZGQEPT09NGzYEPPnz0dWVlax9r6+vpBIJDh79iy2bNmCZs2aoUqVKqhZs6asjZ+fH/r16wcHBwfo6enByMgIrVu3xpYtWxSOQdHpoby8PCxcuBC1a9eGrq4u6tatiw0bNsjarVmzBs7OztDT04OtrS18fX1RWFiosM+rV6+if//+sLS0hLa2Nuzs7DBhwgQ8e/ZM1qbofTt37pxsfIt+2rVrJ9dfTEwMpkyZAgcHB+jo6MDU1BS9e/dGcHCwSmOkLDHHSNX9NTs7G0uXLoWLiwv09fVhZGSEzz77DLt27SrW9u119O/fH+bm5tDQ0ICfn1+pxr0s+2ZgYCCaN28OfX19VKtWDYMGDUJMTIzC7UpOTsYPP/wAZ2dn6Ovrw9jYGI0aNcLs2bPx6tWrYm19fHxQv3596OnpwdjYGB07dlQ4Zjk5OVi1ahWaNGmCqlWrQl9fH3Z2dvj8889x4sQJhbUQUeloVXQBpJyqVati8ODB2Lx5M06ePInOnTvLzX/69CmOHj0Kd3d3uLu7y82LjIxEq1at4OzsjAkTJiAuLg4BAQHo3r07duzYgUGDBsm1Hzt2LDZt2gQ7Ozv069cPxsbG+OeffzB37lycOnUKx48fh1QqlVvmxYsXaNWqFQwNDdG/f38IggALCwu5Nt9++y3Onz+PgQMHwsvLC0FBQVi9ejUuXLiAixcvQldXV9Z22bJlCA8Ph4eHB3r27ImsrCxcunQJCxcuxJkzZ3D69GloaRXfjVesWIGTJ0/i888/R4cOHZCSkiKbN3HiRDRo0ABt27aFlZUVkpKScOTIEYwaNQrh4eFYsmSJwrEfPHgwrl69ih49ekAqlSIwMBBffvkltLW1cf36dezYsQO9evVCp06dcOjQISxYsAB6enqYNWuWXD+bN2/G+PHjoauri969e8PW1hYPHz7Exo0bcejQIfzzzz+wt7eHiYkJ5s+fDz8/P0RHR2P+/PmyPt4MQiEhIejSpQuSk5PRtWtXfPHFF0hKSsL+/fvRpk0b7Nu3Dz169FBqjFQl1hgByu2vubm56NKlCy5cuIAGDRpg8uTJyMzMxJ49ezBkyBDcvHkTy5YtK7aOR48eoWXLlqhbty6GDx+OjIwMuLi4lGrcVd03165di4MHD6J3797w9PTE1atXsXv3bty6dQt37tyBjo6O3Bi0b98e0dHRcHd3x8SJE1FYWIj79+9j1apV+Oqrr1ClShUAQHR0NNq1a4eoqCi0bdsW3bt3R0ZGBg4fPoxu3bph3bp1+PLLL2V9jxw5Ert374azszNGjhwJPT09PHv2DBcvXkRQUFCx3y1EpASBKgwAAYAwf/78En+MjY0FAEJkZKRsuevXrwsAhH79+hXrc+7cuQIA4Y8//pBNi4yMlK3r+++/l2sfHBwsaGlpCSYmJkJqaqps+ubNmwUAQv/+/YWsrCy5ZebPny8AEFatWqVwe0aMGCHk5eUVq23UqFECAMHU1FSIioqSTS8oKBC++OILAYCwcOFCuWUeP34sFBYWFuvLx8dHACDs3LlTYW36+vpCSEhIseUEQRAePXpUbFp2drbQrl07QUtLS3j69KncPE9PTwGA0LRpU+Hly5dytUmlUsHY2FioWbOmEBMTI5uXkpIimJmZCWZmZnJjcf/+fUEqlQp16tQRnj17JreeU6dOCRoaGoKXl5fC9SuSl5cn1K5dW9DV1RUuXLggNy82NlawtrYWqlevLvcelmaMSlL0Hm7evFlhjWKMkSr76+LFiwUAQq9eveT6io+PF+zs7AQAcuPz5jp8fHwUbuu7xr1o21TZNw0NDYU7d+7IzRsyZIgAQNi1a5fcdA8PDwGAsGTJkmLrSUxMlHtfPT09BYlEIuzevVuu3cuXL4VGjRoJurq6QlxcnCAIr8deIpEI7u7uQn5+frG+k5KSStxuIno/hqsKVPTLvTQ/b4YrQRCEZs2aCVKpVIiPj5dNy8/PF6ytrQVDQ0MhIyNDNr3og8TY2FhIS0srVkfRB6afn59sWuPGjQWpVCr3QfnmekxNTYWmTZsW2x5tbW3h+fPnCre3aD1vByhBeP1BpaGhIdSsWVPhsm9LSkoSAAje3t5y04s+wKZNm1aqft4UGBgoABD8/f3lphd9yJ46darYMu3btxcACH/++Wexed7e3gIAuSD5zTffCACEI0eOKKyhT58+goaGhlxweNeH/P79+wUAwowZMxTOX716tQBAOHz4sGxaWcbofeFKjDFSZX+tXbu2IJFIhPv37xdr/8cffxTbV4rWUb16dSE7O1vhtr4vXJXkffvmjz/+WGyZ06dPCwCE6dOny6YV/SeqcePGQkFBwTvXeevWLQGAMGDAAIXzi/aT33//XRAEQUhLSxMACB4eHgoDIhGVDU8LVgJCCddOAa9PQ0RHRxebPmnSJHh7e2PTpk3w8fEBABw6dAjPnj3DxIkTZacK3uTm5gZDQ8Ni09u1awd/f3/cvHkTo0aNQmZmJm7fvg0zMzOsXr1aYV06OjoIDw9XWO/bpwHf5unpWWyag4MD7OzsEBUVhZSUFJiYmAAAXr16hV9//RX79u3DgwcPkJ6eLjdesbGxCtfRokWLEtf/5MkTLFu2DKdOncKTJ0+KXR9TUp9vn2YFAGtr6/fOi4mJQY0aNQAAV65cAQCcPXsW165dK7ZMQkICCgsL8fDhQ4V9vq2ov6ioKPj6+hab//DhQwBAeHg4evbsKTfvXWOkKjHGqEhp99f09HQ8fvwYtra2cHJyKta+U6dOAF6fPn1bo0aN5E7DKUPVfbNp06bFptnZ2QF4fU1lkX/++QcA0LVrV2hovPvy2KL9ICUlReF+kJiYCACyf7OGhob4/PPPcejQITRp0gT9+vVDmzZt0KJFC+jr679zXUT0fgxXamrQoEGYPn06Nm7ciNmzZ0MikWD9+vUAgK+++krhMtWrV1c43dLSEgCQmpoK4PUveEEQkJiYiAULFihVV1Ff7/KuOqKjo5GamgoTExPk5eWhQ4cOuHbtGpydnTFo0CCYm5vLrvNasGCBwgvr31VHREQEmjdvjpcvX+Kzzz5Dly5dYGxsDE1NTURFRcHf37/EPo2NjYtNK7qm5l3z8vLyZNNevHgBAPjPf/6jcB1FMjIy3jn/7f727NmjdH+lea+UJcYYFSnt/lr0Z0nbY2VlJddOUV/KKsu++a5xKCgokE0rugbOxsbmvfUU7QcnTpx458Xob+4HAQEBWLZsGXbs2IF58+YBAHR1dTFw4ECsWLEC5ubm710vESnGcKWm9PT0MHr0aKxcuRInTpyAk5MTjh8/jpYtW8LV1VXhMs+fP1c4PT4+HsD/fukX/dmkSROF/9t/l9I8dPH58+eoW7fue+s4cOAArl27hlGjRhV7aGVcXNw7g19JdaxcuRIvXrzA5s2bMXr0aLl5O3fuhL+//3vrL4uibUtNTYWRkZFo/R04cAC9e/dWatnK/oBMZffXoulvi4uLk2v3JlXHoCz7ZmkVHb0t6QjYm4q27ddff8XUqVNL1b+enh58fX3h6+uLp0+f4vz58/Dz88OWLVsQFRUlu1uSiJTHRzGosYkTJ8qOWG3YsAGFhYWYMGFCie1DQkKQnp5ebPrZs2cBvA5TAGBgYICGDRsiLCwMycnJotet6Jd2REQEnj59ipo1a8o+VB49egQA6NevX6n6KI3y6FMZLVu2BABcuHCh1MtoamoCkD+qUZb+1EVp91dDQ0PUrl0bsbGxstOgbzpz5gyA16cZlfGucf8Q+1HRe3vixIl3XjrwZltV9wM7OzsMGzYMQUFBqFOnDs6fP18u//aJPhUMV2rM0dERnTt3xsGDB/HHH3/AxMSk2OMU3pSamoqFCxfKTbt+/Tq2b98OY2Nj9O3bVzb9u+++Q25uLsaMGaPwFv2XL18qfVSryK+//ip3HVlhYSFmzJiBwsJCeHt7y6YX3fZe9OFYJCIiQuGt+6VRUp9BQUHYuHGjSn0qY8qUKZBKpfj222/x4MGDYvNzc3OLfUCampoCeP2Yjbd5eXmhdu3aWLNmDf7++2+F67xy5QoyMzNFqP7DUmZ/HTNmDARBwIwZM+TCUFJSEn766SdZG2W8a9zLY998m7u7Ozw8PBASEoIVK1YUm//ixQtkZ2cDeH0d12effYa9e/di06ZNCvu7e/cuEhISALy+Buvq1avF2rx69Qrp6enQ1NRU+BgJIiod/utRcxMnTsTx48eRlJSEqVOnQk9Pr8S2bdu2xcaNG3H16lW0bt1a9tygwsJCrF+/Xu401ZgxY3Djxg2sXbsWtWvXRteuXWFvb4/k5GRERkbi/Pnz8Pb2xrp165SuuU2bNmjcuDEGDRoEY2NjBAUF4fbt23B3d8fMmTNl7T7//HM4Ojpi1apVCA0NRZMmTfDkyRMcPnwYPXv2xJMnT5Re96RJk7B582YMHDgQ/fr1g42NDUJDQ3Hs2DEMHDgQAQEBSvepjHr16mHTpk0YM2YMGjZsiG7dusHJyQl5eXl48uQJLly4AHNzc7mbBTp27Ig9e/bgiy++QPfu3aGnp4caNWpgxIgRkEql2Lt3L7p27YqePXvCw8MDjRs3hr6+Pp4+fYrg4GBEREQgLi5O7S5UVmZ//f7773H06FEcOHAAjRo1Qo8ePWTPuUpISMDMmTPRpk0bpdb/rnEvj31TkW3btqFdu3aYOXMmdu/eDU9PTwiCgIcPH+L48eMIDw+XBb0dO3agQ4cOGDt2LH777Te0aNECJiYmiImJwZ07dxAaGoorV67AwsICsbGxaNmyJerXrw83NzfY2dkhLS0Nhw8fRnx8PKZMmSLKaWuiT1YF3qn4ycP/f8zCu9SoUUPhoxiK5OfnC2ZmZgIAISwsTGGbotvOR40aJfz7779C7969BRMTE0FPT0/w8PAQjh07VuL6Dx06JPTs2VMwNzcXpFKpUL16daFZs2bCDz/8IPz777/FtsfT07PEvopuoX/8+LGwYsUKoW7duoKOjo5gbW0tTJs2Te7xA0WePHkiDB06VLC2thZ0dXWFBg0aCMuWLRPy8vIUrq/odvczZ86UWMelS5eE9u3bCyYmJoKBgYHQunVrYd++fcKZM2dkzx1707tuyS/aJkXvz7tquXPnjjBq1CjB3t5e0NbWFqpWrSo0bNhQ+PLLL4s9ziA/P1/w8fERatWqJWhpaSnc7ufPnwuzZs0SGjZsKOjp6QlVqlQRHB0dhX79+glbt26Ve/ZTacaoJO97FMO7lintGKm6v2ZlZQmLFy8WGjZsKOjq6sre2x07dhRr++Y6SvK+cRdz33xXPUlJScLMmTMFJycnQUdHRzA2NhYaNWokzJkzR3j16pVc27S0NGHx4sWCm5ubUKVKFUFXV1eoWbOm0KNHD2H9+vWyR7S8fPlSWLBggdC+fXvB2tpa0NbWFiwtLQVPT09hx44dfDwDURlJBOE9J/OpUnv8+DHq1KmDNm3a4Pz58wrbREVFoVatWgovvv2QRo8eDX9/f0RGRpbpq1bo41ZZ9lciIlXxmis195///AeCIGDKlCkVXQoRERGB11yppejoaGzduhUPHz7E1q1b0aRJE/Tv37+iyyIiIiIwXKmlyMhIzJ07F1WqVEHXrl3xf//3f+99gjMRERF9GLzmioiIiEhEPNxBREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEvFuwgrx8+RL5+fkVXYbaMjc3R2JiYkWXobY4fmXHMSwbjl/ZcQzLRtnx09LSQtWqVUvXVtWiqGzy8/ORl5dX0WWoJYlEAuD1GPJmV+Vx/MqOY1g2HL+y4xiWTXmPH08LEhEREYmI4YqIiIhIRAxXRERERCJiuCIiIiISEcMVERERkYgYroiIiIhExHBFREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiESkVdEFfKqm7Y9EeHxGRZehxv6t6ALUHMev7DiGZcPxK7vKP4aHx9ar6BIqBI9cEREREYmI4YqIiIhIRAxXRERERCJiuCIiIiISEcMVERERkYgYroiIiIhExHBFREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcEVEREQkIoYrIiIiIhExXBERERGJiOGKiIiISEQMV0REREQiYrgiIiIiEhHDFREREZGIGK6IiIiIRMRwRURERCQihisiIiIiETFcEREREYmI4YqIiIhIRAxXRERERCJiuCIiIiISEcMVERERkYgYroiIiIhExHBFREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcEVEREQkIoYrIiIiIhExXBERERGJiOGKiIiISEQMV0REREQiYrgiIiIiEhHDFREREZGIGK6IiIiIRMRwRURERCQihisiIiIiETFcEREREYmI4YqIiIhIRAxXRERERCJiuCIiIiISEcMVERERkYgYroiIiIhExHBFREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcEVEREQkIoYrIiIiIhExXBERERGJiOGKiIiISEQMV0REREQiYrgiIiIiEhHDFREREZU7Pz8/tGzZEg4ODujWrRuuXr1aYtvLly/Dxsam2M+jR49kbQICAhS2yc7O/hCb805aFV0AERERfdwOHDgAX19fLFmyBM2aNcPWrVsxfPhwnD17FjY2NiUud/78eRgaGspem5qays03NDTE+fPn5abp6uqKW7wKeOSKiIiIytWGDRswePBgDB06FHXq1MHChQthbW2NLVu2vHM5MzMzWFhYyH40NTXl5kskErn5FhYW5bkZpVbhR66ysrKwYcMGBAcHQ09PD71798b169dRs2ZNjB49GufPn8fff/+NZ8+eQUdHB87Ozhg9ejSMjY0BAGFhYViwYAHmzJmDHTt2IDY2Fk5OTvjmm28QERGBLVu2IDk5GU2aNMHEiROho6MDAPD19YW9vT00NDRw7tw5aGlpYdCgQWjTpg02bdqEf/75B8bGxhgzZgyaNGkCACgsLMT69esRGhqKlJQUmJmZoWvXrujRo0eFjR8REVFllpubizt37mDy5Mly0z09PXH9+vV3Ltu1a1fk5OSgTp06mDZtGlq3bi03/9WrV2jevDkKCgrQsGFDzJw5E87OzqJvg7IqPFz5+/vj/v37mDlzJoyNjbF7925ERkaiZs2aAID8/HwMGjQI1tbWSE1Nhb+/P9auXQsfHx+5fvbs2YMxY8ZAR0cHq1atwqpVqyCVSjF16lRkZ2djxYoVOHr0KPr06SNb5ty5c+jduzeWLFmCy5cvy0Jes2bN0LdvXxw5cgS///471q5dCx0dHRQWFsLU1BTffvstjIyMcP/+ffzxxx8wMTGBh4fHBxw1IiIi9ZCcnIyCggKYmZnJTTczM0NCQoLCZSwsLLB8+XK4uroiJycHf/31FwYNGoTAwEC0bNkSAODo6IhVq1ahXr16yMjIwMaNG+Hl5YUTJ07AwcGh3LfrXSo0XGVlZeHcuXOYNm0aXFxcAACTJk3ChAkTZG06dOgg+3v16tXh7e2NOXPmIDs7W+686uDBg1GvXj3ZMjt27MB///tfVK9eHQDQokULhIWFyYWrGjVqoF+/fgCAvn37Yv/+/TA0NESnTp0AAP3798fx48cRHR0NJycnaGlpYeDAgbLlLSwscP/+fVy5cqXEcJWXl4e8vDzZa4lEAj09PZXGi4iISJ1IJBJIJBIAgIaGhuzviua/qU6dOqhTp47sdbNmzfDs2TOsW7cOrVq1AgA0bdoUTZs2lbVp3rw5unTpgs2bN2PRokXvrevNP8VWoeHq+fPnKCgogKOjo2yavr4+rK2tZa8jIyOxZ88eREVFISMjA4IgAACSkpJga2sra1ejRg3Z342NjaGjoyMLVgBgYmKCx48fy63f3t5e9ncNDQ0YGhrKTSs69ZiWliabdvz4cZw+fRqJiYnIzc1Ffn6+7CibIvv27UNgYKDsda1atbBs2bKSB4WIiOgjYWVlBVNTU2hqaiI/Px9WVlayeVlZWbCxsZGb9i7t2rXDtm3b3tnew8MDMTExpe7T0tKyVO2UVeGnBRUpClDZ2dlYtGgRGjVqhK+//hpGRkZISkrC4sWLkZ+fL7fMmxe5SSSSYhe9Aa+vmXqTlpb85r+9XFGiLVru8uXL8Pf3x8iRI+Hk5AQ9PT0cPHgQDx8+LHFb+vbti169ehXrk4iI6GMXFxcHAHB1dcWBAwdkp/QA4OjRo+jatauszftcuXIFpqamJbYXBAHBwcGoV6/ee/uUSCSwtLREfHy8LHO8j5aWFszNzUvXtlStykn16tWhqamJR48eyc7FZmZmIi4uDg0aNMCzZ8+Qnp6OoUOHyua/ffTpQwoPD0fdunXRtWtX2bTnz5+/cxmpVAqpVFrepREREVU6RcFl/PjxmDZtGlxdXeHu7o5t27YhNjYWI0aMgCAIWLp0KeLi4vDbb78BeH13oZ2dHZycnJCXl4e9e/fiyJEj2LBhg6zPlStXws3NDbVq1UJ6ejo2bdqEsLAwLF68uNSBSRCEUrdVRoWGKz09PXh6emLbtm0wMDCQXdCuofH6CRFmZmbQ0tLCsWPH0LlzZzx9+hR//fVXhdVraWmJc+fO4datW7CwsMD58+fx6NGjSnPrJxERUWXk5eWFly9fYtWqVUhISEDdunWxdetW2eU9z58/x7Nnz2Tt8/Ly8NNPPyE+Ph66urpwcnLCli1b0LFjR1mb1NRUzJw5E4mJiTA0NISzszP++usv2R3+FanCTwuOGjUKGzZswLJly2SPYnjx4gW0tbVhZGSESZMmYefOnTh69Chq1aqFESNGYPny5RVSa+fOnREVFYXVq1dDIpGgdevW6Nq1K27evFkh9RAREamL0aNHY/To0QrnrV69Wu71pEmTMGnSpHf2t2DBAixYsECk6sQlEcrjeFgZZGdn46uvvsLIkSPl7hT82AzdcA3h8RkVXQYREVG5OTy2XkWXoJBEIoGVlRXi4uJKfVpQKpWqxzVXwOu7AWNjY+Ho6IjMzEzZnXVv3l5JREREpC4qPFwBwKFDh/Ds2TNoaWnBwcEBCxcuhJGRUUWXRURERKS0Cg9XfO4TERERfUz4xc1EREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcEVEREQkIoYrIiIiIhExXBERERGJiOGKiIiISEQMV0REREQiYrgiIiIiEhHDFREREZGIGK6IiIiIRMRwRURERCQihisiIiIiETFcEREREYmI4YqIiIhIRAxXRERERCJSKVzl5ubi5MmTiImJEbseIiIiIrWmUrjS1tbG5s2bkZaWJnY9RERERGpN5dOCFhYWSElJEbEUIiIiIvWncrjq0aMH9u/fj8zMTDHrISIiIlJrWqou+PTpU6Snp2Py5MlwdnZG1apV5eZLJBJ4e3uXuUAiIiIidaJyuAoKCpL9/dq1awrbMFwRERHRp0blcBUQECBmHUREREQfBT7nioiIiEhEKh+5KnLr1i3cu3cPaWlp6N+/P8zMzPDo0SNYWFjAyMhIjBqJiIiI1IbK4SonJwfLly9HaGiobFqXLl1gZmaGQ4cOwdTUFCNHjhSlSCIiIiJ1ofJpwZ07dyIiIgLTp0+Hv7+/3LxGjRrh7t27ZS6OiIiISN2ofOTqn3/+waBBg9C8eXMUFhbKzTMzM0NSUlKZiyMiIiJSNyofuUpLS4Otra3CeRKJBLm5uSoXRURERKSuVA5X1apVw5MnTxTOi46OhoWFhcpFEREREakrlcNV8+bNsW/fPkRGRsqmSSQSJCYm4siRI2jVqpUoBRIRERGpE5WvuRowYABCQ0MxZ84c2NnZAQDWrl2L58+fw9raGn369BGrRiIiIiK1oXK40tPTw6JFi/D3338jJCQElpaW0NHRQZ8+fdCzZ09oa2uLWScRERGRWijTQ0S1tbXRp08fHqUiIiIi+v9UvuZqypQpiIqKUjjvyZMnmDJliqpdExEREaktlcNVYmIi8vPzFc7Ly8tDYmKiykURERERqaty+eLm58+fQ09Przy6JiIiIqrUlLrm6uzZszh37pzs9caNG4uFqNzcXERHR6NBgwbiVEhERESkRpQKV7m5uUhLS5O9fvXqFfLy8uTaSKVSeHh4YODAgeJUSERERKRGlApXXbp0QZcuXQAAkydPxvTp01GzZs3yqIuIiIhILan8KIY1a9aIWQcRERHRR6FMz7nKy8vD2bNnERYWhvT0dIwbNw5WVlYIDg6Gvb09qlevLladRERERGpB5XCVlpaGBQsWICYmBiYmJkhJSUFWVhYAIDg4GLdv38a4ceNEK5SIiIhIHaj8KIZt27YhMzMTS5cuxdq1a+XmNWzYEPfu3StzcURERETqRuVwFRISgoEDB8LBwQESiURunqmpKV68eFHm4oiIiIjUjcrhKisrC+bm5grn5efno7CwUOWiiIiIiNSVyuHKwsICDx48UDjv0aNHsLa2VrkoIiIiInWlcrhq06YNDhw4gODgYAiCAACQSCR49OgRjh49is8++0y0IomIiIjUhcp3C3p5eeH+/ftYsWIFqlSpAgBYvHgx0tPT0bhxY/To0UO0IomIiIjUhcrhSktLCz4+Prh8+TJCQkKQmpoKQ0NDuLu7w8PDAxoa5fKd0ERERESVWpkeIiqRSNC6dWu0bt1arHqIiIiI1BoPLxERERGJSOUjV4WFhTh69CguXryIxMRE5OXlFWvj7+9fpuKIiIiI1I3K4Wr79u04fPgwatasCVdXV2hplekMIxEREdFHQeVEdPHiRXh5eWHo0KFi1kNERESk1lS+5io3Nxeurq5i1kJERESk9lQOV66urnj48KGYtRARERGpPZVPC3p7e+Pnn3+Gjo4O3NzcYGBgUKyNomlEREREHzOVw5W+vj6sra3h7+9f4l2BAQEBKhdGREREpI5UDld//PEHrly5gmbNmsHGxoZ3CxIRERGhDOEqODgYQ4YMQe/evcWsh4iIiEitqXxBu5aWFmrVqiVmLURERERqT+Vw1bx5c9y+fVvMWoiIiIjUnsqnBVu3bo3169cjPz+/xLsFHRwcylQcERERkbqRCIIgqLLgoEGD3tuGdwuWrKTvY6T3k0gksLKyQlxcHFTcfT9pHL+y4xiWDcev7DiGZaPK+EmlUpibm5eqrcpHriZOnKjqokREREQfLZXDVbt27UQsg4iIiOjjoPIF7URERERUXJme/JmRkYGLFy8iJiYGubm5cvMkEglPHRIREdEnR+VwlZSUBB8fH+Tk5CAnJwdGRkbIyMhAYWEhqlSpAn19fTHrJCIiIlILKoer7du3w9bWFrNnz8bIkSPh4+MDe3t7nDx5Evv27cPs2bPFrJOIiD5B+fn5yMzMrOgyKqWsrKxiZ42o9N4eP0EQoKWlhSpVqpS5b5XD1YMHDzB8+HBIpdL/daalhW7duiE1NRXbtm1jwCIiIpXl5+fj1atXMDQ0hIYGLxF+m1Qq5SN9ykDR+L169Qo5OTnQ0dEpU98q762pqamoWrUqNDQ0oKGhIfc/iwYNGiA8PLxMhRER0actMzOTwYo+KH19feTk5JS5H5X3WGNjY2RkZAAAzM3NERERIZuXmJgITU3NMhdHRESfNgYr+pAkEoko/ah8WrBOnTqIjIxE06ZN0bx5cwQGBiIvLw9aWlo4ePAgGjZsKEqBREREROpE5XDVu3dvJCQkAAD69++P2NhY7N69GwBQv359eHt7i1MhERERkRpR+Xirg4MDWrZsCQDQ1dXFrFmzsHnzZvj5+cHX1xdVq1YVrUgiIqKPTYsWLbBhw4YytymrgIAA1K9fv1zXIQZ1qRNQ8chVbm4uvv76a4wfPx5NmzaVTeezrYiIqLz1+vPD3TB1eGw9pZeJjY3FypUrcebMGSQnJ8PCwgLdunXDN998g2rVqinV199//y3qZ2uLFi0wbtw4jB8/Xjatd+/e6Nixo2jreNuRI0fw1Vdf4Z9//oGNjU2x+W3btoWnpyd++umncqvhQ1PpyJW2tjZyc3Ohq6srdj1ERERqKzo6Gj169EBERATWrFmDS5cu4eeff8bFixfRu3dvvHz5Uqn+TE1NoaenV07VvqanpwczM7Ny679Lly6oWrWq7NKhNwUHB+Px48cYPHhwua2/Iqh8WtDFxQV37twRsxYiIiK19sMPP0AqlWLHjh1o1aoVbGxs0KFDB+zatQvx8fFYtmyZXPuMjAxMnjwZderUgZubGzZt2iQ3/+3TgmlpaZg5cyZcXV3h4OCAAQMGICwsTG6Z48ePo3v37nBwcICzszPGjRsH4PX10TExMfD19YWNjY3sKNKbp9sePXoEGxsbPHr0SK7P9evXo0WLFhAEAcDrZ12OGDECderUQaNGjfD1118jOTlZ4ZhIpVL069cPe/bskS1fZNeuXXB1dUXDhg2xfv16dOzYEY6OjmjatCl8fHzw6tWrEsf6m2++wZgxY+SmzZs3D/3795e9FgQBa9euRatWrVC7dm106tQJhw8fLrFPsagcrvr27YvLly8jMDAQT548QXp6OjIyMuR+iIiIPhUvX77E2bNnMWrUqGJHmywsLPDFF1/g0KFDcgFj3bp1qF+/Po4dO4YpU6bA19cX58+fV9i/IAgYOXIkEhISsHXrVpw8eRIuLi4YNGiQ7IjYyZMnMW7cOHTs2BFBQUEICAiAq6srAGDDhg2wsrLC999/j5s3b+LmzZvF1uHo6AhXV1fs3btXbvr+/fvRp08fSCQSPH/+HP369UODBg1w9OhRbN++HUlJSZgwYUKJYzNkyBBER0fjypUrsmmZmZk4dOiQ7KiVhoYGFi5ciNOnT2P16tW4dOkSFi1a9K4hf69ly5YhICAAS5cuxenTpzF+/HhMnTpVro7yoPLdgkVPX9+zZw/27NmjsE1AQICq3RMREamVyMhICIKAOnXqKJzv6OiIlJQUvHjxQnYarlmzZpgyZQoAoHbt2ggODsaGDRvQtm3bYstfunQJ4eHhuH37NnR0dCCVSjFv3jwEBQXhyJEjGD58OH777Td4eXnh+++/ly1X9GikqlWrQlNTEwYGBrCwsChxO/r27Qs/Pz/MnDkTAPD48WPcuXMHv/76KwBgy5YtcHFxgY+Pj2yZX375Bc2aNcPjx49Ru3btYn06OTmhSZMmCAgIgIeHBwDg0KFDKCgoQJ8+fQBA7jowe3t7zJgxAz4+Pli6dGmJtb5LZmYmNmzYgICAANn14TVq1EBwcDC2bdumcIzFonK46tevn2gP2yIiIvrYFR2xevOz093dXa6Nu7s7Nm7cqHD5u3fv4tWrV3B2dpabnp2djejoaABAWFgYhg0bVqY6vby8sGjRIty4cQPu7u7Yt28fGjZsCCcnJwDAnTt3cPnyZYUhMjo6WmG4Al4fvZo/fz4WL14MAwMD7Nq1Cz169ICxsTGA1+Hxv//9Lx4+fIj09HQUFBQgOzsbmZmZKl3U/+DBA2RnZ2PIkCFy0/Py8oqNodhUDlcDBw4Usw4iIiK1VrNmTUgkEjx48ADdunUrNv/x48cwMTF57x2DJR24KCwshIWFBQIDAwG8/j7f/Px8AJAFFDFuNKtevTo8PDywf/9+uLu7Y//+/Rg+fLhsviAI6Ny5M+bMmaNw2ZJ4eXnB19cXBw8eRKtWrXDt2jXZEbaYmBiMHDkSw4cPx4wZM2BiYoLg4GBMnz69xO9P1NDQKHYNV9F4AK/HC3h9pM3S0lKunba29ntGoWxUDldERET0P9WqVUPbtm3h7++P8ePHy113lZCQgL1796J///5y4SkkJESuj5CQEDg6Oirs38XFBYmJidDS0oKdnZ3CLx6uX78+Ll68iEGDBinsQyqVoqCg4L3b0rdvXyxZsgReXl6Ijo6Gl5eXbJ6zszP+/vtv2NnZQUur9DHCwMAAvXr1QkBAAKKjo1GjRg3ZKcLbt28jPz8f8+fPl33l0aFDh97Zn6mpKe7fvy83LSwsDFKpFMDrU5E6OjqIjY1Fq1atSl2nGMr0pU2FhYW4ceMGDh48iMDAwGI/REREn5JFixYhNzcXw4YNwz///IPY2FicOXMGQ4YMgaWlJWbNmiXXPjg4GGvXrsXjx4/h5+eHw4cPY+zYsQr7/uyzz+Du7o4xY8bg7NmzePLkCYKDg7Fs2TLcvn0bAPDdd99h//79WLFiBR4+fIh///0Xa9eulfVhZ2eHq1evIi4ursS7+wCgR48eyMjIgI+PDzw8PGBlZSWbN3r0aKSkpGDSpEm4efMmoqOjce7cOXz33XfvDW5DhgzB9evXsXXrVgwaNEgWNGvUqIH8/Hxs2rQJ0dHRCAwMxNatW9/ZV+vWrXH79m3s2bMHERERWLFihVzYMjAwwIQJE+Dr64vdu3cjKioKoaGh8PPzU/hYCDGpfOQqPT0d8+bNw7Nnz0ps8+btkERERB87BwcHHD16FL/88gsmTpyIly9fwtzcHN26dcO3335b7NtLJkyYgDt37mDlypUwMDDAvHnz0K5dO4V9SyQSbN26FcuWLcP06dPx4sULmJubo2XLlrIL5D08PLB+/XqsXr0aa9asgYGBgezbVADg+++/x6xZs9C6dWvk5OQgNjZW4boMDQ1ljy1YuXKl3DxLS0vs378fS5YswbBhw5CTkwNbW1u0a9fuvV+03bx5c9SuXRuRkZEYMGCAbLqzszPmz5+PtWvXYunSpWjZsiV8fHwwbdq0Evtq164dvvnmGyxevBg5OTkYNGgQ+vfvj/Dw/z1kdubMmTAzM8Pvv/+OJ0+ewMjICC4uLvj666/fWWdZSYS3T1iW0h9//IHHjx9jxowZmDx5suwCtRMnTiAkJARz585V+km0n5LExMQSzyPTu0kkElhZWSEuLq7Y+XZ6P45f2XEMy6a045eWlgYjI6MPWFnl06RJE8yYMQNDhw4tNk/RaUEqvZLGr6T9TiqVwtzcvFR9q3xaMDQ0FD179pQFKA0NDVhaWmLEiBFwcXHBli1bVO2aiIjok5aVlYXz588jMTFRdpceqQ+Vw9WLFy9gYWEBDQ0NSCQSZGdny+a5u7vj7t27ohRIRET0qdm2bRsmTpyIcePGyX2HL6kHla+5MjIyQmZmJoDXDyZ7+vQpGjRoAOD14/xLczcCERERFTd+/Hi5h2qSelE5XNWqVQtPnz6Fm5sbmjRpgsDAQOjp6UFLSws7d+4s8Qm1RERERB8zlcNVt27d8Pz5cwDA4MGD8fDhQ6xZswbA64eIeXt7i1MhERERkRpROVwVfREk8PoU4fLly/H06VMAgI2NDTQ1NcteHRERfbJ4JyapK9Ge0C6RSGBvby9Wd0RE9InT0tLCq1evoK+vz++ypQ8iNzdXlH2tTOEqMzMTQUFBCAsLQ3p6OgwNDdGwYUN06dIFVapUKXNxRET06apSpQpycnKQnp5e0aVUStra2sjNza3oMtSWovGTSCQwMDAoc98qh6uEhAQsWLAASUlJMDMzg4mJCeLi4nD37l2cOHEC8+fPf+cXOBIREb2Pjo4OdHR0KrqMSocPsi2b8h4/lcPV5s2bkZubi59++knuAWf379/HihUr4OfnV+w7lIiIiIg+dmV6QvuQIUOKPTm2bt26GDx4MEJDQ8tcHBEREZG6UTlcSaVSmJqaKpxnZmYGqVSqclFERERE6krlcNW0aVNcuXJF4bwrV67Azc1N5aKIiIiI1JXK11y1adMG69atw8qVK9GmTRuYmJggJSUFFy5cQEREBL766itERETI2js4OIhSMBEREVFlJhFUvEx+0KBBSrUPCAhQZTUfraEbriE8PqOiyyAiorccHluvokt4L94tWDaqjJ9UKoW5uXmp2qp85GrixImqLkpERET00VIpXBUWFsLJyQnGxsZ8WCgRERHRG1S6oF0QBHz33Xd48OCB2PUQERERqTWVwpWmpiZMTEx4npeIiIjoLSo/isHDwwPnzp0TsxYiIiIitafyBe01a9bElStXsGDBArRo0QImJibFvkm6RYsWZS6QiIiISJ2oHK7WrFkDAEhOTsa9e/cUtuHjF4iIiOhTo3K4mj9/vph1EBEREX0UVA5XDRo0ELMOIiIioo+CyuGqSGZmJh48eID09HQ0adIEBgYGYtRFREREpJbKFK4CAwNx4MAB5ObmAgCWLl0KAwMDLFy4EK6urujTp48YNRIRERGpDZUfxRAUFITAwEC0b98es2fPlpvn5uaGkJCQMhdHREREpG5UPnJ17Ngx9OrVC8OHD0dhYaHcvKIvQyQiIiL61Kh85CohIQGNGjVSOE9PTw+ZmZkqF0VERESkrlQOV/r6+khNTVU4LyEhAUZGRioXRURERKSuVA5Xzs7OOHDgALKzs2XTJBIJCgoKcOLEiRKPahERERF9zFS+5mrQoEHw8fHBd999h+bNmwN4fR1WVFQUkpKS8O2334pWJBEREZG6UPnIlaWlJX766SfY2NggKCgIAHD+/HkYGhpiwYIFMDMzE61IIiIiInVRpudc2dra4ocffkBeXh7S09NhYGAAbW1tsWojIiIiUjsqH7l6k5aWFvT09CCVSsXojoiIiEhtlenI1cOHD7F7927cu3cP+fn50NLSQoMGDTBgwAA4OTmJVSMRERGR2lD5yFVoaCjmz5+PiIgItG7dGl5eXmjdujUiIiLg6+uLu3fvilknERERkVpQ+cjV9u3bUatWLcydOxe6urqy6VlZWVi4cCF27NiBpUuXilIkERERkbpQ+cjVkydP0Lt3b7lgBbx+OruXlxeePHlS5uKIiIiI1I3K4crY2BgSiURxpxoafEI7ERERfZJUDledOnXCkSNHkJ+fLzc9Pz8fR44cQadOncpcHBEREZG6UfmaKy0tLSQmJuLrr79G8+bNYWJigpSUFFy7dg0aGhqQSqU4fPiwrH2vXr1EKZiIiIioMivTBe1Fjh079s75AMMVERERfRpUDle///67mHUQERERfRRUDlfm5uZi1kFERET0UVD5gvaff/4Zt27dErEUIiIiIvWn8pGr2NhYLF26FJaWlujatSvatWsHfX19MWsjIiIiUjsqh6v//ve/CAkJQVBQEPz9/bFr1y60adMG3bp1g729vZg1EhEREamNMn1xs5ubG9zc3BAfH4+goCCcPXsWp06dQv369dGtWzc0b94cGhoqn3kkIiIiUjtlCldFLC0tMWrUKPTr1w8rV65EWFgY/v33X1SrVg29e/dGt27dSnyaOxEREdHHRJRw9eLFC5w4cQKnTp1CWloaGjduDA8PDwQHB8PPzw/Pnj3D2LFjxVgVERERUaVWpnAVGhqKY8eO4caNG9DW1oanpye6d+8OKysrAICnpyf+/vtv7Nmzh+GKiIiIPgkqh6tvv/0Wz549g4WFBYYPH4727dsrvFvQ0dERmZmZZSqSiIiISF2oHK6qVauGYcOGwd3d/Z3XUzk4OPBp7kRERPTJUDlczZ07t3Qr0NLi09yJiIjok6FUuJoyZUqp20okEvz3v/9VuiAiIiIidaZUuLK1tS027ebNm6hXrx709PREK4qIiIhIXSkVrmbPni33uqCgAEOHDsWoUaPg4OAgamFERERE6qhMj0/ng0GJiIiI5PG7aYiIiIhExHBFREREJCKGKyIiIiIRKXVBe0REhNzrwsJCAMCzZ88UtudF7kRERPSpUSpc+fj4KJxe0vOsAgIClK+IiIiISI0pFa4mTpxYXnUQERERfRSUClft2rUrpzKIiIiIPg68oJ2IiIhIRAxXRERERCJiuCIiIiISEcMVERERkYgYroiIiIhExHBFREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcEVEREQkIoYrIiIiIhExXBERERGJiOGKiIioBH5+fmjZsiUcHBzQrVs3XL16tVTLBQcHw97eHp07d5abHhAQABsbm2I/2dnZ5VE+VRCtii6AiIioMjpw4AB8fX2xZMkSNGvWDFu3bsXw4cNx9uxZ2NjYlLhcWloapk2bhjZt2iAxMbHYfENDQ5w/f15umq6uruj1U8VhuFJBSkoK9u7di5CQECQnJ8PY2Bg1atRAz5494eLiUtHlERGRCDZs2IDBgwdj6NChAICFCxfi3Llz2LJlC3x8fEpcbtasWejTpw80NTVx7NixYvMlEgksLCzKrW6qeDwtqKSEhATMmjULoaGhGD58OFasWIE5c+bA2dkZf/75Z0WXR0REIsjNzcWdO3fg6ekpN93T0xPXr18vcbmAgABER0fju+++K7HNq1ev0Lx5c7i7u2PkyJEIDQ0VrW6qHHjkSkl//vknJBIJlixZIncY187ODu3bt6/AyoiISCzJyckoKCiAmZmZ3HQzMzMkJCQoXCYiIgJLlizB3r17oaWl+OPV0dERq1atQr169ZCRkYGNGzfCy8sLJ06cgIODg+jbQRWD4UoJGRkZuHXrFgYPHqzw/HiVKlWKTcvLy0NeXp7stUQigZ6eXrnWSUREqpNIJJBIJAAADQ0N2d8VzS9SUFCAKVOm4Pvvv4ejo2Ox9kWaNm2Kpk2byl43b94cXbp0webNm7Fo0SKlany7byq98h4/hislxMfHQxCEd17I+LZ9+/YhMDBQ9rpWrVpYtmxZeZRHREQisLKygqmpKTQ1NZGfnw8rKyvZvKysLNjY2MhNA15fi3v79m2Ehobihx9+AAAUFhZCEATY2dnh+PHj6NChg8L1eXh4ICYmplifpWFpaan0MvQ/5TV+DFdKEARB6WX69u2LXr16yV7zfxlERJVbXFwcAMDV1RUHDhxAy5YtZfOOHj2Krl27ytoUKSwsxOnTp+Wm+fv74+LFi9iwYQPs7e2LLQO8/lwJDg5GvXr1FM4viUQigaWlpew//aQcVcZPS0sL5ubmpWtbluI+NVZWVpBIJIiNjS31MlKpFFKptByrIiIiMRV92I4fPx7Tpk2Dq6sr3N3dsW3bNsTGxmLEiBEQBAFLly5FXFwcfvvtN0gkEtStW1euH1NTU+jo6MimC4KAlStXws3NDbVq1UJ6ejo2bdqEsLAwLF68WKWQJAgCw1UZlNf4MVwpwcDAAI0aNUJQUBC6d+9e7LqrV69eKbzuioiI1I+XlxdevnyJVatWISEhAXXr1sXWrVtha2sLAHj+/DmePXumVJ+pqamYOXMmEhMTYWhoCGdnZ/z1119o0qRJeWwCVRCJwMirlISEBPz4448wMDDAwIEDUaNGDRQUFODOnTs4ceIEVq1aVap+hm64hvD4jHKuloiIlHV4bL2KLuG9JBIJrKysEBcXxyNXKlBl/KRSKU8LlhcLCwssW7YMe/fuxdatW/Hy5UsYGRnBwcEB48aNq+jyiIiIqIIxXKmgatWqGDt2LMaOHVvRpRAREVElwye0ExEREYmI4YqIiIhIRAxXRERERCJiuCIiIiISEcMVERERkYgYroiIiIhExHBFREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcEVEREQkIoYrIiIiIhExXBERERGJiOGKiIiISEQMV0REREQiYrgiIiIiEhHDFREREZGIGK6IiIiIRMRwRURERCQihisiIiIiETFcEREREYmI4YqIiIhIRAxXRERERCJiuCIiIiISEcMVERERkYgYroiIiIhExHBFREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcEVEREQkIoYrIiIiIhExXBERERGJiOGKiIiISEQMV0REREQiYrgiIiIiEhHDFREREZGIGK6IiIiIRMRwRURERCQihisiIiIiETFcEREREYmI4YqIiIhIRAxXRERERCJiuCIiIiISEcMVERERkYgYroiIiIhExHBFREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcEVEREQkIoYrIiIiIhExXBERERGJiOGKiIiISEQMV0REREQiYrgiIiIiEpFWRRfwqfq1Ty3k5eVVdBlqSSKRwMrKCnFxcRAEoaLLUTscv7LjGJYNx48+djxyRURERCQihisiIiIiETFcEREREYmI4YqIiIhIRAxXRERERCJiuCIiIiISEcMVERERkYgYroiIiIhExHBFREREJCKGKyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiLQquoBPlZYWh76sOIZlw/ErO45h2XD8yo5jWDbKjJ8ybSWCIAiqFESqycvLg1QqregyiIiIqJzwtOAHlpeXh19//RVZWVkVXYraysrKwqxZsziGKuL4lR3HsGw4fmXHMSyb8h4/hqsKcOnSJfCAoeoEQUBkZCTHUEUcv7LjGJYNx6/sOIZlU97jx3BFREREJCKGKyIiIiIRMVx9YFKpFP379+dF7WXAMSwbjl/ZcQzLhuNXdhzDsinv8ePdgkREREQi4pErIiIiIhExXBERERGJiOGKiIiISEQMV0REREQi4pcSlYOgoCAcPHgQKSkpsLW1xejRo1G/fv0S29+7dw/+/v6IiYlB1apV0bt3b3Tp0uUDVlz5KDOGL1++xJYtWxAREYH4+Hh0794do0eP/rAFVzLKjN/Vq1dx/PhxREVFIT8/H7a2thgwYAAaN278YYuuRJQZv/DwcGzfvh2xsbHIycmBubk5OnXqhF69en3gqisXZX8PFgkPD4evry/s7Ozwn//85wNUWjkpM35hYWFYsGBBsemrVq2CjY1NeZdaaSm7D+bl5SEwMBAXLlxASkoKTE1N0bdvX3To0EHpdTNciezy5cvw8/PDuHHjULduXZw8eRJLlizBqlWrYGZmVqx9QkICli5dio4dO+Lrr7/G/fv3sXHjRhgZGaFly5YVsAUVT9kxzMvLg5GREb744gscOXKkAiquXJQdv3///Reurq4YMmQIqlSpgjNnzmDZsmVYsmQJatWqVQFbULGUHT8dHR107doVNWrUgI6ODsLDw7Fhwwbo6uqiU6dOFbAFFU/ZMSySmZmJNWvWwMXFBSkpKR+u4EpG1fFbvXo19PX1Za+NjIw+RLmVkipjuGrVKqSmpuKrr76CpaUl0tLSUFBQoNL6eVpQZIcPH0aHDh3QsWNHWVI2MzPD8ePHFbY/fvw4zMzMMHr0aNja2qJjx45o3749Dh069IErrzyUHUMLCwt4e3vD09NT7hfLp0rZ8Rs9ejS8vLzg6OgIKysrDB06FFZWVrhx48YHrrxyUHb8atWqhTZt2sDOzg4WFhZo27YtGjVqhH///fcDV155KDuGRf744w+0bt0aderU+UCVVk6qjp+xsTFMTExkPxoan+5HvLJjeOvWLdy7dw8+Pj5wdXWFhYUFHB0dUbduXZXW/+mOfDnIz89HREQEGjVqJDfd1dUV9+/fV7jMw4cP4erqKjetcePGiIiIQH5+frnVWlmpMob0P2KMX2FhIbKysmBgYFAeJVZqYoxfZGQk7t+/jwYNGpRHiZWeqmN45swZPH/+HAMGDCjvEiu1suyDM2fOxJdffomFCxciNDS0PMus1FQZw+vXr6N27do4cOAAJkyYgGnTpmHLli3Izc1VqQaeFhRRWloaCgsLYWxsLDfd2Ni4xEPcKSkpCtsXFBQgPT0dVatWLa9yKyVVxpD+R4zxO3z4MHJyctCqVatyqLByK8v4ffXVV7LTCAMGDEDHjh3LsdLKS5UxjIuLw44dO7BgwQJoamp+gCorL1XGr2rVqvjyyy/h4OCA/Px8nD9/Hj/99BPmz5//SYZ8Vcbw+fPnCA8Ph1QqxYwZM5CWloY///wTGRkZmDRpktI1MFyVA4lEUqppJc0remj+u5b52Ck7hiRP1fG7ePEi9uzZgxkzZhT7xfQpUWX8Fi5ciOzsbDx48AA7duyApaUl2rRpU14lVnqlHcPCwkL89ttvGDBgAKytrT9EaWpBmX3Q2tpabuycnJyQlJSEQ4cOfZLhqogyY1j0uTt16lTZ5SV5eXlYuXIlxo0bB21tbaXWzXAlIiMjI2hoaBRLxqmpqSV+UJmYmBRrn5aWBk1NzU/ytIwqY0j/U5bxu3z5MtatW4fvvvuu2KnqT0VZxs/CwgIAYG9vj9TUVOzZs+eTDFfKjmFWVhYeP36MyMhIbNq0CcDrDzpBEDB48GD8+OOPcHZ2/hClVwpi/Q50cnLChQsXRK5OPaj6WVytWjW563ZtbGwgCAJevHgBKysrpWrgNVci0tLSgoODA+7cuSM3/c6dOyVeFFenTp1i7W/fvg0HBwdoaX162VeVMaT/UXX8Ll68iDVr1mDq1Klwc3Mr7zIrLbH2P0EQPslrJgHlx1BPTw8rVqzA8uXLZT+dO3eGtbU1li9fDkdHxw9VeqUg1j4YGRkJExMTkatTD6qMYb169fDy5UtkZ2fLpsXFxUEikcDU1FTpGhiuRNarVy+cOnUKp0+fRkxMDPz8/JCUlITOnTsDAHbs2IHff/9d1r5Lly5ISkqSPefq9OnTOH36ND7//POK2oQKp+wYAkBUVBSioqKQnZ2NtLQ0REVFISYmpiLKr3DKjl9RsBo5ciScnJyQkpKClJQUZGZmVtQmVChlx+/YsWO4fv064uLiEBcXhzNnzuDQoUP47LPPKmoTKpwyY6ihoQF7e3u5HyMjI0ilUtjb20NXV7ciN6VCKLsPHjlyBNeuXUNcXByePn2KHTt24OrVq+jWrVtFbUKFU3YM27RpA0NDQ6xduxYxMTG4d+8etm3bhvbt2yt9ShDgaUHReXh4ID09HX/99RdevnwJOzs7+Pj4wNzcHMDrB14mJSXJ2ltYWMDHxwf+/v4ICgpC1apV4e3t/ck+4wpQfgyB13fJFImIiMDFixdhbm6ONWvWfNDaKwNlx+/kyZMoKCjAn3/+iT///FM23dPTE5MnT/7g9Vc0ZcdPEATs3LkTCQkJ0NDQgKWlJYYNG/bJPuMKUO3fMP2PsuOXn5+PrVu3Ijk5Gdra2rCzs8Ps2bM/6aPQyo6hrq4ufvzxR2zatAmzZ8+GoaEhWrVqhcGDB6u0folQdBUXEREREZUZTwsSERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcEVEREQkIoYrogpy9uxZDBw4EI8fP1Y4/+eff/4kH+KpjoKCgnD27NkPuk5fX19Mnz79g65TTDk5Odi9ezfCwsIquhQi0TFcERGV0fHjxz94uFJ3OTk5CAwMZLiijxLDFRGpJD8/HwUFBR9sfTk5OR9sXZWBIAjIzc2t6DJE97FuF9Gb+N2CRGpi4cKFSE5OxqpVqyCRSGTTBUHA1KlTYW1tDR8fHyQkJGDKlCkYNmwYCgoKcOLECaSlpcHOzg7Dhg2Di4uLXL9xcXHYvXs37t69i8zMTFSvXh1du3aV+9LXsLAwLFiwAFOmTEFUVBQuXbqElJQUrFy5Eg8fPsTatWvx448/4uLFiwgODkZ+fj4aNmwIb29vVK9eXdbPnTt3cOzYMURERCA9PR3VqlWDi4sLBg8eDCMjI1m73bt3IzAwED///DP27duH0NBQSKVS/PHHH3j8+DEOHTqEhw8fIiUlBSYmJqhTpw6GDRsm+94w4PVp17Vr12LevHm4ePEirl27hoKCAjRr1gzjxo1DdnY2Nm3ahDt37kBbWxtt2rTB0KFDoaX1v1+L+fn5OHDgAC5cuICEhATo6enB3d0dw4cPl9U7efJkJCYmAgAGDhwIAHLfa5mZmYnAwEBcvXoVycnJMDIykn1n2ZtfSjxw4EB07doVdnZ2OHr0KOLj4+Ht7Y0uXbqUeh8p6sPBwQH79+9HUlIS7OzsMGbMGNSpUweHDh1CUFAQ0tLS4OjoiAkTJsDS0lK2vK+vL9LT0zFu3Dhs27YNUVFRMDAwQPv27TFw4EBoaPzv/+MZGRnYtWsXgoODkZaWBlNTU7Ru3Rr9+/eHVCp973Zt3LgRABAYGIjAwEAA//s+y/j4eOzduxfh4eFITk5GlSpVUKtWLQwdOhT29vbF9supU6fi6dOnOHv2LLKzs+Ho6IixY8fC2tpabnxu3bqFgwcP4vHjxygoKIC5uTnatm2Lvn37yto8fvwYgYGBCA8PR25uLmxsbNCnTx94eHiU+n0gYrgiqmCFhYUKjwC9/bWfPXr0wPLly3H37l24urrKpt+8eRPPnz+Ht7e3XPtjx47B3Nwco0ePhiAIOHDgAJYsWYIFCxbAyckJABATE4Mff/wRZmZmGDlyJExMTHDr1i1s3rwZ6enpGDBggFyfO3bsgJOTE8aPHw8NDQ0YGxvL5v3f//0fXF1dMW3aNCQlJSEgIAC+vr5YsWIFqlSpAgCIj4+Hk5MTOnToAH19fSQmJuLw4cOYN28eVqxYIRdsAOCXX36Bh4cHOnfuLDtylZiYCGtra3h4eMDAwAApKSk4fvw4fHx8sHLlSrmQBgDr1q1D8+bN8c033yAyMhI7d+5EQUEBnj17hhYtWqBTp064e/cuDhw4gGrVqqFXr16y92X58uX4999/4eXlBScnJyQlJWH37t3w9fXFzz//DG1tbXz//fdYuXIl9PX1MXbsWACQhYucnBz4+vrixYsX6Nu3L2rUqIGnT59i9+7dePLkCebOnSsXlIODgxEeHo5+/frBxMREbnxLKyQkBFFRURg2bBgAYPv27fj555/h6emJ58+fY+zYscjMzIS/vz9++eUXLF++XK6GlJQUrF69Gn369MHAgQMREhKCvXv34tWrV7Lty83NxYIFCxAfH4+BAweiRo0a+Pfff7F//35ERUXBx8dHrqa3t8vAwABz5szBkiVL0KFDB3To0AEAZO9dcnIyDAwMMHToUBgZGSEjIwPnzp3DnDlzsHz58mKhaefOnahbty4mTJiArKwsbN++HcuWLcOqVatkgfD06dNYv349GjRogPHjx8PY2BhxcXF48uSJrJ/Q0FAsWbIEderUwfjx46Gvr4/Lly9j9erVyM3NRbt27ZR+P+jTxHBFVMF++OGHEue9eSTGzc0N1atXx7Fjx+TCVVBQEKpXr44mTZrILVtYWIgff/wR2traAIBGjRph8uTJCAgIwNy5cwEA/v7+0NPTw8KFC6Gvrw8AcHV1RX5+Pvbv34/u3bvDwMBA1mf16tXx3XffKay1du3amDhxouy1nZ0d5s6di6CgIHzxxRcAIHcURhAE1K1bFw0bNsSkSZNw69YtNG3aVK5PT09P2dGgIi1btkTLli3lttPNzQ3jx4/HxYsX0aNHD7n2bm5uGDlypGzbHjx4gEuXLmHkyJGyIOXq6orbt2/jwoULsmlXrlzBrVu3MH36dLRo0ULWX40aNeDj44OzZ8+iS5cuqFWrFrS1taGnpycLrUWOHj2K6OhoLFmyBLVr1wYAuLi4oFq1ali5ciVu3bol975lZ2djxYoVcmOurLy8PPzwww+yo2ISiQT/+c9/EBYWhmXLlsmCVFpaGvz8/PD06VO5o0Hp6emYOXOm7L1o1KgRcnNzcfz4cXh5ecHMzAznzp1DdHQ0vv32W7Rq1Uo2hrq6uti+fTvu3Lkjt48q2q60tDQAQLVq1YqNW4MGDdCgQQPZ66L3ePr06Thx4gRGjRol197W1hZTp06VvdbQ0MCqVavw6NEjODk5ITs7G/7+/qhbty7mzZsnG4O3j+L++eefsLOzw7x586CpqQkAaNy4MdLS0rBz5060bdtW7ugdUUkYrogq2JQpU2BjY1Nsur+/P168eCF7raGhga5du2Lbtm1ISkqCmZkZ4uPjcevWLYwYMULu6AMAtGjRQhasAMhOaV26dAmFhYXIz89HaGgoOnfuDB0dHbmjZ02aNMGxY8fw8OFDuQ//N0PG29q0aSP3um7dujA3N0dYWJgsXKWmpiIgIAA3b95EcnKy3NG5mJiYYuFK0fqys7Nlp9kSExNRWFgomxcbG1usvbu7u9xrGxsbBAcHw83Nrdj0O3fuyF7fuHEDVapUgbu7u9zY1KxZEyYmJggLC3vvKbsbN27A3t4eNWvWlOujcePGkEgkCAsLkxtfZ2fnMgUrAGjYsKHc6caifatonW9PT0xMlAtXenp6xd6HNm3a4NSpU7h37x7atm2L0NBQ6OjoyIVcAGjXrh22b99e7OiqsttVUFAgOx0bHx8vN3aK3uO3661RowYAICkpCU5OTrh//z6ysrLQpUuXYv9OisTHxyM2NhYjRoyQ1VDEzc0NISEhePbsGWxtbUu9HfTpYrgiqmA2Njayoxpv0tfXlwtXANChQwfs3r0bx48fx9ChQxEUFARtbW20b9++2PImJiYKp+Xn5yM7OxvZ2dkoKCjAsWPHcOzYMYW1paeny72uWrVqidtR0vqK+igsLMSiRYvw8uVL9OvXD/b29tDR0YEgCPjhhx8UXuSsaH2//vorQkND0a9fP9SuXRt6enqQSCRYunSpwj7e/lAvOvWoaPqby6empuLVq1cYOnSowu19e2wUSU1NRXx8PIYMGVKqPhSNobKU2V7g9ZGuNyk6FVlUV0ZGhuxPExOTYkHF2NgYmpqaZd4uf39/BAUFwcvLCw0aNICBgQEkEgnWrVun8D02NDRUuG1FbYuOkpmampa4zpSUFADA1q1bsXXrVoVtSvOeEwEMV0RqRV9fH56enjh9+jR69+6Ns2fPonXr1rJrmt5U9GHx9jQtLS3o6upCU1MTGhoaaNu2Lbp27apwfRYWFnKvS/pf/7vWV3TB9NOnTxEdHY1JkybJXbsSHx9fYp9vy8zMREhICPr3748+ffrIpufl5ck++MViaGgIQ0NDzJkzR+F8PT29UvWhra0td7r07flvetf4fiipqanFphW9t0UBzcDAAA8fPoQgCHI1p6amoqCgoNh1b8pu14ULF+Dp6Vks2Kanpyvc19+nqJ63/7OiqE2fPn1KPEL79rVeRCVhuCJSM927d8fx48fxyy+/4NWrV3J39b3p6tWrGD58uOzUYFZWFm7cuIH69etDQ0MDOjo6aNiwISIjI1GjRo1iF5Mr6+LFi3Knie7fv4/ExETZxcpFH7Bv3kkGACdOnFBqPYIgFOvj1KlTcqcHxeDu7o7Lly+jsLAQderUeWfbt496vdnHvn37YGhoWCyoVlZZWVm4fv263Km2ixcvQiKRyK6DcnFxwZUrVxAcHIzmzZvL2p07dw7A69OA71P0HioaN4lEUmx/DAkJQXJystzdjaVVt25d6Ovr48SJE2jdurXCsGdtbQ0rKytER0eXeLSSqLQYrojUjLW1NRo3boybN2+iXr16qFmzpsJ2GhoaWLRoEXr16oXCwkIcOHAAWVlZcncAent7Y+7cuZg3bx66dOkCc3NzZGVlIT4+Hjdu3MD8+fNLXdfjx4+xbt06tGzZEi9evMCuXbtQrVo12VExa2trVK9eHTt27IAgCDAwMMCNGzfkrnN6H319fdSvXx8HDx6EoaEhzM3Nce/ePZw5c0alIxrv0rp1a1y8eBFLly5Fjx494OjoCE1NTbx48QJhYWFo1qyZLFjY29vj8uXLuHz5MiwsLKCtrQ17e3v06NEDV69exfz589GzZ0/Y29tDEAQkJSXh9u3b+Pzzz98b3D40Q0NDbNiwAUlJSbCyssLNmzdx6tQpdOnSBWZmZgCAtm3bIigoCGvWrEFCQgLs7e0RHh6Offv2oUmTJnLXW5VET08P5ubmuH79OlxcXGBgYCALoW5ubjh37hxsbGxQo0YNRERE4ODBg+88rfcuurq6GDlyJNatW4effvoJHTt2hLGxMeLj4xEdHS27C3L8+PFYunQpFi9eDE9PT1SrVg0ZGRmIjY1FZGRkiTdzEL2N4YpIDbVq1Qo3b94s8agVAHTr1g15eXnYvHkzUlNTYWdnh9mzZ6NevXqyNra2tli2bBn++usv7Nq1C6mpqahSpQqsrKyK3X34PhMnTsT58+fx66+/Ii8vT/acq6JTSVpaWpg1axb8/PywYcMGaGhowMXFBXPnzsWkSZNKvZ5p06Zh8+bN2LZtGwoLC1G3bl38+OOP+Pnnn5Wq9300NDQwc+ZM/P333zh//jz27dsHTU1NmJqaon79+nIXgQ8cOBApKSlYv349srKyZM+50tXVxYIFC7B//36cPHkSCQkJ0NbWhpmZGVxcXOTuBq0sTExMMHbsWGzduhVPnjyBgYEB+vbtK3fXpra2NubPn4+dO3fi0KFDSEtLQ7Vq1fD5558Xe3zHu3z11VfYtm0bli9fjry8PNlzrry9vaGlpYX9+/cjOzsbtWrVwvfff49du3apvF0dOnRA1apVceDAAaxbtw7A67txPT09ZW2cnZ2xZMkS7N27F/7+/sjIyIChoSFsbW1ld0USlYZEePthOkRU6a1YsQIPHz7EmjVrip0+KXqI6PDhw9G7d+9yr6XoYZ1Lly5VeGE+qY+ih4j+8ssvFV0KkVrjkSsiNZGXl4fIyEg8evQIwcHBGDlyZJmvkyIiIvHxNzORmnj58iV+/PFH6OnpoVOnTujevXtFl0RERArwtCARERGRiPgcfyIiIiIRMVwRERERiYjhioiIiEhEDFdEREREImK4IiIiIhIRwxURERGRiBiuiIiIiETEcEVEREQkIoYrIiIiIhH9P4/GuXNAMKVjAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>211.400000</td>\n",
       "      <td>8.871928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>179.800000</td>\n",
       "      <td>10.580905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>7.438638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>3.590110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.851925</td>\n",
       "      <td>0.017543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.844569</td>\n",
       "      <td>0.026590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.879453</td>\n",
       "      <td>0.013465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.821840</td>\n",
       "      <td>0.031689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.861433</td>\n",
       "      <td>0.015941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.851687</td>\n",
       "      <td>0.017729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.851014</td>\n",
       "      <td>0.017639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.850650</td>\n",
       "      <td>0.017708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.703341</td>\n",
       "      <td>0.034048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.860820</td>\n",
       "      <td>0.017945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.850650</td>\n",
       "      <td>0.017708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       211.400000     8.871928\n",
       "1                    TN       179.800000    10.580905\n",
       "2                    FP        39.000000     7.438638\n",
       "3                    FN        29.000000     3.590110\n",
       "4              Accuracy         0.851925     0.017543\n",
       "5             Precision         0.844569     0.026590\n",
       "6           Sensitivity         0.879453     0.013465\n",
       "7           Specificity         0.821840     0.031689\n",
       "8              F1 score         0.861433     0.015941\n",
       "9   F1 score (weighted)         0.851687     0.017729\n",
       "10     F1 score (macro)         0.851014     0.017639\n",
       "11    Balanced Accuracy         0.850650     0.017708\n",
       "12                  MCC         0.703341     0.034048\n",
       "13                  NPV         0.860820     0.017945\n",
       "14              ROC_AUC         0.850650     0.017708"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>425.000000</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>447.000000</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>423.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>349.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>79.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>66.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.830250</td>\n",
       "      <td>0.861806</td>\n",
       "      <td>0.824810</td>\n",
       "      <td>0.848749</td>\n",
       "      <td>0.828074</td>\n",
       "      <td>0.854189</td>\n",
       "      <td>0.841132</td>\n",
       "      <td>0.841132</td>\n",
       "      <td>0.820457</td>\n",
       "      <td>0.865071</td>\n",
       "      <td>0.841567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.826848</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.856061</td>\n",
       "      <td>0.846960</td>\n",
       "      <td>0.854251</td>\n",
       "      <td>0.851429</td>\n",
       "      <td>0.831721</td>\n",
       "      <td>0.814433</td>\n",
       "      <td>0.845691</td>\n",
       "      <td>0.842801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.863821</td>\n",
       "      <td>0.882231</td>\n",
       "      <td>0.832998</td>\n",
       "      <td>0.877670</td>\n",
       "      <td>0.826176</td>\n",
       "      <td>0.871901</td>\n",
       "      <td>0.867961</td>\n",
       "      <td>0.879346</td>\n",
       "      <td>0.840426</td>\n",
       "      <td>0.899787</td>\n",
       "      <td>0.864232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.791600</td>\n",
       "      <td>0.839100</td>\n",
       "      <td>0.815200</td>\n",
       "      <td>0.811900</td>\n",
       "      <td>0.830200</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.797700</td>\n",
       "      <td>0.799600</td>\n",
       "      <td>0.828900</td>\n",
       "      <td>0.815560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.844930</td>\n",
       "      <td>0.870540</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.866731</td>\n",
       "      <td>0.836439</td>\n",
       "      <td>0.862986</td>\n",
       "      <td>0.859615</td>\n",
       "      <td>0.854871</td>\n",
       "      <td>0.827225</td>\n",
       "      <td>0.871901</td>\n",
       "      <td>0.853245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.829862</td>\n",
       "      <td>0.861674</td>\n",
       "      <td>0.824883</td>\n",
       "      <td>0.848455</td>\n",
       "      <td>0.828189</td>\n",
       "      <td>0.854087</td>\n",
       "      <td>0.840900</td>\n",
       "      <td>0.840669</td>\n",
       "      <td>0.820342</td>\n",
       "      <td>0.864835</td>\n",
       "      <td>0.841390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.828715</td>\n",
       "      <td>0.861174</td>\n",
       "      <td>0.823787</td>\n",
       "      <td>0.845944</td>\n",
       "      <td>0.827623</td>\n",
       "      <td>0.853586</td>\n",
       "      <td>0.838329</td>\n",
       "      <td>0.839695</td>\n",
       "      <td>0.820181</td>\n",
       "      <td>0.864686</td>\n",
       "      <td>0.840372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.827695</td>\n",
       "      <td>0.860656</td>\n",
       "      <td>0.824082</td>\n",
       "      <td>0.844776</td>\n",
       "      <td>0.828204</td>\n",
       "      <td>0.853192</td>\n",
       "      <td>0.837446</td>\n",
       "      <td>0.838510</td>\n",
       "      <td>0.819990</td>\n",
       "      <td>0.864338</td>\n",
       "      <td>0.839889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.658396</td>\n",
       "      <td>0.722697</td>\n",
       "      <td>0.647624</td>\n",
       "      <td>0.692238</td>\n",
       "      <td>0.655530</td>\n",
       "      <td>0.707375</td>\n",
       "      <td>0.676863</td>\n",
       "      <td>0.680976</td>\n",
       "      <td>0.640800</td>\n",
       "      <td>0.731227</td>\n",
       "      <td>0.681373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>0.864900</td>\n",
       "      <td>0.805600</td>\n",
       "      <td>0.838900</td>\n",
       "      <td>0.807700</td>\n",
       "      <td>0.854100</td>\n",
       "      <td>0.827400</td>\n",
       "      <td>0.853200</td>\n",
       "      <td>0.827200</td>\n",
       "      <td>0.888100</td>\n",
       "      <td>0.840170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.827695</td>\n",
       "      <td>0.860656</td>\n",
       "      <td>0.824082</td>\n",
       "      <td>0.844776</td>\n",
       "      <td>0.828204</td>\n",
       "      <td>0.853192</td>\n",
       "      <td>0.837446</td>\n",
       "      <td>0.838510</td>\n",
       "      <td>0.819990</td>\n",
       "      <td>0.864338</td>\n",
       "      <td>0.839889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  425.000000  427.000000  414.000000  452.000000   \n",
       "1                    TN  338.000000  365.000000  344.000000  328.000000   \n",
       "2                    FP   89.000000   70.000000   78.000000   76.000000   \n",
       "3                    FN   67.000000   57.000000   83.000000   63.000000   \n",
       "4              Accuracy    0.830250    0.861806    0.824810    0.848749   \n",
       "5             Precision    0.826848    0.859155    0.841463    0.856061   \n",
       "6           Sensitivity    0.863821    0.882231    0.832998    0.877670   \n",
       "7           Specificity    0.791600    0.839100    0.815200    0.811900   \n",
       "8              F1 score    0.844930    0.870540    0.837209    0.866731   \n",
       "9   F1 score (weighted)    0.829862    0.861674    0.824883    0.848455   \n",
       "10     F1 score (macro)    0.828715    0.861174    0.823787    0.845944   \n",
       "11    Balanced Accuracy    0.827695    0.860656    0.824082    0.844776   \n",
       "12                  MCC    0.658396    0.722697    0.647624    0.692238   \n",
       "13                  NPV    0.834600    0.864900    0.805600    0.838900   \n",
       "14              ROC_AUC    0.827695    0.860656    0.824082    0.844776   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   404.000000  422.000000  447.000000  430.000000  395.000000  422.000000   \n",
       "1   357.000000  363.000000  326.000000  343.000000  359.000000  373.000000   \n",
       "2    73.000000   72.000000   78.000000   87.000000   90.000000   77.000000   \n",
       "3    85.000000   62.000000   68.000000   59.000000   75.000000   47.000000   \n",
       "4     0.828074    0.854189    0.841132    0.841132    0.820457    0.865071   \n",
       "5     0.846960    0.854251    0.851429    0.831721    0.814433    0.845691   \n",
       "6     0.826176    0.871901    0.867961    0.879346    0.840426    0.899787   \n",
       "7     0.830200    0.834500    0.806900    0.797700    0.799600    0.828900   \n",
       "8     0.836439    0.862986    0.859615    0.854871    0.827225    0.871901   \n",
       "9     0.828189    0.854087    0.840900    0.840669    0.820342    0.864835   \n",
       "10    0.827623    0.853586    0.838329    0.839695    0.820181    0.864686   \n",
       "11    0.828204    0.853192    0.837446    0.838510    0.819990    0.864338   \n",
       "12    0.655530    0.707375    0.676863    0.680976    0.640800    0.731227   \n",
       "13    0.807700    0.854100    0.827400    0.853200    0.827200    0.888100   \n",
       "14    0.828204    0.853192    0.837446    0.838510    0.819990    0.864338   \n",
       "\n",
       "           ave  \n",
       "0   423.800000  \n",
       "1   349.600000  \n",
       "2    79.000000  \n",
       "3    66.600000  \n",
       "4     0.841567  \n",
       "5     0.842801  \n",
       "6     0.864232  \n",
       "7     0.815560  \n",
       "8     0.853245  \n",
       "9     0.841390  \n",
       "10    0.840372  \n",
       "11    0.839889  \n",
       "12    0.681373  \n",
       "13    0.840170  \n",
       "14    0.839889  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.852743</td>\n",
       "      <td>0.018051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.844309</td>\n",
       "      <td>0.022414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.881314</td>\n",
       "      <td>0.023558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.821482</td>\n",
       "      <td>0.025408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.862182</td>\n",
       "      <td>0.017939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.852497</td>\n",
       "      <td>0.018081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.851853</td>\n",
       "      <td>0.018093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.851401</td>\n",
       "      <td>0.018129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.705149</td>\n",
       "      <td>0.036261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.863202</td>\n",
       "      <td>0.025551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.851401</td>\n",
       "      <td>0.018129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.852743     0.018051\n",
       "1             Precision         0.844309     0.022414\n",
       "2           Sensitivity         0.881314     0.023558\n",
       "3           Specificity         0.821482     0.025408\n",
       "4              F1 score         0.862182     0.017939\n",
       "5   F1 score (weighted)         0.852497     0.018081\n",
       "6      F1 score (macro)         0.851853     0.018093\n",
       "7     Balanced Accuracy         0.851401     0.018129\n",
       "8                   MCC         0.705149     0.036261\n",
       "9                   NPV         0.863202     0.025551\n",
       "10              ROC_AUC         0.851401     0.018129"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_svm)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_svm))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_svm))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_svm))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_svm))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_svm, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_svm, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_svm))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_svm))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_svm))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model f1_score 0.8172 with a standard deviation of 0.0162\n",
      "SVM optimized model f1_score 0.8388 with a standard deviation of 0.0150\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVC \n",
    "svm_baseline_CVscore = cross_val_score(svm_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"SVM baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm_clf.joblib']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the modesls, both the one with optimized hyperparameters and the initial one\n",
    "joblib.dump(svm_clf, \"OUTPUT/svm_clf.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm_clf.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "484e48eb-6732-43c3-9476-635047d3319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4cdd104c-7c12-406a-9bc8-4cfabb5c45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
