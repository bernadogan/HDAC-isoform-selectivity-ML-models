{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1and6 = Path(HERE).resolve().parents[1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3633769</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[119259, 7882477, 4733773, 6560150, 1301427, 1...</td>\n",
       "      <td>7.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3797314</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[18823350, 26971653, 8033062, 19609762, 104214...</td>\n",
       "      <td>5.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3415967</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1821977, 756812, 526260, 718486, 1799526, 521...</td>\n",
       "      <td>5.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3759625</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[8089733, 16948577, 8033062, 5710863, 5733837,...</td>\n",
       "      <td>5.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3675763</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1106022, 7938102, 5371099, 20209854, 3554414,...</td>\n",
       "      <td>7.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL3633769  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL3797314  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3415967  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3759625  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3675763  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC6  \n",
       "0  [119259, 7882477, 4733773, 6560150, 1301427, 1...           7.59  \n",
       "1  [18823350, 26971653, 8033062, 19609762, 104214...           5.52  \n",
       "2  [1821977, 756812, 526260, 718486, 1799526, 521...           5.09  \n",
       "3  [8089733, 16948577, 8033062, 5710863, 5733837,...           5.68  \n",
       "4  [1106022, 7938102, 5371099, 20209854, 3554414,...           7.89  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC6/\"HDAC6_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type</th>\n",
       "      <th>Standard_Value_HDAC6</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4082520</td>\n",
       "      <td>CN1C(=O)C2CN(Cc3c2c2cc(OCc4ccccc4)ccc2n3Cc2ccc...</td>\n",
       "      <td>Ki</td>\n",
       "      <td>0.08</td>\n",
       "      <td>10.10</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9.85</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4100534</td>\n",
       "      <td>COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...</td>\n",
       "      <td>Ki</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9.82</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4101480</td>\n",
       "      <td>COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...</td>\n",
       "      <td>Ki</td>\n",
       "      <td>0.16</td>\n",
       "      <td>9.80</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                             smiles  type  \\\n",
       "0      CHEMBL4082520  CN1C(=O)C2CN(Cc3c2c2cc(OCc4ccccc4)ccc2n3Cc2ccc...    Ki   \n",
       "1      CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...  IC50   \n",
       "2      CHEMBL4100534  COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...    Ki   \n",
       "3      CHEMBL4101480  COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...    Ki   \n",
       "\n",
       "   Standard_Value_HDAC6  pChEMBL_HDAC6            label  \n",
       "0                  0.08          10.10    Single points  \n",
       "1                  0.14           9.85  HDAC6-selective  \n",
       "2                  0.15           9.82    Single points  \n",
       "3                  0.16           9.80    Single points  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC6/\"HDAC6_dataset.csv\", )\n",
    "df_labeled.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3633769</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[119259, 7882477, 4733773, 6560150, 1301427, 1...</td>\n",
       "      <td>7.59</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3797314</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[18823350, 26971653, 8033062, 19609762, 104214...</td>\n",
       "      <td>5.52</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3415967</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1821977, 756812, 526260, 718486, 1799526, 521...</td>\n",
       "      <td>5.09</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3759625</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[8089733, 16948577, 8033062, 5710863, 5733837,...</td>\n",
       "      <td>5.68</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL3633769  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL3797314  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3415967  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3759625  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC6  \\\n",
       "0  [119259, 7882477, 4733773, 6560150, 1301427, 1...           7.59   \n",
       "1  [18823350, 26971653, 8033062, 19609762, 104214...           5.52   \n",
       "2  [1821977, 756812, 526260, 718486, 1799526, 521...           5.09   \n",
       "3  [8089733, 16948577, 8033062, 5710863, 5733837,...           5.68   \n",
       "\n",
       "           label  Class  \n",
       "0    Dual-binder    3.0  \n",
       "1     Non-binder    4.0  \n",
       "2  Single points    0.0  \n",
       "3  Single points    0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'HDAC6-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'HDAC6-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'HDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"activity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.pChEMBL_HDAC6 >= 6.6].index, \"activity\"] = 1.0\n",
    "\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y =  df[\"activity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['activity'])).astype(float)\n",
    "    \n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['activity'])).astype(float)\n",
    "    \n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       165.800000     5.808040\n",
      "1                    TN        86.700000     4.029061\n",
      "2                    FP        26.700000     4.877385\n",
      "3                    FN        17.900000     3.107339\n",
      "4              Accuracy         0.849885     0.021285\n",
      "5             Precision         0.861365     0.024380\n",
      "6           Sensitivity         0.902474     0.017110\n",
      "7           Specificity         0.765240     0.037807\n",
      "8              F1 score         0.881292     0.017653\n",
      "9   F1 score (weighted)         0.848573     0.021768\n",
      "10     F1 score (macro)         0.838401     0.022402\n",
      "11    Balanced Accuracy         0.833857     0.023038\n",
      "12                  MCC         0.678989     0.043826\n",
      "13                  NPV         0.829160     0.026955\n",
      "14              ROC_AUC         0.833857     0.023038\n",
      "CPU times: user 2min 3s, sys: 165 ms, total: 2min 4s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_clf =  RandomForestClassifier(random_state=1121218, max_features = None, n_jobs=16,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_clf.fit(x_train, y_train)\n",
    "        y_pred = rf_clf.predict(x_test)  \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggestegorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggestegorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestClassifier(**param_grid, n_jobs=16, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred,  average=\"macro\")\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggestegorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggestegorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestClassifier(**param_grid, n_jobs=16, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 14:01:37,391] A new study created in memory with name: RFclassifier\n",
      "[I 2023-12-04 14:02:00,786] Trial 0 finished with value: 0.8382051836875716 and parameters: {'n_estimators': 317}. Best is trial 0 with value: 0.8382051836875716.\n",
      "[I 2023-12-04 14:02:26,596] Trial 1 finished with value: 0.8382275025918178 and parameters: {'n_estimators': 350}. Best is trial 1 with value: 0.8382275025918178.\n",
      "[I 2023-12-04 14:03:24,244] Trial 2 finished with value: 0.8370473580666802 and parameters: {'n_estimators': 803}. Best is trial 1 with value: 0.8382275025918178.\n",
      "[I 2023-12-04 14:04:31,724] Trial 3 finished with value: 0.8362283621536057 and parameters: {'n_estimators': 950}. Best is trial 1 with value: 0.8382275025918178.\n",
      "[I 2023-12-04 14:04:50,537] Trial 4 finished with value: 0.8390855384805878 and parameters: {'n_estimators': 247}. Best is trial 4 with value: 0.8390855384805878.\n",
      "[I 2023-12-04 14:05:53,593] Trial 5 finished with value: 0.8375281039267654 and parameters: {'n_estimators': 888}. Best is trial 4 with value: 0.8390855384805878.\n",
      "[I 2023-12-04 14:06:51,409] Trial 6 finished with value: 0.8375271765324523 and parameters: {'n_estimators': 804}. Best is trial 4 with value: 0.8390855384805878.\n",
      "[I 2023-12-04 14:07:52,553] Trial 7 finished with value: 0.8371109553341082 and parameters: {'n_estimators': 855}. Best is trial 4 with value: 0.8390855384805878.\n",
      "[I 2023-12-04 14:08:09,947] Trial 8 finished with value: 0.8374218897750326 and parameters: {'n_estimators': 226}. Best is trial 4 with value: 0.8390855384805878.\n",
      "[I 2023-12-04 14:09:16,930] Trial 9 finished with value: 0.8371109553341082 and parameters: {'n_estimators': 892}. Best is trial 4 with value: 0.8390855384805878.\n",
      "[I 2023-12-04 14:09:56,167] Trial 10 finished with value: 0.835490350883361 and parameters: {'n_estimators': 533}. Best is trial 4 with value: 0.8390855384805878.\n",
      "[I 2023-12-04 14:10:05,086] Trial 11 finished with value: 0.8404792928577681 and parameters: {'n_estimators': 104}. Best is trial 11 with value: 0.8404792928577681.\n",
      "[I 2023-12-04 14:10:14,561] Trial 12 finished with value: 0.8376856960287477 and parameters: {'n_estimators': 114}. Best is trial 11 with value: 0.8404792928577681.\n",
      "[I 2023-12-04 14:10:23,333] Trial 13 finished with value: 0.8404792928577681 and parameters: {'n_estimators': 104}. Best is trial 11 with value: 0.8404792928577681.\n",
      "[I 2023-12-04 14:10:32,888] Trial 14 finished with value: 0.8389492632195747 and parameters: {'n_estimators': 111}. Best is trial 11 with value: 0.8404792928577681.\n",
      "[I 2023-12-04 14:11:09,185] Trial 15 finished with value: 0.836381979795331 and parameters: {'n_estimators': 491}. Best is trial 11 with value: 0.8404792928577681.\n",
      "[I 2023-12-04 14:11:58,105] Trial 16 finished with value: 0.8374162149438709 and parameters: {'n_estimators': 671}. Best is trial 11 with value: 0.8404792928577681.\n",
      "[I 2023-12-04 14:12:29,157] Trial 17 finished with value: 0.8357362802103812 and parameters: {'n_estimators': 420}. Best is trial 11 with value: 0.8404792928577681.\n",
      "[I 2023-12-04 14:13:17,797] Trial 18 finished with value: 0.836867004477161 and parameters: {'n_estimators': 665}. Best is trial 11 with value: 0.8404792928577681.\n",
      "[I 2023-12-04 14:13:33,047] Trial 19 finished with value: 0.8406693457458252 and parameters: {'n_estimators': 195}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:13:50,474] Trial 20 finished with value: 0.8374218897750326 and parameters: {'n_estimators': 225}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:14:02,776] Trial 21 finished with value: 0.8385066258203008 and parameters: {'n_estimators': 153}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:14:17,242] Trial 22 finished with value: 0.8404597767760604 and parameters: {'n_estimators': 185}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:14:40,839] Trial 23 finished with value: 0.8368619441275674 and parameters: {'n_estimators': 310}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:14:49,948] Trial 24 finished with value: 0.838403727220198 and parameters: {'n_estimators': 107}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:15:18,947] Trial 25 finished with value: 0.8383171136699245 and parameters: {'n_estimators': 389}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:15:39,641] Trial 26 finished with value: 0.8380621731100713 and parameters: {'n_estimators': 275}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:15:54,665] Trial 27 finished with value: 0.8400508378028825 and parameters: {'n_estimators': 188}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:16:27,314] Trial 28 finished with value: 0.8368749905183952 and parameters: {'n_estimators': 443}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:16:52,502] Trial 29 finished with value: 0.8382574184888141 and parameters: {'n_estimators': 329}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:17:06,243] Trial 30 finished with value: 0.840161661424047 and parameters: {'n_estimators': 172}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:17:20,085] Trial 31 finished with value: 0.8393864810426173 and parameters: {'n_estimators': 170}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:17:41,309] Trial 32 finished with value: 0.8386150847828683 and parameters: {'n_estimators': 279}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:17:57,107] Trial 33 finished with value: 0.8400921028694865 and parameters: {'n_estimators': 204}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:18:06,082] Trial 34 finished with value: 0.8389868049988998 and parameters: {'n_estimators': 106}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:18:32,933] Trial 35 finished with value: 0.8377518331926496 and parameters: {'n_estimators': 358}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:18:53,412] Trial 36 finished with value: 0.8380621731100713 and parameters: {'n_estimators': 273}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:19:05,308] Trial 37 finished with value: 0.8384974615511928 and parameters: {'n_estimators': 147}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:19:52,482] Trial 38 finished with value: 0.8368791415853719 and parameters: {'n_estimators': 655}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:20:36,336] Trial 39 finished with value: 0.8359727925636747 and parameters: {'n_estimators': 605}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:20:54,266] Trial 40 finished with value: 0.8382618277411549 and parameters: {'n_estimators': 237}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:21:07,838] Trial 41 finished with value: 0.8404922370954628 and parameters: {'n_estimators': 171}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:21:19,694] Trial 42 finished with value: 0.8389122327442149 and parameters: {'n_estimators': 149}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:21:35,234] Trial 43 finished with value: 0.8395428924027767 and parameters: {'n_estimators': 201}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:21:53,866] Trial 44 finished with value: 0.8390855384805878 and parameters: {'n_estimators': 245}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:22:05,334] Trial 45 finished with value: 0.840198325641268 and parameters: {'n_estimators': 141}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:22:27,894] Trial 46 finished with value: 0.8377096592129435 and parameters: {'n_estimators': 299}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:22:43,409] Trial 47 finished with value: 0.8397039196919799 and parameters: {'n_estimators': 198}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:23:37,538] Trial 48 finished with value: 0.8360285023672226 and parameters: {'n_estimators': 752}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:23:46,710] Trial 49 finished with value: 0.8375172054171169 and parameters: {'n_estimators': 108}. Best is trial 19 with value: 0.8406693457458252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8407\n",
      "\tBest params:\n",
      "\t\tn_estimators: 195\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFclassifier\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  330.000000\n",
      "1                    TN  170.000000\n",
      "2                    FP   57.000000\n",
      "3                    FN   38.000000\n",
      "4              Accuracy    0.840336\n",
      "5             Precision    0.852713\n",
      "6           Sensitivity    0.896739\n",
      "7           Specificity    0.748900\n",
      "8              F1 score    0.874172\n",
      "9   F1 score (weighted)    0.838858\n",
      "10     F1 score (macro)    0.827891\n",
      "11    Balanced Accuracy    0.822819\n",
      "12                  MCC    0.657716\n",
      "13                  NPV    0.817300\n",
      "14              ROC_AUC    0.822819\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_rf_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_rf_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_rf_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_rf_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_rf_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_rf_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_rf_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_rf_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_rf_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_rf_0)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP), np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                           np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 14:24:15,014] Trial 50 finished with value: 0.8349731008768506 and parameters: {'n_estimators': 365}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:24:25,476] Trial 51 finished with value: 0.8282447001849347 and parameters: {'n_estimators': 130}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:24:37,872] Trial 52 finished with value: 0.8298255490035931 and parameters: {'n_estimators': 155}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:24:54,272] Trial 53 finished with value: 0.8315386512872228 and parameters: {'n_estimators': 214}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:26:04,209] Trial 54 finished with value: 0.8353839401740695 and parameters: {'n_estimators': 986}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:26:23,884] Trial 55 finished with value: 0.8328537868782213 and parameters: {'n_estimators': 258}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:26:34,639] Trial 56 finished with value: 0.8300087307951799 and parameters: {'n_estimators': 133}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:26:49,542] Trial 57 finished with value: 0.831702028312822 and parameters: {'n_estimators': 186}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:27:07,182] Trial 58 finished with value: 0.8321463073809685 and parameters: {'n_estimators': 231}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:27:21,031] Trial 59 finished with value: 0.8301746889678568 and parameters: {'n_estimators': 170}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:27:29,816] Trial 60 finished with value: 0.8332835066579845 and parameters: {'n_estimators': 104}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:27:44,067] Trial 61 finished with value: 0.8311424902171849 and parameters: {'n_estimators': 177}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:27:55,080] Trial 62 finished with value: 0.8300910893217346 and parameters: {'n_estimators': 135}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:28:08,469] Trial 63 finished with value: 0.8296061756664059 and parameters: {'n_estimators': 165}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:28:32,321] Trial 64 finished with value: 0.8343415018768645 and parameters: {'n_estimators': 316}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:28:49,209] Trial 65 finished with value: 0.8318700887234097 and parameters: {'n_estimators': 215}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:28:57,833] Trial 66 finished with value: 0.8323730059287463 and parameters: {'n_estimators': 102}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:29:08,645] Trial 67 finished with value: 0.8287726124588424 and parameters: {'n_estimators': 132}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:29:28,055] Trial 68 finished with value: 0.8328537868782213 and parameters: {'n_estimators': 258}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:29:42,794] Trial 69 finished with value: 0.831702028312822 and parameters: {'n_estimators': 186}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:30:17,206] Trial 70 finished with value: 0.8352204235083533 and parameters: {'n_estimators': 479}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:30:33,389] Trial 71 finished with value: 0.8328755306687384 and parameters: {'n_estimators': 208}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:30:54,934] Trial 72 finished with value: 0.8350056318304004 and parameters: {'n_estimators': 288}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:31:07,681] Trial 73 finished with value: 0.8289646014514359 and parameters: {'n_estimators': 157}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:31:25,490] Trial 74 finished with value: 0.8317379176358803 and parameters: {'n_estimators': 234}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:31:39,974] Trial 75 finished with value: 0.8306685836771239 and parameters: {'n_estimators': 184}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:31:50,584] Trial 76 finished with value: 0.8282447001849347 and parameters: {'n_estimators': 130}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:32:14,923] Trial 77 finished with value: 0.833952001759528 and parameters: {'n_estimators': 328}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:32:31,426] Trial 78 finished with value: 0.8329005320571907 and parameters: {'n_estimators': 212}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:32:51,469] Trial 79 finished with value: 0.8337339035209297 and parameters: {'n_estimators': 265}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:33:30,963] Trial 80 finished with value: 0.8343419172429914 and parameters: {'n_estimators': 550}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:33:43,002] Trial 81 finished with value: 0.831546511060278 and parameters: {'n_estimators': 150}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:33:58,210] Trial 82 finished with value: 0.8329309415368519 and parameters: {'n_estimators': 194}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:34:09,075] Trial 83 finished with value: 0.8300087307951799 and parameters: {'n_estimators': 133}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:34:22,822] Trial 84 finished with value: 0.8307318761796505 and parameters: {'n_estimators': 173}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:34:40,785] Trial 85 finished with value: 0.8321387992384507 and parameters: {'n_estimators': 235}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:34:50,359] Trial 86 finished with value: 0.8321057451106325 and parameters: {'n_estimators': 115}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:35:03,056] Trial 87 finished with value: 0.8301069735617462 and parameters: {'n_estimators': 159}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:35:18,555] Trial 88 finished with value: 0.8319235433506698 and parameters: {'n_estimators': 202}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:35:46,932] Trial 89 finished with value: 0.833970108855301 and parameters: {'n_estimators': 387}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:35:56,957] Trial 90 finished with value: 0.8315226245244679 and parameters: {'n_estimators': 122}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:36:11,934] Trial 91 finished with value: 0.8313331628823646 and parameters: {'n_estimators': 193}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:36:30,619] Trial 92 finished with value: 0.8322889718411831 and parameters: {'n_estimators': 248}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:36:42,567] Trial 93 finished with value: 0.8297175620224561 and parameters: {'n_estimators': 148}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:36:51,023] Trial 94 finished with value: 0.8313576453785313 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:37:07,750] Trial 95 finished with value: 0.8328763414070867 and parameters: {'n_estimators': 216}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:38:00,102] Trial 96 finished with value: 0.834569586829638 and parameters: {'n_estimators': 736}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:38:22,062] Trial 97 finished with value: 0.8349805092526372 and parameters: {'n_estimators': 295}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:39:24,799] Trial 98 finished with value: 0.835809255486603 and parameters: {'n_estimators': 886}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:39:38,485] Trial 99 finished with value: 0.8307318761796505 and parameters: {'n_estimators': 173}. Best is trial 19 with value: 0.8406693457458252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8407\n",
      "\tBest params:\n",
      "\t\tn_estimators: 195\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  330.000000  320.000000\n",
      "1                    TN  170.000000  184.000000\n",
      "2                    FP   57.000000   53.000000\n",
      "3                    FN   38.000000   38.000000\n",
      "4              Accuracy    0.840336    0.847059\n",
      "5             Precision    0.852713    0.857909\n",
      "6           Sensitivity    0.896739    0.893855\n",
      "7           Specificity    0.748900    0.776400\n",
      "8              F1 score    0.874172    0.875513\n",
      "9   F1 score (weighted)    0.838858    0.846129\n",
      "10     F1 score (macro)    0.827891    0.838628\n",
      "11    Balanced Accuracy    0.822819    0.835113\n",
      "12                  MCC    0.657716    0.678432\n",
      "13                  NPV    0.817300    0.828800\n",
      "14              ROC_AUC    0.822819    0.835113\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_rf_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_rf_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_rf_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_rf_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_rf_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_rf_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_rf_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_rf_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_rf_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_rf_1)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 14:39:54,969] Trial 100 finished with value: 0.8330518354272506 and parameters: {'n_estimators': 191}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:40:06,647] Trial 101 finished with value: 0.835450102010534 and parameters: {'n_estimators': 144}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:40:22,333] Trial 102 finished with value: 0.833230426662283 and parameters: {'n_estimators': 204}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:40:39,488] Trial 103 finished with value: 0.8352842472951053 and parameters: {'n_estimators': 225}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:40:49,354] Trial 104 finished with value: 0.8370866126183664 and parameters: {'n_estimators': 119}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:41:02,284] Trial 105 finished with value: 0.8338766893955561 and parameters: {'n_estimators': 163}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:41:21,329] Trial 106 finished with value: 0.831254328077609 and parameters: {'n_estimators': 255}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:41:41,500] Trial 107 finished with value: 0.8311821064272126 and parameters: {'n_estimators': 272}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:41:55,504] Trial 108 finished with value: 0.8340330839344174 and parameters: {'n_estimators': 180}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:42:07,165] Trial 109 finished with value: 0.83430559688768 and parameters: {'n_estimators': 145}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:42:17,115] Trial 110 finished with value: 0.8368181049260859 and parameters: {'n_estimators': 122}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:42:30,325] Trial 111 finished with value: 0.8340486490316306 and parameters: {'n_estimators': 167}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:42:45,605] Trial 112 finished with value: 0.8314250551880938 and parameters: {'n_estimators': 198}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:43:02,773] Trial 113 finished with value: 0.8343185451787736 and parameters: {'n_estimators': 227}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:43:16,847] Trial 114 finished with value: 0.8339427257243603 and parameters: {'n_estimators': 179}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:43:29,698] Trial 115 finished with value: 0.833959282912317 and parameters: {'n_estimators': 159}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:43:41,278] Trial 116 finished with value: 0.8361231454684808 and parameters: {'n_estimators': 143}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:43:59,270] Trial 117 finished with value: 0.8321713428059601 and parameters: {'n_estimators': 238}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:44:15,330] Trial 118 finished with value: 0.8357175684278166 and parameters: {'n_estimators': 209}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:44:24,325] Trial 119 finished with value: 0.8389148009632436 and parameters: {'n_estimators': 111}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:44:34,934] Trial 120 finished with value: 0.8367979101626721 and parameters: {'n_estimators': 131}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:44:49,373] Trial 121 finished with value: 0.8327974390644097 and parameters: {'n_estimators': 186}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:45:06,181] Trial 122 finished with value: 0.834767528351127 and parameters: {'n_estimators': 219}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:45:19,418] Trial 123 finished with value: 0.8336453846339833 and parameters: {'n_estimators': 166}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:45:34,893] Trial 124 finished with value: 0.8324042618235934 and parameters: {'n_estimators': 200}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:45:53,235] Trial 125 finished with value: 0.8326100070856274 and parameters: {'n_estimators': 245}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:46:01,699] Trial 126 finished with value: 0.8398067092646142 and parameters: {'n_estimators': 101}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:46:11,310] Trial 127 finished with value: 0.8394235007411354 and parameters: {'n_estimators': 117}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:46:19,698] Trial 128 finished with value: 0.8398067092646142 and parameters: {'n_estimators': 101}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:46:28,479] Trial 129 finished with value: 0.8368326231065899 and parameters: {'n_estimators': 106}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:46:39,438] Trial 130 finished with value: 0.8368376191396159 and parameters: {'n_estimators': 136}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:46:47,744] Trial 131 finished with value: 0.8398067092646142 and parameters: {'n_estimators': 101}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:46:56,097] Trial 132 finished with value: 0.8385874475786117 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:47:06,255] Trial 133 finished with value: 0.836290381034319 and parameters: {'n_estimators': 125}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:47:18,478] Trial 134 finished with value: 0.8335819926510857 and parameters: {'n_estimators': 153}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:47:26,909] Trial 135 finished with value: 0.8398067092646142 and parameters: {'n_estimators': 101}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:47:38,044] Trial 136 finished with value: 0.8357933678372443 and parameters: {'n_estimators': 139}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:47:46,603] Trial 137 finished with value: 0.8380638246902665 and parameters: {'n_estimators': 102}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:47:56,755] Trial 138 finished with value: 0.836290381034319 and parameters: {'n_estimators': 125}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:48:09,314] Trial 139 finished with value: 0.8335471461972279 and parameters: {'n_estimators': 157}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:48:17,785] Trial 140 finished with value: 0.8398067092646142 and parameters: {'n_estimators': 101}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:48:27,456] Trial 141 finished with value: 0.8370866126183664 and parameters: {'n_estimators': 119}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:48:35,836] Trial 142 finished with value: 0.8385874475786117 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:48:46,946] Trial 143 finished with value: 0.8351292945955621 and parameters: {'n_estimators': 140}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:48:56,771] Trial 144 finished with value: 0.8385997802016556 and parameters: {'n_estimators': 121}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:49:10,401] Trial 145 finished with value: 0.8343892390175837 and parameters: {'n_estimators': 175}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:49:22,270] Trial 146 finished with value: 0.8358336351472732 and parameters: {'n_estimators': 149}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:49:32,993] Trial 147 finished with value: 0.8358978518971704 and parameters: {'n_estimators': 133}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:49:45,773] Trial 148 finished with value: 0.8338786284803688 and parameters: {'n_estimators': 165}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:49:54,671] Trial 149 finished with value: 0.838647041671601 and parameters: {'n_estimators': 110}. Best is trial 19 with value: 0.8406693457458252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8407\n",
      "\tBest params:\n",
      "\t\tn_estimators: 195\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  330.000000  320.000000  333.000000\n",
      "1                    TN  170.000000  184.000000  170.000000\n",
      "2                    FP   57.000000   53.000000   59.000000\n",
      "3                    FN   38.000000   38.000000   33.000000\n",
      "4              Accuracy    0.840336    0.847059    0.845378\n",
      "5             Precision    0.852713    0.857909    0.849490\n",
      "6           Sensitivity    0.896739    0.893855    0.909836\n",
      "7           Specificity    0.748900    0.776400    0.742400\n",
      "8              F1 score    0.874172    0.875513    0.878628\n",
      "9   F1 score (weighted)    0.838858    0.846129    0.843377\n",
      "10     F1 score (macro)    0.827891    0.838628    0.832833\n",
      "11    Balanced Accuracy    0.822819    0.835113    0.826097\n",
      "12                  MCC    0.657716    0.678432    0.669336\n",
      "13                  NPV    0.817300    0.828800    0.837400\n",
      "14              ROC_AUC    0.822819    0.835113    0.826097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_rf_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_rf_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_rf_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_rf_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_rf_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_rf_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_rf_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_rf_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_rf_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_rf_2)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 14:50:39,851] Trial 150 finished with value: 0.8246136963764427 and parameters: {'n_estimators': 598}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:50:54,982] Trial 151 finished with value: 0.8246138774002821 and parameters: {'n_estimators': 188}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:51:07,255] Trial 152 finished with value: 0.8224925152397002 and parameters: {'n_estimators': 150}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:51:17,331] Trial 153 finished with value: 0.8204987202725184 and parameters: {'n_estimators': 121}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:51:52,312] Trial 154 finished with value: 0.8236167694360796 and parameters: {'n_estimators': 472}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:52:00,879] Trial 155 finished with value: 0.8249997691039319 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:52:15,473] Trial 156 finished with value: 0.8244486452864574 and parameters: {'n_estimators': 180}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:52:27,157] Trial 157 finished with value: 0.8214210555739122 and parameters: {'n_estimators': 142}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:52:43,758] Trial 158 finished with value: 0.8243570525295063 and parameters: {'n_estimators': 213}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:52:59,290] Trial 159 finished with value: 0.8246156565282272 and parameters: {'n_estimators': 198}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:53:12,581] Trial 160 finished with value: 0.8230937301214654 and parameters: {'n_estimators': 164}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:53:23,155] Trial 161 finished with value: 0.8191787915066291 and parameters: {'n_estimators': 127}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:53:32,651] Trial 162 finished with value: 0.8234293236224459 and parameters: {'n_estimators': 112}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:53:42,775] Trial 163 finished with value: 0.8204987202725184 and parameters: {'n_estimators': 121}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:53:54,455] Trial 164 finished with value: 0.8223365410151724 and parameters: {'n_estimators': 141}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:54:03,077] Trial 165 finished with value: 0.823957177806304 and parameters: {'n_estimators': 101}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:54:16,617] Trial 166 finished with value: 0.8228088770025778 and parameters: {'n_estimators': 169}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:54:27,200] Trial 167 finished with value: 0.8191787915066291 and parameters: {'n_estimators': 127}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:54:39,497] Trial 168 finished with value: 0.8229019648046204 and parameters: {'n_estimators': 151}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:54:54,896] Trial 169 finished with value: 0.825531226726927 and parameters: {'n_estimators': 193}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:55:03,503] Trial 170 finished with value: 0.8249997691039319 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:55:17,712] Trial 171 finished with value: 0.823038815286474 and parameters: {'n_estimators': 176}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:55:30,466] Trial 172 finished with value: 0.8254041296811432 and parameters: {'n_estimators': 155}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:55:48,001] Trial 173 finished with value: 0.8257680948120119 and parameters: {'n_estimators': 225}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:56:46,024] Trial 174 finished with value: 0.8253872549492153 and parameters: {'n_estimators': 801}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:56:56,178] Trial 175 finished with value: 0.8204987202725184 and parameters: {'n_estimators': 121}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:57:07,562] Trial 176 finished with value: 0.8230957183368579 and parameters: {'n_estimators': 137}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:57:23,647] Trial 177 finished with value: 0.8241137904625255 and parameters: {'n_estimators': 204}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:57:38,342] Trial 178 finished with value: 0.8241032161883949 and parameters: {'n_estimators': 183}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:57:51,534] Trial 179 finished with value: 0.8239244373332701 and parameters: {'n_estimators': 162}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:58:01,606] Trial 180 finished with value: 0.8193873774983214 and parameters: {'n_estimators': 120}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:58:19,574] Trial 181 finished with value: 0.8272128768511606 and parameters: {'n_estimators': 232}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:58:36,131] Trial 182 finished with value: 0.8247084092938131 and parameters: {'n_estimators': 210}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:58:47,539] Trial 183 finished with value: 0.8220241263972143 and parameters: {'n_estimators': 138}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:59:02,019] Trial 184 finished with value: 0.8247010491979676 and parameters: {'n_estimators': 181}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:59:10,530] Trial 185 finished with value: 0.8249997691039319 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:59:23,347] Trial 186 finished with value: 0.8245546851005339 and parameters: {'n_estimators': 158}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:59:33,280] Trial 187 finished with value: 0.8214939265979204 and parameters: {'n_estimators': 118}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 14:59:53,004] Trial 188 finished with value: 0.8242443240183699 and parameters: {'n_estimators': 255}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:00:08,550] Trial 189 finished with value: 0.8253293985251634 and parameters: {'n_estimators': 195}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:00:40,054] Trial 190 finished with value: 0.825846975084 and parameters: {'n_estimators': 424}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:00:58,785] Trial 191 finished with value: 0.8267638523340304 and parameters: {'n_estimators': 240}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:01:16,160] Trial 192 finished with value: 0.82704730898273 and parameters: {'n_estimators': 221}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:01:38,012] Trial 193 finished with value: 0.8239347308206687 and parameters: {'n_estimators': 287}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:01:58,704] Trial 194 finished with value: 0.8241035993886513 and parameters: {'n_estimators': 268}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:02:10,184] Trial 195 finished with value: 0.8221730574000367 and parameters: {'n_estimators': 139}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:02:23,889] Trial 196 finished with value: 0.8220884250736689 and parameters: {'n_estimators': 170}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:02:40,084] Trial 197 finished with value: 0.8247600155491682 and parameters: {'n_estimators': 205}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:03:48,927] Trial 198 finished with value: 0.8244119307459583 and parameters: {'n_estimators': 950}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:04:27,633] Trial 199 finished with value: 0.8219390919405178 and parameters: {'n_estimators': 526}. Best is trial 19 with value: 0.8406693457458252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8407\n",
      "\tBest params:\n",
      "\t\tn_estimators: 195\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  330.000000  320.000000  333.000000  329.000000\n",
      "1                    TN  170.000000  184.000000  170.000000  175.000000\n",
      "2                    FP   57.000000   53.000000   59.000000   63.000000\n",
      "3                    FN   38.000000   38.000000   33.000000   28.000000\n",
      "4              Accuracy    0.840336    0.847059    0.845378    0.847059\n",
      "5             Precision    0.852713    0.857909    0.849490    0.839286\n",
      "6           Sensitivity    0.896739    0.893855    0.909836    0.921569\n",
      "7           Specificity    0.748900    0.776400    0.742400    0.735300\n",
      "8              F1 score    0.874172    0.875513    0.878628    0.878505\n",
      "9   F1 score (weighted)    0.838858    0.846129    0.843377    0.844563\n",
      "10     F1 score (macro)    0.827891    0.838628    0.832833    0.836078\n",
      "11    Balanced Accuracy    0.822819    0.835113    0.826097    0.828431\n",
      "12                  MCC    0.657716    0.678432    0.669336    0.678744\n",
      "13                  NPV    0.817300    0.828800    0.837400    0.862100\n",
      "14              ROC_AUC    0.822819    0.835113    0.826097    0.828431\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_rf_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_rf_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_rf_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_rf_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_rf_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_rf_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_rf_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_rf_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_rf_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_rf_3)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 15:04:39,338] Trial 200 finished with value: 0.820857282657703 and parameters: {'n_estimators': 117}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:04:48,843] Trial 201 finished with value: 0.8202121074794751 and parameters: {'n_estimators': 112}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:04:57,298] Trial 202 finished with value: 0.8200636350085431 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:05:08,300] Trial 203 finished with value: 0.8179592446110447 and parameters: {'n_estimators': 134}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:05:21,191] Trial 204 finished with value: 0.8189415342098698 and parameters: {'n_estimators': 158}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:05:31,960] Trial 205 finished with value: 0.8201958693124789 and parameters: {'n_estimators': 130}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:05:46,418] Trial 206 finished with value: 0.8194523786192619 and parameters: {'n_estimators': 180}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:05:58,748] Trial 207 finished with value: 0.8191326046765012 and parameters: {'n_estimators': 148}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:06:07,324] Trial 208 finished with value: 0.8200636350085431 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:06:17,467] Trial 209 finished with value: 0.8208149277958199 and parameters: {'n_estimators': 121}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:06:32,755] Trial 210 finished with value: 0.8190445873684272 and parameters: {'n_estimators': 190}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:06:42,322] Trial 211 finished with value: 0.821786731322438 and parameters: {'n_estimators': 113}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:06:53,615] Trial 212 finished with value: 0.8190130280556629 and parameters: {'n_estimators': 137}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:07:02,275] Trial 213 finished with value: 0.8200636350085431 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:07:14,659] Trial 214 finished with value: 0.81923858292111 and parameters: {'n_estimators': 152}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:07:24,765] Trial 215 finished with value: 0.8208149277958199 and parameters: {'n_estimators': 121}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:07:38,259] Trial 216 finished with value: 0.8202824738627355 and parameters: {'n_estimators': 167}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:07:54,767] Trial 217 finished with value: 0.8181356245098327 and parameters: {'n_estimators': 209}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:08:05,750] Trial 218 finished with value: 0.8179592446110447 and parameters: {'n_estimators': 134}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:08:15,637] Trial 219 finished with value: 0.820857282657703 and parameters: {'n_estimators': 117}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:08:33,168] Trial 220 finished with value: 0.8187047389597071 and parameters: {'n_estimators': 226}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:08:43,120] Trial 221 finished with value: 0.8224237579683598 and parameters: {'n_estimators': 118}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:08:52,425] Trial 222 finished with value: 0.8213624133007087 and parameters: {'n_estimators': 109}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:09:04,396] Trial 223 finished with value: 0.8189987004021191 and parameters: {'n_estimators': 144}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:09:14,994] Trial 224 finished with value: 0.8225342132715247 and parameters: {'n_estimators': 126}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:09:23,690] Trial 225 finished with value: 0.8200845772113752 and parameters: {'n_estimators': 102}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:09:37,561] Trial 226 finished with value: 0.8203650509138217 and parameters: {'n_estimators': 173}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:10:03,360] Trial 227 finished with value: 0.818353000719644 and parameters: {'n_estimators': 338}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:10:15,677] Trial 228 finished with value: 0.8194547322855812 and parameters: {'n_estimators': 150}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:11:07,470] Trial 229 finished with value: 0.8202877379615071 and parameters: {'n_estimators': 711}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:11:22,813] Trial 230 finished with value: 0.8182692515336338 and parameters: {'n_estimators': 193}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:11:31,504] Trial 231 finished with value: 0.8200636350085431 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:11:41,990] Trial 232 finished with value: 0.8205006104021569 and parameters: {'n_estimators': 129}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:11:55,109] Trial 233 finished with value: 0.818869685863038 and parameters: {'n_estimators': 162}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:12:06,851] Trial 234 finished with value: 0.8185149553482894 and parameters: {'n_estimators': 142}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:12:16,805] Trial 235 finished with value: 0.8212603621895923 and parameters: {'n_estimators': 119}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:12:31,937] Trial 236 finished with value: 0.8190445873684272 and parameters: {'n_estimators': 190}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:12:42,944] Trial 237 finished with value: 0.818288051972077 and parameters: {'n_estimators': 133}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:12:56,686] Trial 238 finished with value: 0.8203650509138217 and parameters: {'n_estimators': 171}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:13:13,593] Trial 239 finished with value: 0.8176305418291825 and parameters: {'n_estimators': 213}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:13:23,351] Trial 240 finished with value: 0.8215287489662717 and parameters: {'n_estimators': 116}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:13:32,711] Trial 241 finished with value: 0.8210432293558266 and parameters: {'n_estimators': 110}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:13:41,273] Trial 242 finished with value: 0.8200636350085431 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:13:53,488] Trial 243 finished with value: 0.8203447862922614 and parameters: {'n_estimators': 149}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:14:03,484] Trial 244 finished with value: 0.820857282657703 and parameters: {'n_estimators': 117}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:14:14,623] Trial 245 finished with value: 0.818874830401292 and parameters: {'n_estimators': 135}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:14:24,268] Trial 246 finished with value: 0.8212777288106607 and parameters: {'n_estimators': 115}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:14:38,963] Trial 247 finished with value: 0.8193294632710872 and parameters: {'n_estimators': 183}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:14:47,484] Trial 248 finished with value: 0.8200636350085431 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:15:06,254] Trial 249 finished with value: 0.8191656406711992 and parameters: {'n_estimators': 242}. Best is trial 19 with value: 0.8406693457458252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8407\n",
      "\tBest params:\n",
      "\t\tn_estimators: 195\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  330.000000  320.000000  333.000000  329.000000   \n",
      "1                    TN  170.000000  184.000000  170.000000  175.000000   \n",
      "2                    FP   57.000000   53.000000   59.000000   63.000000   \n",
      "3                    FN   38.000000   38.000000   33.000000   28.000000   \n",
      "4              Accuracy    0.840336    0.847059    0.845378    0.847059   \n",
      "5             Precision    0.852713    0.857909    0.849490    0.839286   \n",
      "6           Sensitivity    0.896739    0.893855    0.909836    0.921569   \n",
      "7           Specificity    0.748900    0.776400    0.742400    0.735300   \n",
      "8              F1 score    0.874172    0.875513    0.878628    0.878505   \n",
      "9   F1 score (weighted)    0.838858    0.846129    0.843377    0.844563   \n",
      "10     F1 score (macro)    0.827891    0.838628    0.832833    0.836078   \n",
      "11    Balanced Accuracy    0.822819    0.835113    0.826097    0.828431   \n",
      "12                  MCC    0.657716    0.678432    0.669336    0.678744   \n",
      "13                  NPV    0.817300    0.828800    0.837400    0.862100   \n",
      "14              ROC_AUC    0.822819    0.835113    0.826097    0.828431   \n",
      "\n",
      "          Set4  \n",
      "0   337.000000  \n",
      "1   178.000000  \n",
      "2    60.000000  \n",
      "3    20.000000  \n",
      "4     0.865546  \n",
      "5     0.848866  \n",
      "6     0.943978  \n",
      "7     0.747900  \n",
      "8     0.893899  \n",
      "9     0.862945  \n",
      "10    0.855206  \n",
      "11    0.845938  \n",
      "12    0.719322  \n",
      "13    0.899000  \n",
      "14    0.845938  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_rf_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_rf_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_rf_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_rf_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_rf_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_rf_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_rf_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_rf_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_rf_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_rf_4)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 15:15:20,511] Trial 250 finished with value: 0.8330586073579676 and parameters: {'n_estimators': 157}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:15:31,319] Trial 251 finished with value: 0.8286917452885711 and parameters: {'n_estimators': 134}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:15:47,031] Trial 252 finished with value: 0.8310720026795015 and parameters: {'n_estimators': 204}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:15:56,576] Trial 253 finished with value: 0.8290262949702052 and parameters: {'n_estimators': 118}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:16:04,837] Trial 254 finished with value: 0.8284093411794785 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:16:17,972] Trial 255 finished with value: 0.8306659292558802 and parameters: {'n_estimators': 167}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:17:01,024] Trial 256 finished with value: 0.8334914665242723 and parameters: {'n_estimators': 603}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:17:12,531] Trial 257 finished with value: 0.8314715478436632 and parameters: {'n_estimators': 146}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:17:26,874] Trial 258 finished with value: 0.8312453526432508 and parameters: {'n_estimators': 185}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:17:49,704] Trial 259 finished with value: 0.8328559014167537 and parameters: {'n_estimators': 309}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:17:59,598] Trial 260 finished with value: 0.8301865280537122 and parameters: {'n_estimators': 121}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:18:10,216] Trial 261 finished with value: 0.8297088032872482 and parameters: {'n_estimators': 132}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:18:22,891] Trial 262 finished with value: 0.832145616002142 and parameters: {'n_estimators': 160}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:18:39,496] Trial 263 finished with value: 0.8322555065488679 and parameters: {'n_estimators': 220}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:18:48,926] Trial 264 finished with value: 0.83002941946745 and parameters: {'n_estimators': 115}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:18:57,314] Trial 265 finished with value: 0.8284093411794785 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:19:08,854] Trial 266 finished with value: 0.8291379536628657 and parameters: {'n_estimators': 144}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:19:24,127] Trial 267 finished with value: 0.8305525069030562 and parameters: {'n_estimators': 198}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:19:37,943] Trial 268 finished with value: 0.829293389870335 and parameters: {'n_estimators': 175}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:19:48,090] Trial 269 finished with value: 0.8295489767084472 and parameters: {'n_estimators': 127}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:19:59,997] Trial 270 finished with value: 0.8321759756995896 and parameters: {'n_estimators': 150}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:20:10,629] Trial 271 finished with value: 0.8276582364757374 and parameters: {'n_estimators': 133}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:20:19,930] Trial 272 finished with value: 0.8293316220561449 and parameters: {'n_estimators': 114}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:20:37,548] Trial 273 finished with value: 0.832270799039019 and parameters: {'n_estimators': 234}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:20:52,499] Trial 274 finished with value: 0.8320321599081911 and parameters: {'n_estimators': 194}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:21:11,734] Trial 275 finished with value: 0.8304906404293331 and parameters: {'n_estimators': 256}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:21:24,654] Trial 276 finished with value: 0.8319872583121347 and parameters: {'n_estimators': 162}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:21:35,172] Trial 277 finished with value: 0.8296419568534532 and parameters: {'n_estimators': 129}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:21:51,743] Trial 278 finished with value: 0.8329656672351321 and parameters: {'n_estimators': 216}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:22:00,162] Trial 279 finished with value: 0.8284093411794785 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:22:13,977] Trial 280 finished with value: 0.829293389870335 and parameters: {'n_estimators': 175}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:22:25,911] Trial 281 finished with value: 0.8328671329455715 and parameters: {'n_estimators': 148}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:22:35,432] Trial 282 finished with value: 0.8288320466847274 and parameters: {'n_estimators': 116}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:22:46,291] Trial 283 finished with value: 0.8284867836219618 and parameters: {'n_estimators': 137}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:23:01,696] Trial 284 finished with value: 0.8300581958246693 and parameters: {'n_estimators': 197}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:23:10,982] Trial 285 finished with value: 0.8285879204060838 and parameters: {'n_estimators': 113}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:23:24,494] Trial 286 finished with value: 0.8307068681943139 and parameters: {'n_estimators': 173}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:23:35,058] Trial 287 finished with value: 0.8306368873112321 and parameters: {'n_estimators': 130}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:23:43,383] Trial 288 finished with value: 0.829631143456551 and parameters: {'n_estimators': 101}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:24:29,389] Trial 289 finished with value: 0.8344780368831719 and parameters: {'n_estimators': 648}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:24:41,665] Trial 290 finished with value: 0.831605380046551 and parameters: {'n_estimators': 153}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:24:55,922] Trial 291 finished with value: 0.830899022804708 and parameters: {'n_estimators': 182}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:25:05,335] Trial 292 finished with value: 0.8290262949702052 and parameters: {'n_estimators': 118}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:25:33,154] Trial 293 finished with value: 0.8312686394408599 and parameters: {'n_estimators': 377}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:25:49,462] Trial 294 finished with value: 0.8329067670978489 and parameters: {'n_estimators': 213}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:26:01,031] Trial 295 finished with value: 0.8304564805903883 and parameters: {'n_estimators': 145}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:26:13,874] Trial 296 finished with value: 0.8310828899011128 and parameters: {'n_estimators': 161}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:26:23,759] Trial 297 finished with value: 0.8299527371000954 and parameters: {'n_estimators': 122}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:27:22,432] Trial 298 finished with value: 0.8340393280974521 and parameters: {'n_estimators': 833}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:27:43,364] Trial 299 finished with value: 0.8318354321711181 and parameters: {'n_estimators': 280}. Best is trial 19 with value: 0.8406693457458252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8407\n",
      "\tBest params:\n",
      "\t\tn_estimators: 195\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  330.000000  320.000000  333.000000  329.000000   \n",
      "1                    TN  170.000000  184.000000  170.000000  175.000000   \n",
      "2                    FP   57.000000   53.000000   59.000000   63.000000   \n",
      "3                    FN   38.000000   38.000000   33.000000   28.000000   \n",
      "4              Accuracy    0.840336    0.847059    0.845378    0.847059   \n",
      "5             Precision    0.852713    0.857909    0.849490    0.839286   \n",
      "6           Sensitivity    0.896739    0.893855    0.909836    0.921569   \n",
      "7           Specificity    0.748900    0.776400    0.742400    0.735300   \n",
      "8              F1 score    0.874172    0.875513    0.878628    0.878505   \n",
      "9   F1 score (weighted)    0.838858    0.846129    0.843377    0.844563   \n",
      "10     F1 score (macro)    0.827891    0.838628    0.832833    0.836078   \n",
      "11    Balanced Accuracy    0.822819    0.835113    0.826097    0.828431   \n",
      "12                  MCC    0.657716    0.678432    0.669336    0.678744   \n",
      "13                  NPV    0.817300    0.828800    0.837400    0.862100   \n",
      "14              ROC_AUC    0.822819    0.835113    0.826097    0.828431   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   337.000000  328.000000  \n",
      "1   178.000000  178.000000  \n",
      "2    60.000000   56.000000  \n",
      "3    20.000000   33.000000  \n",
      "4     0.865546    0.850420  \n",
      "5     0.848866    0.854167  \n",
      "6     0.943978    0.908587  \n",
      "7     0.747900    0.760700  \n",
      "8     0.893899    0.880537  \n",
      "9     0.862945    0.848864  \n",
      "10    0.855206    0.840268  \n",
      "11    0.845938    0.834636  \n",
      "12    0.719322    0.683371  \n",
      "13    0.899000    0.843600  \n",
      "14    0.845938    0.834636  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_rf_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_rf_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_rf_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_rf_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_rf_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_rf_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_rf_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_rf_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_rf_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_rf_5)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 15:28:03,324] Trial 300 finished with value: 0.8255482780261911 and parameters: {'n_estimators': 235}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:28:11,985] Trial 301 finished with value: 0.8269391867575898 and parameters: {'n_estimators': 102}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:28:27,915] Trial 302 finished with value: 0.8289344322444663 and parameters: {'n_estimators': 198}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:28:39,084] Trial 303 finished with value: 0.8295443405789709 and parameters: {'n_estimators': 135}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:28:53,567] Trial 304 finished with value: 0.8297985718545606 and parameters: {'n_estimators': 180}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:29:03,150] Trial 305 finished with value: 0.8278593502166116 and parameters: {'n_estimators': 114}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:29:15,907] Trial 306 finished with value: 0.8271479745059672 and parameters: {'n_estimators': 158}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:29:27,121] Trial 307 finished with value: 0.8296043330885897 and parameters: {'n_estimators': 136}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:29:36,894] Trial 308 finished with value: 0.8263619563267074 and parameters: {'n_estimators': 117}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:30:10,745] Trial 309 finished with value: 0.8299369049693665 and parameters: {'n_estimators': 460}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:30:27,469] Trial 310 finished with value: 0.827521479972075 and parameters: {'n_estimators': 212}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:30:36,184] Trial 311 finished with value: 0.8269391867575898 and parameters: {'n_estimators': 102}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:30:44,747] Trial 312 finished with value: 0.826832089007917 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:30:56,885] Trial 313 finished with value: 0.8300835714061121 and parameters: {'n_estimators': 150}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:31:38,223] Trial 314 finished with value: 0.830014212547604 and parameters: {'n_estimators': 563}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:31:52,115] Trial 315 finished with value: 0.8273983447991041 and parameters: {'n_estimators': 172}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:32:03,036] Trial 316 finished with value: 0.829202344313044 and parameters: {'n_estimators': 132}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:32:18,401] Trial 317 finished with value: 0.8290627807301041 and parameters: {'n_estimators': 192}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:32:37,564] Trial 318 finished with value: 0.8283735465532839 and parameters: {'n_estimators': 248}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:32:48,156] Trial 319 finished with value: 0.8315225314484873 and parameters: {'n_estimators': 126}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:33:01,083] Trial 320 finished with value: 0.8271479745059672 and parameters: {'n_estimators': 158}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:33:11,401] Trial 321 finished with value: 0.8284179051933244 and parameters: {'n_estimators': 122}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:33:19,830] Trial 322 finished with value: 0.826832089007917 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:33:31,581] Trial 323 finished with value: 0.8292401734047671 and parameters: {'n_estimators': 142}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:33:46,183] Trial 324 finished with value: 0.8295039133390167 and parameters: {'n_estimators': 184}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:34:02,473] Trial 325 finished with value: 0.8285438628129225 and parameters: {'n_estimators': 207}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:34:16,306] Trial 326 finished with value: 0.8284732083501993 and parameters: {'n_estimators': 171}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:34:52,999] Trial 327 finished with value: 0.8305068854509461 and parameters: {'n_estimators': 498}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:35:03,114] Trial 328 finished with value: 0.8274310444847132 and parameters: {'n_estimators': 120}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:35:20,808] Trial 329 finished with value: 0.8255724213972154 and parameters: {'n_estimators': 225}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:35:32,874] Trial 330 finished with value: 0.8282965966406817 and parameters: {'n_estimators': 148}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:35:42,643] Trial 331 finished with value: 0.8286913165212226 and parameters: {'n_estimators': 116}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:36:13,356] Trial 332 finished with value: 0.8286555968381215 and parameters: {'n_estimators': 416}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:36:21,841] Trial 333 finished with value: 0.826832089007917 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:36:33,114] Trial 334 finished with value: 0.8294790508975802 and parameters: {'n_estimators': 138}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:36:46,642] Trial 335 finished with value: 0.8277797281134157 and parameters: {'n_estimators': 167}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:37:02,252] Trial 336 finished with value: 0.8305685806114302 and parameters: {'n_estimators': 196}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:37:22,450] Trial 337 finished with value: 0.8259456591735155 and parameters: {'n_estimators': 263}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:37:33,286] Trial 338 finished with value: 0.8295523260739858 and parameters: {'n_estimators': 129}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:37:51,327] Trial 339 finished with value: 0.8259732615062525 and parameters: {'n_estimators': 227}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:38:01,120] Trial 340 finished with value: 0.8281947198382584 and parameters: {'n_estimators': 116}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:38:13,431] Trial 341 finished with value: 0.8300835714061121 and parameters: {'n_estimators': 150}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:38:21,960] Trial 342 finished with value: 0.826832089007917 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:38:36,717] Trial 343 finished with value: 0.8314101621190261 and parameters: {'n_estimators': 185}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:38:47,137] Trial 344 finished with value: 0.8315225314484873 and parameters: {'n_estimators': 126}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:38:59,929] Trial 345 finished with value: 0.8269764462806901 and parameters: {'n_estimators': 157}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:39:11,358] Trial 346 finished with value: 0.8289191161488747 and parameters: {'n_estimators': 139}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:39:27,937] Trial 347 finished with value: 0.8264495674505504 and parameters: {'n_estimators': 210}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:39:41,744] Trial 348 finished with value: 0.82703544317147 and parameters: {'n_estimators': 170}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:39:51,432] Trial 349 finished with value: 0.8285345951649626 and parameters: {'n_estimators': 115}. Best is trial 19 with value: 0.8406693457458252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8407\n",
      "\tBest params:\n",
      "\t\tn_estimators: 195\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  330.000000  320.000000  333.000000  329.000000   \n",
      "1                    TN  170.000000  184.000000  170.000000  175.000000   \n",
      "2                    FP   57.000000   53.000000   59.000000   63.000000   \n",
      "3                    FN   38.000000   38.000000   33.000000   28.000000   \n",
      "4              Accuracy    0.840336    0.847059    0.845378    0.847059   \n",
      "5             Precision    0.852713    0.857909    0.849490    0.839286   \n",
      "6           Sensitivity    0.896739    0.893855    0.909836    0.921569   \n",
      "7           Specificity    0.748900    0.776400    0.742400    0.735300   \n",
      "8              F1 score    0.874172    0.875513    0.878628    0.878505   \n",
      "9   F1 score (weighted)    0.838858    0.846129    0.843377    0.844563   \n",
      "10     F1 score (macro)    0.827891    0.838628    0.832833    0.836078   \n",
      "11    Balanced Accuracy    0.822819    0.835113    0.826097    0.828431   \n",
      "12                  MCC    0.657716    0.678432    0.669336    0.678744   \n",
      "13                  NPV    0.817300    0.828800    0.837400    0.862100   \n",
      "14              ROC_AUC    0.822819    0.835113    0.826097    0.828431   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   337.000000  328.000000  332.000000  \n",
      "1   178.000000  178.000000  166.000000  \n",
      "2    60.000000   56.000000   47.000000  \n",
      "3    20.000000   33.000000   50.000000  \n",
      "4     0.865546    0.850420    0.836975  \n",
      "5     0.848866    0.854167    0.875989  \n",
      "6     0.943978    0.908587    0.869110  \n",
      "7     0.747900    0.760700    0.779300  \n",
      "8     0.893899    0.880537    0.872536  \n",
      "9     0.862945    0.848864    0.837223  \n",
      "10    0.855206    0.840268    0.823214  \n",
      "11    0.845938    0.834636    0.824226  \n",
      "12    0.719322    0.683371    0.646477  \n",
      "13    0.899000    0.843600    0.768500  \n",
      "14    0.845938    0.834636    0.824226  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_rf_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_rf_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_rf_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_rf_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_rf_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_rf_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_rf_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_rf_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_rf_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_rf_6)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 15:40:04,076] Trial 350 finished with value: 0.8373351844724007 and parameters: {'n_estimators': 135}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:40:19,581] Trial 351 finished with value: 0.8364020491959435 and parameters: {'n_estimators': 199}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:40:29,168] Trial 352 finished with value: 0.8348637972059254 and parameters: {'n_estimators': 115}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:40:43,530] Trial 353 finished with value: 0.8366270784803144 and parameters: {'n_estimators': 183}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:40:56,072] Trial 354 finished with value: 0.8359160610501343 and parameters: {'n_estimators': 157}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:41:07,252] Trial 355 finished with value: 0.8384799703604585 and parameters: {'n_estimators': 138}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:41:25,548] Trial 356 finished with value: 0.8353349837308205 and parameters: {'n_estimators': 241}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:41:35,183] Trial 357 finished with value: 0.8355793959912402 and parameters: {'n_estimators': 116}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:41:48,424] Trial 358 finished with value: 0.8365356172144853 and parameters: {'n_estimators': 167}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:41:58,869] Trial 359 finished with value: 0.8356007684376768 and parameters: {'n_estimators': 130}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:42:15,612] Trial 360 finished with value: 0.8374924814171714 and parameters: {'n_estimators': 219}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:42:24,029] Trial 361 finished with value: 0.8372965674982928 and parameters: {'n_estimators': 101}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:42:36,274] Trial 362 finished with value: 0.8372886294297416 and parameters: {'n_estimators': 152}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:42:44,654] Trial 363 finished with value: 0.8348927789194376 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:42:58,762] Trial 364 finished with value: 0.8366320008567463 and parameters: {'n_estimators': 180}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:43:08,679] Trial 365 finished with value: 0.8350324648291506 and parameters: {'n_estimators': 121}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:43:24,439] Trial 366 finished with value: 0.8359835623427339 and parameters: {'n_estimators': 201}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:43:35,532] Trial 367 finished with value: 0.8388050923740469 and parameters: {'n_estimators': 137}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:43:47,646] Trial 368 finished with value: 0.8377028844669017 and parameters: {'n_estimators': 150}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:44:09,766] Trial 369 finished with value: 0.8381492839828594 and parameters: {'n_estimators': 295}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:44:23,568] Trial 370 finished with value: 0.8351456789711277 and parameters: {'n_estimators': 173}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:44:34,634] Trial 371 finished with value: 0.8388050923740469 and parameters: {'n_estimators': 137}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:44:46,982] Trial 372 finished with value: 0.8368204224720925 and parameters: {'n_estimators': 153}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:45:01,563] Trial 373 finished with value: 0.8361205813719377 and parameters: {'n_estimators': 187}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:45:11,739] Trial 374 finished with value: 0.8349433600252392 and parameters: {'n_estimators': 127}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:45:28,798] Trial 375 finished with value: 0.8388052911990597 and parameters: {'n_estimators': 225}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:45:48,889] Trial 376 finished with value: 0.8348943069466065 and parameters: {'n_estimators': 269}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:46:07,697] Trial 377 finished with value: 0.8358914832999258 and parameters: {'n_estimators': 248}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:46:25,677] Trial 378 finished with value: 0.8357459296943925 and parameters: {'n_estimators': 235}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:46:42,527] Trial 379 finished with value: 0.8383935325808947 and parameters: {'n_estimators': 221}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:46:58,172] Trial 380 finished with value: 0.8372951628928111 and parameters: {'n_estimators': 202}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:47:14,987] Trial 381 finished with value: 0.8374924814171714 and parameters: {'n_estimators': 219}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:47:33,974] Trial 382 finished with value: 0.8367330363436556 and parameters: {'n_estimators': 251}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:47:49,698] Trial 383 finished with value: 0.8372951628928111 and parameters: {'n_estimators': 202}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:48:06,848] Trial 384 finished with value: 0.8383902169783802 and parameters: {'n_estimators': 223}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:48:21,972] Trial 385 finished with value: 0.836307182783837 and parameters: {'n_estimators': 196}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:48:39,841] Trial 386 finished with value: 0.8350253084757379 and parameters: {'n_estimators': 236}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:48:53,320] Trial 387 finished with value: 0.8361220847937103 and parameters: {'n_estimators': 171}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:49:03,109] Trial 388 finished with value: 0.8376872620702415 and parameters: {'n_estimators': 119}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:49:17,757] Trial 389 finished with value: 0.8361205813719377 and parameters: {'n_estimators': 189}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:49:26,353] Trial 390 finished with value: 0.8361580270533684 and parameters: {'n_estimators': 102}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:49:34,817] Trial 391 finished with value: 0.8348927789194376 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:49:50,950] Trial 392 finished with value: 0.8363274380167862 and parameters: {'n_estimators': 209}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:50:54,140] Trial 393 finished with value: 0.8357265533813789 and parameters: {'n_estimators': 888}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:51:07,085] Trial 394 finished with value: 0.8372912890001437 and parameters: {'n_estimators': 162}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:51:16,960] Trial 395 finished with value: 0.837137183290673 and parameters: {'n_estimators': 120}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:51:31,262] Trial 396 finished with value: 0.8361220847937103 and parameters: {'n_estimators': 181}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:51:43,223] Trial 397 finished with value: 0.8350282446877909 and parameters: {'n_estimators': 148}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:51:51,587] Trial 398 finished with value: 0.8348927789194376 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:52:02,488] Trial 399 finished with value: 0.8373351844724007 and parameters: {'n_estimators': 135}. Best is trial 19 with value: 0.8406693457458252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8407\n",
      "\tBest params:\n",
      "\t\tn_estimators: 195\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  330.000000  320.000000  333.000000  329.000000   \n",
      "1                    TN  170.000000  184.000000  170.000000  175.000000   \n",
      "2                    FP   57.000000   53.000000   59.000000   63.000000   \n",
      "3                    FN   38.000000   38.000000   33.000000   28.000000   \n",
      "4              Accuracy    0.840336    0.847059    0.845378    0.847059   \n",
      "5             Precision    0.852713    0.857909    0.849490    0.839286   \n",
      "6           Sensitivity    0.896739    0.893855    0.909836    0.921569   \n",
      "7           Specificity    0.748900    0.776400    0.742400    0.735300   \n",
      "8              F1 score    0.874172    0.875513    0.878628    0.878505   \n",
      "9   F1 score (weighted)    0.838858    0.846129    0.843377    0.844563   \n",
      "10     F1 score (macro)    0.827891    0.838628    0.832833    0.836078   \n",
      "11    Balanced Accuracy    0.822819    0.835113    0.826097    0.828431   \n",
      "12                  MCC    0.657716    0.678432    0.669336    0.678744   \n",
      "13                  NPV    0.817300    0.828800    0.837400    0.862100   \n",
      "14              ROC_AUC    0.822819    0.835113    0.826097    0.828431   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   337.000000  328.000000  332.000000  332.000000  \n",
      "1   178.000000  178.000000  166.000000  174.000000  \n",
      "2    60.000000   56.000000   47.000000   54.000000  \n",
      "3    20.000000   33.000000   50.000000   35.000000  \n",
      "4     0.865546    0.850420    0.836975    0.850420  \n",
      "5     0.848866    0.854167    0.875989    0.860104  \n",
      "6     0.943978    0.908587    0.869110    0.904632  \n",
      "7     0.747900    0.760700    0.779300    0.763200  \n",
      "8     0.893899    0.880537    0.872536    0.881806  \n",
      "9     0.862945    0.848864    0.837223    0.849056  \n",
      "10    0.855206    0.840268    0.823214    0.839072  \n",
      "11    0.845938    0.834636    0.824226    0.833895  \n",
      "12    0.719322    0.683371    0.646477    0.680101  \n",
      "13    0.899000    0.843600    0.768500    0.832500  \n",
      "14    0.845938    0.834636    0.824226    0.833895  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_rf_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_rf_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_rf_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_rf_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_rf_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_rf_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_rf_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_rf_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_rf_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_rf_7)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 15:52:17,134] Trial 400 finished with value: 0.832447006170377 and parameters: {'n_estimators': 163}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:53:02,169] Trial 401 finished with value: 0.8334973355518684 and parameters: {'n_estimators': 629}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:53:19,450] Trial 402 finished with value: 0.8310469345799694 and parameters: {'n_estimators': 228}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:53:29,434] Trial 403 finished with value: 0.8313693984408654 and parameters: {'n_estimators': 121}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:53:45,687] Trial 404 finished with value: 0.8313589870961694 and parameters: {'n_estimators': 212}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:54:00,155] Trial 405 finished with value: 0.8330859695224613 and parameters: {'n_estimators': 186}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:54:11,628] Trial 406 finished with value: 0.8326450303445512 and parameters: {'n_estimators': 143}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:54:31,491] Trial 407 finished with value: 0.8336937539487955 and parameters: {'n_estimators': 268}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:55:21,742] Trial 408 finished with value: 0.8326777987166218 and parameters: {'n_estimators': 712}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:56:16,191] Trial 409 finished with value: 0.8317333038300434 and parameters: {'n_estimators': 769}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:56:25,663] Trial 410 finished with value: 0.8301794330844112 and parameters: {'n_estimators': 117}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:56:36,147] Trial 411 finished with value: 0.8344725124957456 and parameters: {'n_estimators': 130}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:56:44,532] Trial 412 finished with value: 0.8344190719212939 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:57:53,726] Trial 413 finished with value: 0.8330665162599438 and parameters: {'n_estimators': 985}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:58:06,753] Trial 414 finished with value: 0.83211509138183 and parameters: {'n_estimators': 166}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:58:25,663] Trial 415 finished with value: 0.8318564004315607 and parameters: {'n_estimators': 251}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:58:41,343] Trial 416 finished with value: 0.833125453334671 and parameters: {'n_estimators': 204}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:58:50,947] Trial 417 finished with value: 0.8312492377848312 and parameters: {'n_estimators': 118}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:59:02,765] Trial 418 finished with value: 0.8338813319409691 and parameters: {'n_estimators': 147}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:59:17,231] Trial 419 finished with value: 0.833850693056362 and parameters: {'n_estimators': 183}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:59:30,116] Trial 420 finished with value: 0.8321295846497622 and parameters: {'n_estimators': 164}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:59:39,743] Trial 421 finished with value: 0.8312492377848312 and parameters: {'n_estimators': 118}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 15:59:57,158] Trial 422 finished with value: 0.8300878988236604 and parameters: {'n_estimators': 229}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:00:05,454] Trial 423 finished with value: 0.8344190719212939 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:00:16,374] Trial 424 finished with value: 0.8334585238268257 and parameters: {'n_estimators': 137}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:00:32,104] Trial 425 finished with value: 0.833125453334671 and parameters: {'n_estimators': 204}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:00:44,013] Trial 426 finished with value: 0.8321567964381957 and parameters: {'n_estimators': 150}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:00:58,072] Trial 427 finished with value: 0.8334391693036171 and parameters: {'n_estimators': 179}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:01:08,520] Trial 428 finished with value: 0.8342583217609946 and parameters: {'n_estimators': 129}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:01:18,582] Trial 429 finished with value: 0.8319816841649189 and parameters: {'n_estimators': 122}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:01:33,241] Trial 430 finished with value: 0.8324503970197641 and parameters: {'n_estimators': 189}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:01:46,247] Trial 431 finished with value: 0.8319251069752145 and parameters: {'n_estimators': 161}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:01:55,823] Trial 432 finished with value: 0.8312492377848312 and parameters: {'n_estimators': 118}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:02:12,425] Trial 433 finished with value: 0.8337612380733473 and parameters: {'n_estimators': 216}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:02:23,625] Trial 434 finished with value: 0.8331656177571283 and parameters: {'n_estimators': 142}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:02:32,024] Trial 435 finished with value: 0.8344190719212939 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:02:56,852] Trial 436 finished with value: 0.8326723575394513 and parameters: {'n_estimators': 338}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:04:01,354] Trial 437 finished with value: 0.8312746285312427 and parameters: {'n_estimators': 918}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:04:19,755] Trial 438 finished with value: 0.832260688896602 and parameters: {'n_estimators': 241}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:04:33,164] Trial 439 finished with value: 0.8351630507586402 and parameters: {'n_estimators': 172}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:04:43,802] Trial 440 finished with value: 0.8325738036463409 and parameters: {'n_estimators': 134}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:04:53,259] Trial 441 finished with value: 0.8303819879739389 and parameters: {'n_estimators': 116}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:05:05,723] Trial 442 finished with value: 0.8304179247264157 and parameters: {'n_estimators': 156}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:05:21,184] Trial 443 finished with value: 0.8320768422906429 and parameters: {'n_estimators': 199}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:05:29,563] Trial 444 finished with value: 0.8344190719212939 and parameters: {'n_estimators': 100}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:05:40,335] Trial 445 finished with value: 0.834466545368989 and parameters: {'n_estimators': 136}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:05:49,803] Trial 446 finished with value: 0.8296637307838648 and parameters: {'n_estimators': 115}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:06:04,652] Trial 447 finished with value: 0.8335718562496861 and parameters: {'n_estimators': 192}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:06:21,801] Trial 448 finished with value: 0.8309884105836653 and parameters: {'n_estimators': 227}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:06:33,935] Trial 449 finished with value: 0.8319131749137567 and parameters: {'n_estimators': 152}. Best is trial 19 with value: 0.8406693457458252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8407\n",
      "\tBest params:\n",
      "\t\tn_estimators: 195\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  330.000000  320.000000  333.000000  329.000000   \n",
      "1                    TN  170.000000  184.000000  170.000000  175.000000   \n",
      "2                    FP   57.000000   53.000000   59.000000   63.000000   \n",
      "3                    FN   38.000000   38.000000   33.000000   28.000000   \n",
      "4              Accuracy    0.840336    0.847059    0.845378    0.847059   \n",
      "5             Precision    0.852713    0.857909    0.849490    0.839286   \n",
      "6           Sensitivity    0.896739    0.893855    0.909836    0.921569   \n",
      "7           Specificity    0.748900    0.776400    0.742400    0.735300   \n",
      "8              F1 score    0.874172    0.875513    0.878628    0.878505   \n",
      "9   F1 score (weighted)    0.838858    0.846129    0.843377    0.844563   \n",
      "10     F1 score (macro)    0.827891    0.838628    0.832833    0.836078   \n",
      "11    Balanced Accuracy    0.822819    0.835113    0.826097    0.828431   \n",
      "12                  MCC    0.657716    0.678432    0.669336    0.678744   \n",
      "13                  NPV    0.817300    0.828800    0.837400    0.862100   \n",
      "14              ROC_AUC    0.822819    0.835113    0.826097    0.828431   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   337.000000  328.000000  332.000000  332.000000  333.000000  \n",
      "1   178.000000  178.000000  166.000000  174.000000  180.000000  \n",
      "2    60.000000   56.000000   47.000000   54.000000   49.000000  \n",
      "3    20.000000   33.000000   50.000000   35.000000   33.000000  \n",
      "4     0.865546    0.850420    0.836975    0.850420    0.862185  \n",
      "5     0.848866    0.854167    0.875989    0.860104    0.871728  \n",
      "6     0.943978    0.908587    0.869110    0.904632    0.909836  \n",
      "7     0.747900    0.760700    0.779300    0.763200    0.786000  \n",
      "8     0.893899    0.880537    0.872536    0.881806    0.890374  \n",
      "9     0.862945    0.848864    0.837223    0.849056    0.861164  \n",
      "10    0.855206    0.840268    0.823214    0.839072    0.852427  \n",
      "11    0.845938    0.834636    0.824226    0.833895    0.847931  \n",
      "12    0.719322    0.683371    0.646477    0.680101    0.706253  \n",
      "13    0.899000    0.843600    0.768500    0.832500    0.845100  \n",
      "14    0.845938    0.834636    0.824226    0.833895    0.847931  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_rf_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_rf_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_rf_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_rf_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_rf_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_rf_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_rf_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_rf_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_rf_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_rf_8)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:06:49,301] Trial 450 finished with value: 0.840098281925515 and parameters: {'n_estimators': 167}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:07:02,825] Trial 451 finished with value: 0.8398925258728532 and parameters: {'n_estimators': 166}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:07:16,387] Trial 452 finished with value: 0.8388235722393083 and parameters: {'n_estimators': 169}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:07:30,682] Trial 453 finished with value: 0.8386144742607511 and parameters: {'n_estimators': 175}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:07:43,125] Trial 454 finished with value: 0.8365773420390342 and parameters: {'n_estimators': 150}. Best is trial 19 with value: 0.8406693457458252.\n",
      "[I 2023-12-04 16:08:13,078] Trial 455 finished with value: 0.8417761021073946 and parameters: {'n_estimators': 399}. Best is trial 455 with value: 0.8417761021073946.\n",
      "[I 2023-12-04 16:08:43,391] Trial 456 finished with value: 0.8427090878578014 and parameters: {'n_estimators': 402}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:09:05,118] Trial 457 finished with value: 0.8407656940892014 and parameters: {'n_estimators': 282}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:09:42,767] Trial 458 finished with value: 0.8413060242713065 and parameters: {'n_estimators': 509}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:10:14,782] Trial 459 finished with value: 0.840759587809108 and parameters: {'n_estimators': 426}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:10:44,354] Trial 460 finished with value: 0.8412854452565226 and parameters: {'n_estimators': 393}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:11:21,492] Trial 461 finished with value: 0.8416940438832892 and parameters: {'n_estimators': 501}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:11:51,618] Trial 462 finished with value: 0.8427090878578014 and parameters: {'n_estimators': 404}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:12:23,577] Trial 463 finished with value: 0.8417878914852768 and parameters: {'n_estimators': 428}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:12:54,066] Trial 464 finished with value: 0.8416787448209613 and parameters: {'n_estimators': 407}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:13:24,743] Trial 465 finished with value: 0.8416787448209613 and parameters: {'n_estimators': 407}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:13:54,851] Trial 466 finished with value: 0.8416787448209613 and parameters: {'n_estimators': 403}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:14:25,239] Trial 467 finished with value: 0.8421927433023745 and parameters: {'n_estimators': 406}. Best is trial 456 with value: 0.8427090878578014.\n",
      "[I 2023-12-04 16:14:55,247] Trial 468 finished with value: 0.8427578241870324 and parameters: {'n_estimators': 400}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:15:26,767] Trial 469 finished with value: 0.8411820693125511 and parameters: {'n_estimators': 421}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:15:57,934] Trial 470 finished with value: 0.8416392627941045 and parameters: {'n_estimators': 414}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:16:28,550] Trial 471 finished with value: 0.8420861291625308 and parameters: {'n_estimators': 408}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:16:59,311] Trial 472 finished with value: 0.8407836336481683 and parameters: {'n_estimators': 410}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:17:30,520] Trial 473 finished with value: 0.8411706985517441 and parameters: {'n_estimators': 417}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:18:00,728] Trial 474 finished with value: 0.8421927433023745 and parameters: {'n_estimators': 406}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:18:30,979] Trial 475 finished with value: 0.8427090878578014 and parameters: {'n_estimators': 404}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:19:01,445] Trial 476 finished with value: 0.8421927433023745 and parameters: {'n_estimators': 406}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:19:32,417] Trial 477 finished with value: 0.8406872896535031 and parameters: {'n_estimators': 411}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:20:03,240] Trial 478 finished with value: 0.8406872896535031 and parameters: {'n_estimators': 411}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:20:33,390] Trial 479 finished with value: 0.8427090878578014 and parameters: {'n_estimators': 402}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:21:04,048] Trial 480 finished with value: 0.8412073935687884 and parameters: {'n_estimators': 409}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:21:34,153] Trial 481 finished with value: 0.8421950893763881 and parameters: {'n_estimators': 401}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:22:06,946] Trial 482 finished with value: 0.8412952598386712 and parameters: {'n_estimators': 438}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:22:36,360] Trial 483 finished with value: 0.8418008411137183 and parameters: {'n_estimators': 390}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:23:05,869] Trial 484 finished with value: 0.8412854452565226 and parameters: {'n_estimators': 391}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:23:35,052] Trial 485 finished with value: 0.8421056719458871 and parameters: {'n_estimators': 388}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:24:04,119] Trial 486 finished with value: 0.842208066802176 and parameters: {'n_estimators': 386}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:24:33,070] Trial 487 finished with value: 0.8421056719458871 and parameters: {'n_estimators': 387}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:25:00,636] Trial 488 finished with value: 0.8405818174201288 and parameters: {'n_estimators': 363}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:25:34,103] Trial 489 finished with value: 0.8417103586882728 and parameters: {'n_estimators': 447}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:26:07,663] Trial 490 finished with value: 0.8412982564451846 and parameters: {'n_estimators': 449}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:26:40,842] Trial 491 finished with value: 0.8419102721344599 and parameters: {'n_estimators': 446}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:27:14,377] Trial 492 finished with value: 0.8418115174722439 and parameters: {'n_estimators': 452}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:27:42,778] Trial 493 finished with value: 0.8420943685083888 and parameters: {'n_estimators': 377}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:28:11,503] Trial 494 finished with value: 0.8421111514293187 and parameters: {'n_estimators': 381}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:28:39,749] Trial 495 finished with value: 0.8411229528103743 and parameters: {'n_estimators': 375}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:29:08,593] Trial 496 finished with value: 0.8412889747483774 and parameters: {'n_estimators': 383}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:29:38,208] Trial 497 finished with value: 0.8404486843349955 and parameters: {'n_estimators': 394}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:30:11,246] Trial 498 finished with value: 0.8419102721344599 and parameters: {'n_estimators': 446}. Best is trial 468 with value: 0.8427578241870324.\n",
      "[I 2023-12-04 16:30:46,469] Trial 499 finished with value: 0.8412804432301723 and parameters: {'n_estimators': 474}. Best is trial 468 with value: 0.8427578241870324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8428\n",
      "\tBest params:\n",
      "\t\tn_estimators: 400\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  330.000000  320.000000  333.000000  329.000000   \n",
      "1                    TN  170.000000  184.000000  170.000000  175.000000   \n",
      "2                    FP   57.000000   53.000000   59.000000   63.000000   \n",
      "3                    FN   38.000000   38.000000   33.000000   28.000000   \n",
      "4              Accuracy    0.840336    0.847059    0.845378    0.847059   \n",
      "5             Precision    0.852713    0.857909    0.849490    0.839286   \n",
      "6           Sensitivity    0.896739    0.893855    0.909836    0.921569   \n",
      "7           Specificity    0.748900    0.776400    0.742400    0.735300   \n",
      "8              F1 score    0.874172    0.875513    0.878628    0.878505   \n",
      "9   F1 score (weighted)    0.838858    0.846129    0.843377    0.844563   \n",
      "10     F1 score (macro)    0.827891    0.838628    0.832833    0.836078   \n",
      "11    Balanced Accuracy    0.822819    0.835113    0.826097    0.828431   \n",
      "12                  MCC    0.657716    0.678432    0.669336    0.678744   \n",
      "13                  NPV    0.817300    0.828800    0.837400    0.862100   \n",
      "14              ROC_AUC    0.822819    0.835113    0.826097    0.828431   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   337.000000  328.000000  332.000000  332.000000  333.000000  330.000000  \n",
      "1   178.000000  178.000000  166.000000  174.000000  180.000000  161.000000  \n",
      "2    60.000000   56.000000   47.000000   54.000000   49.000000   66.000000  \n",
      "3    20.000000   33.000000   50.000000   35.000000   33.000000   38.000000  \n",
      "4     0.865546    0.850420    0.836975    0.850420    0.862185    0.825210  \n",
      "5     0.848866    0.854167    0.875989    0.860104    0.871728    0.833333  \n",
      "6     0.943978    0.908587    0.869110    0.904632    0.909836    0.896739  \n",
      "7     0.747900    0.760700    0.779300    0.763200    0.786000    0.709300  \n",
      "8     0.893899    0.880537    0.872536    0.881806    0.890374    0.863874  \n",
      "9     0.862945    0.848864    0.837223    0.849056    0.861164    0.822669  \n",
      "10    0.855206    0.840268    0.823214    0.839072    0.852427    0.809871  \n",
      "11    0.845938    0.834636    0.824226    0.833895    0.847931    0.802995  \n",
      "12    0.719322    0.683371    0.646477    0.680101    0.706253    0.623919  \n",
      "13    0.899000    0.843600    0.768500    0.832500    0.845100    0.809000  \n",
      "14    0.845938    0.834636    0.824226    0.833895    0.847931    0.802995  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_rf_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_rf_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_rf_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_rf_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_rf_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_rf_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_rf_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_rf_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_rf_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_rf_9)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })  \n",
    "\n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8428\n",
      "\tBest params:\n",
      "\t\tn_estimators: 400\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Optimization History Plot'}, xlabel='Trial', ylabel='Objective Value'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>167.300000</td>\n",
       "      <td>6.254776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>87.300000</td>\n",
       "      <td>3.233505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>4.954235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>3.438346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.856951</td>\n",
       "      <td>0.022401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.865046</td>\n",
       "      <td>0.025351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.910601</td>\n",
       "      <td>0.019079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.770750</td>\n",
       "      <td>0.036690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.887078</td>\n",
       "      <td>0.018965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.855573</td>\n",
       "      <td>0.022796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.845828</td>\n",
       "      <td>0.023047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.840674</td>\n",
       "      <td>0.023228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.694260</td>\n",
       "      <td>0.045196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.842450</td>\n",
       "      <td>0.028881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.840674</td>\n",
       "      <td>0.023228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       167.300000     6.254776\n",
       "1                    TN        87.300000     3.233505\n",
       "2                    FP        26.100000     4.954235\n",
       "3                    FN        16.400000     3.438346\n",
       "4              Accuracy         0.856951     0.022401\n",
       "5             Precision         0.865046     0.025351\n",
       "6           Sensitivity         0.910601     0.019079\n",
       "7           Specificity         0.770750     0.036690\n",
       "8              F1 score         0.887078     0.018965\n",
       "9   F1 score (weighted)         0.855573     0.022796\n",
       "10     F1 score (macro)         0.845828     0.023047\n",
       "11    Balanced Accuracy         0.840674     0.023228\n",
       "12                  MCC         0.694260     0.045196\n",
       "13                  NPV         0.842450     0.028881\n",
       "14              ROC_AUC         0.840674     0.023228"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>330.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>173.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>56.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>34.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.865546</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.836975</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.862185</td>\n",
       "      <td>0.825210</td>\n",
       "      <td>0.847059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>0.857909</td>\n",
       "      <td>0.849490</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.848866</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.875989</td>\n",
       "      <td>0.860104</td>\n",
       "      <td>0.871728</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.854358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.896739</td>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.909836</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.943978</td>\n",
       "      <td>0.908587</td>\n",
       "      <td>0.869110</td>\n",
       "      <td>0.904632</td>\n",
       "      <td>0.909836</td>\n",
       "      <td>0.896739</td>\n",
       "      <td>0.905488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.748900</td>\n",
       "      <td>0.776400</td>\n",
       "      <td>0.742400</td>\n",
       "      <td>0.735300</td>\n",
       "      <td>0.747900</td>\n",
       "      <td>0.760700</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>0.763200</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.709300</td>\n",
       "      <td>0.754940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.874172</td>\n",
       "      <td>0.875513</td>\n",
       "      <td>0.878628</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.893899</td>\n",
       "      <td>0.880537</td>\n",
       "      <td>0.872536</td>\n",
       "      <td>0.881806</td>\n",
       "      <td>0.890374</td>\n",
       "      <td>0.863874</td>\n",
       "      <td>0.878984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.838858</td>\n",
       "      <td>0.846129</td>\n",
       "      <td>0.843377</td>\n",
       "      <td>0.844563</td>\n",
       "      <td>0.862945</td>\n",
       "      <td>0.848864</td>\n",
       "      <td>0.837223</td>\n",
       "      <td>0.849056</td>\n",
       "      <td>0.861164</td>\n",
       "      <td>0.822669</td>\n",
       "      <td>0.845485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.827891</td>\n",
       "      <td>0.838628</td>\n",
       "      <td>0.832833</td>\n",
       "      <td>0.836078</td>\n",
       "      <td>0.855206</td>\n",
       "      <td>0.840268</td>\n",
       "      <td>0.823214</td>\n",
       "      <td>0.839072</td>\n",
       "      <td>0.852427</td>\n",
       "      <td>0.809871</td>\n",
       "      <td>0.835549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.822819</td>\n",
       "      <td>0.835113</td>\n",
       "      <td>0.826097</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.845938</td>\n",
       "      <td>0.834636</td>\n",
       "      <td>0.824226</td>\n",
       "      <td>0.833895</td>\n",
       "      <td>0.847931</td>\n",
       "      <td>0.802995</td>\n",
       "      <td>0.830208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.657716</td>\n",
       "      <td>0.678432</td>\n",
       "      <td>0.669336</td>\n",
       "      <td>0.678744</td>\n",
       "      <td>0.719322</td>\n",
       "      <td>0.683371</td>\n",
       "      <td>0.646477</td>\n",
       "      <td>0.680101</td>\n",
       "      <td>0.706253</td>\n",
       "      <td>0.623919</td>\n",
       "      <td>0.674367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.817300</td>\n",
       "      <td>0.828800</td>\n",
       "      <td>0.837400</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.843600</td>\n",
       "      <td>0.768500</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.845100</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.834330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.822819</td>\n",
       "      <td>0.835113</td>\n",
       "      <td>0.826097</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.845938</td>\n",
       "      <td>0.834636</td>\n",
       "      <td>0.824226</td>\n",
       "      <td>0.833895</td>\n",
       "      <td>0.847931</td>\n",
       "      <td>0.802995</td>\n",
       "      <td>0.830208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  330.000000  320.000000  333.000000  329.000000   \n",
       "1                    TN  170.000000  184.000000  170.000000  175.000000   \n",
       "2                    FP   57.000000   53.000000   59.000000   63.000000   \n",
       "3                    FN   38.000000   38.000000   33.000000   28.000000   \n",
       "4              Accuracy    0.840336    0.847059    0.845378    0.847059   \n",
       "5             Precision    0.852713    0.857909    0.849490    0.839286   \n",
       "6           Sensitivity    0.896739    0.893855    0.909836    0.921569   \n",
       "7           Specificity    0.748900    0.776400    0.742400    0.735300   \n",
       "8              F1 score    0.874172    0.875513    0.878628    0.878505   \n",
       "9   F1 score (weighted)    0.838858    0.846129    0.843377    0.844563   \n",
       "10     F1 score (macro)    0.827891    0.838628    0.832833    0.836078   \n",
       "11    Balanced Accuracy    0.822819    0.835113    0.826097    0.828431   \n",
       "12                  MCC    0.657716    0.678432    0.669336    0.678744   \n",
       "13                  NPV    0.817300    0.828800    0.837400    0.862100   \n",
       "14              ROC_AUC    0.822819    0.835113    0.826097    0.828431   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   337.000000  328.000000  332.000000  332.000000  333.000000  330.000000   \n",
       "1   178.000000  178.000000  166.000000  174.000000  180.000000  161.000000   \n",
       "2    60.000000   56.000000   47.000000   54.000000   49.000000   66.000000   \n",
       "3    20.000000   33.000000   50.000000   35.000000   33.000000   38.000000   \n",
       "4     0.865546    0.850420    0.836975    0.850420    0.862185    0.825210   \n",
       "5     0.848866    0.854167    0.875989    0.860104    0.871728    0.833333   \n",
       "6     0.943978    0.908587    0.869110    0.904632    0.909836    0.896739   \n",
       "7     0.747900    0.760700    0.779300    0.763200    0.786000    0.709300   \n",
       "8     0.893899    0.880537    0.872536    0.881806    0.890374    0.863874   \n",
       "9     0.862945    0.848864    0.837223    0.849056    0.861164    0.822669   \n",
       "10    0.855206    0.840268    0.823214    0.839072    0.852427    0.809871   \n",
       "11    0.845938    0.834636    0.824226    0.833895    0.847931    0.802995   \n",
       "12    0.719322    0.683371    0.646477    0.680101    0.706253    0.623919   \n",
       "13    0.899000    0.843600    0.768500    0.832500    0.845100    0.809000   \n",
       "14    0.845938    0.834636    0.824226    0.833895    0.847931    0.802995   \n",
       "\n",
       "           ave  \n",
       "0   330.400000  \n",
       "1   173.600000  \n",
       "2    56.400000  \n",
       "3    34.600000  \n",
       "4     0.847059  \n",
       "5     0.854358  \n",
       "6     0.905488  \n",
       "7     0.754940  \n",
       "8     0.878984  \n",
       "9     0.845485  \n",
       "10    0.835549  \n",
       "11    0.830208  \n",
       "12    0.674367  \n",
       "13    0.834330  \n",
       "14    0.830208  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.855131</td>\n",
       "      <td>0.018239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.862623</td>\n",
       "      <td>0.021765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.911342</td>\n",
       "      <td>0.020318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.764104</td>\n",
       "      <td>0.040967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.886051</td>\n",
       "      <td>0.014395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.853555</td>\n",
       "      <td>0.018668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.843254</td>\n",
       "      <td>0.020481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.837727</td>\n",
       "      <td>0.021568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.689867</td>\n",
       "      <td>0.040502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.842104</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.837727</td>\n",
       "      <td>0.021568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.855131     0.018239\n",
       "1             Precision         0.862623     0.021765\n",
       "2           Sensitivity         0.911342     0.020318\n",
       "3           Specificity         0.764104     0.040967\n",
       "4              F1 score         0.886051     0.014395\n",
       "5   F1 score (weighted)         0.853555     0.018668\n",
       "6      F1 score (macro)         0.843254     0.020481\n",
       "7     Balanced Accuracy         0.837727     0.021568\n",
       "8                   MCC         0.689867     0.040502\n",
       "9                   NPV         0.842104     0.032700\n",
       "10              ROC_AUC         0.837727     0.021568"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=16, \n",
    "                                            random_state=1121218, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "   \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_rf)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "    \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_rf))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_rf))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_rf))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_rf))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_rf, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_rf, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_rf))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_rf))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_rf))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model f1_score 0.8493 with a standard deviation of 0.0152\n",
      "RF optimized model f1_score 0.8468 with a standard deviation of 0.0170\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"RF baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf_clf.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the modesls, both the one with optimized hyperparameters and the initial one\n",
    "joblib.dump(rf_clf, \"OUTPUT/rf_clf.joblib\")\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf_clf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       163.100000     7.030884\n",
      "1                    TN        86.700000     2.869379\n",
      "2                    FP        26.700000     5.271517\n",
      "3                    FN        20.600000     4.550946\n",
      "4              Accuracy         0.840793     0.027458\n",
      "5             Precision         0.859203     0.028214\n",
      "6           Sensitivity         0.887707     0.025231\n",
      "7           Specificity         0.765640     0.038518\n",
      "8              F1 score         0.873049     0.023727\n",
      "9   F1 score (weighted)         0.839888     0.027627\n",
      "10     F1 score (macro)         0.829649     0.027813\n",
      "11    Balanced Accuracy         0.826671     0.027792\n",
      "12                  MCC         0.660645     0.054975\n",
      "13                  NPV         0.808880     0.035126\n",
      "14              ROC_AUC         0.826671     0.027792\n",
      "CPU times: user 31.5 s, sys: 112 ms, total: 31.6 s\n",
      "Wall time: 2.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_clf = lgbm.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=16,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_clf.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"logloss\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_clf.predict(X_test) \n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggestegorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=16,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggestegorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "  \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=16,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:35:30,167] A new study created in memory with name: LGBMClassifier\n",
      "[I 2023-12-04 16:35:33,647] Trial 0 finished with value: 0.8180666193942565 and parameters: {'n_estimators': 555, 'learning_rate': 0.01604322072608594, 'max_depth': 5, 'max_bin': 271, 'num_leaves': 326}. Best is trial 0 with value: 0.8180666193942565.\n",
      "[I 2023-12-04 16:35:36,352] Trial 1 finished with value: 0.7680868517924951 and parameters: {'n_estimators': 650, 'learning_rate': 0.00977564476192363, 'max_depth': 3, 'max_bin': 196, 'num_leaves': 147}. Best is trial 0 with value: 0.8180666193942565.\n",
      "[I 2023-12-04 16:35:37,802] Trial 2 finished with value: 0.8320797648692204 and parameters: {'n_estimators': 131, 'learning_rate': 0.15597513238311134, 'max_depth': 8, 'max_bin': 239, 'num_leaves': 570}. Best is trial 2 with value: 0.8320797648692204.\n",
      "[I 2023-12-04 16:35:39,892] Trial 3 finished with value: 0.8396938988659011 and parameters: {'n_estimators': 556, 'learning_rate': 0.1109654657244034, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 401}. Best is trial 3 with value: 0.8396938988659011.\n",
      "[I 2023-12-04 16:35:41,114] Trial 4 finished with value: 0.8210830723008874 and parameters: {'n_estimators': 362, 'learning_rate': 0.13478111311075508, 'max_depth': 4, 'max_bin': 208, 'num_leaves': 538}. Best is trial 3 with value: 0.8396938988659011.\n",
      "[I 2023-12-04 16:35:42,303] Trial 5 finished with value: 0.8308991642642181 and parameters: {'n_estimators': 869, 'learning_rate': 0.19047988049410614, 'max_depth': 6, 'max_bin': 177, 'num_leaves': 355}. Best is trial 3 with value: 0.8396938988659011.\n",
      "[I 2023-12-04 16:35:43,900] Trial 6 finished with value: 0.8272938497857577 and parameters: {'n_estimators': 117, 'learning_rate': 0.15271447395545204, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 326}. Best is trial 3 with value: 0.8396938988659011.\n",
      "[I 2023-12-04 16:35:45,712] Trial 7 finished with value: 0.8330050941388466 and parameters: {'n_estimators': 566, 'learning_rate': 0.08218502159515446, 'max_depth': 6, 'max_bin': 207, 'num_leaves': 676}. Best is trial 3 with value: 0.8396938988659011.\n",
      "[I 2023-12-04 16:35:46,512] Trial 8 finished with value: 0.8193982024150903 and parameters: {'n_estimators': 111, 'learning_rate': 0.14493481455654733, 'max_depth': 4, 'max_bin': 273, 'num_leaves': 271}. Best is trial 3 with value: 0.8396938988659011.\n",
      "[I 2023-12-04 16:35:52,731] Trial 9 finished with value: 0.8299558650728815 and parameters: {'n_estimators': 508, 'learning_rate': 0.014669264207717969, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 346}. Best is trial 3 with value: 0.8396938988659011.\n",
      "[I 2023-12-04 16:35:54,810] Trial 10 finished with value: 0.8341904531214835 and parameters: {'n_estimators': 770, 'learning_rate': 0.07497997641892662, 'max_depth': 9, 'max_bin': 151, 'num_leaves': 44}. Best is trial 3 with value: 0.8396938988659011.\n",
      "[I 2023-12-04 16:35:56,856] Trial 11 finished with value: 0.8402527340548698 and parameters: {'n_estimators': 801, 'learning_rate': 0.08365602812036363, 'max_depth': 9, 'max_bin': 163, 'num_leaves': 43}. Best is trial 11 with value: 0.8402527340548698.\n",
      "[I 2023-12-04 16:35:59,083] Trial 12 finished with value: 0.8404561416162684 and parameters: {'n_estimators': 372, 'learning_rate': 0.10311480870086734, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 481}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:01,671] Trial 13 finished with value: 0.8332231974911333 and parameters: {'n_estimators': 322, 'learning_rate': 0.06021706859179528, 'max_depth': 9, 'max_bin': 154, 'num_leaves': 530}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:03,727] Trial 14 finished with value: 0.8380849604038291 and parameters: {'n_estimators': 391, 'learning_rate': 0.10671148772609163, 'max_depth': 12, 'max_bin': 181, 'num_leaves': 203}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:06,459] Trial 15 finished with value: 0.8318068385737598 and parameters: {'n_estimators': 262, 'learning_rate': 0.04925723911274947, 'max_depth': 9, 'max_bin': 225, 'num_leaves': 444}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:08,680] Trial 16 finished with value: 0.831233677084956 and parameters: {'n_estimators': 708, 'learning_rate': 0.09808607878624474, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 745}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:10,255] Trial 17 finished with value: 0.8381427626967026 and parameters: {'n_estimators': 852, 'learning_rate': 0.12633485897493077, 'max_depth': 10, 'max_bin': 176, 'num_leaves': 57}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:12,946] Trial 18 finished with value: 0.8261802656600384 and parameters: {'n_estimators': 438, 'learning_rate': 0.04458842898704293, 'max_depth': 7, 'max_bin': 299, 'num_leaves': 455}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:15,030] Trial 19 finished with value: 0.8374391165772483 and parameters: {'n_estimators': 238, 'learning_rate': 0.08520188061030227, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 148}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:16,685] Trial 20 finished with value: 0.8365372064879629 and parameters: {'n_estimators': 656, 'learning_rate': 0.11608498671820802, 'max_depth': 8, 'max_bin': 245, 'num_leaves': 627}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:18,725] Trial 21 finished with value: 0.8361571957102004 and parameters: {'n_estimators': 792, 'learning_rate': 0.10444717184381364, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 435}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:20,722] Trial 22 finished with value: 0.8380274224602232 and parameters: {'n_estimators': 456, 'learning_rate': 0.09386136607016524, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 250}. Best is trial 12 with value: 0.8404561416162684.\n",
      "[I 2023-12-04 16:36:22,638] Trial 23 finished with value: 0.8454651418857668 and parameters: {'n_estimators': 614, 'learning_rate': 0.11855097491368037, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 491}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:24,344] Trial 24 finished with value: 0.8408847332165846 and parameters: {'n_estimators': 628, 'learning_rate': 0.12315722488576168, 'max_depth': 10, 'max_bin': 276, 'num_leaves': 490}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:25,908] Trial 25 finished with value: 0.8396624789217411 and parameters: {'n_estimators': 631, 'learning_rate': 0.12742410561638115, 'max_depth': 10, 'max_bin': 282, 'num_leaves': 493}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:27,606] Trial 26 finished with value: 0.8359123410445399 and parameters: {'n_estimators': 729, 'learning_rate': 0.12100131879446109, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 625}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:29,221] Trial 27 finished with value: 0.8402288149169876 and parameters: {'n_estimators': 506, 'learning_rate': 0.13821120970069478, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 486}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:30,903] Trial 28 finished with value: 0.8416467187810139 and parameters: {'n_estimators': 607, 'learning_rate': 0.16312991388704673, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 596}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:32,052] Trial 29 finished with value: 0.8267744250553968 and parameters: {'n_estimators': 594, 'learning_rate': 0.17831609865100911, 'max_depth': 7, 'max_bin': 287, 'num_leaves': 595}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:33,238] Trial 30 finished with value: 0.834840090252771 and parameters: {'n_estimators': 707, 'learning_rate': 0.17123857345836252, 'max_depth': 8, 'max_bin': 271, 'num_leaves': 684}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:35,022] Trial 31 finished with value: 0.8379140884457799 and parameters: {'n_estimators': 517, 'learning_rate': 0.11839712940871627, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 520}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:36,406] Trial 32 finished with value: 0.8299902880620469 and parameters: {'n_estimators': 607, 'learning_rate': 0.16384168694997564, 'max_depth': 10, 'max_bin': 289, 'num_leaves': 398}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:38,074] Trial 33 finished with value: 0.8399745672207187 and parameters: {'n_estimators': 658, 'learning_rate': 0.1456780325839151, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 573}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:39,530] Trial 34 finished with value: 0.8356225956664313 and parameters: {'n_estimators': 426, 'learning_rate': 0.15448719665404426, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 476}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:41,203] Trial 35 finished with value: 0.8411483795929003 and parameters: {'n_estimators': 553, 'learning_rate': 0.13196922641770933, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 553}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:42,776] Trial 36 finished with value: 0.8364668781743948 and parameters: {'n_estimators': 549, 'learning_rate': 0.13395786609217875, 'max_depth': 10, 'max_bin': 292, 'num_leaves': 555}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:44,215] Trial 37 finished with value: 0.833388847324736 and parameters: {'n_estimators': 660, 'learning_rate': 0.16329599781001322, 'max_depth': 9, 'max_bin': 278, 'num_leaves': 612}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:45,287] Trial 38 finished with value: 0.8261302582853389 and parameters: {'n_estimators': 56, 'learning_rate': 0.19402119719328195, 'max_depth': 11, 'max_bin': 265, 'num_leaves': 672}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:46,415] Trial 39 finished with value: 0.8177910261261145 and parameters: {'n_estimators': 575, 'learning_rate': 0.12979746546745927, 'max_depth': 3, 'max_bin': 293, 'num_leaves': 421}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:47,992] Trial 40 finished with value: 0.832899051555629 and parameters: {'n_estimators': 489, 'learning_rate': 0.14570482277735516, 'max_depth': 10, 'max_bin': 284, 'num_leaves': 720}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:49,859] Trial 41 finished with value: 0.8395141595726725 and parameters: {'n_estimators': 330, 'learning_rate': 0.10853831076284445, 'max_depth': 11, 'max_bin': 233, 'num_leaves': 499}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:51,651] Trial 42 finished with value: 0.8393673543198645 and parameters: {'n_estimators': 550, 'learning_rate': 0.1141218156004926, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 368}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:53,450] Trial 43 finished with value: 0.8405951354129002 and parameters: {'n_estimators': 623, 'learning_rate': 0.1361864902206092, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 568}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:55,298] Trial 44 finished with value: 0.838045201921824 and parameters: {'n_estimators': 622, 'learning_rate': 0.14163393100306268, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 576}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:56,905] Trial 45 finished with value: 0.8378204981402083 and parameters: {'n_estimators': 736, 'learning_rate': 0.15157448133229057, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 515}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:36:58,610] Trial 46 finished with value: 0.8376996694138958 and parameters: {'n_estimators': 694, 'learning_rate': 0.1326847483021273, 'max_depth': 9, 'max_bin': 256, 'num_leaves': 644}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:00,448] Trial 47 finished with value: 0.8417462355970221 and parameters: {'n_estimators': 534, 'learning_rate': 0.12512959060008533, 'max_depth': 10, 'max_bin': 277, 'num_leaves': 541}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:02,288] Trial 48 finished with value: 0.8364860510694273 and parameters: {'n_estimators': 535, 'learning_rate': 0.11958591501963989, 'max_depth': 9, 'max_bin': 298, 'num_leaves': 541}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:03,842] Trial 49 finished with value: 0.8351495650359169 and parameters: {'n_estimators': 576, 'learning_rate': 0.1251121842327323, 'max_depth': 8, 'max_bin': 279, 'num_leaves': 461}. Best is trial 23 with value: 0.8454651418857668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 614\n",
      "\t\tlearning_rate: 0.11855097491368037\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 263\n",
      "\t\tnum_leaves: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  320.000000\n",
      "1                    TN  170.000000\n",
      "2                    FP   57.000000\n",
      "3                    FN   48.000000\n",
      "4              Accuracy    0.823529\n",
      "5             Precision    0.848806\n",
      "6           Sensitivity    0.869565\n",
      "7           Specificity    0.748900\n",
      "8              F1 score    0.859060\n",
      "9   F1 score (weighted)    0.822811\n",
      "10     F1 score (macro)    0.811553\n",
      "11    Balanced Accuracy    0.809232\n",
      "12                  MCC    0.623523\n",
      "13                  NPV    0.779800\n",
      "14              ROC_AUC    0.809232\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_lgbm_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_lgbm_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_lgbm_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_lgbm_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_lgbm_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_lgbm_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_lgbm_0)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:37:05,752] Trial 50 finished with value: 0.8326985363371546 and parameters: {'n_estimators': 474, 'learning_rate': 0.11232660653472355, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 307}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:07,466] Trial 51 finished with value: 0.830986403276835 and parameters: {'n_estimators': 678, 'learning_rate': 0.13910172109308172, 'max_depth': 10, 'max_bin': 267, 'num_leaves': 586}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:09,301] Trial 52 finished with value: 0.83188373025879 and parameters: {'n_estimators': 611, 'learning_rate': 0.13575642969109664, 'max_depth': 11, 'max_bin': 240, 'num_leaves': 545}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:11,006] Trial 53 finished with value: 0.8319291535373292 and parameters: {'n_estimators': 637, 'learning_rate': 0.14919381504167228, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 650}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:12,799] Trial 54 finished with value: 0.8316482871349521 and parameters: {'n_estimators': 582, 'learning_rate': 0.12725756363974947, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 599}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:13,830] Trial 55 finished with value: 0.8195738837298577 and parameters: {'n_estimators': 403, 'learning_rate': 0.15575327651814996, 'max_depth': 5, 'max_bin': 286, 'num_leaves': 558}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:15,740] Trial 56 finished with value: 0.8356309835850727 and parameters: {'n_estimators': 758, 'learning_rate': 0.12159891333306157, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 423}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:17,310] Trial 57 finished with value: 0.8329206760814298 and parameters: {'n_estimators': 525, 'learning_rate': 0.14011366724762872, 'max_depth': 9, 'max_bin': 276, 'num_leaves': 511}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:19,411] Trial 58 finished with value: 0.834710461795639 and parameters: {'n_estimators': 898, 'learning_rate': 0.09846379589511198, 'max_depth': 11, 'max_bin': 265, 'num_leaves': 379}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:21,384] Trial 59 finished with value: 0.8284133115710777 and parameters: {'n_estimators': 478, 'learning_rate': 0.11537268892549117, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 532}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:23,357] Trial 60 finished with value: 0.8358999965741637 and parameters: {'n_estimators': 677, 'learning_rate': 0.13197471850979883, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 471}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:25,506] Trial 61 finished with value: 0.8344739463157971 and parameters: {'n_estimators': 257, 'learning_rate': 0.10484344322219402, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 495}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:27,434] Trial 62 finished with value: 0.8287692377658583 and parameters: {'n_estimators': 372, 'learning_rate': 0.10733185915227958, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 442}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:29,523] Trial 63 finished with value: 0.8321186668708964 and parameters: {'n_estimators': 205, 'learning_rate': 0.09285819591129664, 'max_depth': 11, 'max_bin': 235, 'num_leaves': 564}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:31,374] Trial 64 finished with value: 0.8387483905966441 and parameters: {'n_estimators': 438, 'learning_rate': 0.1233560455477456, 'max_depth': 10, 'max_bin': 248, 'num_leaves': 611}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:33,357] Trial 65 finished with value: 0.8284299446046315 and parameters: {'n_estimators': 597, 'learning_rate': 0.11236316922315687, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 413}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:35,183] Trial 66 finished with value: 0.8367267658766016 and parameters: {'n_estimators': 304, 'learning_rate': 0.14462674706921835, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 532}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:36,455] Trial 67 finished with value: 0.8370940451782524 and parameters: {'n_estimators': 504, 'learning_rate': 0.12641633936614463, 'max_depth': 6, 'max_bin': 283, 'num_leaves': 454}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:38,353] Trial 68 finished with value: 0.8318270577786248 and parameters: {'n_estimators': 554, 'learning_rate': 0.1019600501839721, 'max_depth': 10, 'max_bin': 222, 'num_leaves': 484}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:40,253] Trial 69 finished with value: 0.8247660293049085 and parameters: {'n_estimators': 638, 'learning_rate': 0.11855350540411008, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 652}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:42,113] Trial 70 finished with value: 0.8315916185471846 and parameters: {'n_estimators': 456, 'learning_rate': 0.133840083755049, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 690}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:43,874] Trial 71 finished with value: 0.8287088909352424 and parameters: {'n_estimators': 821, 'learning_rate': 0.08970572139829136, 'max_depth': 9, 'max_bin': 189, 'num_leaves': 113}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:45,746] Trial 72 finished with value: 0.835688710968923 and parameters: {'n_estimators': 810, 'learning_rate': 0.07783575377147296, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 210}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:47,452] Trial 73 finished with value: 0.8275464053975613 and parameters: {'n_estimators': 727, 'learning_rate': 0.09901131422911771, 'max_depth': 9, 'max_bin': 161, 'num_leaves': 30}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:49,090] Trial 74 finished with value: 0.8268356975343789 and parameters: {'n_estimators': 536, 'learning_rate': 0.10958313205968116, 'max_depth': 8, 'max_bin': 278, 'num_leaves': 509}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:50,912] Trial 75 finished with value: 0.8321728661653773 and parameters: {'n_estimators': 600, 'learning_rate': 0.13045948530040083, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 341}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:53,042] Trial 76 finished with value: 0.8283664493138113 and parameters: {'n_estimators': 778, 'learning_rate': 0.08448880939396733, 'max_depth': 10, 'max_bin': 272, 'num_leaves': 584}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:54,464] Trial 77 finished with value: 0.8349842412025789 and parameters: {'n_estimators': 671, 'learning_rate': 0.16031881163331319, 'max_depth': 9, 'max_bin': 171, 'num_leaves': 94}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:56,278] Trial 78 finished with value: 0.8276519727080347 and parameters: {'n_estimators': 566, 'learning_rate': 0.1163865885820357, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 555}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:58,073] Trial 79 finished with value: 0.8326100579309387 and parameters: {'n_estimators': 415, 'learning_rate': 0.10487830532037187, 'max_depth': 10, 'max_bin': 288, 'num_leaves': 309}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:37:59,432] Trial 80 finished with value: 0.8340592630684383 and parameters: {'n_estimators': 852, 'learning_rate': 0.14938573409288713, 'max_depth': 7, 'max_bin': 196, 'num_leaves': 603}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:01,250] Trial 81 finished with value: 0.8362420850016992 and parameters: {'n_estimators': 616, 'learning_rate': 0.14139180528431886, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 477}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:03,172] Trial 82 finished with value: 0.8377547304491386 and parameters: {'n_estimators': 512, 'learning_rate': 0.1370527936020169, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 626}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:05,162] Trial 83 finished with value: 0.830773094604203 and parameters: {'n_estimators': 647, 'learning_rate': 0.12277941037381557, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 517}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:06,946] Trial 84 finished with value: 0.8359317467899384 and parameters: {'n_estimators': 500, 'learning_rate': 0.1271749052874281, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 541}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:08,864] Trial 85 finished with value: 0.8255020290175106 and parameters: {'n_estimators': 453, 'learning_rate': 0.11106446590521465, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 494}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:10,560] Trial 86 finished with value: 0.8356080394819536 and parameters: {'n_estimators': 698, 'learning_rate': 0.13854646354590786, 'max_depth': 11, 'max_bin': 243, 'num_leaves': 463}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:12,236] Trial 87 finished with value: 0.8311665705600543 and parameters: {'n_estimators': 542, 'learning_rate': 0.12052401839381288, 'max_depth': 10, 'max_bin': 285, 'num_leaves': 394}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:13,878] Trial 88 finished with value: 0.8336467692072442 and parameters: {'n_estimators': 583, 'learning_rate': 0.1294573668394016, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 432}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:15,375] Trial 89 finished with value: 0.8278667158452118 and parameters: {'n_estimators': 341, 'learning_rate': 0.13552493889527234, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 522}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:16,817] Trial 90 finished with value: 0.8247805172173367 and parameters: {'n_estimators': 564, 'learning_rate': 0.1457506708147411, 'max_depth': 9, 'max_bin': 281, 'num_leaves': 571}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:18,333] Trial 91 finished with value: 0.8339565859293001 and parameters: {'n_estimators': 657, 'learning_rate': 0.14629578369823795, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 566}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:19,952] Trial 92 finished with value: 0.8342352512711615 and parameters: {'n_estimators': 635, 'learning_rate': 0.15520023440393454, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 587}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:21,711] Trial 93 finished with value: 0.8329796037888755 and parameters: {'n_estimators': 616, 'learning_rate': 0.13235937018658708, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 549}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:23,328] Trial 94 finished with value: 0.8316924133355552 and parameters: {'n_estimators': 588, 'learning_rate': 0.14192042312788727, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 501}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:25,016] Trial 95 finished with value: 0.8328989756748506 and parameters: {'n_estimators': 527, 'learning_rate': 0.1497109182062073, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 633}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:26,790] Trial 96 finished with value: 0.8265067684923049 and parameters: {'n_estimators': 747, 'learning_rate': 0.12379491142558834, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 484}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:28,313] Trial 97 finished with value: 0.8346906260435887 and parameters: {'n_estimators': 720, 'learning_rate': 0.1689927411408121, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 613}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:30,037] Trial 98 finished with value: 0.8246735018676995 and parameters: {'n_estimators': 485, 'learning_rate': 0.11707047545468278, 'max_depth': 10, 'max_bin': 230, 'num_leaves': 572}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:31,668] Trial 99 finished with value: 0.8290133698792834 and parameters: {'n_estimators': 375, 'learning_rate': 0.13689450028025532, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 530}. Best is trial 23 with value: 0.8454651418857668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 614\n",
      "\t\tlearning_rate: 0.11855097491368037\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 263\n",
      "\t\tnum_leaves: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  320.000000  314.000000\n",
      "1                    TN  170.000000  188.000000\n",
      "2                    FP   57.000000   49.000000\n",
      "3                    FN   48.000000   44.000000\n",
      "4              Accuracy    0.823529    0.843697\n",
      "5             Precision    0.848806    0.865014\n",
      "6           Sensitivity    0.869565    0.877095\n",
      "7           Specificity    0.748900    0.793200\n",
      "8              F1 score    0.859060    0.871012\n",
      "9   F1 score (weighted)    0.822811    0.843406\n",
      "10     F1 score (macro)    0.811553    0.836359\n",
      "11    Balanced Accuracy    0.809232    0.835172\n",
      "12                  MCC    0.623523    0.672847\n",
      "13                  NPV    0.779800    0.810300\n",
      "14              ROC_AUC    0.809232    0.835172\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_lgbm_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_lgbm_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_lgbm_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_lgbm_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_lgbm_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_lgbm_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_lgbm_1)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:38:33,723] Trial 100 finished with value: 0.8332097358592241 and parameters: {'n_estimators': 685, 'learning_rate': 0.09554604139082454, 'max_depth': 11, 'max_bin': 265, 'num_leaves': 449}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:35,379] Trial 101 finished with value: 0.8302633127582004 and parameters: {'n_estimators': 552, 'learning_rate': 0.11155171727976343, 'max_depth': 10, 'max_bin': 248, 'num_leaves': 471}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:36,944] Trial 102 finished with value: 0.8256203214822785 and parameters: {'n_estimators': 520, 'learning_rate': 0.10268785353646916, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 183}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:38,687] Trial 103 finished with value: 0.8300184703057241 and parameters: {'n_estimators': 599, 'learning_rate': 0.128976472339857, 'max_depth': 11, 'max_bin': 245, 'num_leaves': 590}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:40,443] Trial 104 finished with value: 0.8315988499924923 and parameters: {'n_estimators': 623, 'learning_rate': 0.11543492964601255, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 418}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:42,031] Trial 105 finished with value: 0.8287310073524381 and parameters: {'n_estimators': 474, 'learning_rate': 0.1088356524505701, 'max_depth': 10, 'max_bin': 236, 'num_leaves': 257}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:43,843] Trial 106 finished with value: 0.8327950250213234 and parameters: {'n_estimators': 572, 'learning_rate': 0.12465890967433561, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 407}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:45,732] Trial 107 finished with value: 0.8288838672092664 and parameters: {'n_estimators': 661, 'learning_rate': 0.11912794048839255, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 378}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:47,133] Trial 108 finished with value: 0.8313822417172482 and parameters: {'n_estimators': 498, 'learning_rate': 0.15256780802661263, 'max_depth': 9, 'max_bin': 291, 'num_leaves': 357}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:48,840] Trial 109 finished with value: 0.829895135588745 and parameters: {'n_estimators': 153, 'learning_rate': 0.1340230391467653, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 543}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:50,060] Trial 110 finished with value: 0.8094493299460078 and parameters: {'n_estimators': 557, 'learning_rate': 0.10123890643411636, 'max_depth': 3, 'max_bin': 251, 'num_leaves': 512}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:51,765] Trial 111 finished with value: 0.8357700606681988 and parameters: {'n_estimators': 642, 'learning_rate': 0.1277730863628139, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 487}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:53,407] Trial 112 finished with value: 0.8353905225214934 and parameters: {'n_estimators': 625, 'learning_rate': 0.13095865021019268, 'max_depth': 10, 'max_bin': 277, 'num_leaves': 436}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:54,959] Trial 113 finished with value: 0.8181504614412777 and parameters: {'n_estimators': 606, 'learning_rate': 0.11366689225515407, 'max_depth': 8, 'max_bin': 281, 'num_leaves': 459}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:56,707] Trial 114 finished with value: 0.8287367793091288 and parameters: {'n_estimators': 579, 'learning_rate': 0.12039495028349305, 'max_depth': 10, 'max_bin': 288, 'num_leaves': 501}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:38:58,451] Trial 115 finished with value: 0.82583598374427 and parameters: {'n_estimators': 536, 'learning_rate': 0.106307149771162, 'max_depth': 9, 'max_bin': 298, 'num_leaves': 556}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:00,122] Trial 116 finished with value: 0.8317578500312329 and parameters: {'n_estimators': 712, 'learning_rate': 0.14185511325362965, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 537}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:01,949] Trial 117 finished with value: 0.8303983889592971 and parameters: {'n_estimators': 670, 'learning_rate': 0.12506857290215373, 'max_depth': 11, 'max_bin': 239, 'num_leaves': 524}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:03,601] Trial 118 finished with value: 0.8339021039104354 and parameters: {'n_estimators': 296, 'learning_rate': 0.13969460045868987, 'max_depth': 10, 'max_bin': 262, 'num_leaves': 581}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:05,367] Trial 119 finished with value: 0.8269248192746914 and parameters: {'n_estimators': 590, 'learning_rate': 0.1335053490724807, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 603}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:06,949] Trial 120 finished with value: 0.8288208994797619 and parameters: {'n_estimators': 630, 'learning_rate': 0.12070021119331464, 'max_depth': 9, 'max_bin': 271, 'num_leaves': 671}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:08,654] Trial 121 finished with value: 0.8285212692161412 and parameters: {'n_estimators': 338, 'learning_rate': 0.1080914856470432, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 500}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:10,299] Trial 122 finished with value: 0.8326277758013705 and parameters: {'n_estimators': 275, 'learning_rate': 0.11557588736986545, 'max_depth': 11, 'max_bin': 233, 'num_leaves': 470}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:12,244] Trial 123 finished with value: 0.8265490708308718 and parameters: {'n_estimators': 893, 'learning_rate': 0.09764425349935178, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 486}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:13,226] Trial 124 finished with value: 0.8146191888046067 and parameters: {'n_estimators': 356, 'learning_rate': 0.15858538542916825, 'max_depth': 4, 'max_bin': 241, 'num_leaves': 519}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:15,160] Trial 125 finished with value: 0.8336443279442622 and parameters: {'n_estimators': 232, 'learning_rate': 0.08975051861650477, 'max_depth': 10, 'max_bin': 225, 'num_leaves': 560}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:16,914] Trial 126 finished with value: 0.8263128989349735 and parameters: {'n_estimators': 610, 'learning_rate': 0.12404864579485538, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 508}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:18,392] Trial 127 finished with value: 0.8204354907920072 and parameters: {'n_estimators': 319, 'learning_rate': 0.14514760365535617, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 289}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:19,797] Trial 128 finished with value: 0.8253487749848436 and parameters: {'n_estimators': 563, 'learning_rate': 0.11278393258887015, 'max_depth': 7, 'max_bin': 256, 'num_leaves': 440}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:21,339] Trial 129 finished with value: 0.8294600134546549 and parameters: {'n_estimators': 648, 'learning_rate': 0.1279872092535161, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 531}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:23,189] Trial 130 finished with value: 0.8319444462882103 and parameters: {'n_estimators': 518, 'learning_rate': 0.10712904775985618, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 574}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:24,986] Trial 131 finished with value: 0.8255624564657303 and parameters: {'n_estimators': 548, 'learning_rate': 0.10233461344316269, 'max_depth': 12, 'max_bin': 186, 'num_leaves': 142}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:26,795] Trial 132 finished with value: 0.8299114621693466 and parameters: {'n_estimators': 468, 'learning_rate': 0.11090800129784764, 'max_depth': 12, 'max_bin': 170, 'num_leaves': 454}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:28,412] Trial 133 finished with value: 0.8271271364448489 and parameters: {'n_estimators': 433, 'learning_rate': 0.13737604954153051, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 371}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:30,139] Trial 134 finished with value: 0.8296626813957027 and parameters: {'n_estimators': 591, 'learning_rate': 0.11705986607248574, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 326}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:31,755] Trial 135 finished with value: 0.8306591758794768 and parameters: {'n_estimators': 503, 'learning_rate': 0.12171247998407905, 'max_depth': 11, 'max_bin': 236, 'num_leaves': 75}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:33,387] Trial 136 finished with value: 0.8373915492229462 and parameters: {'n_estimators': 534, 'learning_rate': 0.13122154054988977, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 545}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:35,032] Trial 137 finished with value: 0.831340966758369 and parameters: {'n_estimators': 405, 'learning_rate': 0.12657378555537105, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 491}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:36,719] Trial 138 finished with value: 0.8287061964520855 and parameters: {'n_estimators': 692, 'learning_rate': 0.1150354508974192, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 216}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:38,262] Trial 139 finished with value: 0.8309711258616881 and parameters: {'n_estimators': 564, 'learning_rate': 0.17927923039142693, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 620}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:39,991] Trial 140 finished with value: 0.8294889912614011 and parameters: {'n_estimators': 387, 'learning_rate': 0.1363632558091199, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 641}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:41,655] Trial 141 finished with value: 0.8304883837943459 and parameters: {'n_estimators': 444, 'learning_rate': 0.12352629749150393, 'max_depth': 10, 'max_bin': 249, 'num_leaves': 601}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:43,362] Trial 142 finished with value: 0.82853841743418 and parameters: {'n_estimators': 813, 'learning_rate': 0.11732099555720524, 'max_depth': 9, 'max_bin': 248, 'num_leaves': 615}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:45,050] Trial 143 finished with value: 0.8304867166949628 and parameters: {'n_estimators': 489, 'learning_rate': 0.11050636869264856, 'max_depth': 10, 'max_bin': 244, 'num_leaves': 474}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:46,677] Trial 144 finished with value: 0.8286561290357973 and parameters: {'n_estimators': 613, 'learning_rate': 0.13076108023219418, 'max_depth': 10, 'max_bin': 239, 'num_leaves': 663}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:48,381] Trial 145 finished with value: 0.8297956043399953 and parameters: {'n_estimators': 654, 'learning_rate': 0.10506992067448409, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 562}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:50,216] Trial 146 finished with value: 0.8320529101065534 and parameters: {'n_estimators': 577, 'learning_rate': 0.12051492051532306, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 591}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:51,839] Trial 147 finished with value: 0.8262531925677923 and parameters: {'n_estimators': 416, 'learning_rate': 0.14816719499391248, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 529}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:53,500] Trial 148 finished with value: 0.8303638547269727 and parameters: {'n_estimators': 352, 'learning_rate': 0.14351936070970478, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 714}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:55,561] Trial 149 finished with value: 0.8326667129348699 and parameters: {'n_estimators': 551, 'learning_rate': 0.07183241426136182, 'max_depth': 9, 'max_bin': 237, 'num_leaves': 548}. Best is trial 23 with value: 0.8454651418857668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 614\n",
      "\t\tlearning_rate: 0.11855097491368037\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 263\n",
      "\t\tnum_leaves: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  320.000000  314.000000  323.000000\n",
      "1                    TN  170.000000  188.000000  174.000000\n",
      "2                    FP   57.000000   49.000000   55.000000\n",
      "3                    FN   48.000000   44.000000   43.000000\n",
      "4              Accuracy    0.823529    0.843697    0.835294\n",
      "5             Precision    0.848806    0.865014    0.854497\n",
      "6           Sensitivity    0.869565    0.877095    0.882514\n",
      "7           Specificity    0.748900    0.793200    0.759800\n",
      "8              F1 score    0.859060    0.871012    0.868280\n",
      "9   F1 score (weighted)    0.822811    0.843406    0.834407\n",
      "10     F1 score (macro)    0.811553    0.836359    0.824274\n",
      "11    Balanced Accuracy    0.809232    0.835172    0.821169\n",
      "12                  MCC    0.623523    0.672847    0.649302\n",
      "13                  NPV    0.779800    0.810300    0.801800\n",
      "14              ROC_AUC    0.809232    0.835172    0.821169\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_lgbm_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_lgbm_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_lgbm_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_lgbm_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_lgbm_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_lgbm_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_lgbm_2)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:39:57,709] Trial 150 finished with value: 0.8295897135977197 and parameters: {'n_estimators': 515, 'learning_rate': 0.09961682961448037, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 511}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:39:59,068] Trial 151 finished with value: 0.8288743868610364 and parameters: {'n_estimators': 872, 'learning_rate': 0.1273795199202139, 'max_depth': 10, 'max_bin': 152, 'num_leaves': 52}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:00,628] Trial 152 finished with value: 0.8250853220327858 and parameters: {'n_estimators': 854, 'learning_rate': 0.13277893732989338, 'max_depth': 10, 'max_bin': 233, 'num_leaves': 577}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:02,182] Trial 153 finished with value: 0.8270832034430384 and parameters: {'n_estimators': 840, 'learning_rate': 0.12396398717656906, 'max_depth': 10, 'max_bin': 265, 'num_leaves': 62}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:03,586] Trial 154 finished with value: 0.8205456696943477 and parameters: {'n_estimators': 629, 'learning_rate': 0.13930389061737777, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 84}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:04,666] Trial 155 finished with value: 0.8198759021538045 and parameters: {'n_estimators': 601, 'learning_rate': 0.1522836788435399, 'max_depth': 5, 'max_bin': 158, 'num_leaves': 486}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:06,049] Trial 156 finished with value: 0.8277987808943414 and parameters: {'n_estimators': 315, 'learning_rate': 0.11951615018649292, 'max_depth': 10, 'max_bin': 166, 'num_leaves': 36}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:07,501] Trial 157 finished with value: 0.8235524841515408 and parameters: {'n_estimators': 462, 'learning_rate': 0.11237310814602652, 'max_depth': 9, 'max_bin': 182, 'num_leaves': 54}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:09,058] Trial 158 finished with value: 0.8184210528083626 and parameters: {'n_estimators': 765, 'learning_rate': 0.12948086036567083, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 505}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:10,471] Trial 159 finished with value: 0.8258854271628447 and parameters: {'n_estimators': 381, 'learning_rate': 0.12452442117692358, 'max_depth': 8, 'max_bin': 241, 'num_leaves': 120}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:12,096] Trial 160 finished with value: 0.8254678920575331 and parameters: {'n_estimators': 533, 'learning_rate': 0.1367775092392448, 'max_depth': 12, 'max_bin': 174, 'num_leaves': 600}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:13,775] Trial 161 finished with value: 0.8211467137882178 and parameters: {'n_estimators': 834, 'learning_rate': 0.10302303494296208, 'max_depth': 12, 'max_bin': 156, 'num_leaves': 103}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:15,485] Trial 162 finished with value: 0.8187207226915698 and parameters: {'n_estimators': 394, 'learning_rate': 0.10703747758307137, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 242}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:17,424] Trial 163 finished with value: 0.8190973127731146 and parameters: {'n_estimators': 781, 'learning_rate': 0.09409383495662144, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 133}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:19,025] Trial 164 finished with value: 0.823606830919517 and parameters: {'n_estimators': 365, 'learning_rate': 0.11460642661433285, 'max_depth': 11, 'max_bin': 191, 'num_leaves': 72}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:20,726] Trial 165 finished with value: 0.8227322939613929 and parameters: {'n_estimators': 798, 'learning_rate': 0.10916096581928611, 'max_depth': 10, 'max_bin': 180, 'num_leaves': 342}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:22,634] Trial 166 finished with value: 0.8215468560301794 and parameters: {'n_estimators': 342, 'learning_rate': 0.1210115966599419, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 459}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:24,339] Trial 167 finished with value: 0.8205698290398453 and parameters: {'n_estimators': 880, 'learning_rate': 0.11730819154295657, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 290}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:26,039] Trial 168 finished with value: 0.8306992788033034 and parameters: {'n_estimators': 578, 'learning_rate': 0.1331186783966442, 'max_depth': 12, 'max_bin': 174, 'num_leaves': 168}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:28,058] Trial 169 finished with value: 0.8230999218549604 and parameters: {'n_estimators': 418, 'learning_rate': 0.09776099709985939, 'max_depth': 10, 'max_bin': 287, 'num_leaves': 473}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:29,784] Trial 170 finished with value: 0.8297601985413902 and parameters: {'n_estimators': 669, 'learning_rate': 0.12767797163974737, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 198}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:31,526] Trial 171 finished with value: 0.8184881419119637 and parameters: {'n_estimators': 640, 'learning_rate': 0.14254076706354593, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 567}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:33,269] Trial 172 finished with value: 0.8264186489404594 and parameters: {'n_estimators': 618, 'learning_rate': 0.14002744610138104, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 551}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:35,091] Trial 173 finished with value: 0.8215600624247841 and parameters: {'n_estimators': 599, 'learning_rate': 0.1349583229725142, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 628}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:36,785] Trial 174 finished with value: 0.8218357052969086 and parameters: {'n_estimators': 629, 'learning_rate': 0.1495806622547897, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 589}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:38,525] Trial 175 finished with value: 0.8218372436177924 and parameters: {'n_estimators': 439, 'learning_rate': 0.15785241260130875, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 532}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:40,346] Trial 176 finished with value: 0.8175175410172326 and parameters: {'n_estimators': 650, 'learning_rate': 0.11322785054948944, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 576}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:42,023] Trial 177 finished with value: 0.8272687190952489 and parameters: {'n_estimators': 550, 'learning_rate': 0.13085168405517567, 'max_depth': 10, 'max_bin': 273, 'num_leaves': 424}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:43,823] Trial 178 finished with value: 0.8266455558559823 and parameters: {'n_estimators': 593, 'learning_rate': 0.14587190800451807, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 500}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:45,567] Trial 179 finished with value: 0.8213718816559835 and parameters: {'n_estimators': 565, 'learning_rate': 0.12435165144704811, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 399}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:47,262] Trial 180 finished with value: 0.8209627136779754 and parameters: {'n_estimators': 505, 'learning_rate': 0.16189901643317867, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 521}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:49,431] Trial 181 finished with value: 0.8192095275117556 and parameters: {'n_estimators': 430, 'learning_rate': 0.09402244762688741, 'max_depth': 12, 'max_bin': 186, 'num_leaves': 447}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:51,732] Trial 182 finished with value: 0.8256784353754393 and parameters: {'n_estimators': 491, 'learning_rate': 0.08603988905547025, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 612}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:53,637] Trial 183 finished with value: 0.812425586778542 and parameters: {'n_estimators': 458, 'learning_rate': 0.10071942002769961, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 41}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:55,482] Trial 184 finished with value: 0.8247377376434895 and parameters: {'n_estimators': 480, 'learning_rate': 0.10497894457083974, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 266}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:57,238] Trial 185 finished with value: 0.8262111991124328 and parameters: {'n_estimators': 613, 'learning_rate': 0.15463529784549912, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 165}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:40:59,106] Trial 186 finished with value: 0.8246574031294729 and parameters: {'n_estimators': 683, 'learning_rate': 0.12070075546787272, 'max_depth': 10, 'max_bin': 247, 'num_leaves': 560}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:01,288] Trial 187 finished with value: 0.8263315422982356 and parameters: {'n_estimators': 407, 'learning_rate': 0.09099132901708804, 'max_depth': 9, 'max_bin': 280, 'num_leaves': 489}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:03,011] Trial 188 finished with value: 0.823119408909383 and parameters: {'n_estimators': 530, 'learning_rate': 0.13946636292943237, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 241}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:04,849] Trial 189 finished with value: 0.8267713643303163 and parameters: {'n_estimators': 583, 'learning_rate': 0.12787159560735112, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 358}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:06,717] Trial 190 finished with value: 0.8263049688653789 and parameters: {'n_estimators': 332, 'learning_rate': 0.10962923722728166, 'max_depth': 10, 'max_bin': 285, 'num_leaves': 540}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:08,655] Trial 191 finished with value: 0.8226990105963473 and parameters: {'n_estimators': 520, 'learning_rate': 0.11454566127043095, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 525}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:10,141] Trial 192 finished with value: 0.8198115313312486 and parameters: {'n_estimators': 477, 'learning_rate': 0.11739013012913424, 'max_depth': 6, 'max_bin': 262, 'num_leaves': 512}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:11,877] Trial 193 finished with value: 0.8259370324171998 and parameters: {'n_estimators': 548, 'learning_rate': 0.16609720303795877, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 473}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:13,695] Trial 194 finished with value: 0.8261059821345214 and parameters: {'n_estimators': 289, 'learning_rate': 0.13530227012294588, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 497}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:15,618] Trial 195 finished with value: 0.822915109618639 and parameters: {'n_estimators': 506, 'learning_rate': 0.12365036620837812, 'max_depth': 10, 'max_bin': 274, 'num_leaves': 586}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:17,299] Trial 196 finished with value: 0.8276034502632685 and parameters: {'n_estimators': 638, 'learning_rate': 0.11964123503794967, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 317}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:19,660] Trial 197 finished with value: 0.8280387121450747 and parameters: {'n_estimators': 568, 'learning_rate': 0.07995547569072918, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 289}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:21,629] Trial 198 finished with value: 0.8250733392739662 and parameters: {'n_estimators': 371, 'learning_rate': 0.10616720572922494, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 548}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:23,359] Trial 199 finished with value: 0.8261829458798002 and parameters: {'n_estimators': 391, 'learning_rate': 0.1434957435876398, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 574}. Best is trial 23 with value: 0.8454651418857668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 614\n",
      "\t\tlearning_rate: 0.11855097491368037\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 263\n",
      "\t\tnum_leaves: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  320.000000  314.000000  323.000000  324.000000\n",
      "1                    TN  170.000000  188.000000  174.000000  179.000000\n",
      "2                    FP   57.000000   49.000000   55.000000   59.000000\n",
      "3                    FN   48.000000   44.000000   43.000000   33.000000\n",
      "4              Accuracy    0.823529    0.843697    0.835294    0.845378\n",
      "5             Precision    0.848806    0.865014    0.854497    0.845953\n",
      "6           Sensitivity    0.869565    0.877095    0.882514    0.907563\n",
      "7           Specificity    0.748900    0.793200    0.759800    0.752100\n",
      "8              F1 score    0.859060    0.871012    0.868280    0.875676\n",
      "9   F1 score (weighted)    0.822811    0.843406    0.834407    0.843628\n",
      "10     F1 score (macro)    0.811553    0.836359    0.824274    0.835616\n",
      "11    Balanced Accuracy    0.809232    0.835172    0.821169    0.829832\n",
      "12                  MCC    0.623523    0.672847    0.649302    0.674804\n",
      "13                  NPV    0.779800    0.810300    0.801800    0.844300\n",
      "14              ROC_AUC    0.809232    0.835172    0.821169    0.829832\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_lgbm_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_lgbm_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_lgbm_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_lgbm_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_lgbm_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_lgbm_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_lgbm_3)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:41:25,106] Trial 200 finished with value: 0.8064591212346006 and parameters: {'n_estimators': 538, 'learning_rate': 0.17341484951954542, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 515}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:26,672] Trial 201 finished with value: 0.8040715997698143 and parameters: {'n_estimators': 738, 'learning_rate': 0.14753118347823505, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 481}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:30,353] Trial 202 finished with value: 0.80883616141404 and parameters: {'n_estimators': 661, 'learning_rate': 0.028217984613787966, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 540}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:31,845] Trial 203 finished with value: 0.8022076757628703 and parameters: {'n_estimators': 709, 'learning_rate': 0.15039404501447604, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 519}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:33,692] Trial 204 finished with value: 0.8075625489211214 and parameters: {'n_estimators': 754, 'learning_rate': 0.08647211354238359, 'max_depth': 10, 'max_bin': 237, 'num_leaves': 499}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:35,281] Trial 205 finished with value: 0.8009364296123135 and parameters: {'n_estimators': 622, 'learning_rate': 0.11025753308572336, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 560}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:36,631] Trial 206 finished with value: 0.8120358499848621 and parameters: {'n_estimators': 825, 'learning_rate': 0.1547500985383768, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 466}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:38,153] Trial 207 finished with value: 0.8091448983195948 and parameters: {'n_estimators': 608, 'learning_rate': 0.12983946852570596, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 608}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:39,837] Trial 208 finished with value: 0.8066657358306669 and parameters: {'n_estimators': 450, 'learning_rate': 0.09776255831318253, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 506}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:41,403] Trial 209 finished with value: 0.8067604075410703 and parameters: {'n_estimators': 636, 'learning_rate': 0.11634784896497516, 'max_depth': 10, 'max_bin': 269, 'num_leaves': 532}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:42,854] Trial 210 finished with value: 0.8059695032141775 and parameters: {'n_estimators': 807, 'learning_rate': 0.1336626488230786, 'max_depth': 11, 'max_bin': 174, 'num_leaves': 387}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:44,290] Trial 211 finished with value: 0.8043618112853794 and parameters: {'n_estimators': 520, 'learning_rate': 0.13697177350322626, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 226}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:46,024] Trial 212 finished with value: 0.814313054239646 and parameters: {'n_estimators': 850, 'learning_rate': 0.14338208849637743, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 623}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:47,633] Trial 213 finished with value: 0.8028326073082315 and parameters: {'n_estimators': 495, 'learning_rate': 0.137636540915606, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 639}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:49,350] Trial 214 finished with value: 0.8029742303977869 and parameters: {'n_estimators': 515, 'learning_rate': 0.12686681330362024, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 598}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:50,946] Trial 215 finished with value: 0.8100704246978176 and parameters: {'n_estimators': 545, 'learning_rate': 0.14037415850926122, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 582}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:52,549] Trial 216 finished with value: 0.8090525697838056 and parameters: {'n_estimators': 786, 'learning_rate': 0.12285676258453898, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 486}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:53,887] Trial 217 finished with value: 0.8089977982837407 and parameters: {'n_estimators': 597, 'learning_rate': 0.13292029418710635, 'max_depth': 8, 'max_bin': 240, 'num_leaves': 565}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:55,396] Trial 218 finished with value: 0.8116173987902217 and parameters: {'n_estimators': 528, 'learning_rate': 0.10100237100440888, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 61}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:56,832] Trial 219 finished with value: 0.8016545853451695 and parameters: {'n_estimators': 468, 'learning_rate': 0.15831552392848, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 629}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:58,160] Trial 220 finished with value: 0.8087382060363962 and parameters: {'n_estimators': 560, 'learning_rate': 0.14795091629327234, 'max_depth': 10, 'max_bin': 281, 'num_leaves': 332}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:41:59,344] Trial 221 finished with value: 0.8043375096679872 and parameters: {'n_estimators': 694, 'learning_rate': 0.1311211516360804, 'max_depth': 7, 'max_bin': 257, 'num_leaves': 614}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:00,856] Trial 222 finished with value: 0.8119785646064723 and parameters: {'n_estimators': 724, 'learning_rate': 0.12574287503882672, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 654}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:02,455] Trial 223 finished with value: 0.8094357170144052 and parameters: {'n_estimators': 635, 'learning_rate': 0.11321391305063444, 'max_depth': 9, 'max_bin': 251, 'num_leaves': 649}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:04,242] Trial 224 finished with value: 0.8061461348449104 and parameters: {'n_estimators': 680, 'learning_rate': 0.12019151266382662, 'max_depth': 10, 'max_bin': 252, 'num_leaves': 591}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:05,662] Trial 225 finished with value: 0.805800572534358 and parameters: {'n_estimators': 659, 'learning_rate': 0.1366949375832823, 'max_depth': 8, 'max_bin': 259, 'num_leaves': 637}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:06,916] Trial 226 finished with value: 0.8042945350565587 and parameters: {'n_estimators': 650, 'learning_rate': 0.14142592831699524, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 30}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:09,074] Trial 227 finished with value: 0.8094960838921905 and parameters: {'n_estimators': 674, 'learning_rate': 0.0713697277670148, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 495}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:10,784] Trial 228 finished with value: 0.8069141439887157 and parameters: {'n_estimators': 498, 'learning_rate': 0.12867745957152488, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 548}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:12,399] Trial 229 finished with value: 0.8097407018888154 and parameters: {'n_estimators': 613, 'learning_rate': 0.10585600365511566, 'max_depth': 9, 'max_bin': 250, 'num_leaves': 510}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:13,810] Trial 230 finished with value: 0.7995456710055471 and parameters: {'n_estimators': 704, 'learning_rate': 0.13366629126538046, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 463}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:15,516] Trial 231 finished with value: 0.8033722552433126 and parameters: {'n_estimators': 199, 'learning_rate': 0.08519033678920926, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 91}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:17,200] Trial 232 finished with value: 0.8071561912650411 and parameters: {'n_estimators': 439, 'learning_rate': 0.08216725243409119, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 166}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:18,736] Trial 233 finished with value: 0.8106149974238883 and parameters: {'n_estimators': 142, 'learning_rate': 0.09197143459065435, 'max_depth': 10, 'max_bin': 167, 'num_leaves': 115}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:20,263] Trial 234 finished with value: 0.7978403267740971 and parameters: {'n_estimators': 108, 'learning_rate': 0.08999307050560215, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 223}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:21,783] Trial 235 finished with value: 0.812467159127513 and parameters: {'n_estimators': 229, 'learning_rate': 0.11727416767820771, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 151}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:23,218] Trial 236 finished with value: 0.8047257114635391 and parameters: {'n_estimators': 579, 'learning_rate': 0.12531926180637723, 'max_depth': 10, 'max_bin': 225, 'num_leaves': 524}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:24,475] Trial 237 finished with value: 0.8056663069243367 and parameters: {'n_estimators': 261, 'learning_rate': 0.1518401896091779, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 47}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:25,764] Trial 238 finished with value: 0.8019664497748668 and parameters: {'n_estimators': 513, 'learning_rate': 0.1617888931220271, 'max_depth': 12, 'max_bin': 201, 'num_leaves': 250}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:27,247] Trial 239 finished with value: 0.8083022899013402 and parameters: {'n_estimators': 619, 'learning_rate': 0.10923198468843193, 'max_depth': 10, 'max_bin': 255, 'num_leaves': 196}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:28,545] Trial 240 finished with value: 0.8052970073621684 and parameters: {'n_estimators': 193, 'learning_rate': 0.1443207161483157, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 200}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:30,065] Trial 241 finished with value: 0.8083078379100636 and parameters: {'n_estimators': 538, 'learning_rate': 0.13093007002698165, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 686}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:31,469] Trial 242 finished with value: 0.8032065843632091 and parameters: {'n_estimators': 527, 'learning_rate': 0.1394822218459942, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 548}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:32,894] Trial 243 finished with value: 0.8053631114133015 and parameters: {'n_estimators': 485, 'learning_rate': 0.09531886324446233, 'max_depth': 10, 'max_bin': 223, 'num_leaves': 182}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:34,306] Trial 244 finished with value: 0.8086495699251562 and parameters: {'n_estimators': 510, 'learning_rate': 0.13131634923457247, 'max_depth': 10, 'max_bin': 286, 'num_leaves': 568}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:35,803] Trial 245 finished with value: 0.8085754689289422 and parameters: {'n_estimators': 629, 'learning_rate': 0.12199117355935465, 'max_depth': 9, 'max_bin': 273, 'num_leaves': 542}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:37,665] Trial 246 finished with value: 0.8085186807006698 and parameters: {'n_estimators': 564, 'learning_rate': 0.07855018793409045, 'max_depth': 10, 'max_bin': 160, 'num_leaves': 484}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:39,255] Trial 247 finished with value: 0.8078588055473489 and parameters: {'n_estimators': 540, 'learning_rate': 0.12805242622178342, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 603}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:40,845] Trial 248 finished with value: 0.8114697136145477 and parameters: {'n_estimators': 423, 'learning_rate': 0.13497674537879334, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 578}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:42,176] Trial 249 finished with value: 0.8033592219418052 and parameters: {'n_estimators': 589, 'learning_rate': 0.16695213855756952, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 532}. Best is trial 23 with value: 0.8454651418857668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 614\n",
      "\t\tlearning_rate: 0.11855097491368037\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 263\n",
      "\t\tnum_leaves: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  320.000000  314.000000  323.000000  324.000000   \n",
      "1                    TN  170.000000  188.000000  174.000000  179.000000   \n",
      "2                    FP   57.000000   49.000000   55.000000   59.000000   \n",
      "3                    FN   48.000000   44.000000   43.000000   33.000000   \n",
      "4              Accuracy    0.823529    0.843697    0.835294    0.845378   \n",
      "5             Precision    0.848806    0.865014    0.854497    0.845953   \n",
      "6           Sensitivity    0.869565    0.877095    0.882514    0.907563   \n",
      "7           Specificity    0.748900    0.793200    0.759800    0.752100   \n",
      "8              F1 score    0.859060    0.871012    0.868280    0.875676   \n",
      "9   F1 score (weighted)    0.822811    0.843406    0.834407    0.843628   \n",
      "10     F1 score (macro)    0.811553    0.836359    0.824274    0.835616   \n",
      "11    Balanced Accuracy    0.809232    0.835172    0.821169    0.829832   \n",
      "12                  MCC    0.623523    0.672847    0.649302    0.674804   \n",
      "13                  NPV    0.779800    0.810300    0.801800    0.844300   \n",
      "14              ROC_AUC    0.809232    0.835172    0.821169    0.829832   \n",
      "\n",
      "          Set4  \n",
      "0   328.000000  \n",
      "1   171.000000  \n",
      "2    67.000000  \n",
      "3    29.000000  \n",
      "4     0.838655  \n",
      "5     0.830380  \n",
      "6     0.918768  \n",
      "7     0.718500  \n",
      "8     0.872340  \n",
      "9     0.835733  \n",
      "10    0.826581  \n",
      "11    0.818627  \n",
      "12    0.660879  \n",
      "13    0.855000  \n",
      "14    0.818627  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_lgbm_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_lgbm_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_lgbm_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_lgbm_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_lgbm_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_lgbm_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_lgbm_4)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:42:55,443] Trial 250 finished with value: 0.7977671394521437 and parameters: {'n_estimators': 739, 'learning_rate': 0.002685262019634052, 'max_depth': 11, 'max_bin': 177, 'num_leaves': 275}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:57,183] Trial 251 finished with value: 0.8168893837110405 and parameters: {'n_estimators': 651, 'learning_rate': 0.12383381982440606, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 504}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:42:59,050] Trial 252 finished with value: 0.8287182914806219 and parameters: {'n_estimators': 399, 'learning_rate': 0.10324229622693507, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 129}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:00,920] Trial 253 finished with value: 0.8263761520561209 and parameters: {'n_estimators': 552, 'learning_rate': 0.11367522093570051, 'max_depth': 10, 'max_bin': 248, 'num_leaves': 558}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:02,760] Trial 254 finished with value: 0.8208754728289464 and parameters: {'n_estimators': 357, 'learning_rate': 0.13760203453528158, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 525}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:04,454] Trial 255 finished with value: 0.8189627649795167 and parameters: {'n_estimators': 768, 'learning_rate': 0.08795170259664352, 'max_depth': 9, 'max_bin': 264, 'num_leaves': 76}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:06,262] Trial 256 finished with value: 0.8186212068468068 and parameters: {'n_estimators': 607, 'learning_rate': 0.11801291413491581, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 618}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:07,653] Trial 257 finished with value: 0.8138371926346208 and parameters: {'n_estimators': 466, 'learning_rate': 0.14515022952390536, 'max_depth': 8, 'max_bin': 244, 'num_leaves': 511}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:09,533] Trial 258 finished with value: 0.8229721701908014 and parameters: {'n_estimators': 526, 'learning_rate': 0.12698636805666244, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 479}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:11,176] Trial 259 finished with value: 0.8222807803410452 and parameters: {'n_estimators': 506, 'learning_rate': 0.14097119355034224, 'max_depth': 12, 'max_bin': 194, 'num_leaves': 595}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:12,530] Trial 260 finished with value: 0.8131839793877074 and parameters: {'n_estimators': 869, 'learning_rate': 0.18722139325706472, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 495}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:14,364] Trial 261 finished with value: 0.8226805104991406 and parameters: {'n_estimators': 322, 'learning_rate': 0.08352795314241088, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 366}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:15,997] Trial 262 finished with value: 0.8251827692260119 and parameters: {'n_estimators': 487, 'learning_rate': 0.13236426071548477, 'max_depth': 10, 'max_bin': 237, 'num_leaves': 555}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:17,716] Trial 263 finished with value: 0.8318764949155755 and parameters: {'n_estimators': 168, 'learning_rate': 0.11006476691429097, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 572}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:19,393] Trial 264 finished with value: 0.8301538978338989 and parameters: {'n_estimators': 637, 'learning_rate': 0.12219679251754971, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 538}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:20,808] Trial 265 finished with value: 0.824136700997807 and parameters: {'n_estimators': 240, 'learning_rate': 0.16324938552191404, 'max_depth': 10, 'max_bin': 288, 'num_leaves': 516}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:22,485] Trial 266 finished with value: 0.818814662258385 and parameters: {'n_estimators': 663, 'learning_rate': 0.1353021308075104, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 585}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:23,961] Trial 267 finished with value: 0.8225677206954828 and parameters: {'n_estimators': 580, 'learning_rate': 0.15546163098419447, 'max_depth': 10, 'max_bin': 277, 'num_leaves': 636}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:25,588] Trial 268 finished with value: 0.8233852064036056 and parameters: {'n_estimators': 376, 'learning_rate': 0.12915904442199655, 'max_depth': 11, 'max_bin': 163, 'num_leaves': 672}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:27,338] Trial 269 finished with value: 0.8220881098187384 and parameters: {'n_estimators': 452, 'learning_rate': 0.09828496341065296, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 492}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:28,864] Trial 270 finished with value: 0.8264882820210623 and parameters: {'n_estimators': 897, 'learning_rate': 0.10690727379837246, 'max_depth': 9, 'max_bin': 155, 'num_leaves': 104}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:30,423] Trial 271 finished with value: 0.8264559595298862 and parameters: {'n_estimators': 548, 'learning_rate': 0.14781228613770064, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 437}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:32,246] Trial 272 finished with value: 0.8218516040047454 and parameters: {'n_estimators': 528, 'learning_rate': 0.07596217408775369, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 64}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:34,020] Trial 273 finished with value: 0.8252964740936175 and parameters: {'n_estimators': 593, 'learning_rate': 0.1185542779964102, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 151}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:36,186] Trial 274 finished with value: 0.8280367269032108 and parameters: {'n_estimators': 628, 'learning_rate': 0.12545330595399448, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 457}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:38,235] Trial 275 finished with value: 0.8221620092698638 and parameters: {'n_estimators': 689, 'learning_rate': 0.1131564143003045, 'max_depth': 10, 'max_bin': 171, 'num_leaves': 478}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:39,894] Trial 276 finished with value: 0.8138917095817624 and parameters: {'n_estimators': 88, 'learning_rate': 0.13772617408136092, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 653}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:41,958] Trial 277 finished with value: 0.8230224870694904 and parameters: {'n_estimators': 294, 'learning_rate': 0.10328339922018227, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 414}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:43,428] Trial 278 finished with value: 0.8095512333775974 and parameters: {'n_estimators': 566, 'learning_rate': 0.19999465571742825, 'max_depth': 12, 'max_bin': 184, 'num_leaves': 607}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:44,948] Trial 279 finished with value: 0.8212314175169046 and parameters: {'n_estimators': 497, 'learning_rate': 0.17065843899495153, 'max_depth': 10, 'max_bin': 282, 'num_leaves': 554}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:46,810] Trial 280 finished with value: 0.8231179185235427 and parameters: {'n_estimators': 800, 'learning_rate': 0.09353295631960243, 'max_depth': 9, 'max_bin': 235, 'num_leaves': 530}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:48,592] Trial 281 finished with value: 0.8235455943482475 and parameters: {'n_estimators': 612, 'learning_rate': 0.14250147922110823, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 570}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:50,173] Trial 282 finished with value: 0.8217060360346398 and parameters: {'n_estimators': 409, 'learning_rate': 0.12968134091853345, 'max_depth': 10, 'max_bin': 279, 'num_leaves': 308}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:51,839] Trial 283 finished with value: 0.8235119079674931 and parameters: {'n_estimators': 545, 'learning_rate': 0.15139773935953285, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 505}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:53,780] Trial 284 finished with value: 0.8231931497772902 and parameters: {'n_estimators': 715, 'learning_rate': 0.12027397673388163, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 541}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:55,717] Trial 285 finished with value: 0.820860685505185 and parameters: {'n_estimators': 646, 'learning_rate': 0.1160788911549557, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 520}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:57,274] Trial 286 finished with value: 0.8251913961986064 and parameters: {'n_estimators': 515, 'learning_rate': 0.13363844615919648, 'max_depth': 10, 'max_bin': 247, 'num_leaves': 46}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:43:59,304] Trial 287 finished with value: 0.8184712559984186 and parameters: {'n_estimators': 475, 'learning_rate': 0.08857058211349464, 'max_depth': 10, 'max_bin': 285, 'num_leaves': 230}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:01,188] Trial 288 finished with value: 0.8225986603823252 and parameters: {'n_estimators': 428, 'learning_rate': 0.12433538209113998, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 181}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:03,463] Trial 289 finished with value: 0.8218612057756992 and parameters: {'n_estimators': 600, 'learning_rate': 0.08170501449166083, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 591}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:05,462] Trial 290 finished with value: 0.8232074526576983 and parameters: {'n_estimators': 671, 'learning_rate': 0.11195600459019277, 'max_depth': 11, 'max_bin': 243, 'num_leaves': 620}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:07,123] Trial 291 finished with value: 0.8184179747976913 and parameters: {'n_estimators': 823, 'learning_rate': 0.15997845299437388, 'max_depth': 10, 'max_bin': 256, 'num_leaves': 463}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:08,960] Trial 292 finished with value: 0.8187440722178941 and parameters: {'n_estimators': 628, 'learning_rate': 0.13938478247562663, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 492}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:12,137] Trial 293 finished with value: 0.8266702309878532 and parameters: {'n_estimators': 339, 'learning_rate': 0.05116806763985962, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 555}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:14,332] Trial 294 finished with value: 0.821661244669768 and parameters: {'n_estimators': 534, 'learning_rate': 0.10050974665676657, 'max_depth': 10, 'max_bin': 287, 'num_leaves': 473}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:16,261] Trial 295 finished with value: 0.8200465275013518 and parameters: {'n_estimators': 846, 'learning_rate': 0.13170786243618923, 'max_depth': 12, 'max_bin': 179, 'num_leaves': 507}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:17,930] Trial 296 finished with value: 0.8203701792008218 and parameters: {'n_estimators': 559, 'learning_rate': 0.10811156223538215, 'max_depth': 8, 'max_bin': 265, 'num_leaves': 210}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:19,702] Trial 297 finished with value: 0.8128955652922099 and parameters: {'n_estimators': 576, 'learning_rate': 0.14435078585409195, 'max_depth': 10, 'max_bin': 272, 'num_leaves': 580}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:21,729] Trial 298 finished with value: 0.8138020841885002 and parameters: {'n_estimators': 653, 'learning_rate': 0.1216162852960759, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 631}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:23,458] Trial 299 finished with value: 0.8203784697466945 and parameters: {'n_estimators': 515, 'learning_rate': 0.13508051362897958, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 30}. Best is trial 23 with value: 0.8454651418857668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 614\n",
      "\t\tlearning_rate: 0.11855097491368037\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 263\n",
      "\t\tnum_leaves: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  320.000000  314.000000  323.000000  324.000000   \n",
      "1                    TN  170.000000  188.000000  174.000000  179.000000   \n",
      "2                    FP   57.000000   49.000000   55.000000   59.000000   \n",
      "3                    FN   48.000000   44.000000   43.000000   33.000000   \n",
      "4              Accuracy    0.823529    0.843697    0.835294    0.845378   \n",
      "5             Precision    0.848806    0.865014    0.854497    0.845953   \n",
      "6           Sensitivity    0.869565    0.877095    0.882514    0.907563   \n",
      "7           Specificity    0.748900    0.793200    0.759800    0.752100   \n",
      "8              F1 score    0.859060    0.871012    0.868280    0.875676   \n",
      "9   F1 score (weighted)    0.822811    0.843406    0.834407    0.843628   \n",
      "10     F1 score (macro)    0.811553    0.836359    0.824274    0.835616   \n",
      "11    Balanced Accuracy    0.809232    0.835172    0.821169    0.829832   \n",
      "12                  MCC    0.623523    0.672847    0.649302    0.674804   \n",
      "13                  NPV    0.779800    0.810300    0.801800    0.844300   \n",
      "14              ROC_AUC    0.809232    0.835172    0.821169    0.829832   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   328.000000  322.000000  \n",
      "1   171.000000  176.000000  \n",
      "2    67.000000   58.000000  \n",
      "3    29.000000   39.000000  \n",
      "4     0.838655    0.836975  \n",
      "5     0.830380    0.847368  \n",
      "6     0.918768    0.891967  \n",
      "7     0.718500    0.752100  \n",
      "8     0.872340    0.869096  \n",
      "9     0.835733    0.835616  \n",
      "10    0.826581    0.826530  \n",
      "11    0.818627    0.822052  \n",
      "12    0.660879    0.654947  \n",
      "13    0.855000    0.818600  \n",
      "14    0.818627    0.822052  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_lgbm_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_lgbm_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_lgbm_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_lgbm_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_lgbm_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_lgbm_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_lgbm_5)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:44:25,653] Trial 300 finished with value: 0.8237174739427718 and parameters: {'n_estimators': 383, 'learning_rate': 0.12708733939901687, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 540}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:27,576] Trial 301 finished with value: 0.8239320368021245 and parameters: {'n_estimators': 619, 'learning_rate': 0.11574731908665219, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 565}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:30,060] Trial 302 finished with value: 0.8267666643517433 and parameters: {'n_estimators': 534, 'learning_rate': 0.09510405237243047, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 600}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:31,928] Trial 303 finished with value: 0.8188093219058169 and parameters: {'n_estimators': 494, 'learning_rate': 0.1300168905861699, 'max_depth': 9, 'max_bin': 227, 'num_leaves': 519}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:33,603] Trial 304 finished with value: 0.8243123580926287 and parameters: {'n_estimators': 452, 'learning_rate': 0.1523219235530796, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 489}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:35,567] Trial 305 finished with value: 0.8228907030247038 and parameters: {'n_estimators': 752, 'learning_rate': 0.10437570106121205, 'max_depth': 10, 'max_bin': 289, 'num_leaves': 449}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:37,342] Trial 306 finished with value: 0.8269885791494813 and parameters: {'n_estimators': 359, 'learning_rate': 0.1767663794618602, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 528}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:38,393] Trial 307 finished with value: 0.8043274084332095 and parameters: {'n_estimators': 599, 'learning_rate': 0.14698927441016957, 'max_depth': 3, 'max_bin': 174, 'num_leaves': 646}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:40,227] Trial 308 finished with value: 0.8295879758303562 and parameters: {'n_estimators': 648, 'learning_rate': 0.14047373984299308, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 503}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:42,827] Trial 309 finished with value: 0.8183733231097831 and parameters: {'n_estimators': 278, 'learning_rate': 0.07193321549691499, 'max_depth': 10, 'max_bin': 282, 'num_leaves': 704}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:44,159] Trial 310 finished with value: 0.8152420392800375 and parameters: {'n_estimators': 730, 'learning_rate': 0.11968813231980911, 'max_depth': 5, 'max_bin': 252, 'num_leaves': 400}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:46,232] Trial 311 finished with value: 0.8311476026260379 and parameters: {'n_estimators': 699, 'learning_rate': 0.11135828528940214, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 577}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:48,378] Trial 312 finished with value: 0.8205779797578409 and parameters: {'n_estimators': 312, 'learning_rate': 0.0855510033529391, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 283}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:50,223] Trial 313 finished with value: 0.8235681859010808 and parameters: {'n_estimators': 559, 'learning_rate': 0.12488727208874284, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 383}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:51,575] Trial 314 finished with value: 0.8234173993059196 and parameters: {'n_estimators': 780, 'learning_rate': 0.16565604284590418, 'max_depth': 9, 'max_bin': 255, 'num_leaves': 259}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:52,937] Trial 315 finished with value: 0.8202384503763271 and parameters: {'n_estimators': 479, 'learning_rate': 0.1357415710065692, 'max_depth': 7, 'max_bin': 223, 'num_leaves': 615}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:54,460] Trial 316 finished with value: 0.8202629573390908 and parameters: {'n_estimators': 668, 'learning_rate': 0.15817818874892203, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 345}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:56,407] Trial 317 finished with value: 0.8257031272457984 and parameters: {'n_estimators': 517, 'learning_rate': 0.07851699797162762, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 85}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:44:58,511] Trial 318 finished with value: 0.8241629098817181 and parameters: {'n_estimators': 585, 'learning_rate': 0.09793931467258907, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 747}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:00,328] Trial 319 finished with value: 0.8306633058646511 and parameters: {'n_estimators': 636, 'learning_rate': 0.13229372489010408, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 547}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:02,175] Trial 320 finished with value: 0.825508935101565 and parameters: {'n_estimators': 611, 'learning_rate': 0.11734306748965932, 'max_depth': 10, 'max_bin': 169, 'num_leaves': 481}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:04,370] Trial 321 finished with value: 0.8204259497832422 and parameters: {'n_estimators': 436, 'learning_rate': 0.09032030134870517, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 659}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:06,472] Trial 322 finished with value: 0.8293254667338676 and parameters: {'n_estimators': 505, 'learning_rate': 0.10751096839815619, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 596}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:08,478] Trial 323 finished with value: 0.8272382902916829 and parameters: {'n_estimators': 538, 'learning_rate': 0.12721185755288317, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 557}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:10,156] Trial 324 finished with value: 0.8233451236398988 and parameters: {'n_estimators': 863, 'learning_rate': 0.13739884079728468, 'max_depth': 10, 'max_bin': 291, 'num_leaves': 533}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:11,936] Trial 325 finished with value: 0.8222329821406162 and parameters: {'n_estimators': 574, 'learning_rate': 0.12210881273576699, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 514}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:13,450] Trial 326 finished with value: 0.8247039481633018 and parameters: {'n_estimators': 392, 'learning_rate': 0.1690409806488055, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 474}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:15,388] Trial 327 finished with value: 0.8197852724875888 and parameters: {'n_estimators': 170, 'learning_rate': 0.10228193039865606, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 497}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:17,060] Trial 328 finished with value: 0.8172438883177213 and parameters: {'n_estimators': 415, 'learning_rate': 0.14427540936393404, 'max_depth': 10, 'max_bin': 297, 'num_leaves': 565}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:18,852] Trial 329 finished with value: 0.8253047038574917 and parameters: {'n_estimators': 550, 'learning_rate': 0.1291014881617139, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 589}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:20,474] Trial 330 finished with value: 0.8266032889624355 and parameters: {'n_estimators': 685, 'learning_rate': 0.14787792150198106, 'max_depth': 9, 'max_bin': 177, 'num_leaves': 608}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:22,463] Trial 331 finished with value: 0.8264908128433927 and parameters: {'n_estimators': 218, 'learning_rate': 0.11400395369680731, 'max_depth': 10, 'max_bin': 162, 'num_leaves': 540}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:24,203] Trial 332 finished with value: 0.8242297590759129 and parameters: {'n_estimators': 526, 'learning_rate': 0.13249348879023248, 'max_depth': 11, 'max_bin': 232, 'num_leaves': 300}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:25,989] Trial 333 finished with value: 0.8305676492838723 and parameters: {'n_estimators': 618, 'learning_rate': 0.14161274302298124, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 134}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:27,684] Trial 334 finished with value: 0.8224308553608897 and parameters: {'n_estimators': 465, 'learning_rate': 0.12394080859150093, 'max_depth': 9, 'max_bin': 287, 'num_leaves': 515}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:29,720] Trial 335 finished with value: 0.8208860331763814 and parameters: {'n_estimators': 633, 'learning_rate': 0.11103857543184724, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 428}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:31,799] Trial 336 finished with value: 0.8214014627414361 and parameters: {'n_estimators': 496, 'learning_rate': 0.08318617402929691, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 61}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:33,384] Trial 337 finished with value: 0.8274805528355701 and parameters: {'n_estimators': 251, 'learning_rate': 0.15470629903278282, 'max_depth': 10, 'max_bin': 283, 'num_leaves': 327}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:35,190] Trial 338 finished with value: 0.820982921036793 and parameters: {'n_estimators': 593, 'learning_rate': 0.16282208421386973, 'max_depth': 11, 'max_bin': 257, 'num_leaves': 575}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:36,704] Trial 339 finished with value: 0.8272806147626156 and parameters: {'n_estimators': 661, 'learning_rate': 0.11981339592970892, 'max_depth': 8, 'max_bin': 252, 'num_leaves': 626}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:38,475] Trial 340 finished with value: 0.8227707866310194 and parameters: {'n_estimators': 832, 'learning_rate': 0.1377222352013886, 'max_depth': 10, 'max_bin': 152, 'num_leaves': 492}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:40,274] Trial 341 finished with value: 0.825808095140205 and parameters: {'n_estimators': 643, 'learning_rate': 0.1059717853579966, 'max_depth': 12, 'max_bin': 181, 'num_leaves': 45}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:42,485] Trial 342 finished with value: 0.82190843216604 and parameters: {'n_estimators': 568, 'learning_rate': 0.061699228073916014, 'max_depth': 10, 'max_bin': 245, 'num_leaves': 163}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:44,503] Trial 343 finished with value: 0.8239128231383507 and parameters: {'n_estimators': 544, 'learning_rate': 0.09214544888313837, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 462}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:46,274] Trial 344 finished with value: 0.8215009864326511 and parameters: {'n_estimators': 483, 'learning_rate': 0.14990619431704394, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 525}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:48,008] Trial 345 finished with value: 0.8191564993536053 and parameters: {'n_estimators': 351, 'learning_rate': 0.1273320717173355, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 550}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:49,769] Trial 346 finished with value: 0.8311709127812594 and parameters: {'n_estimators': 607, 'learning_rate': 0.11667574535041936, 'max_depth': 10, 'max_bin': 281, 'num_leaves': 500}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:51,584] Trial 347 finished with value: 0.8245990794132962 and parameters: {'n_estimators': 522, 'learning_rate': 0.10030888102172779, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 238}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:53,418] Trial 348 finished with value: 0.8249452480419658 and parameters: {'n_estimators': 444, 'learning_rate': 0.13328297522135385, 'max_depth': 11, 'max_bin': 186, 'num_leaves': 639}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:45:55,125] Trial 349 finished with value: 0.8239502704847098 and parameters: {'n_estimators': 794, 'learning_rate': 0.14214960803274468, 'max_depth': 11, 'max_bin': 235, 'num_leaves': 478}. Best is trial 23 with value: 0.8454651418857668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.845465\n",
      "\tBest params:\n",
      "\t\tn_estimators: 614\n",
      "\t\tlearning_rate: 0.11855097491368037\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 263\n",
      "\t\tnum_leaves: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  320.000000  314.000000  323.000000  324.000000   \n",
      "1                    TN  170.000000  188.000000  174.000000  179.000000   \n",
      "2                    FP   57.000000   49.000000   55.000000   59.000000   \n",
      "3                    FN   48.000000   44.000000   43.000000   33.000000   \n",
      "4              Accuracy    0.823529    0.843697    0.835294    0.845378   \n",
      "5             Precision    0.848806    0.865014    0.854497    0.845953   \n",
      "6           Sensitivity    0.869565    0.877095    0.882514    0.907563   \n",
      "7           Specificity    0.748900    0.793200    0.759800    0.752100   \n",
      "8              F1 score    0.859060    0.871012    0.868280    0.875676   \n",
      "9   F1 score (weighted)    0.822811    0.843406    0.834407    0.843628   \n",
      "10     F1 score (macro)    0.811553    0.836359    0.824274    0.835616   \n",
      "11    Balanced Accuracy    0.809232    0.835172    0.821169    0.829832   \n",
      "12                  MCC    0.623523    0.672847    0.649302    0.674804   \n",
      "13                  NPV    0.779800    0.810300    0.801800    0.844300   \n",
      "14              ROC_AUC    0.809232    0.835172    0.821169    0.829832   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   328.000000  322.000000  336.000000  \n",
      "1   171.000000  176.000000  169.000000  \n",
      "2    67.000000   58.000000   44.000000  \n",
      "3    29.000000   39.000000   46.000000  \n",
      "4     0.838655    0.836975    0.848739  \n",
      "5     0.830380    0.847368    0.884211  \n",
      "6     0.918768    0.891967    0.879581  \n",
      "7     0.718500    0.752100    0.793400  \n",
      "8     0.872340    0.869096    0.881890  \n",
      "9     0.835733    0.835616    0.848894  \n",
      "10    0.826581    0.826530    0.835805  \n",
      "11    0.818627    0.822052    0.836504  \n",
      "12    0.660879    0.654947    0.671631  \n",
      "13    0.855000    0.818600    0.786000  \n",
      "14    0.818627    0.822052    0.836504  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_lgbm_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_lgbm_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_lgbm_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_lgbm_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_lgbm_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_lgbm_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_lgbm_6)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:45:56,821] Trial 350 finished with value: 0.8270006967671566 and parameters: {'n_estimators': 367, 'learning_rate': 0.12466295132900364, 'max_depth': 10, 'max_bin': 172, 'num_leaves': 102}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:03,802] Trial 351 finished with value: 0.8312503595286325 and parameters: {'n_estimators': 627, 'learning_rate': 0.012054521877515073, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 195}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:05,921] Trial 352 finished with value: 0.833931304846088 and parameters: {'n_estimators': 885, 'learning_rate': 0.08779661507780165, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 580}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:07,586] Trial 353 finished with value: 0.8277290284564689 and parameters: {'n_estimators': 503, 'learning_rate': 0.11028917946812614, 'max_depth': 9, 'max_bin': 266, 'num_leaves': 511}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:08,968] Trial 354 finished with value: 0.8213711284903914 and parameters: {'n_estimators': 711, 'learning_rate': 0.1733046616005361, 'max_depth': 10, 'max_bin': 286, 'num_leaves': 442}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:10,529] Trial 355 finished with value: 0.8181041287486241 and parameters: {'n_estimators': 589, 'learning_rate': 0.13629320399838585, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 531}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:13,950] Trial 356 finished with value: 0.8261000489793406 and parameters: {'n_estimators': 763, 'learning_rate': 0.03241584178231735, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 565}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:15,840] Trial 357 finished with value: 0.8206773094853783 and parameters: {'n_estimators': 556, 'learning_rate': 0.09600852703935177, 'max_depth': 10, 'max_bin': 254, 'num_leaves': 601}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:17,354] Trial 358 finished with value: 0.8285611822403836 and parameters: {'n_estimators': 533, 'learning_rate': 0.12939494336534363, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 72}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:18,495] Trial 359 finished with value: 0.8184794864483388 and parameters: {'n_estimators': 673, 'learning_rate': 0.1140622469110124, 'max_depth': 6, 'max_bin': 230, 'num_leaves': 545}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:20,037] Trial 360 finished with value: 0.8242565469233767 and parameters: {'n_estimators': 403, 'learning_rate': 0.12101469242230381, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 484}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:21,557] Trial 361 finished with value: 0.8183410396991532 and parameters: {'n_estimators': 645, 'learning_rate': 0.1469051696250671, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 621}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:23,209] Trial 362 finished with value: 0.8226004356046136 and parameters: {'n_estimators': 333, 'learning_rate': 0.10638961056193333, 'max_depth': 12, 'max_bin': 193, 'num_leaves': 358}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:24,549] Trial 363 finished with value: 0.8176173503385019 and parameters: {'n_estimators': 461, 'learning_rate': 0.13949742949975438, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 119}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:26,449] Trial 364 finished with value: 0.8247873818897119 and parameters: {'n_estimators': 619, 'learning_rate': 0.08135658309748574, 'max_depth': 10, 'max_bin': 244, 'num_leaves': 561}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:27,522] Trial 365 finished with value: 0.8116224072193925 and parameters: {'n_estimators': 514, 'learning_rate': 0.15576191728397942, 'max_depth': 7, 'max_bin': 206, 'num_leaves': 666}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:29,120] Trial 366 finished with value: 0.8228792086613014 and parameters: {'n_estimators': 814, 'learning_rate': 0.1302782309112515, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 504}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:30,426] Trial 367 finished with value: 0.8248333977647244 and parameters: {'n_estimators': 600, 'learning_rate': 0.18689031925677446, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 585}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:31,905] Trial 368 finished with value: 0.8236951739987933 and parameters: {'n_estimators': 425, 'learning_rate': 0.13474484718565707, 'max_depth': 9, 'max_bin': 247, 'num_leaves': 466}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:32,852] Trial 369 finished with value: 0.8164193376627453 and parameters: {'n_estimators': 51, 'learning_rate': 0.11619431325122136, 'max_depth': 10, 'max_bin': 280, 'num_leaves': 212}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:34,919] Trial 370 finished with value: 0.8253253670219143 and parameters: {'n_estimators': 539, 'learning_rate': 0.07599326789444856, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 525}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:36,270] Trial 371 finished with value: 0.8245822418089747 and parameters: {'n_estimators': 566, 'learning_rate': 0.12415018658700787, 'max_depth': 10, 'max_bin': 250, 'num_leaves': 179}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:37,879] Trial 372 finished with value: 0.823063845227933 and parameters: {'n_estimators': 487, 'learning_rate': 0.14378221344765055, 'max_depth': 12, 'max_bin': 167, 'num_leaves': 609}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:39,252] Trial 373 finished with value: 0.8198927941259508 and parameters: {'n_estimators': 664, 'learning_rate': 0.1650111401042943, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 495}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:40,648] Trial 374 finished with value: 0.8215881976665866 and parameters: {'n_estimators': 622, 'learning_rate': 0.12724213795125444, 'max_depth': 10, 'max_bin': 288, 'num_leaves': 142}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:42,383] Trial 375 finished with value: 0.8266262431340042 and parameters: {'n_estimators': 683, 'learning_rate': 0.10310655846078741, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 542}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:44,100] Trial 376 finished with value: 0.8321476294119918 and parameters: {'n_estimators': 582, 'learning_rate': 0.11842906928326431, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 515}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:45,593] Trial 377 finished with value: 0.8219950222654167 and parameters: {'n_estimators': 741, 'learning_rate': 0.15071984802008923, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 555}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:46,833] Trial 378 finished with value: 0.8142363707145316 and parameters: {'n_estimators': 512, 'learning_rate': 0.15947747126039724, 'max_depth': 8, 'max_bin': 275, 'num_leaves': 577}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:48,374] Trial 379 finished with value: 0.8225755620634718 and parameters: {'n_estimators': 377, 'learning_rate': 0.10989456858111657, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 52}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:50,443] Trial 380 finished with value: 0.830758381617884 and parameters: {'n_estimators': 306, 'learning_rate': 0.09149657496219224, 'max_depth': 11, 'max_bin': 234, 'num_leaves': 451}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:52,261] Trial 381 finished with value: 0.8232155602057126 and parameters: {'n_estimators': 606, 'learning_rate': 0.13314440457210244, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 632}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:53,986] Trial 382 finished with value: 0.8156317612735723 and parameters: {'n_estimators': 647, 'learning_rate': 0.13946779493913028, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 596}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:46:56,170] Trial 383 finished with value: 0.8245713067864328 and parameters: {'n_estimators': 471, 'learning_rate': 0.09679539467615596, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 485}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:00,473] Trial 384 finished with value: 0.8298747258384719 and parameters: {'n_estimators': 555, 'learning_rate': 0.023337291978164185, 'max_depth': 9, 'max_bin': 272, 'num_leaves': 256}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:02,864] Trial 385 finished with value: 0.827013382110348 and parameters: {'n_estimators': 535, 'learning_rate': 0.08749642201483385, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 534}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:04,799] Trial 386 finished with value: 0.8228281815255094 and parameters: {'n_estimators': 449, 'learning_rate': 0.12019454057787415, 'max_depth': 10, 'max_bin': 293, 'num_leaves': 647}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:06,629] Trial 387 finished with value: 0.8262245556043342 and parameters: {'n_estimators': 493, 'learning_rate': 0.1264750020893516, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 372}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:08,702] Trial 388 finished with value: 0.8259642895242957 and parameters: {'n_estimators': 635, 'learning_rate': 0.1139552820840004, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 504}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:10,053] Trial 389 finished with value: 0.8210852903220826 and parameters: {'n_estimators': 721, 'learning_rate': 0.17940796678302812, 'max_depth': 10, 'max_bin': 238, 'num_leaves': 273}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:11,982] Trial 390 finished with value: 0.8252773753425 and parameters: {'n_estimators': 859, 'learning_rate': 0.13105088002652315, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 567}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:13,883] Trial 391 finished with value: 0.8217211446836572 and parameters: {'n_estimators': 526, 'learning_rate': 0.12287582021389033, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 470}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:15,581] Trial 392 finished with value: 0.8205160517602623 and parameters: {'n_estimators': 697, 'learning_rate': 0.13587764586700082, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 521}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:17,505] Trial 393 finished with value: 0.8187150017271152 and parameters: {'n_estimators': 570, 'learning_rate': 0.10130496933903045, 'max_depth': 10, 'max_bin': 241, 'num_leaves': 589}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:19,129] Trial 394 finished with value: 0.8196394355784099 and parameters: {'n_estimators': 508, 'learning_rate': 0.14564272131133343, 'max_depth': 12, 'max_bin': 177, 'num_leaves': 154}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:21,083] Trial 395 finished with value: 0.8214624113485863 and parameters: {'n_estimators': 655, 'learning_rate': 0.10697360729764241, 'max_depth': 11, 'max_bin': 243, 'num_leaves': 550}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:23,308] Trial 396 finished with value: 0.8243266158957777 and parameters: {'n_estimators': 397, 'learning_rate': 0.08495493410838965, 'max_depth': 10, 'max_bin': 164, 'num_leaves': 616}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:24,890] Trial 397 finished with value: 0.8243984573865376 and parameters: {'n_estimators': 592, 'learning_rate': 0.13940534941483268, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 30}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:27,140] Trial 398 finished with value: 0.8254010802462514 and parameters: {'n_estimators': 550, 'learning_rate': 0.07884497073922192, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 494}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:28,922] Trial 399 finished with value: 0.8265617207123285 and parameters: {'n_estimators': 284, 'learning_rate': 0.11367434680756966, 'max_depth': 9, 'max_bin': 280, 'num_leaves': 516}. Best is trial 23 with value: 0.8454651418857668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8454651\n",
      "\tBest params:\n",
      "\t\tn_estimators: 614\n",
      "\t\tlearning_rate: 0.11855097491368037\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 263\n",
      "\t\tnum_leaves: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  320.000000  314.000000  323.000000  324.000000   \n",
      "1                    TN  170.000000  188.000000  174.000000  179.000000   \n",
      "2                    FP   57.000000   49.000000   55.000000   59.000000   \n",
      "3                    FN   48.000000   44.000000   43.000000   33.000000   \n",
      "4              Accuracy    0.823529    0.843697    0.835294    0.845378   \n",
      "5             Precision    0.848806    0.865014    0.854497    0.845953   \n",
      "6           Sensitivity    0.869565    0.877095    0.882514    0.907563   \n",
      "7           Specificity    0.748900    0.793200    0.759800    0.752100   \n",
      "8              F1 score    0.859060    0.871012    0.868280    0.875676   \n",
      "9   F1 score (weighted)    0.822811    0.843406    0.834407    0.843628   \n",
      "10     F1 score (macro)    0.811553    0.836359    0.824274    0.835616   \n",
      "11    Balanced Accuracy    0.809232    0.835172    0.821169    0.829832   \n",
      "12                  MCC    0.623523    0.672847    0.649302    0.674804   \n",
      "13                  NPV    0.779800    0.810300    0.801800    0.844300   \n",
      "14              ROC_AUC    0.809232    0.835172    0.821169    0.829832   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   328.000000  322.000000  336.000000  328.000000  \n",
      "1   171.000000  176.000000  169.000000  183.000000  \n",
      "2    67.000000   58.000000   44.000000   45.000000  \n",
      "3    29.000000   39.000000   46.000000   39.000000  \n",
      "4     0.838655    0.836975    0.848739    0.858824  \n",
      "5     0.830380    0.847368    0.884211    0.879357  \n",
      "6     0.918768    0.891967    0.879581    0.893733  \n",
      "7     0.718500    0.752100    0.793400    0.802600  \n",
      "8     0.872340    0.869096    0.881890    0.886486  \n",
      "9     0.835733    0.835616    0.848894    0.858455  \n",
      "10    0.826581    0.826530    0.835805    0.849910  \n",
      "11    0.818627    0.822052    0.836504    0.848182  \n",
      "12    0.660879    0.654947    0.671631    0.700013  \n",
      "13    0.855000    0.818600    0.786000    0.824300  \n",
      "14    0.818627    0.822052    0.836504    0.848182  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_lgbm_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_lgbm_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_lgbm_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_lgbm_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_lgbm_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_lgbm_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_lgbm_7)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:47:31,046] Trial 400 finished with value: 0.8235504109062297 and parameters: {'n_estimators': 624, 'learning_rate': 0.13104115359869675, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 536}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:33,074] Trial 401 finished with value: 0.8209162469851794 and parameters: {'n_estimators': 611, 'learning_rate': 0.12192241245288812, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 573}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:34,226] Trial 402 finished with value: 0.8133731836019769 and parameters: {'n_estimators': 776, 'learning_rate': 0.1536249939214529, 'max_depth': 5, 'max_bin': 266, 'num_leaves': 600}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:42,385] Trial 403 finished with value: 0.7175415169006369 and parameters: {'n_estimators': 436, 'learning_rate': 0.0013232726690461938, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 482}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:44,328] Trial 404 finished with value: 0.8242099272569237 and parameters: {'n_estimators': 524, 'learning_rate': 0.11823081518955937, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 419}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:46,298] Trial 405 finished with value: 0.8234924604077671 and parameters: {'n_estimators': 418, 'learning_rate': 0.09424839690613424, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 560}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:47,458] Trial 406 finished with value: 0.8181252491779176 and parameters: {'n_estimators': 573, 'learning_rate': 0.12767929422719115, 'max_depth': 4, 'max_bin': 260, 'num_leaves': 83}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:48,987] Trial 407 finished with value: 0.8183931175936662 and parameters: {'n_estimators': 839, 'learning_rate': 0.1703565583646023, 'max_depth': 10, 'max_bin': 269, 'num_leaves': 503}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:50,427] Trial 408 finished with value: 0.8191342480452655 and parameters: {'n_estimators': 472, 'learning_rate': 0.14290960628297036, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 389}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:52,010] Trial 409 finished with value: 0.8222503053593734 and parameters: {'n_estimators': 543, 'learning_rate': 0.13718930149876418, 'max_depth': 10, 'max_bin': 252, 'num_leaves': 455}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:53,901] Trial 410 finished with value: 0.8244503902456503 and parameters: {'n_estimators': 637, 'learning_rate': 0.11052069199610202, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 530}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:55,530] Trial 411 finished with value: 0.8227434230252848 and parameters: {'n_estimators': 496, 'learning_rate': 0.14984057671504628, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 623}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:56,796] Trial 412 finished with value: 0.8137088890221701 and parameters: {'n_estimators': 667, 'learning_rate': 0.16119619332741442, 'max_depth': 9, 'max_bin': 190, 'num_leaves': 194}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:47:58,384] Trial 413 finished with value: 0.8209696392855491 and parameters: {'n_estimators': 346, 'learning_rate': 0.1341431052226714, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 301}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:00,043] Trial 414 finished with value: 0.8300084974416571 and parameters: {'n_estimators': 603, 'learning_rate': 0.12670171561560994, 'max_depth': 10, 'max_bin': 248, 'num_leaves': 585}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:01,907] Trial 415 finished with value: 0.8250689653787491 and parameters: {'n_estimators': 515, 'learning_rate': 0.09942812468964753, 'max_depth': 11, 'max_bin': 185, 'num_leaves': 548}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:03,443] Trial 416 finished with value: 0.8216440036199966 and parameters: {'n_estimators': 322, 'learning_rate': 0.11746690641166167, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 61}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:05,178] Trial 417 finished with value: 0.8227400024880241 and parameters: {'n_estimators': 262, 'learning_rate': 0.12183988035085151, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 680}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:06,870] Trial 418 finished with value: 0.8268113269811674 and parameters: {'n_estimators': 586, 'learning_rate': 0.1056867485935879, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 491}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:12,123] Trial 419 finished with value: 0.8206966282189324 and parameters: {'n_estimators': 808, 'learning_rate': 0.015863604978877427, 'max_depth': 10, 'max_bin': 289, 'num_leaves': 513}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:14,264] Trial 420 finished with value: 0.821839270976669 and parameters: {'n_estimators': 115, 'learning_rate': 0.04785676051238345, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 474}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:16,115] Trial 421 finished with value: 0.8136955231635785 and parameters: {'n_estimators': 877, 'learning_rate': 0.08294675254557257, 'max_depth': 8, 'max_bin': 173, 'num_leaves': 639}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:17,912] Trial 422 finished with value: 0.8235405042741801 and parameters: {'n_estimators': 366, 'learning_rate': 0.13026833404986804, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 334}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:19,582] Trial 423 finished with value: 0.8306249272570391 and parameters: {'n_estimators': 558, 'learning_rate': 0.14229911240324233, 'max_depth': 10, 'max_bin': 242, 'num_leaves': 406}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:21,879] Trial 424 finished with value: 0.822079241639811 and parameters: {'n_estimators': 650, 'learning_rate': 0.0903175642789854, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 318}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:23,444] Trial 425 finished with value: 0.8191481985063287 and parameters: {'n_estimators': 211, 'learning_rate': 0.16763215133313095, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 224}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:25,300] Trial 426 finished with value: 0.8153897883727256 and parameters: {'n_estimators': 623, 'learning_rate': 0.1127537655723982, 'max_depth': 10, 'max_bin': 245, 'num_leaves': 568}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:27,710] Trial 427 finished with value: 0.819437030108815 and parameters: {'n_estimators': 534, 'learning_rate': 0.07122452737000304, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 609}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:29,360] Trial 428 finished with value: 0.8220276294001628 and parameters: {'n_estimators': 756, 'learning_rate': 0.12348228259317229, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 42}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:30,968] Trial 429 finished with value: 0.8294375138269728 and parameters: {'n_estimators': 702, 'learning_rate': 0.13540717563132648, 'max_depth': 10, 'max_bin': 181, 'num_leaves': 99}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:34,341] Trial 430 finished with value: 0.8215217757438434 and parameters: {'n_estimators': 456, 'learning_rate': 0.03533047921063619, 'max_depth': 10, 'max_bin': 250, 'num_leaves': 529}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:36,098] Trial 431 finished with value: 0.8211234258855045 and parameters: {'n_estimators': 678, 'learning_rate': 0.14801257319292843, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 435}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:38,060] Trial 432 finished with value: 0.8199914518809466 and parameters: {'n_estimators': 496, 'learning_rate': 0.13156421565694668, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 655}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:46,335] Trial 433 finished with value: 0.8294498602169906 and parameters: {'n_estimators': 550, 'learning_rate': 0.009901364241073171, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 546}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:48,342] Trial 434 finished with value: 0.8216882391726836 and parameters: {'n_estimators': 383, 'learning_rate': 0.10947801181812618, 'max_depth': 11, 'max_bin': 154, 'num_leaves': 583}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:49,811] Trial 435 finished with value: 0.81322035815445 and parameters: {'n_estimators': 481, 'learning_rate': 0.15787825207054504, 'max_depth': 9, 'max_bin': 257, 'num_leaves': 500}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:51,642] Trial 436 finished with value: 0.8265550822090069 and parameters: {'n_estimators': 577, 'learning_rate': 0.10337172435894715, 'max_depth': 10, 'max_bin': 229, 'num_leaves': 173}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:53,485] Trial 437 finished with value: 0.8227969829640251 and parameters: {'n_estimators': 633, 'learning_rate': 0.1397835600386884, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 563}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:55,203] Trial 438 finished with value: 0.8190674110804856 and parameters: {'n_estimators': 730, 'learning_rate': 0.11483490833227766, 'max_depth': 10, 'max_bin': 279, 'num_leaves': 128}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:56,943] Trial 439 finished with value: 0.8191343708832429 and parameters: {'n_estimators': 609, 'learning_rate': 0.12533366322351788, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 239}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:48:58,848] Trial 440 finished with value: 0.8254961919210828 and parameters: {'n_estimators': 522, 'learning_rate': 0.11893456167059706, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 467}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:00,673] Trial 441 finished with value: 0.826079282150137 and parameters: {'n_estimators': 651, 'learning_rate': 0.1284167329054419, 'max_depth': 11, 'max_bin': 245, 'num_leaves': 523}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:02,350] Trial 442 finished with value: 0.8233128803697708 and parameters: {'n_estimators': 510, 'learning_rate': 0.13685548001309825, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 487}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:04,009] Trial 443 finished with value: 0.8257836579495412 and parameters: {'n_estimators': 540, 'learning_rate': 0.15343025664066293, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 604}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:05,644] Trial 444 finished with value: 0.8275602435894264 and parameters: {'n_estimators': 407, 'learning_rate': 0.1457566504016858, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 516}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:08,371] Trial 445 finished with value: 0.8182348971584161 and parameters: {'n_estimators': 592, 'learning_rate': 0.04030940401285216, 'max_depth': 10, 'max_bin': 248, 'num_leaves': 553}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:10,351] Trial 446 finished with value: 0.8201777253897646 and parameters: {'n_estimators': 619, 'learning_rate': 0.09458669560744139, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 625}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:12,252] Trial 447 finished with value: 0.8239772629247375 and parameters: {'n_estimators': 562, 'learning_rate': 0.0993822272840977, 'max_depth': 12, 'max_bin': 170, 'num_leaves': 506}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:13,688] Trial 448 finished with value: 0.819380097482037 and parameters: {'n_estimators': 191, 'learning_rate': 0.1641543265185394, 'max_depth': 9, 'max_bin': 253, 'num_leaves': 592}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:15,524] Trial 449 finished with value: 0.8163729172148241 and parameters: {'n_estimators': 427, 'learning_rate': 0.06788642101855197, 'max_depth': 8, 'max_bin': 276, 'num_leaves': 70}. Best is trial 23 with value: 0.8454651418857668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.84546514\n",
      "\tBest params:\n",
      "\t\tn_estimators: 614\n",
      "\t\tlearning_rate: 0.11855097491368037\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 263\n",
      "\t\tnum_leaves: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  320.000000  314.000000  323.000000  324.000000   \n",
      "1                    TN  170.000000  188.000000  174.000000  179.000000   \n",
      "2                    FP   57.000000   49.000000   55.000000   59.000000   \n",
      "3                    FN   48.000000   44.000000   43.000000   33.000000   \n",
      "4              Accuracy    0.823529    0.843697    0.835294    0.845378   \n",
      "5             Precision    0.848806    0.865014    0.854497    0.845953   \n",
      "6           Sensitivity    0.869565    0.877095    0.882514    0.907563   \n",
      "7           Specificity    0.748900    0.793200    0.759800    0.752100   \n",
      "8              F1 score    0.859060    0.871012    0.868280    0.875676   \n",
      "9   F1 score (weighted)    0.822811    0.843406    0.834407    0.843628   \n",
      "10     F1 score (macro)    0.811553    0.836359    0.824274    0.835616   \n",
      "11    Balanced Accuracy    0.809232    0.835172    0.821169    0.829832   \n",
      "12                  MCC    0.623523    0.672847    0.649302    0.674804   \n",
      "13                  NPV    0.779800    0.810300    0.801800    0.844300   \n",
      "14              ROC_AUC    0.809232    0.835172    0.821169    0.829832   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   328.000000  322.000000  336.000000  328.000000  324.000000  \n",
      "1   171.000000  176.000000  169.000000  183.000000  178.000000  \n",
      "2    67.000000   58.000000   44.000000   45.000000   51.000000  \n",
      "3    29.000000   39.000000   46.000000   39.000000   42.000000  \n",
      "4     0.838655    0.836975    0.848739    0.858824    0.843697  \n",
      "5     0.830380    0.847368    0.884211    0.879357    0.864000  \n",
      "6     0.918768    0.891967    0.879581    0.893733    0.885246  \n",
      "7     0.718500    0.752100    0.793400    0.802600    0.777300  \n",
      "8     0.872340    0.869096    0.881890    0.886486    0.874494  \n",
      "9     0.835733    0.835616    0.848894    0.858455    0.843080  \n",
      "10    0.826581    0.826530    0.835805    0.849910    0.833683  \n",
      "11    0.818627    0.822052    0.836504    0.848182    0.831269  \n",
      "12    0.660879    0.654947    0.671631    0.700013    0.667794  \n",
      "13    0.855000    0.818600    0.786000    0.824300    0.809100  \n",
      "14    0.818627    0.822052    0.836504    0.848182    0.831269  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_lgbm_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_lgbm_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_lgbm_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_lgbm_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_lgbm_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_lgbm_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_lgbm_8)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:49:18,195] Trial 450 finished with value: 0.8363728574744167 and parameters: {'n_estimators': 503, 'learning_rate': 0.07911136356372417, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 534}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:19,660] Trial 451 finished with value: 0.8326318422467421 and parameters: {'n_estimators': 828, 'learning_rate': 0.19711530165804014, 'max_depth': 10, 'max_bin': 243, 'num_leaves': 352}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:21,498] Trial 452 finished with value: 0.8365462735217337 and parameters: {'n_estimators': 637, 'learning_rate': 0.10922791886088676, 'max_depth': 10, 'max_bin': 282, 'num_leaves': 469}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:23,279] Trial 453 finished with value: 0.8304494872595146 and parameters: {'n_estimators': 681, 'learning_rate': 0.1223853855064141, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 489}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:31,241] Trial 454 finished with value: 0.8195630662113892 and parameters: {'n_estimators': 462, 'learning_rate': 0.005834579905174303, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 571}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:32,935] Trial 455 finished with value: 0.8305978487849865 and parameters: {'n_estimators': 246, 'learning_rate': 0.17500624642638438, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 506}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:34,800] Trial 456 finished with value: 0.8378916254029759 and parameters: {'n_estimators': 524, 'learning_rate': 0.13251650243109697, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 551}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:36,602] Trial 457 finished with value: 0.8355493598443131 and parameters: {'n_estimators': 790, 'learning_rate': 0.13387650592498831, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 574}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:38,435] Trial 458 finished with value: 0.8241759342818529 and parameters: {'n_estimators': 662, 'learning_rate': 0.1409594058851524, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 448}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:40,653] Trial 459 finished with value: 0.834454267577301 and parameters: {'n_estimators': 482, 'learning_rate': 0.10448494928477052, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 541}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:43,617] Trial 460 finished with value: 0.8369088620352633 and parameters: {'n_estimators': 525, 'learning_rate': 0.059436406493746395, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 555}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:45,667] Trial 461 finished with value: 0.8380202407091355 and parameters: {'n_estimators': 601, 'learning_rate': 0.11676479994632857, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 614}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:47,681] Trial 462 finished with value: 0.8345276815997484 and parameters: {'n_estimators': 601, 'learning_rate': 0.11587267847083682, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 619}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:49,728] Trial 463 finished with value: 0.8403378603494843 and parameters: {'n_estimators': 584, 'learning_rate': 0.11919550676643398, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 632}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:51,696] Trial 464 finished with value: 0.8342472341262797 and parameters: {'n_estimators': 573, 'learning_rate': 0.11924550069646375, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 626}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:53,738] Trial 465 finished with value: 0.8295030957038998 and parameters: {'n_estimators': 586, 'learning_rate': 0.11248681502985074, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 607}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:55,798] Trial 466 finished with value: 0.8305681566257095 and parameters: {'n_estimators': 583, 'learning_rate': 0.11621296034060177, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 641}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:57,902] Trial 467 finished with value: 0.8336246796227142 and parameters: {'n_estimators': 601, 'learning_rate': 0.1083615096076236, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 602}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:49:59,890] Trial 468 finished with value: 0.8345141627896362 and parameters: {'n_estimators': 576, 'learning_rate': 0.11992001167014704, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 583}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:02,084] Trial 469 finished with value: 0.8342845522423851 and parameters: {'n_estimators': 611, 'learning_rate': 0.11281950008212503, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 595}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:04,038] Trial 470 finished with value: 0.831012133086622 and parameters: {'n_estimators': 561, 'learning_rate': 0.12401739419396975, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 590}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:06,111] Trial 471 finished with value: 0.8345909557451966 and parameters: {'n_estimators': 588, 'learning_rate': 0.1163359761025737, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 628}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:08,152] Trial 472 finished with value: 0.8354358129414369 and parameters: {'n_estimators': 551, 'learning_rate': 0.12046568841978207, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 483}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:10,326] Trial 473 finished with value: 0.8332068599115287 and parameters: {'n_estimators': 619, 'learning_rate': 0.12605246030406753, 'max_depth': 11, 'max_bin': 240, 'num_leaves': 613}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:12,540] Trial 474 finished with value: 0.827303725501966 and parameters: {'n_estimators': 599, 'learning_rate': 0.10580572695748434, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 636}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:14,568] Trial 475 finished with value: 0.8348932012859474 and parameters: {'n_estimators': 565, 'learning_rate': 0.11122072004802683, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 615}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:16,508] Trial 476 finished with value: 0.8326132181997871 and parameters: {'n_estimators': 353, 'learning_rate': 0.11759049234399874, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 519}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:18,283] Trial 477 finished with value: 0.833768488963357 and parameters: {'n_estimators': 619, 'learning_rate': 0.15665377598758246, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 575}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:20,435] Trial 478 finished with value: 0.8298694231647289 and parameters: {'n_estimators': 546, 'learning_rate': 0.10244114259067666, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 497}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:22,506] Trial 479 finished with value: 0.8310980790909502 and parameters: {'n_estimators': 335, 'learning_rate': 0.12305877845017461, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 561}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:24,498] Trial 480 finished with value: 0.8317268324299345 and parameters: {'n_estimators': 596, 'learning_rate': 0.12851599837826705, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 535}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:26,294] Trial 481 finished with value: 0.8323570283962057 and parameters: {'n_estimators': 576, 'learning_rate': 0.14953660517003325, 'max_depth': 12, 'max_bin': 298, 'num_leaves': 457}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:28,245] Trial 482 finished with value: 0.834371065751148 and parameters: {'n_estimators': 606, 'learning_rate': 0.14485014015508887, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 594}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:30,298] Trial 483 finished with value: 0.8319033012906427 and parameters: {'n_estimators': 527, 'learning_rate': 0.1083294634893557, 'max_depth': 12, 'max_bin': 158, 'num_leaves': 476}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:32,281] Trial 484 finished with value: 0.8353178015305129 and parameters: {'n_estimators': 622, 'learning_rate': 0.11404084989447635, 'max_depth': 11, 'max_bin': 243, 'num_leaves': 551}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:34,157] Trial 485 finished with value: 0.8352507629169518 and parameters: {'n_estimators': 561, 'learning_rate': 0.12488185118247273, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 371}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:36,112] Trial 486 finished with value: 0.8325271748119072 and parameters: {'n_estimators': 849, 'learning_rate': 0.1197882266205308, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 505}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:38,037] Trial 487 finished with value: 0.8354242821985253 and parameters: {'n_estimators': 634, 'learning_rate': 0.1612359148493473, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 654}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:40,189] Trial 488 finished with value: 0.8324352700616459 and parameters: {'n_estimators': 541, 'learning_rate': 0.096863924640488, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 526}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:45,680] Trial 489 finished with value: 0.8318926288118146 and parameters: {'n_estimators': 391, 'learning_rate': 0.020130736146374048, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 422}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:47,506] Trial 490 finished with value: 0.8348987808122461 and parameters: {'n_estimators': 505, 'learning_rate': 0.12904469716920064, 'max_depth': 11, 'max_bin': 236, 'num_leaves': 292}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:49,362] Trial 491 finished with value: 0.8242574426841983 and parameters: {'n_estimators': 590, 'learning_rate': 0.1379696689054884, 'max_depth': 11, 'max_bin': 268, 'num_leaves': 606}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:51,344] Trial 492 finished with value: 0.8394454604075705 and parameters: {'n_estimators': 573, 'learning_rate': 0.13276554581220495, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 580}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:53,209] Trial 493 finished with value: 0.8297782707028976 and parameters: {'n_estimators': 580, 'learning_rate': 0.13180614670058513, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 577}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:55,227] Trial 494 finished with value: 0.8385683250443087 and parameters: {'n_estimators': 569, 'learning_rate': 0.12213122633761116, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 567}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:57,291] Trial 495 finished with value: 0.8331131710035337 and parameters: {'n_estimators': 568, 'learning_rate': 0.12246673386766484, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 572}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:50:59,331] Trial 496 finished with value: 0.8404184750754785 and parameters: {'n_estimators': 898, 'learning_rate': 0.1258680200295308, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 560}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:51:01,382] Trial 497 finished with value: 0.8290524000009107 and parameters: {'n_estimators': 883, 'learning_rate': 0.1262200306820432, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 581}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:51:03,123] Trial 498 finished with value: 0.8352175090058441 and parameters: {'n_estimators': 854, 'learning_rate': 0.1809504186255105, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 568}. Best is trial 23 with value: 0.8454651418857668.\n",
      "[I 2023-12-04 16:51:05,184] Trial 499 finished with value: 0.8376733783336459 and parameters: {'n_estimators': 585, 'learning_rate': 0.11798791738868929, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 580}. Best is trial 23 with value: 0.8454651418857668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.845465142\n",
      "\tBest params:\n",
      "\t\tn_estimators: 614\n",
      "\t\tlearning_rate: 0.11855097491368037\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 263\n",
      "\t\tnum_leaves: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  320.000000  314.000000  323.000000  324.000000   \n",
      "1                    TN  170.000000  188.000000  174.000000  179.000000   \n",
      "2                    FP   57.000000   49.000000   55.000000   59.000000   \n",
      "3                    FN   48.000000   44.000000   43.000000   33.000000   \n",
      "4              Accuracy    0.823529    0.843697    0.835294    0.845378   \n",
      "5             Precision    0.848806    0.865014    0.854497    0.845953   \n",
      "6           Sensitivity    0.869565    0.877095    0.882514    0.907563   \n",
      "7           Specificity    0.748900    0.793200    0.759800    0.752100   \n",
      "8              F1 score    0.859060    0.871012    0.868280    0.875676   \n",
      "9   F1 score (weighted)    0.822811    0.843406    0.834407    0.843628   \n",
      "10     F1 score (macro)    0.811553    0.836359    0.824274    0.835616   \n",
      "11    Balanced Accuracy    0.809232    0.835172    0.821169    0.829832   \n",
      "12                  MCC    0.623523    0.672847    0.649302    0.674804   \n",
      "13                  NPV    0.779800    0.810300    0.801800    0.844300   \n",
      "14              ROC_AUC    0.809232    0.835172    0.821169    0.829832   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   328.000000  322.000000  336.000000  328.000000  324.000000  324.000000  \n",
      "1   171.000000  176.000000  169.000000  183.000000  178.000000  158.000000  \n",
      "2    67.000000   58.000000   44.000000   45.000000   51.000000   69.000000  \n",
      "3    29.000000   39.000000   46.000000   39.000000   42.000000   44.000000  \n",
      "4     0.838655    0.836975    0.848739    0.858824    0.843697    0.810084  \n",
      "5     0.830380    0.847368    0.884211    0.879357    0.864000    0.824427  \n",
      "6     0.918768    0.891967    0.879581    0.893733    0.885246    0.880435  \n",
      "7     0.718500    0.752100    0.793400    0.802600    0.777300    0.696000  \n",
      "8     0.872340    0.869096    0.881890    0.886486    0.874494    0.851511  \n",
      "9     0.835733    0.835616    0.848894    0.858455    0.843080    0.807670  \n",
      "10    0.826581    0.826530    0.835805    0.849910    0.833683    0.794054  \n",
      "11    0.818627    0.822052    0.836504    0.848182    0.831269    0.788235  \n",
      "12    0.660879    0.654947    0.671631    0.700013    0.667794    0.591346  \n",
      "13    0.855000    0.818600    0.786000    0.824300    0.809100    0.782200  \n",
      "14    0.818627    0.822052    0.836504    0.848182    0.831269    0.788235  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=5, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_lgbm_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_lgbm_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_lgbm_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_lgbm_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_lgbm_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_lgbm_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_lgbm_9)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAHJCAYAAAAb9zQwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB5HUlEQVR4nO3dd1gU1/s28HtpAlKlVxEFbGBvaMTeYkCjInZsMUZjSWLBREWSaFBji12jYi98o1iiaCyosWFFjNhQUaQKSJc27x++7M+VRWFZis79uS4v3TNnzjzP7CqPZ2bOSgRBEEBEREREnzyVyg6AiIiIiCoGCz8iIiIikWDhR0RERCQSLPyIiIiIRIKFHxEREZFIsPAjIiIiEgkWfkREREQiwcKPiIiISCRY+BERERGJBAs/IiIiIpFg4UdySSQSSCSS9/axs7ODRCLBkydPKiYoqnI6dOjwwc9JRfH29oZEIsGWLVsqO5RyV5XOOxF9XFj4EREREYkECz8iIiIikWDhR0qTnJwMbW1t1K5dG4IgyO3Tu3dvSCQSXLt2DQDw5MkTSCQSeHt7IyIiAn369EGNGjVQvXp1tGvXDsePHy/2eLt27ULHjh1haGgITU1N1KtXD7/88gtev35dpK9EIkGHDh3w4sULjBw5EhYWFlBVVZVeFiy8TBgZGYklS5agbt260NTUhLW1NaZOnYrU1NQiY54+fRpfffUV6tevDz09PWhpaaFBgwaYO3cusrKyivT39fWFRCLBmTNnsHXrVrRo0QLVq1eHnZ2dtM+WLVvQr18/2NvbQ0tLC3p6emjbti22bt0q9xwUXvLLzc2Fn58fateuDU1NTTg5OWHDhg3SfqtWrULDhg2hpaUFa2tr+Pr6oqCgQO6Yly9fRv/+/WFubg4NDQ3Y2Nhg3LhxePHihbRP4fsWEhIiPb+Fvzp06CAz3vPnzzFx4kTY29ujWrVqMDIygru7O0JDQxU6R6WlzHOk6Oc1OzsbCxYsgLOzM7S1taGnp4fPPvsMu3fvLtL33WP0798fJiYmUFFRwZYtW0p03svy2QwMDETLli2hra2NGjVqYODAgXj+/LncvJKSkvDjjz+iYcOG0NbWhr6+Pho1aoSZM2ciIyOjSF8fHx/Uq1cPWlpa0NfXR+fOneWes9evX2Pp0qVo0qQJDA0Noa2tDRsbG3zxxRc4ceKE3FiIqGTUKjsA+nQYGhrCy8sLmzdvxj///IOuXbvKbH/27BmOHj2KZs2aoVmzZjLbHj9+jDZt2qBhw4YYN24cYmJisGfPHvTs2RM7d+7EwIEDZfqPHj0amzZtgo2NDfr16wd9fX1cunQJs2fPxsmTJ3H8+HGoq6vL7PPy5Uu0adMGurq66N+/PwRBgKmpqUyfqVOn4uzZs/D09ISHhweCg4OxbNkynDt3DufPn4empqa0r7+/PyIiIuDq6orPP/8cWVlZ+Pfff+Hn54fTp0/j1KlTUFMr+lds8eLF+Oeff/DFF1+gU6dOSElJkW4bP3486tevj/bt28PCwgKJiYk4cuQIRowYgYiICMyfP1/uuffy8sLly5fRq1cvqKurIzAwEF999RU0NDRw9epV7Ny5E71790aXLl1w6NAhzJs3D1paWpgxY4bMOJs3b8bYsWOhqakJd3d3WFtb48GDB9i4cSMOHTqES5cuwdbWFgYGBpg7dy62bNmCp0+fYu7cudIx3i7Srl+/jm7duiEpKQndu3fHl19+icTERBw4cADt2rXD/v370atXr1KdI0Up6xwBpfu85uTkoFu3bjh37hzq16+PCRMmIDMzE/v27cOgQYNw48YN+Pv7FznGw4cP0bp1azg5OWHo0KFIT0+Hs7Nzic67op/N1atX4+DBg3B3d4ebmxsuX76MvXv34ubNmwgLC0O1atVkzkHHjh3x9OlTNGvWDOPHj0dBQQHu3buHpUuX4uuvv0b16tUBAE+fPkWHDh3w5MkTtG/fHj179kR6ejoOHz6MHj16YO3atfjqq6+kYw8fPhx79+5Fw4YNMXz4cGhpaeHFixc4f/48goODi/zbQkSlIBDJAUAAIMydO7fYX/r6+gIA4fHjx9L9rl69KgAQ+vXrV2TM2bNnCwCE9evXS9seP34sPdYPP/wg0z80NFRQU1MTDAwMhFevXknbN2/eLAAQ+vfvL2RlZcnsM3fuXAGAsHTpUrn5DBs2TMjNzS0S24gRIwQAgpGRkfDkyRNpe35+vvDll18KAAQ/Pz+ZfR49eiQUFBQUGcvHx0cAIOzatUtubNra2sL169eL7CcIgvDw4cMibdnZ2UKHDh0ENTU14dmzZzLb3NzcBABC8+bNheTkZJnY1NXVBX19fcHOzk54/vy5dFtKSopgbGwsGBsby5yLe/fuCerq6oKDg4Pw4sULmeOcPHlSUFFRETw8POQeX57c3Fyhdu3agqampnDu3DmZbdHR0YKlpaVgZmYm8x6W5BwVp/A93Lx5s9wYlXGOFPm8/vrrrwIAoXfv3jJjxcbGCjY2NgIAmfPz9jF8fHzk5vq+816YmyKfTV1dXSEsLExm26BBgwQAwu7du2XaXV1dBQDC/PnzixwnISFB5n11c3MTJBKJsHfvXpl+ycnJQqNGjQRNTU0hJiZGEIQ3514ikQjNmjUT8vLyioydmJhYbN5E9GEs/Eiuwh88Jfn1duEnCILQokULQV1dXYiNjZW25eXlCZaWloKurq6Qnp4ubS/8Iaevry+kpqYWiaPwh/mWLVukbY0bNxbU1dVlfoi/fRwjIyOhefPmRfLR0NAQ4uLi5OZbeJx3iztBePNDVEVFRbCzs5O777sSExMFAMLIkSNl2gt/uE6ePLlE47wtMDBQACAEBATItBcWACdPniyyT8eOHQUAwp9//llk28iRIwUAMkXulClTBADCkSNH5MbQp08fQUVFRaaoeV8BcuDAAQGAMG3aNLnbly1bJgAQDh8+LG0ryzn6UOGnjHOkyOe1du3agkQiEe7du1ek//r164t8VgqPYWZmJmRnZ8vN9UOFX3E+9Nn86aefiuxz6tQpAYDw/fffS9sK/4PXuHFjIT8//73HvHnzpgBAGDBggNzthZ+TlStXCoIgCKmpqQIAwdXVVW7xSkRlw0u99F5CMffqAW8uLT19+rRI+zfffIORI0di06ZN8PHxAQAcOnQIL168wPjx46WXf97WtGlT6OrqFmnv0KEDAgICcOPGDYwYMQKZmZm4desWjI2NsWzZMrlxVatWDREREXLjfffS7rvc3NyKtNnb28PGxgZPnjxBSkoKDAwMAAAZGRlYvnw59u/fj/v37yMtLU3mfEVHR8s9RqtWrYo9flRUFPz9/XHy5ElERUUVuR+ruDHfvXQOAJaWlh/c9vz5c9SsWRMAcPHiRQDAmTNncOXKlSL7xMfHo6CgAA8ePJA75rsKx3vy5Al8fX2LbH/w4AEAICIiAp9//rnMtvedI0Up4xwVKunnNS0tDY8ePYK1tTUcHR2L9O/SpQuAN5fE39WoUSOZS6uloehns3nz5kXabGxsALy5h7fQpUuXAADdu3eHisr7bxUv/BykpKTI/RwkJCQAgPTvrK6uLr744gscOnQITZo0Qb9+/dCuXTu0atUK2tra7z0WEX0YCz9SuoEDB+L777/Hxo0bMXPmTEgkEqxbtw4A8PXXX8vdx8zMTG67ubk5AODVq1cA3vzwEQQBCQkJmDdvXqniKhzrfd4Xx9OnT/Hq1SsYGBggNzcXnTp1wpUrV9CwYUMMHDgQJiYm0vsK582bJ/chk/fFERkZiZYtWyI5ORmfffYZunXrBn19faiqquLJkycICAgodkx9ff0ibYX3cL1vW25urrTt5cuXAIBFixbJPUah9PT0925/d7x9+/aVerySvFelpYxzVKikn9fC34vLx8LCQqafvLFKqyyfzfedh/z8fGlb4T2XVlZWH4yn8HNw4sSJ9z6Y8fbnYM+ePfD398fOnTsxZ84cAICmpiY8PT2xePFimJiYfPC4RCQfCz9SOi0tLXh7e2PJkiU4ceIEHB0dcfz4cbRu3RouLi5y94mLi5PbHhsbC+D/fiAV/t6kSRO5syTvU5IFb+Pi4uDk5PTBOIKCgnDlyhWMGDGiyILBMTEx7y1Ki4tjyZIlePnyJTZv3gxvb2+Zbbt27UJAQMAH4y+LwtxevXoFPT09pY0XFBQEd3f3Uu1b1RcnLu3ntbD9XTExMTL93qboOSjLZ7OkCme9i5s5fFthbsuXL8ekSZNKNL6WlhZ8fX3h6+uLZ8+e4ezZs9iyZQu2bt2KJ0+eSJ9qJqLS43IuVC7Gjx8vnenbsGEDCgoKMG7cuGL7X79+HWlpaUXaz5w5A+BNoQcAOjo6aNCgAe7cuYOkpCSlxy3vB0pkZCSePXsGOzs76Q+8hw8fAgD69etXojFKojzGLI3WrVsDAM6dO1fifVRVVQHIzgaVZbyPRUk/r7q6uqhduzaio6Oll7bfdvr0aQBvLh2XxvvOe0V8jgrf2xMnTrz3dpC3+yr6ObCxscGQIUMQHBwMBwcHnD17tlz+7hOJBQs/Khd16tRB165dcfDgQaxfvx4GBgZFlmR526tXr+Dn5yfTdvXqVezYsQP6+vro27evtP27775DTk4ORo0aJXeZj+Tk5FLPBhZavny5zH2LBQUFmDZtGgoKCjBy5Ehpe+HSGYU/uAtFRkbKXf6jJIobMzg4GBs3blRozNKYOHEi1NXVMXXqVNy/f7/I9pycnCI/vI2MjAC8WarnXR4eHqhduzZWrVqFv//+W+4xL168iMzMTCVEX7FK83kdNWoUBEHAtGnTZAq1xMRE/Pzzz9I+pfG+814en813NWvWDK6urrh+/ToWL15cZPvLly+RnZ0N4M19g5999hn++usvbNq0Se54t2/fRnx8PIA39/xdvny5SJ+MjAykpaVBVVVV7lI0RFQy/NtD5Wb8+PE4fvw4EhMTMWnSJGhpaRXbt3379ti4cSMuX76Mtm3bStdFKygowLp162QuPY4aNQrXrl3D6tWrUbt2bXTv3h22trZISkrC48ePcfbsWYwcORJr164tdczt2rVD48aNMXDgQOjr6yM4OBi3bt1Cs2bNMH36dGm/L774AnXq1MHSpUsRHh6OJk2aICoqCocPH8bnn3+OqKioUh/7m2++webNm+Hp6Yl+/frBysoK4eHhOHbsGDw9PbFnz55Sj1kadevWxaZNmzBq1Cg0aNAAPXr0gKOjI3JzcxEVFYVz587BxMRE5sGZzp07Y9++ffjyyy/Rs2dPaGlpoWbNmhg2bBjU1dXx119/oXv37vj888/h6uqKxo0bQ1tbG8+ePUNoaCgiIyMRExPz0d20X5rP6w8//ICjR48iKCgIjRo1Qq9evaTr+MXHx2P69Olo165dqY7/vvNeHp9NebZv344OHTpg+vTp2Lt3L9zc3CAIAh48eIDjx48jIiJCWoTu3LkTnTp1wujRo7FixQq0atUKBgYGeP78OcLCwhAeHo6LFy/C1NQU0dHRaN26NerVq4emTZvCxsYGqampOHz4MGJjYzFx4kSl3IpAJFqV+EQxVWH4/0u1vE/NmjXlLudSKC8vTzA2NhYACHfu3JHbp3DpihEjRgh3794V3N3dBQMDA0FLS0twdXUVjh07VuzxDx06JHz++eeCiYmJoK6uLpiZmQktWrQQfvzxR+Hu3btF8nFzcyt2rMJlOB49eiQsXrxYcHJyEqpVqyZYWloKkydPllnCpFBUVJQwePBgwdLSUtDU1BTq168v+Pv7C7m5uXKPV7hkxunTp4uN499//xU6duwoGBgYCDo6OkLbtm2F/fv3C6dPn5auq/i29y3rUZiTvPfnfbGEhYUJI0aMEGxtbQUNDQ3B0NBQaNCggfDVV18VWRIlLy9P8PHxEWrVqiWoqanJzTsuLk6YMWOG0KBBA0FLS0uoXr26UKdOHaFfv37Ctm3bZNa2K8k5Ks6HlnN53z4lPUeKfl6zsrKEX3/9VWjQoIGgqakpfW937txZpO/bxyjOh867Mj+b74snMTFRmD59uuDo6ChUq1ZN0NfXFxo1aiTMmjVLyMjIkOmbmpoq/Prrr0LTpk2F6tWrC5qamoKdnZ3Qq1cvYd26ddJlnpKTk4V58+YJHTt2FCwtLQUNDQ3B3NxccHNzE3bu3MklXojKSCIIH7hBg0hBjx49goODA9q1a4ezZ8/K7fPkyRPUqlVL7o3oFcnb2xsBAQF4/Phxmb4ejD5tVeXzSkSkKN7jR+Vm0aJFEAQBEydOrOxQiIiICLzHj5Ts6dOn2LZtGx48eIBt27ahSZMm6N+/f2WHRURERGDhR0r2+PFjzJ49G9WrV0f37t2xZs2aD67sT0RERBWD9/gRERERiQSnYoiIiIhEgoUfERERkUiw8CMiIiISCRZ+RERERCLBp3qpiOTkZOTl5VV2GOXOxMQECQkJlR1GuRNLnoB4chVLnoB4chVLnoB4cq3IPNXU1GBoaFiyvuUcC32E8vLykJubW9lhlCuJRALgTa6f8oPtYskTEE+uYskTEE+uYskTEE+uVTlPXuolIiIiEgkWfkREREQiwcKPiIiISCRY+BERERGJBAs/IiIiIpFg4UdEREQkEiz8iIiIiESChR8RERGRSLDwIyIiIhIJFn5EREREIsHCj4iIiEgkWPgRERERiQQLPyIiIiKRYOFHREREJBISQRCEyg6CqpbBG64gIja9ssMgIiIqN4dH1y23sSUSCSwsLBATE4OKKLPU1dVhYmJSor6c8SMiIiISCRZ+RERERCLBwo+IiIhIJFj4EREREYkECz8iIiIikWDhR0RERCQSLPyIiIiIRIKFHxEREZFIsPAjIiIiEgkWfkREREQiwcKPiIiISCRY+BERERGJBAs/IiIiIpFg4UdEREQkEiz8iIiIiESChR8RERGRSLDwIyIiIhIJFn5EREREIsHCj4iIiEgkWPgRERERiQQLPyIiIiKRYOFHREREorZlyxa0bt0a9vb26NGjBy5fvlxs3ylTpsDKyqrIr44dO8r0S0lJgY+PD5o0aQJ7e3u4ubnh5MmT5Z3KB1V64efr64stW7ZUdhjYu3cvpk2bVtlhEBERUQUKCgqCr68vJk2ahODgYLRs2RJDhw5FdHS03P5+fn64ceOG9FdoaCgMDAzQu3dvaZ+cnBx07doVz58/x/r163H27FksWrQI5ubmFZVWsdQqO4Cqwt3dHT179qzsMEpk1apVyMjIwPTp0ys7FCIioo/ahg0b4OXlhcGDBwN4U9iFhIRg69at8PHxKdJfT08Penp60tfHjh3Dq1evMHDgQGnb7t27kZSUhP/9739QU3tTallbW5dzJiVT6TN+5S0vL69E/TQ1NaGrq1vO0bxfSWMlIiKissvJyUFYWBjc3Nxk2t3c3HD16tUSjbFr1y589tlnMoXd8ePH0aZNG8yaNQuNGjVCp06dsGLFCuTn5ys1fkVUqRm/vLw87N69G+fOnUNmZiZsbGwwZMgQNGjQAACQlpaGP//8ExEREUhPT4eZmRn69u2Ldu3aScfw9fWFjY0N1NTUcPbsWVhbW8PT0xPz5s3D7NmzsWPHDjx//hx2dnb45ptvYGlpCeDNpd7Q0FAsWrQIwP/NqtWtWxeHDx9GXl4eXF1d4e3tLa3ek5OTsXbtWoSHh8PAwACDBg3Crl270KtXL3z++ecfzNfT0xNjxozBzZs3cfv2bXzxxRfo378/1q1bh/DwcKSkpMDY2Bjdu3dHr169pHGGhIRI9weAuXPnokGDBkhKSkJAQADCwsIgkUhQt25deHt7w9TUVEnvEBER0acjKSkJ+fn5MDY2lmk3NjZGfHz8B/ePi4vD6dOnsXLlSpn2p0+f4t9//0Xfvn2xbds2PH78GLNmzUJ+fj6mTp2q1BxKq0oVfqtXr0ZCQgKmTJkCQ0NDXLlyBfPnz8fixYthYWGB3Nxc2Nvbo0+fPtDS0sL169excuVKmJmZwcHBQTpOSEgIunXrhp9//hmCICAlJQXAm6nX4cOHQ09PDxs2bMCaNWvw888/FxvPnTt3YGhoiLlz5yI2NhbLli2DnZ0dunTpAgBYuXIl0tLS4OvrC1VVVWzduhWvXr0qVc779u3DoEGDMGLECKioqKCgoABGRkaYOnUq9PT0cO/ePaxfvx4GBgZwdXWFu7s7oqOjkZWVhW+++QYAoKOjg9evX2PevHmoW7cu5s2bBxUVFfz111/S81dYrL4tNzcXubm50tcSiQRaWlqlip+IiOhjJJFIIJFIAAAqKirSP8vbXpx9+/ZBT08PPXv2lOkrCAJMTU2xePFiqKiooFGjRoiLi8OaNWvw3XffKT+ZUqgyhV9sbCz+/fdfrFmzBjVq1ADw5r67W7du4fTp0xg8eDBq1KgBd3d36T49e/bEzZs3cfHiRZnCz9zcHEOHDpW+Liz8vLy8UL9+fQCAh4cHfvvtN+Tk5EBDQ0NuTDo6Ohg9ejRUVFRgZWWFJk2aIDw8HF26dEF0dDRu376NBQsWoHbt2gCAr7/+GpMmTSpV3m3btkWnTp1k2gpn8gDA1NQU9+7dw8WLF+Hq6gpNTU1oaGggNzcXBgYG0n5nz56FRCLB119/Lf3wffPNN/D29sadO3fQqFGjIsfev38/AgMDpa9r1aoFf3//UsVPRET0MbKwsICRkRFUVVWRl5cHCwsL6basrCxYWVnJtL1LEATs27cPI0aMQM2aNWW2WVtbQ11dHVZWVtK2Vq1aYd68eTAyMiq27qgIVabwe/z4MQRBwOTJk2Xa8/LyoKOjAwAoKCjAgQMHcOHCBSQlJSE3Nxd5eXmoVq2azD729vZyj/H2G2NoaAgASE1NLTLFW8ja2hoqKioy+0RFRQEAXrx4AVVVVdSqVUu63dzcHNWrVy9pygAgLRrfdvz4cZw6dQoJCQnIyclBXl4e7Ozs3jtOZGQkYmNjMXz4cJn23NxcxMXFyd2nb9++Mk8hfeh/NkRERJ+KmJgYAICLiwuCgoLQunVr6bajR4+ie/fu0j7yXLhwAQ8fPoS7u3uRfo0aNcLBgwfx4sUL6c/Wq1evwszMDC9fvlR6LmpqajAxMSlZX6UfXUGCIEBFRQX+/v4yxRbw5sELADh06BCOHDmCESNGwNbWFpqamtiyZUuRhyIK+79LVVVV+ufCN6KgoKDYmN7uX7iPIAjSeJXh3aL1woULCAgIwPDhw+Ho6AgtLS0cPHgQDx48eO84giDA3t5e7ozj208fvU1dXR3q6uqKB09ERPSRKvw5PnbsWEyePBkuLi5o1qwZtm/fjujoaAwbNgyCIGDBggWIiYnBihUrZPbfuXMnmjRpAicnpyI1wfDhw7F582b89NNPGDlyJB4/fowVK1Zg1KhRSqsfFFVlCj87OzsUFBTg1atXqFevntw+d+/eRfPmzdG+fXsAb4q2mJgYmanUimJlZYX8/Hw8efJEOsMYGxuLjIyMMo0bEREBJycndO/eXdr27oydmppakYK1Vq1auHDhAvT09KCtrV2mGIiIiMTCw8MDycnJWLp0KeLj4+Hk5IRt27ZJn9KNi4vDixcvZPZJTU3F33//DT8/P7ljWllZ4fjx45g4cSK6du0Kc3NzjB49GhMmTCj3fD6kyhR+lpaWaNeuHVauXInhw4ejVq1aSE1NRXh4OGxtbdG0aVOYm5vj8uXLuHfvHqpXr47Dhw8jJSWl0go/Z2dnrFu3DmPHjpU+3KGhoVGmS6bm5uYICQnBzZs3YWpqirNnz+Lhw4cyT+aamJjg1q1bePHiBXR0dKCtrY3PPvsMhw4dwqJFi+Dp6QkjIyMkJibi8uXLcHd3h5GRkTLSJiIi+uR4e3vD29tb7rZly5YVadPT08OjR4/eO2abNm1w+PDhSp/he1eVKfyANw8j/PXXX9i6dSuSkpKgq6sLR0dHNG3aFADQv39/xMfH49dff0W1atXQuXNntGjRApmZmZUS78SJE7F27VrMnTtXupzL8+fPy3T5tGvXrnjy5AmWLVsGiUSCtm3bonv37rhx44a0T5cuXfDff/9h5syZyM7Oli7nMm/ePGzfvh2LFy9GdnY2atSogYYNG/JJXSIiIgIASISqVop+xF6+fInx48dj9uzZcHZ2ruxwFDZ4wxVExKZXdhhERETl5vDouuU2tkQigYWFBWJiYipkxk9dXf3je7jjYxQeHo7s7GzY2toiOTkZ27dvh4mJSbH3KBIRERFVJhZ+ZZCXl4ddu3YhLi4OWlpacHR0xKRJk6CmpoZz585h/fr1cvczMTHBkiVLKjhaIiIiEjsWfmXQuHFjNG7cWO625s2byywq/bZ3l4khIiIiqggs/MqJlpYWH6ogIiKiKkXlw12IiIiI6FPAwo+IiIhIJFj4EREREYkECz8iIiIikWDhR0RERCQSLPyIiIiIRIKFHxEREZFIsPAjIiIiEgkWfkREREQiwcKPiIiISCRY+BERERGJBAs/IiIiIpFg4UdEREQkEiz8iIiIiESChR8RERGRSLDwIyIiIhIJFn5EREREIqFW2QFQ1bO8Ty3k5uZWdhjlSiKRwMLCAjExMRAEobLDKTdiyRMQT65iyRMQT65iyRMQV65VFWf8iIiIiESChR8RERGRSLDwIyIiIhIJFn5EREREIsHCj4iIiEgkWPgRERERiQQLPyIiIiKRYOFHREREJBIs/IiIiIhEgoUfERERkUiw8CMiIiISCRZ+RERERCLBwo+IiIhIJNQqOwCqeiYfeIyI2PTKDqMC3MWRMfUqOwgiIqIKwxk/IiIiIpFg4UdEREQkEiz8iIiIiESChR8RERGRSLDwIyIiIhIJFn5EREREIsHCj4iIiEgkWPgRERERiQQLPyIiIiKRYOFHREREJBIs/IiIiIhEgoUfERERkUiw8CMiIiISCRZ+RERERCLBwo+IiIhIJFj4EREREYkECz8iIiIikWDhR0RERCQSLPyIiIiIRIKFHxEREZFIsPAjIiIiEgkWfkT/35YtW9C6dWvY29ujR48euHz5crF94+LiMGHCBHz22WewtrbGnDlz5PY7cuQIOnTogFq1aqFDhw44evRoeYVPRET0QSz8ytGZM2fg7e1dIcdatWoVFi5cWCHH+hQFBQXB19cXkyZNQnBwMFq2bImhQ4ciOjpabv+cnBwYGRlh0qRJqF+/vtw+V69exfjx49GvXz+cOHEC/fr1w9dff43r16+XZypERETFYuH3kYmPj4enpyeePHlS2aF8UjZs2AAvLy8MHjwYDg4O8PPzg6WlJbZu3Sq3v42NDfz8/DBgwADo6enJ7bNx40a0b98e3377LerUqYNvv/0W7dq1w8aNG8szFSIiomKx8CPRy8nJQVhYGNzc3GTa3dzccPXqVYXHvXbtGtq3b6/UMYmIiMpCrbIDUJSvry9sbW2hoqKCkJAQqKmpYeDAgWjXrh02bdqES5cuQV9fH6NGjUKTJk1QUFCAdevWITw8HCkpKTA2Nkb37t3Rq1cvAG9++M+cORNOTk4YN24cgDeza9OmTcOwYcPQpUuXD8Z05swZ7NmzB2lpaWjUqBHq1q1bpM/Vq1exb98+PH/+HIaGhnBzc8OXX34JVVVVAICnpyfGjBmDq1ev4s6dOzAwMMDQoUPRpk0bAMDEiRMBANOnTwcA1K9fH76+vtLxDx48iMOHDyMvLw+urq7w9vaGmtpH+zZXiKSkJOTn58PY2Fim3djYGPHx8QqPm5CQABMTE5k2ExMTJCQkKDwmERFRWXzUFUFISAjc3d0xf/58XLhwARs2bEBoaChatGiBvn374siRI1i5ciVWr14NVVVVGBkZYerUqdDT08O9e/ewfv16GBgYwNXVFRoaGpg0aRJmzZqFJk2aoHnz5vjjjz/QoEGDEhV9Dx48wJo1azBo0CC0bNkSN2/exL59+2T63Lx5E3/88QdGjhyJevXqIS4uDuvWrQMADBgwQNpvz549GDx4MLy9vXH27FksX74cNjY2sLa2xvz58zFr1izMnj0bNjY2MkXdnTt3YGhoiLlz5yI2NhbLli2DnZ1dsfHn5uYiNzdX+loikUBLS6tU78HHTiKRQCKRAABUVFSkf5a3vaTjfKi9pGMqQ+FxKup4lUksuYolT0A8uYolT0A8uVblPD/qwq9mzZro168fAKBv3744cOAAdHV1pYVO//79cfz4cTx9+hSOjo7w9PSU7mtqaop79+7h4sWLcHV1BQDY2dnBy8tLOjMYFxeHadOmlSiWv//+G40aNUKfPn0AAJaWlrh//z5u3rwp7bN//3706dMHHTp0AACYmZlh4MCB2LFjh0zh17p1a3Tu3BkA4OXlhdu3b+PYsWMYM2aM9H4yXV1dGBgYyMSgo6OD0aNHQ0VFBVZWVmjSpAnCw8OLLfz279+PwMBA6etatWrB39+/RPl+KszNzVGjRg2oqqoiLy8PFhYW0m1ZWVmwsrKSaZNHQ0MD1atXL9LP3Nwcr1+/lmnPycmBmZnZB8dUNnNz8wo9XmUSS65iyRMQT65iyRMQT65VMc+PuvCztbWV/llFRQW6uroybfr6+gCA1NRUAMDx48dx6tQpJCQkICcnB3l5ebCzs5MZs3fv3ggNDcWxY8cwa9asYm/cf1d0dDRatmwp0+bo6ChT+EVGRuLhw4f466+/pG0FBQXIzc3F69evUa1aNel+b3NwcMDTp08/GIO1tTVUVP7vtk1DQ0NERUUV279v377o3bu39HVV/J9JeYuNjYUgCHBxcUFQUBBat24t3Xb06FF0794dMTEx7x0jJycHGRkZRfo1btwYhw8fhpeXl7Tt0KFDaNKkyQfHVBaJRAJzc3Npnp8yseQqljwB8eQqljwB8eRa0XmqqakVubWo2L7lHEu5evfeNYlEIr1XrvA18Ka4unDhAgICAjB8+HA4OjpCS0sLBw8exIMHD2TGSE1NxYsXL6CiooKYmBg0bty4RLGU5I0tKCiAp6cnWrVqVWSburp6iY7zPm/nDrzJ/31xqaurK+W4HzNBECAIAsaOHYvJkyfDxcUFzZo1w/bt2xEdHY1hw4ZBEAQsWLAAMTExWLFihXTf8PBwAEBGRgZevnyJ27dvQ0NDQ1q4jx49Gv369cPKlSvRvXt3BAcH49y5c9i/f3+F/4NXmKcYiCVXseQJiCdXseQJiCfXqpinQoVfTk4Ozp49i7p168La2lrZMZWLiIgIODk5oXv37tK2uLi4Iv3WrFkDW1tbdO7cGWvWrIGzs3OJcrS2ti5SRN6/f1/mtb29PV68ePHBqd8HDx7IPGH64MED1KpVC8D/FbsFBQUfjIlKzsPDA8nJyVi6dCni4+Ph5OSEbdu2Sd/7uLg4vHjxQmaftz9LYWFh2L9/P6ytraULP7do0QKrV6/GwoULsWjRItSsWRNr1qxB06ZNKy4xIiKityhU+GloaGDz5s348ccflR1PuTE3N0dISAhu3rwJU1NTnD17Fg8fPoSpqam0z7Fjx3D//n0sWrQIxsbGuHHjBlasWIH58+d/8MnYnj17Yvbs2QgKCkKLFi0QFhaGW7duyfTp168f/P39YWRkhDZt2kAikSAqKgpRUVEylwMvXrwIe3t71K1bF+fPn8fDhw8xfvx4AG8uX2toaODmzZuoUaMGNDQ0oK2trcQzJV7e3t7FLri9bNmyIm3FLe78tt69e8tcTiciIqpMCq/jZ2pqipSUFCWGUr66du2KVq1aYdmyZfjxxx+Rnp4uM2MTHR2N7du3Y/To0dJlPUaPHo2MjAzs3r37g+M7Ojpi3LhxOHbsGKZPn45bt27hyy+/lOnTuHFjzJgxA7dv34aPjw9+/PFHHD58uMgyIp6enrhw4QKmTZuGkJAQTJo0STrzpKqqipEjR+LEiRMYN24cv62DiIiISkwiKHjx+cSJEzhx4gR8fX0546REnp6e+OGHH4o8KFKRBm+4gojY9Eo7fkU6MqZelbv/QpkkEgksLCwQExPzSecJiCdXseQJiCdXseQJiCfXis5TXV29/B/uePbsGdLS0jBhwgQ0bNgQhoaGMtslEglGjhyp6PBEREREpGQKF37BwcHSP1+5ckVun0+p8Js/fz7u3r0rd1vfvn2LXNYlIiIiqmoULvz27NmjzDiqvK+//ho5OTlyt+no6CjtOHv37lXaWERERERv+6jX8atINWrUqOwQiIiIiMqkzIXfzZs38d9//yE1NRX9+/eHsbGxdJmUkn7rBRERERGVP4ULv9evX2PhwoXSby8AgG7dusHY2BiHDh2CkZERhg8frpQgiYiIiKjsFF7Hb9euXYiMjMT333+PgIAAmW2NGjXC7du3yxwcERERESmPwjN+ly5dwsCBA9GyZcsiXx9mbGyMxMTEMgdHRERERMqj8Ixfampqsd9hK5FIin0CloiIiIgqh8KFX40aNRAVFSV329OnT2W+A5eIiIiIKp/ChV/Lli2xf/9+PH78WNomkUiQkJCAI0eOoE2bNkoJkIiIiIiUQ+F7/AYMGIDw8HDMmjULNjY2AIDVq1cjLi4OlpaW6NOnj7JiJCIiIiIlULjw09LSwi+//IK///4b169fh7m5OapVq4Y+ffrg888/h4aGhjLjJCIiIqIyKtMCzhoaGujTpw9n94iIiIg+Agrf4zdx4kQ8efJE7raoqChMnDhR0aGJiIiIqBwoXPglJCQgLy9P7rbc3FwkJCQoHBQRERERKZ/Chd/7xMXFQUtLqzyGJiIiIiIFleoevzNnziAkJET6euPGjUUKvJycHDx9+hT169dXToREREREpBSlKvxycnKQmpoqfZ2RkYHc3FyZPurq6nB1dYWnp6dyIiQiIiIipShV4detWzd069YNADBhwgR8//33sLOzK4+4iIiIiEjJFF7OZdWqVcqMg4iIiIjKWZnW8cvNzcWZM2dw584dpKWlYcyYMbCwsEBoaChsbW1hZmamrDipAi3vU6vIJfxPjUQigYWFBWJiYio7FCIiogqjcOGXmpqKefPm4fnz5zAwMEBKSgqysrIAAKGhobh16xbGjBmjtECJiIiIqGwUXs5l+/btyMzMxIIFC7B69WqZbQ0aNMB///1X5uCIiIiISHkULvyuX78OT09P2NvbQyKRyGwzMjLCy5cvyxwcERERESmPwoVfVlYWTExM5G7Ly8tDQUGBwkERERERkfIpXPiZmpri/v37crc9fPgQlpaWCgdFRERERMqncOHXrl07BAUFITQ0FIIgAHjzpOTDhw9x9OhRfPbZZ0oLkoiIiIjKTuGnej08PHDv3j0sXrwY1atXBwD8+uuvSEtLQ+PGjdGrVy+lBUlEREREZadw4aempgYfHx9cuHAB169fx6tXr6Crq4tmzZrB1dUVKioKTyYSERERUTko0wLOEokEbdu2Rdu2bZUVDxERERGVE07LEREREYmEwjN+BQUFOHr0KM6fP4+EhAS5X/EVEBBQpuCIiIiISHkULvx27NiBw4cPw87ODi4uLlBTK9NVYyIiIiIqZwpXa+fPn4eHhwcGDx6szHiIiIiIqJwoXPjl5OTAxcVFmbFQFTH5wGNExKZXdhglcnh03coOgYiI6KOh8MMdLi4uePDggTJjISIiIqJypPCM38iRI/Hbb7+hWrVqaNq0KXR0dIr0kddGRERERJVD4cJPW1sblpaWCAgIKPbp3T179igcGBEREREpl8KF3/r163Hx4kW0aNECVlZWfKqXiIiIqIpTuFoLDQ3FoEGD4O7ursx4iIiIiKicKPxwh5qaGmrVqqXMWIiIiIioHClc+LVs2RK3bt1SZixEREREVI4UvtTbtm1brFu3Dnl5ecU+1Wtvb1+m4IiIiIhIeRQu/H7++WcAwNGjR3H06FG5ffhULxEREVHVoXDhN378eGXGQURERETlTOHCr0OHDkoMg4iIiIjKm8IPdxARERHRx6VMqy6np6fj/PnzeP78OXJycmS2SSQSXg4mIiIiqkIULvwSExPh4+OD169f4/Xr19DT00N6ejoKCgpQvXp1aGtrKzNOIiIiIiojhS/17tixA9bW1tiwYQMAwMfHB9u2bcPIkSOhrq6OmTNnKi1IIiIiIio7hQu/+/fvo1u3blBXV5e2qampoUePHujUqRO2b9+ulACJiIiISDkULvxevXoFQ0NDqKioQEVFBZmZmdJt9evXR0REhFICJCIiIiLlULjw09fXR3p6OgDAxMQEkZGR0m0JCQlQVVUte3REREREpDQKP9zh4OCAx48fo3nz5mjZsiUCAwORm5sLNTU1HDx4EA0aNFBmnERERERURgoXfu7u7oiPjwcA9O/fH9HR0di7dy8AoF69ehg5cqRyIiQiIiIipVC48LO3t4e9vT0AQFNTEzNmzEBmZiYkEgm0tLSUFiARERERKYdC9/jl5ORg3LhxuHr1qky7trY2iz6qNFu2bEHr1q1hb2+PHj164PLly+/tHxISgu7du8Pe3h5t2rTB1q1bZbb3798fVlZWRX4NGzasPNMgIiIqNwoVfhoaGsjJyYGmpqay46kyJkyYgCNHjlR2GFRCQUFB8PX1xaRJkxAcHIyWLVti6NChiI6Olts/KioKvXr1QqtWrRAcHIxvv/0Wc+bMkXnPN2zYgBs3bkh/nTp1Cqqqqujdu3dFpUVERKRUCj/V6+zsjLCwMGXGUinOnDkDb2/vIu0LFixAly5dyv34LDCVY8OGDfDy8sLgwYPh4OAAPz8/WFpaFpnFK7R161bY2trCz88PDg4OGDx4MAYOHIi1a9dK+xgaGsLU1FT66+zZs9DS0sIXX3xRUWkREREplcKFX9++fXHhwgUEBgYiKioKaWlpSE9Pl/n1MdPT00O1atUqO4wSy8vLq+wQKk1OTg7CwsLg5uYm0+7m5lbkdoRC165dQ7du3WTaOnTogLCwMOTm5srdZ/fu3fDw8ODXERIR0UdL4Yc7Cr+Sbd++fdi3b5/cPnv27CnxeL6+vrC1tYWGhgZOnjwJNTU1dO3aFZ6enh/cNzMzE9u2bUNoaChyc3Nhb2+PESNGwM7ODgDw5MkTBAQE4NGjR5BIJDA3N8dXX32F7OxsrF69GgCkx+nfvz88PT0xYcIE9OrVC59//rl0+9ixY3Ht2jWEh4fDxMQE48ePh56eHtauXYtHjx7B1tYW3377LczNzQEAsbGx2Lp1Kx48eIDs7GxYW1tj0KBBcHFxkeackJCAgIAABAQEAID0yehLly5h7969iI2NhaGhIXr06CEz0zRhwgR06tQJsbGxuHLlClq0aIGvv/4aAQEBuHz5MjIyMmBgYIAuXbqgb9++JX4fPkZJSUnIz8+HsbGxTLuxsbH0yfN3JSQkwMzMrEj/vLw8JCUlFdl248YNREREYPHixcoNnoiIqAIpXPj169cPEolEmbEgJCQEvXv3xvz583H//n2sXr0adevWlRZK8giCgAULFkBHRwc+Pj7Q1tbGiRMn8PPPP2P58uXQ0dHBH3/8ATs7O4wZMwYqKip48uQJVFVV4eTkBG9vb+zZswfLly8HgPfet/i///0Pw4cPx/Dhw7Fjxw4sX74cZmZm6NOnD4yNjbFmzRps2rQJs2bNAgBkZ2ejSZMm8PLygrq6OkJCQuDv74/ly5fD2NgYP/zwA6ZNm4bOnTvLXFaOjIzE0qVLMWDAALi6uuL+/fvYuHEjdHV10aFDB2m/gwcPol+/fujXrx8A4O+//8bVq1cxdepUGBsb4+XLl0hMTCw2n9zcXJnZrY/xiWyJRCL9HKqoqBT5TL69/X37vk3eOLt370bdunXRtGlTJUVeMQrzUPbf1apILLmKJU9APLmKJU9APLlW5TwVLvxKMhNXWjVr1sSAAQMAABYWFjh27Bhu37793sLvzp07iIqKwsaNG6XfGzx8+HCEhobi0qVL6NKlCxITE/HFF1/AyspKOnYhbW1tSCQSGBgYfDC+Dh06wNXVFQDg4eGBn376Cf369UPjxo0BAL169ZLOIAKAnZ2ddNYRALy8vHDlyhVcvXoVPXr0gI6ODlRUVKClpSVz/MOHD8PZ2Rn9+/cHAFhaWuL58+c4ePCgTOHXsGFDuLu7S18nJibCwsICdevWhUQigYmJyXvz2b9/PwIDA6Wva9WqBX9//w+eh6rEwsICRkZGUFVVRV5ensx7m5WVBSsrK5m2QlZWVoiNjZXOzgJAQUEB1NTUUL9+fZnvoM7MzMTBgwfh5+cnd6yPwdt5furEkqtY8gTEk6tY8gTEk2tVzFPhwq882Nrayrw2NDTEq1ev3rtPZGQksrOzMWrUKJn2nJwcxMbGAgA+//xzrFu3DufOnYOzszNat26t0JtRs2ZN6Z8LC7W3Y9bX10dubi4yMzOhra2N7OxsBAYG4tq1a0hOTkZ+fj5ycnLeOwsHANHR0WjevLlMm5OTE44cOYKCggKoqLy5NbN27doyfTp06IBffvkFU6ZMQaNGjdCsWTM0atSo2OP07dtX5gnVqvg/kw+JiYkBALi4uCAoKAitW7eWbjt69Ci6d+8u7fM2FxcXnDhxArGxsRAEAQBw4MABNGrUqMj7s2fPHrx+/RpdunSRO1ZVVnhrw9t5fqrEkqtY8gTEk6tY8gTEk2tF56mmpvbByR5p37IcqKCgADdu3EB0dDRycnKKbC+csSopNbWi4XzohBUUFMDQ0BC+vr5FthXehO/p6Yl27drh+vXruHnzJvbu3YspU6agZcuWpYpP3vcPvx1zYeFUGPP27dtx69YtDBs2DObm5tDQ0MDvv//+wQcxBEEoUoTJOw/vPnxib2+PlStX4ubNmwgLC8PSpUvh7OyM77//Xu5x1NXVZWa2PkaF52Xs2LGYPHkyXFxc0KxZM2zfvh3R0dEYNmyY9HaAmJgYrFixAgAwbNgwbN68GXPnzsXgwYNx7do17Nq1C6tWrSpyrnft2oXu3bvD0NDwo/2HShCEjzb20hJLrmLJExBPrmLJExBPrlUxT4ULv7S0NMyZMwcvXrwotk9pCz9F2NvbIyUlBSoqKjA1NS22n6WlJSwtLdG7d28sW7YMp0+fRsuWLaGmpoaCgoJyie3u3btwc3OTFpjZ2dlISEiQ6SPv+NbW1oiIiJBpu3//PiwtLaWzfcXR1taGq6srXF1d0bp1a8yfPx/p6enQ0dFRQkZVl4eHB5KTk7F06VLEx8fDyckJ27Ztg7W1NQAgLi5O5rNqa2uLv//+GxMnTsSWLVtgZmYGPz8/6cM8hR49eoQrV65g165dFZoPERFReVC48Nu1axc0NDSwatUqTJgwAb/++it0dHRw4sQJXL9+HbNnz1ZmnMVydnaGo6MjFi1ahCFDhsDS0hLJycm4ceMGWrRoARsbG2zbtg2tW7eGqakpXr58iUePHqFVq1YAABMTE2RnZ+P27duoWbMmqlWrprRlXMzNzXHlyhXpZds9e/YUqfxNTExw9+5dtG3bFmpqatDT00Pv3r3h4+ODwMBA6cMdx44dw5gxY957vMOHD8PQ0BB2dnaQSCS4dOkSDAwMRLP8iLe3t9w1GQFg2bJlRdrc3Nxw/Pjx9/5vrHbt2sUuAk1ERPSxUbjwCw8PR//+/VGjRg0Ab56ENDc3x7Bhw5Cbm4utW7diypQpyoqzWBKJBD4+Pti1axfWrFmD1NRUGBgYoF69etDX14eKigrS0tKwcuVKvHr1Crq6umjVqpX04RQnJyd07doVy5YtQ1pamnQ5F2UYMWIE1qxZg59++gm6urrw8PBAVlaWTB9PT09s2LAB3377LXJzc7F3717Y29tj6tSp2Lt3L/73v//B0NAQnp6eMg92yKOpqYmgoCDExMRARUUFderUgY+PzwdnCYmIiEgcJIKCF5+HDBmC2bNno27duvDy8sKcOXNQv359AMCtW7ewYsUK/Pnnn0oNlirG4A1XEBH7cSzAfXh0XYX2k0gksLCwQExMTJW7/0KZxJInIJ5cxZInIJ5cxZInIJ5cKzpPdXX1Ej/cofBUkJ6eHjIzMwG8efr22bNn0m3p6enIz89XdGgiIiIiKgcKX+qtVasWnj17hqZNm6JJkyYIDAyElpYW1NTUsGvXLjg4OCglwHPnzmH9+vVyt5mYmGDJkiVKOQ4RERHRp07hwq9Hjx6Ii4sD8GZh4gcPHmDVqlUAADMzM4wcOVIpATZv3rzYIlLe8ipEREREJJ/Chd/b36ahp6eHhQsXSi/3WllZKa0o09LS+ui+RoyIiIioKlLaN3dIJJIi37xBRERERFVHmQq/zMxMBAcH486dO0hLS4Ouri4aNGiAbt26oXr16sqKkYiIiIiUQOHCLz4+HvPmzUNiYiKMjY1hYGCAmJgY3L59GydOnMDcuXNhZmamzFiJiIiIqAwULvw2b96MnJwc/Pzzz3B0dJS237t3D4sXL8aWLVswY8YMpQRJRERERGWn8Dp+4eHhGDRokEzRB7z5JgwvLy+Eh4eXOTgiIiIiUh6FCz91dXUYGRnJ3WZsbAx1dXWFgyIiIiIi5VO48GvevDkuXrwod9vFixfRtGlThYMiIiIiIuVT+B6/du3aYe3atViyZAnatWsHAwMDpKSk4Ny5c4iMjMTXX3+NyMhIaX97e3ulBExEREREilG48Pv1118BAC9fvsTly5eLbP/ll19kXu/Zs0fRQxERERGREihc+I0fP16ZcRARERFROVOo8CsoKICjoyP09fW5UDMRERHRR0KhhzsEQcB3332H+/fvKzseIiIiIionChV+qqqqMDAwgCAIyo6HiIiIiMqJwsu5uLq6IiQkRJmxEBEREVE5UvjhDjs7O1y8eBHz5s1Dq1atYGBgAIlEItOnVatWZQ6QiIiIiJRD4cJv1apVAICkpCT8999/cvtwCRciIiKiqkPhwm/u3LnKjIOIiIiIypnChV/9+vWVGQdVIcv71EJubm5lh0FERERKpnDhVygzMxP3799HWloamjRpAh0dHWXERURERERKVqbCLzAwEEFBQcjJyQEALFiwADo6OvDz84OLiwv69OmjjBiJiIiISAkUXs4lODgYgYGB6NixI2bOnCmzrWnTprh+/XqZgyMiIiIi5VF4xu/YsWPo3bs3hg4dioKCApltFhYWiImJKXNwRERERKQ8Cs/4xcfHo1GjRnK3aWlpITMzU+GgiIiIiEj5FC78tLW18erVK7nb4uPjoaenp3BQRERERKR8Chd+DRs2RFBQELKzs6VtEokE+fn5OHHiRLGzgURERERUORS+x2/gwIHw8fHBd999h5YtWwJ4c9/fkydPkJiYiKlTpyotSCIiIiIqO4Vn/MzNzfHzzz/DysoKwcHBAICzZ89CV1cX8+bNg7GxsdKCJCIiIqKyK9M6ftbW1vjxxx+Rm5uLtLQ06OjoQENDQ1mxEREREZESKTzj9zY1NTVoaWlBXV1dGcMRERERUTko04zfgwcPsHfvXvz333/Iy8uDmpoa6tevjwEDBsDR0VFZMRIRERGREig84xceHo65c+ciMjISbdu2hYeHB9q2bYvIyEj4+vri9u3byoyTiIiIiMpI4Rm/HTt2oFatWpg9ezY0NTWl7VlZWfDz88POnTuxYMECpQRJFWvygceIiE0v12McHl23XMcnIiKiohSe8YuKioK7u7tM0Qe8+dYODw8PREVFlTk4IiIiIlIehQs/fX19SCQS+YOqqPCbO4iIiIiqGIULvy5duuDIkSPIy8uTac/Ly8ORI0fQpUuXMgdHRERERMqj8D1+ampqSEhIwLfffouWLVvCwMAAKSkpuHLlClRUVKCuro7Dhw9L+/fu3VspARMRERGRYsr0cEehY8eOvXc7wMKPiIiIqLIpXPitXLlSmXEQERERUTlTuPAzMTFRZhxEREREVM4Ufrjjt99+w82bN5UYChERERGVJ4Vn/KKjo7FgwQKYm5uje/fu6NChA7S1tZUZGxEREREpkcKF3x9//IHr168jODgYAQEB2L17N9q1a4cePXrA1tZWmTESERERkRIoXPgBQNOmTdG0aVPExsYiODgYZ86cwcmTJ1GvXj306NEDLVu2hIqKwleTiYiIiEiJylT4FTI3N8eIESPQr18/LFmyBHfu3MHdu3dRo0YNuLu7o0ePHsV+ywcRERERVQylFH4vX77EiRMncPLkSaSmpqJx48ZwdXVFaGgotmzZghcvXmD06NHKOBQRERERKahMhV94eDiOHTuGa9euQUNDA25ubujZsycsLCwAAG5ubvj777+xb98+Fn5ERERElUzhwm/q1Kl48eIFTE1NMXToUHTs2FHuU7116tRBZmZmmYIkIiIiorJTuPCrUaMGhgwZgmbNmr33/j17e3t+ywcRERFRFaBw4Td79uySHUBNjd/yQURERFQFlKrwmzhxYon7SiQS/PHHH6UOiIiIiIjKR6kKP2tr6yJtN27cQN26daGlpaW0oIiIiIhI+UpV+M2cOVPmdX5+PgYPHowRI0bA3t5eqYERERERkXKV6Ws1uCgzERER0ceD36dGlW7Lli1o3bo17O3t0aNHD1y+fPm9/S9evIgePXrA3t4ebdq0wdatW4vtGxQUBCsrK4waNUrZYRMREX10WPgpyNfXF1u2bKnsMD56QUFB8PX1xaRJkxAcHIyWLVti6NChiI6Olts/KioKw4YNQ8uWLREcHIxvv/0Wc+bMwZEjR4r0ff78Ofz8/NCqVavyToOIiOijwMKPKtWGDRvg5eWFwYMHw8HBAX5+frC0tCx2Fm/btm2wsrKCn58fHBwcMHjwYAwcOBBr166V6Zefn4+JEyfihx9+gK2tbUWkQkREVOWV6uGOyMhImdcFBQUAgBcvXsjtzwc+6H1ycnIQFhaGCRMmyLS7ubnh6tWrcve5du0a3NzcZNo6dOiA3bt3Izc3F+rq6gCApUuXwsjICIMGDfrgpWMiIiKxKFXh5+PjI7e9uPX69uzZU/qI5PD19YWtrS00NDRw8uRJqKmpoWvXrvD09ER8fDwmTpyIhQsXws7ODgCQkZGBkSNHYu7cuWjQoAHu3LmDefPmYdasWdi5cyeio6Ph6OiIKVOmIDIyElu3bkVSUhKaNGmC8ePHo1q1aqWOMS8vD7t378a5c+eQmZkJGxsbDBkyBA0aNAAApKWl4c8//0RERATS09NhZmaGvn37ol27dgCAEydOIDAwEGvWrIGKyv9NxPr7+6N69erSNRSvXr2Kffv24fnz5zA0NISbmxu+/PJLqKqqAgD27t2L06dP49WrV9DV1UWrVq2q7P1tSUlJyM/Ph7GxsUy7sbEx4uPj5e4THx8vt39eXh6SkpJgZmaG0NBQ7Nq1CydOnCi32ImIiD5GpSr8xo8fX15xfFBISAh69+6N+fPn4/79+1i9ejXq1q0Lc3PzEo+xb98+jBo1CtWqVcPSpUuxdOlSqKurY9KkScjOzsbixYtx9OhR9OnTp9TxrV69GgkJCZgyZQoMDQ1x5coVzJ8/H4sXL4aFhQVyc3Nhb2+PPn36QEtLC9evX8fKlSthZmYGBwcHtGnTBps3b8adO3fg7OwMAEhPT8etW7cwY8YMAMDNmzfxxx9/YOTIkahXrx7i4uKwbt06AMCAAQNw6dIlHDlyBFOmTIGNjQ1SUlLw5MmTYmPOzc1Fbm6u9LVEIqmw9RglEon0qXAVFZUiT4i/vf3ddnn9C8fJyMjAt99+i8WLF8PIyEi6z9u/F9f2KRJLnoB4chVLnoB4chVLnoB4cq3KeZaq8OvQoUM5hfFhNWvWxIABAwAAFhYWOHbsGG7fvl2qws/Lywt169YFAHTq1Ak7d+7EH3/8ATMzMwBAq1atcOfOnVIXfrGxsfj333+xZs0a1KhRAwDg7u6OW7du4fTp0xg8eDBq1KgBd3d36T49e/bEzZs3cfHiRTg4OEBHRweNGzfG+fPnpYXfpUuXoKOjI329f/9+9OnTR/o+mJmZYeDAgdixYwcGDBiAxMREGBgYwNnZGWpqajA2NkadOnWKjXv//v0IDAyUvq5Vqxb8/f1LlbuiLCwsYGRkBFVVVeTl5cHCwkK6LSsrC1ZWVjJthaysrJCRkSGzraCgAGpqaqhfvz7u3LmDZ8+eYcSIETLbAcDGxgb37t1D7dq1pdtK8/n5mIklT0A8uYolT0A8uYolT0A8uVbFPBX+rt6K9u4N+oaGhnj16lWpxqhZs6b0z/r6+qhWrZq06AMAAwMDPHr0qNSxPX78GIIgYPLkyTLteXl50NHRAfCm+Dhw4AAuXLiApKQk5ObmIi8vT+aycrt27bB+/XqMGTMG6urqOHfuHFxdXaWXfiMjI/Hw4UP89ddf0n0KCgqQm5uL169fo3Xr1jhy5Ai+/fZbNGrUCE2bNkWzZs2kl4Hf1bdvX/Tu3Vv6uiL/ZxITEwMAcHFxQVBQEFq3bi3ddvToUXTv3l3a523Ozs44evSozGLiBw4cQKNGjZCYmAh9fX2cOnVKZh9/f39kZGTAz88PampqiImJgUQigbm5OWJjYyEIQjllWfnEkicgnlzFkicgnlzFkicgnlwrOk81NTWYmJiUrG85x6I0ampFQxUEQVoUvX1i8/Pz5Y7xdgEkkUjkFkSFs0OlURiHv7+/zP15AKCpqQkAOHToEI4cOYIRI0bA1tYWmpqa2LJlC/Ly8qR9mzdvjnXr1uH69euoXbs2IiIiisxceXp6yl2eRF1dHcbGxli+fDnCwsIQFhaGjRs34uDBg/D19ZV7/tTV1aUPQ1S0wvdr7NixmDx5MlxcXNCsWTNs374d0dHRGDZsGARBwIIFCxATE4MVK1YAAIYNG4bNmzdj7ty5GDJkCK5du4Zdu3Zh1apVEAQB1apVg5OTk8yx9PT0AEDa/vZnRRCET/ofn0JiyRMQT65iyRMQT65iyRMQT65VMc+PpvArTuEP9eTkZNSqVQsA3ntfW3mws7NDQUEBXr16hXr16sntc/fuXTRv3hzt27cH8KaIi4mJgZWVlbSPhoYGWrZsiXPnziE2NhYWFhYyT0bb29vjxYsX75061tDQQPPmzdG8eXP06NEDU6ZMQVRUVJV9wtrDwwPJyclYunQp4uPj4eTkhG3btkm/FzouLk7mqXFbW1ts27YNvr6+CAgIgJmZGfz8/PD5559XVgpEREQfjY++8NPQ0ICDgwOCgoJgamqK1NRU7N69u0JjsLS0RLt27bBy5UoMHz4ctWrVQmpqKsLDw2Fra4umTZvC3Nwcly9fxr1791C9enUcPnwYKSkpMoUfAHz22Wfw9/fH8+fP8dlnn8ls69evH/z9/WFkZIQ2bdpAIpEgKioKUVFR8PLywpkzZ1BQUIA6deqgWrVqOHv2LDQ0NEo8/VtZvL294e3tLXfbsmXLirS1adMGwcHBJR5f3hhERERi9NEXfsCbp43XrFmDmTNnwtLSEkOHDsUvv/xSoTF88803+Ouvv6RLw+jq6sLR0RFNmzYFAPTv3x/x8fH49ddfUa1aNXTu3BktWrRAZmamzDgNGzaEjo4OXrx4IV3qpVDjxo0xY8YM/O9//8PBgwehqqoKKysrdOrUCQCgra2NoKAgBAQEoKCgALa2tpgxYwZ0dXUr5iQQERFRlSYRqtrFZ6p0gzdcQURserke4/DouuU6/odIJBJYWFggJiamyt1/oUxiyRMQT65iyRMQT65iyRMQT64Vnae6unqJr+7xK9uIiIiIROKTuNSrbImJiZg6dWqx25cuXVrk2yOIiIiIqjoWfnIYGhpi0aJF791ORERE9LFh4SeHqqpqlVxtm4iIiKgseI8fERERkUiw8CMiIiISCRZ+RERERCLBwo+IiIhIJFj4EREREYkECz8iIiIikWDhR0RERCQSLPyIiIiIRIKFHxEREZFIsPAjIiIiEgkWfkREREQiwcKPiIiISCRY+BERERGJBAs/IiIiIpFg4UdEREQkEiz8iIiIiESChR8RERGRSKhVdgBU9SzvUwu5ubmVHQYREREpGWf8iIiIiESChR8RERGRSLDwIyIiIhIJFn5EREREIsHCj4iIiEgkWPgRERERiQQLPyIiIiKRYOFHREREJBIs/IiIiIhEgoUfERERkUiw8CMiIiISCRZ+RERERCLBwo+IiIhIJNQqOwCqeiYfeIyI2PQS9z88um45RkNERETKwhk/IiIiIpFg4UdEREQkEiz8iIiIiESChR8RERGRSLDwIyIiIhIJFn5EREREIsHCj4iIiEgkWPgRERERiQQLPyIiIiKRYOFHREREJBIs/IiIiIhEgoUfERERkUiw8CMiIiISCRZ+RERERCLBwo+IiIhIJFj4EREREYkECz8iIiIikWDhR0RERCQSLPyIiIiIRIKFHxEREZFIsPAjIiIiEgkWfqRUW7ZsQevWrWFvb48ePXrg8uXL7+1/8eJF9OjRA/b29mjTpg22bt0qs33Hjh3o27cv6tevj/r162PgwIG4ceNGeaZARET0yWLhVwWdOXMG3t7e7+2zd+9eTJs2rWICKqGgoCD4+vpi0qRJCA4ORsuWLTF06FBER0fL7R8VFYVhw4ahZcuWCA4Oxrfffos5c+bgyJEj0j4XL16Eh4cH9u7di4MHD8LKygqDBw9GTExMRaVFRET0yVCr7ABIMe7u7ujZs2dlhyFjw4YN8PLywuDBgwEAfn5+CAkJwdatW+Hj41Ok/7Zt22BlZQU/Pz8AgIODA27duoW1a9fi888/BwCsXLlSZp9FixbhyJEjOH/+PAYMGFDOGREREX1aOOP3kdLU1ISurm5lhyGVk5ODsLAwuLm5ybS7ubnh6tWrcve5du1akf4dOnRAWFgYcnNz5e6TlZWFvLw8GBgYKCVuIiIiMeGMHwBfX1/Y2tpCRUUFISEhUFNTw8CBA9GuXTts2rQJly5dgr6+PkaNGoUmTZqgoKAA69atQ3h4OFJSUmBsbIzu3bujV69eAN4UQTNnzoSTkxPGjRsHAIiPj8e0adMwbNgwdOnSpURxXblyBTt27EBiYiLq1q2L8ePHw9jYGMCbS72hoaFYtGgRAGDVqlXIyMhA3bp1cfjwYeTl5cHV1RXe3t5QUyv/tzkpKQn5+fnS+AoZGxsjPj5e7j7x8fFy++fl5SEpKQlmZmZF9pk/fz7Mzc3x2WefKS94IiIikWDh9/+FhITA3d0d8+fPx4ULF7BhwwaEhoaiRYsW6Nu3L44cOYKVK1di9erVUFVVhZGREaZOnQo9PT3cu3cP69evh4GBAVxdXaGhoYFJkyZh1qxZaNKkCZo3b44//vgDDRo0KHHR9/r1a+zfvx8TJkyAmpoaNm7ciOXLl+Pnn38udp87d+7A0NAQc+fORWxsLJYtWwY7O7tij5mbmyszsyaRSKClpVW6E/f/95NIJAAAFRUV6Z/lbX+3XV7/4sZZtWoVgoKCEBgYqFCc7x777d8/VWLJExBPrmLJExBPrmLJExBPrlU5TxZ+/1/NmjXRr18/AEDfvn1x4MAB6OrqSoum/v374/jx43j69CkcHR3h6ekp3dfU1BT37t3DxYsX4erqCgCws7ODl5eXdGYwLi6uVA9j5OfnY9SoUXBwcAAATJgwAVOnTsXDhw9Rp04dufvo6Ohg9OjRUFFRgZWVFZo0aYLw8PBiC7/9+/cjMDBQ+rpWrVrw9/cvcYyFLCwsYGRkBFVVVeTl5cHCwkK6LSsrC1ZWVjJthaysrJCRkSGzraCgAGpqaqhfvz7U1dWl7YsXL8bKlSvxzz//oHnz5qWOsTjm5uZKG6sqE0uegHhyFUuegHhyFUuegHhyrYp5svD7/2xtbaV/VlFRga6urkybvr4+ACA1NRUAcPz4cZw6dQoJCQnIyclBXl4e7OzsZMbs3bs3QkNDcezYMcyaNQt6enoljkdVVRW1a9eWvrayskL16tXx/PnzYgs/a2trqKj8322bhoaGiIqKKvYYffv2Re/evaWvFf2fSeETti4uLggKCkLr1q2l244ePYru3bvLfQrX2dkZR48excyZM6VtBw4cQKNGjZCYmChtW716NZYvX46dO3fCyspKKU/0SiQSmJubIzY2FoIglHm8qkoseQLiyVUseQLiyVUseQLiybWi81RTU4OJiUnJ+pZzLB+Nd++Dk0gkUFVVlXkNvJmRunDhAgICAjB8+HA4OjpCS0sLBw8exIMHD2TGSE1NxYsXL6CiooKYmBg0bty4zHG+rzh7O97Cvu/7wKmrq8vMqimq8Bhjx47F5MmT4eLigmbNmmH79u2Ijo7GsGHDIAgCFixYgJiYGKxYsQIAMGzYMGzevBlz587FkCFDcO3aNezatQurVq2Sjrl69WosWrQIK1euhLW1NeLi4gAA1atXR/Xq1ZUS+6f8j08hseQJiCdXseQJiCdXseQJiCfXqpgnCz8FREREwMnJCd27d5e2FRYkb1uzZg1sbW3RuXNnrFmzBs7OzrC2ti7RMfLz8xEZGSmd3Xvx4gUyMjJgZWWlnCTKgYeHB5KTk7F06VLEx8fDyckJ27Ztk+YcFxeHFy9eSPvb2tpi27Zt8PX1RUBAAMzMzODn5yddygUAAgICkJOTg6+++krmWN999x2+//77ikmMiIjoE8HCTwHm5uYICQnBzZs3YWpqirNnz+Lhw4cwNTWV9jl27Bju37+PRYsWwdjYGDdu3MCKFSswf/78Ej1lq6qqik2bNmHkyJHSPzs4OBR7mbeq8Pb2Lnbx6WXLlhVpa9OmDYKDg4sd70Pf/EFEREQlx3X8FNC1a1e0atUKy5Ytw48//oj09HSZ2b/o6Ghs374do0ePli5XMnr0aGRkZGD37t0lOka1atXg4eGBFStW4KeffoKGhgamTJlSHukQERGRSEiEqnbxmSrd4A1XEBGbXuL+h0fXLcdoyodEIoGFhQViYmKq3P0XyiSWPAHx5CqWPAHx5CqWPAHx5FrReaqrq5f44Q7O+BERERGJBO/xqwTz58/H3bt35W7r27cvvvzyywqOiIiIiMSAhV8l+Prrr5GTkyN3m46OTgVHQ0RERGLBwq8S1KhRo7JDICIiIhHiPX5EREREIsHCj4iIiEgkeKmXiIhIAa9fv8br16+VMlZWVlax935/asSSq7LzlEgk0NHRee9Xt5YECz8iIqJSysjIgEQiga6ubpl/EANv1mHLzc1VQmRVn1hyVXaeOTk5SE9Ph66ubpnG4aVeIiKiUsrLy4O2trZSij6iktDQ0FDKYtAs/IiIiEqJBR99rFj4EREREYkECz8iIiKS0apVK2zYsKHMfcpqz549qFevXrkeQxk+ljgBFn5ERESiER0dje+//x5NmzaFnZ0dWrZsiTlz5iApKanUY/39998YOnSo0mKTV0i6u7vj3LlzSjvGu44cOQIbGxtER0fL3d6+fXvMnj273I5fGfhULxERkZL0/jOiwo51eHTdUvV/+vQp3N3dYW9vj1WrVsHW1hb37t3DL7/8glOnTuHQoUMwNDQs8XhGRkalDbnUtLS0oKWlVW7jd+vWDYaGhti7dy+mTp0qsy00NBSPHj3CmjVryu34lYEzfkRERCLw448/Ql1dHTt37kSbNm1gZWWFTp06Yffu3YiNjYW/v79M//T0dEyYMAEODg5o2rQpNm3aJLP93Rm61NRUTJ8+HS4uLnBycsKAAQNw584dmX2OHz+Orl27wt7eHg0bNsSYMWMAAP3798fz58/h6+sLKysrWFlZAZC9hPrw4UNYWVnh4cOHMmOuW7cOrVq1kj7xev/+fQwbNgwODg5o1KgRvv3222JnNNXV1dGvXz/s27evyBOzu3fvhouLCxo0aIB169ahc+fOqFOnDpo3bw4fHx9kZGQUe66nTJmC4cOHy7TNmTMH/fv3l74WBAGrV69GmzZtULt2bXTp0gWHDx8udkxlYeFHRET0iUtOTsaZM2cwYsSIIjNopqam+PLLL3Ho0CGZ4mft2rWoV68ejh07hokTJ8LX1xdnz56VO74gCBg+fDji4+Oxbds2HD16FM7Ozhg4cCCSk5MBAP/88w/GjBmDLl26IDg4GHv27IGLiwsAYMOGDbCwsMAPP/yAGzdu4MaNG0WOUadOHbi4uOCvv/6SaT9w4AD69OkDiUSCuLg49OvXD/Xr18fRo0exY8cOJCYmYty4ccWem0GDBuHp06e4ePGitC0zMxOHDh2Cl5cXAEBFRQV+fn44deoUli1bhn///Re//PLL+075B/n7+2PPnj1YsGABTp06hbFjx2LSpEkycZQHXuolIiL6xD1+/BiCIMDBwUHu9jp16iAlJQUvX76EsbExAKBFixaYOHEiAKB27doIDQ3Fhg0b0L59+yL7//vvv4iIiMCtW7dQrVo1AG9muIKDg3HkyBEMHToUK1asgIeHB2bMmCFd2LhBgwYAAENDQ6iqqkJHRwempqbF5tG3b19s2bIF06dPBwA8evQIYWFhWL58OQBg69atcHZ2ho+Pj3Sf33//HS1atMCjR49Qu3btImM6OjqiSZMm2LNnD1xdXQEAhw4dQn5+Pvr06QMAGDt2rLS/ra0tpk2bBh8fHyxYsKDYWN8nMzMTGzZswJ49e9C8eXMAQM2aNREaGort27ejTZs2Co1bEiz8iIiIRK5wpu/t9QmbNWsm06dZs2bYuHGj3P1v376NjIwMNGzYUKY9OzsbT58+BQDcuXMHQ4YMKVOcHh4e+OWXX3Dt2jU0a9YM+/fvR4MGDeDo6AgACAsLw4ULF+QWuE+fPpVb+AFvZv3mzp2LX3/9FTo6Oti9ezd69eoFfX19AG8K2z/++AMPHjxAWloa8vPzkZ2djczMTGhra5c6j/v37yM7OxuDBg2Sac/NzS1yDpWNhR8REdEnzs7ODhKJBPfv30ePHj2KbH/06BEMDAxQo0aN945T3MLVBQUFMDU1RWBgYJFthcWTpqamApHLMjMzg6urKw4cOIBmzZrhwIEDMk8WC4KArl27YtasWXL3LY6Hhwd8fX1x8OBBtGnTBleuXMEPP/wAAHj+/DmGDx+OoUOHYtq0aTAwMEBoaCi+//77Yr+STUWl6J10eXl50j8XFBQAeDNDaW5uLtNPQ0PjPWeg7Fj4ERERfeJq1KiB9u3bIyAgAGPHjpW5zy8+Ph5//fUX+vfvL1PYXb9+XWaM69evo06dOnLHd3Z2RkJCAtTU1GBjYyO3T7169XD+/Plil4BRV1dHfn7+B3Pp27cv5s+fDw8PDzx9+hQeHh7SbQ0bNsTff/8NGxsbqKmVvMTR0dFB7969sWfPHjx9+hQ1a9aUXva9desW8vLyMHfuXGlBd+jQofeOZ2RkhPv378u03blzB+rq6gDeXF6uVq0aoqOjy/Wyrjx8uIOIiEgEfvnlF+Tk5GDIkCG4dOkSoqOjcfr0aQwaNAjm5uaYMWOGTP/Q0FCsXr0ajx49wpYtW3D48GGMHj1a7tifffYZmjVrhlGjRuHMmTN49uwZQkND4e/vj1u3bgEAvvvuOxw4cAD+/v548OAB7t69i9WrV0vHsLGxweXLlxETE/PedQV79eqF9PR0+Pj4wNXVFRYWFtJt3t7eSElJwTfffIMbN27g6dOnCAkJwXfffffBonLQoEG4evUqtm3bhoEDB0qL4Jo1ayIvLw+bNm3C06dPERgYiG3btr13rLZt2+LmzZvYt28fIiMjsXjxYty7d0+6XUdHB+PGjYOvry/27t2LJ0+eIDw8HFu2bMHevXvfO3ZZccaPiljep1ax09dERPRxsre3x9GjR/H7779j/PjxSE5OhomJCXr06IGpU6cWWcNv3LhxCAsLw5IlS6Cjo4M5c+agQ4cOcseWSCTYtm0b/P398f333+Ply5cwMTFB69atpQ+LuLq6Yt26dVi+fDn++OMP6OjooHXr1tIxfvjhB8yYMQNt27bF69evi11UWVdXV7r0yZIlS2S2mZub48CBA5g/fz6GDBmC169fw9raGh06dJB7+fVtLVu2RO3atfH48WMMGDBA2t6wYUPMnTsXq1evxoIFC9C6dWv4+Phg8uTJxY7VoUMHfPfdd/j111/x+vVrDBw4EP3790dExP+t8zh9+nQYGxtj5cqViIqKgp6eHpydnfHtt9++N86ykgjvLlxDopeQkPDJF34SiQQWFhaIiYkpsnbTp0QseQLiyVUseQJVO9fU1FTo6ekpbTx1dfWP7t/dJk2aYNq0aRg8eHCp9vsYc1VEeeRZ3OdOXV0dJiYmJRqDM35ERERUYllZWQgNDUVCQoL0aVr6ePAePyIiIiqx7du3Y/z48RgzZox0DTr6eHDGj4iIiEps7NixMgsa08eFM35EREREIsHCj4iIiEgkWPgRERERiQQLPyIiIgUUfu0WUUVQ1pJGLPyIiIhKSVtbG2lpaSz+qMJkZmaiWrVqZR6HT/USERGVkpqaGqpXr4709HSljKehoYGcnByljFXViSVXZeYpCALU1NRY+BEREVUWNTU1pXx7R1X+hhJlE0uuVTlPXuolIiIiEgkWfkREREQiwcKPiIiISCRY+BERERGJBB/uoCLU1MTzsRBLrmLJExBPrmLJExBPrmLJExBPrhWVZ2mOIxGq2uMmVGlyc3Ohrq5e2WEQERFROeGlXpLKzc3F8uXLkZWVVdmhlLusrCzMmDHjk89VLHkC4slVLHkC4slVLHkC4sm1KufJwo9k/Pvvv1VuzaHyIAgCHj9+/MnnKpY8AfHkKpY8AfHkKpY8AfHkWpXzZOFHREREJBIs/IiIiIhEgoUfSamrq6N///6ieMBDLLmKJU9APLmKJU9APLmKJU9APLlW5Tz5VC8RERGRSHDGj4iIiEgkWPgRERERiQQLPyIiIiKRYOFHREREJBLi+LI8kgoODsbBgweRkpICa2treHt7o169esX2/++//xAQEIDnz5/D0NAQ7u7u6NatWwVGrLjS5JqcnIytW7ciMjISsbGx6NmzJ7y9vSs2YAWVJs/Lly/j+PHjePLkCfLy8mBtbY0BAwagcePGFRu0gkqTa0REBHbs2IHo6Gi8fv0aJiYm6NKlC3r37l3BUZdeaf+eFoqIiICvry9sbGywaNGiCoi07EqT6507dzBv3rwi7UuXLoWVlVV5h1ompX1Pc3NzERgYiHPnziElJQVGRkbo27cvOnXqVIFRK6Y0ua5atQohISFF2q2trbFkyZLyDrVMSvuenjt3DgcPHkRMTAy0tbXRuHFjDBs2DLq6uhUYNQCBROPff/8VvLy8hH/++Ud49uyZsHnzZmHo0KFCQkKC3P5xcXHC0KFDhc2bNwvPnj0T/vnnH8HLy0u4ePFiBUdeeorkumnTJuHMmTPCtGnThM2bN1dswAoqbZ6bN28WDhw4IDx48EB48eKFsGPHDsHLy0uIjIys4MhLr7S5RkZGCufOnROioqKEuLg4ISQkRBg6dKhw4sSJCo68dEqbZ6GMjAxh4sSJwi+//CL88MMPFRRt2ZQ21/DwcGHAgAFCdHS0kJycLP2Vn59fwZGXjiLvqb+/vzBr1izh1q1bQlxcnPDgwQMhIiKiAqNWTGlzzcjIkHkvExMThZEjRwp79uyp4MhLp7R53r17V/D09BSOHDkixMXFCXfv3hW+++47YeHChRUcuSDwUq+IHD58GJ06dULnzp2l/zsxNjbG8ePH5fY/fvw4jI2N4e3tDWtra3Tu3BkdO3bEoUOHKjjy0ittrqamphg5ciTc3Nygra1dwdEqrrR5ent7w8PDA3Xq1IGFhQUGDx4MCwsLXLt2rYIjL73S5lqrVi20a9cONjY2MDU1Rfv27dGoUSPcvXu3giMvndLmWWj9+vVo27YtHBwcKijSslM0V319fRgYGEh/qahU7R9lpc3z5s2b+O+//+Dj4wMXFxeYmpqiTp06cHJyquDIS6+0uWpra8u8l48ePUJGRgY6duxYwZGXTmnzvH//PkxNTdGrVy+Ympqibt266NKlCyIjIys4ct7jJxp5eXmIjIxEo0aNZNpdXFxw7949ufs8ePAALi4uMm2NGzdGZGQk8vLyyi3WslIk14+RMvIsKChAVlYWdHR0yiNEpVFGro8fP8a9e/dQv3798ghRKRTN8/Tp04iLi8OAAQPKO0SlKct7On36dHz11Vfw8/NDeHh4eYZZZorkefXqVdSuXRtBQUEYN24cJk+ejK1btyInJ6ciQlaYMv6enjp1Cs7OzjAxMSmPEJVCkTydnJzw8uVLXL9+HYIgICUlBZcuXUKTJk0qImQZvMdPJFJTU1FQUAB9fX2Zdn19faSkpMjdJyUlRW7//Px8pKWlwdDQsLzCLRNFcv0YKSPPw4cP4/Xr12jTpk05RKg8Zcn166+/RmpqKvLz8zFgwAB07ty5HCMtG0XyjImJwc6dOzFv3jyoqqpWQJTKoUiuhoaG+Oqrr2Bvb4+8vDycPXsWP//8M+bOnVtlC3pF8oyLi0NERATU1dUxbdo0pKam4s8//0R6ejq++eabCohaMWX9Nyk5ORk3b97EpEmTyilC5VAkTycnJ0yaNAnLli1Dbm4u8vPz0bx5c4waNaoCIpbFwk9kJBJJidqK2yb8/y96ed8+VUVpc/1YKZrn+fPnsW/fPkybNq3IP2BVlSK5+vn5ITs7G/fv38fOnTthbm6Odu3alVeISlHSPAsKCrBixQoMGDAAlpaWFRGa0pXmPbW0tJTJ09HREYmJiTh06FCVLfwKlSbPwn9nJ02aJL31JDc3F0uWLMGYMWOgoaFRfoEqgaL/Jp05cwbVq1dHy5YtyyMspStNns+fP8fmzZvRv39/NGrUCMnJydi+fTs2bNiA8ePHl3eoMlj4iYSenh5UVFSK/G/k1atXxf7QNzAwKNI/NTUVqqqqVfrSoCK5fozKkueFCxewdu1afPfdd0Uu51dFZcnV1NQUAGBra4tXr15h3759VbbwK22eWVlZePToER4/foxNmzYBeFM0CIIALy8v/PTTT2jYsGFFhF5qyvp76ujoiHPnzik5OuVR9N/eGjVqyNxvbGVlBUEQ8PLlS1hYWJRnyAory3sqCAJOnz6Nzz77DGpqVbs0USTP/fv3w8nJCe7u7gCAmjVrQlNTE3PmzIGXl1eFXkHjPX4ioaamBnt7e4SFhcm0h4WFFXvDsIODQ5H+t27dgr29fZX+i6lIrh8jRfM8f/48Vq1ahUmTJqFp06blHaZSKOs9FQShSt+fWto8tbS0sHjxYixcuFD6q2vXrrC0tMTChQtRp06digq91JT1nj5+/BgGBgZKjk55FMmzbt26SE5ORnZ2trQtJiYGEokERkZG5RpvWZTlPf3vv/8QGxv7USxXo0ier1+/LjIbWPhQUuEMb0Vh4ScivXv3xsmTJ3Hq1Ck8f/4cW7ZsQWJiIrp27QoA2LlzJ1auXCnt361bNyQmJkrX8Tt16hROnTqFL774orJSKLHS5goAT548wZMnT5CdnY3U1FQ8efIEz58/r4zwS6y0eRYWfcOHD4ejoyNSUlKQkpKCzMzMykqhxEqb67Fjx3D16lXExMQgJiYGp0+fxqFDh/DZZ59VVgolUpo8VVRUYGtrK/NLT08P6urqsLW1haamZmWm8kGlfU+PHDmCK1euICYmBs+ePcPOnTtx+fJl9OjRo7JSKJHS5tmuXTvo6upi9erVeP78Of777z9s374dHTt2rPKXeRX5txd481CHg4MDbG1tKzpkhZQ2z+bNm+PKlSs4fvy49B7OzZs3o06dOqhRo0aFxl51p21I6VxdXZGWlob//e9/SE5Oho2NDXx8fKRPTyUnJyMxMVHa39TUFD4+PggICEBwcDAMDQ0xcuRItG7durJSKLHS5gq8eVKwUGRkJM6fPw8TExOsWrWqQmMvjdLm+c8//yA/Px9//vkn/vzzT2m7m5sbJkyYUOHxl0ZpcxUEAbt27UJ8fDxUVFRgbm6OIUOGoEuXLpWVQoko8tn9WJU217y8PGzbtg1JSUnQ0NCAjY0NZs6cWeVnrkubp6amJn766Sds2rQJM2fOhK6uLtq0aQMvL6/KSqHEFPn8ZmZm4vLlyx/NovlA6fPs0KEDsrKycOzYMWzduhXVq1dHgwYNMHTo0AqPXSJU9BwjEREREVUKXuolIiIiEgkWfkREREQiwcKPiIiISCRY+BERERGJBAs/IiIiIpFg4UdEREQkEiz8iIiIiESChR8RFXHmzBl4enri0aNHcrf/9ttvVX7BZ3ojODgYZ86cqdBj+vr64vvvv6/QYyrT69evsXfvXty5c6eyQyFSOhZ+RESfsOPHj1d44fexe/36NQIDA1n40SeJhR8RfXLy8vKQn59fYcd7/fp1hR2rKhAEATk5OZUdhtJ9qnkRvY3f1UtEZebn54ekpCQsXboUEolE2i4IAiZNmgRLS0v4+PggPj4eEydOxJAhQ5Cfn48TJ04gNTUVNjY2GDJkCJydnWXGjYmJwd69e3H79m1kZmbCzMwM3bt3R48ePaR97ty5g3nz5mHixIl48uQJ/v33X6SkpGDJkiV48OABVq9ejZ9++gnnz59HaGgo8vLy0KBBA4wcORJmZmbSccLCwnDs2DFERkYiLS0NNWrUgLOzM7y8vKCnpyftt3fvXgQGBuK3337D/v37ER4eDnV1daxfvx6PHj3CoUOH8ODBA6SkpMDAwAAODg4YMmSI9Ds8gTeX0levXo05c+bg/PnzuHLlCvLz89GiRQuMGTMG2dnZ2LRpE8LCwqChoYF27dph8ODBUFP7v3+y8/LyEBQUhHPnziE+Ph5aWlpo1qwZhg4dKo13woQJSEhIAAB4enoCgMz3T2dmZiIwMBCXL19GUlIS9PT0pN8Jq6mpKT2Wp6cnunfvDhsbGxw9ehSxsbEYOXIkunXrVuLPSOEY9vb2OHDgABITE2FjY4NRo0bBwcEBhw4dQnBwMFJTU1GnTh2MGzcO5ubm0v19fX2RlpaGMWPGYPv27Xjy5Al0dHTQsWNHeHp6QkXl/+Yx0tPTsXv3boSGhiI1NRVGRkZo27Yt+vfvD3V19Q/mtXHjRgBAYGAgAgMDAfzf91nHxsbir7/+QkREBJKSklC9enXUqlULgwcPhq2tbZHP5aRJk/Ds2TOcOXMG2dnZqFOnDkaPHg1LS0uZ83Pz5k0cPHgQjx49Qn5+PkxMTNC+fXv07dtX2ufRo0cIDAxEREQEcnJyYGVlhT59+sDV1bXE7wMRCz8iKlZBQYHcmbN3v+K7V69eWLhwIW7fvg0XFxdp+40bNxAXF4eRI0fK9D927BhMTEzg7e0NQRAQFBSE+fPnY968eXB0dAQAPH/+HD/99BOMjY0xfPhwGBgY4ObNm9i8eTPS0tIwYMAAmTF37twJR0dHjB07FioqKtDX15duW7NmDVxcXDB58mQkJiZiz5498PX1xeLFi1G9enUAQGxsLBwdHdGpUydoa2sjISEBhw8fxpw5c7B48WKZogsAfv/9d7i6uqJr167SGb+EhARYWlrC1dUVOjo6SElJwfHjx+Hj44MlS5bIFJAAsHbtWrRs2RJTpkzB48ePsWvXLuTn5+PFixdo1aoVunTpgtu3byMoKAg1atRA7969pe/LwoULcffuXXh4eMDR0RGJiYnYu3cvfH198dtvv0FDQwM//PADlixZAm1tbYwePRoApIXP69ev4evri5cvX6Jv376oWbMmnj17hr179yIqKgqzZ8+WKeJDQ0MRERGBfv36wcDAQOb8ltT169fx5MkTDBkyBACwY8cO/Pbbb3Bzc0NcXBxGjx6NzMxMBAQE4Pfff8fChQtlYkhJScGyZcvQp08feHp64vr16/jrr7+QkZEhzS8nJwfz5s1DbGwsPD09UbNmTdy9excHDhzAkydP4OPjIxPTu3np6Ohg1qxZmD9/Pjp16oROnToBgPS9S0pKgo6ODgYPHgw9PT2kp6cjJCQEs2bNwsKFC4sUdLt27YKTkxPGjRuHrKws7NixA/7+/li6dKm0WD116hTWrVuH+vXrY+zYsdDX10dMTAyioqKk44SHh2P+/PlwcHDA2LFjoa2tjQsXLmDZsmXIyclBhw4dSv1+kDix8COiYv3444/Fbnt7Bqtp06YwMzPDsWPHZAq/4OBgmJmZoUmTJjL7FhQU4KeffoKGhgYAoFGjRpgwYQL27NmD2bNnAwACAgKgpaUFPz8/aGtrAwBcXFyQl5eHAwcOoGfPntDR0ZGOaWZmhu+++05urLVr18b48eOlr21sbDB79mwEBwfjyy+/BACZ2StBEODk5IQGDRrgm2++wc2bN9G8eXOZMd3c3KSzaIVat26N1q1by+TZtGlTjB07FufPn0evXr1k+jdt2hTDhw+X5nb//n38+++/GD58uLTIc3Fxwa1bt3Du3Dlp28WLF3Hz5k18//33aNWqlXS8mjVrwsfHB2fOnEG3bt1Qq1YtaGhoQEtLS1pQFzp69CiePn2K+fPno3bt2gAAZ2dn1KhRA0uWLMHNmzdl3rfs7GwsXrxY5pyXVm5uLn788UfpbKJEIsGiRYtw584d+Pv7S4u81NRUbNmyBc+ePZOZRUtLS8P06dOl70WjRo2Qk5OD48ePw8PDA8bGxggJCcHTp08xdepUtGnTRnoONTU1sWPHDoSFhcl8RuXllZqaCgCoUaNGkfNWv3591K9fX/q68D3+/vvvceLECYwYMUKmv7W1NSZNmiR9raKigqVLl+Lhw4dwdHREdnY2AgIC4OTkhDlz5kjPwbuz33/++SdsbGwwZ84cqKqqAgAaN26M1NRU7Nq1C+3bt5eZ9SQqDgs/IirWxIkTYWVlVaQ9ICAAL1++lL5WUVFB9+7dsX37diQmJsLY2BixsbG4efMmhg0bJjNrAwCtWrWSFn0ApJcp//33XxQUFCAvLw/h4eHo2rUrqlWrJjPr2KRJExw7dgwPHjyQKUzeLoDe1a5dO5nXTk5OMDExwZ07d6SF36tXr7Bnzx7cuHEDSUlJMrOaz58/L1L4yTtedna29NJpQkICCgoKpNuio6OL9G/WrJnMaysrK4SGhqJp06ZF2sPCwqSvr127hurVq6NZs2Yy58bOzg4GBga4c+fOBy/DXrt2Dba2trCzs5MZo3HjxpBIJLhz547M+W3YsGGZij4AaNCggcwl5MLPVuEx321PSEiQKfy0tLSKvA/t2rXDyZMn8d9//6F9+/YIDw9HtWrVZApwAOjQoQN27NhRZFa6tHnl5+dLL7HHxsbKnDt57/G78dasWRMAkJiYCEdHR9y7dw9ZWVno1q1bkb8nhWJjYxEdHY1hw4ZJYyjUtGlTXL9+HS9evIC1tXWJ8yDxYuFHRMWysrKSzga9TVtbW6bwA4BOnTph7969OH78OAYPHozg4GBoaGigY8eORfY3MDCQ25aXl4fs7GxkZ2cjPz8fx44dw7Fjx+TGlpaWJvPa0NCw2DyKO17hGAUFBfjll1+QnJyMfv36wdbWFtWqVYMgCPjxxx/l3vAv73jLly9HeHg4+vXrh9q1a0NLSwsSiQQLFiyQO8a7BUfh5WR57W/v/+rVK2RkZGDw4MFy83333Mjz6tUrxMbGYtCgQSUaQ945LK3S5Au8mSF8m7zLy4VxpaenS383MDAoUkTp6+tDVVW1zHkFBAQgODgYHh4eqF+/PnR0dCCRSLB27Vq577Gurq7c3Ar7Fs4uGhkZFXvMlJQUAMC2bduwbds2uX1K8p4TASz8iEhJtLW14ebmhlOnTsHd3R1nzpxB27ZtpffQva3wB9m7bWpqatDU1ISqqipUVFTQvn17dO/eXe7xTE1NZV4XN1vyvuMVPjzw7NkzPH36FN98843MvVKxsbHFjvmuzMxMXL9+Hf3790efPn2k7bm5udKiRFl0dXWhq6uLWbNmyd2upaVVojE0NDRkLoG/u/1t7zu/FeXVq1dF2grf28LiUUdHBw8ePIAgCDIxv3r1Cvn5+UXusyxtXufOnYObm1uRojstLU3uZ/1DCuN59z9S8vr06dOn2Jntd+8tJCoOCz8iUpqePXvi+PHj+P3335GRkSHz9O3bLl++jKFDh0ov92ZlZeHatWuoV68eVFRUUK1aNTRo0ACPHz9GzZo1izxYUVrnz5+XufR37949JCQkSG/cL/zh//YTnwBw4sSJUh1HEIQiY5w8eVLmkq8yNGvWDBcuXEBBQQEcHBze2/fd2cK3x9i/fz90dXWLFNFVVVZWFq5evSpz+fT8+fOQSCTS++6cnZ1x8eJFhIaGomXLltJ+ISEhAN5c2v2QwvdQ3nmTSCRFPo/Xr19HUlKSzFPIJeXk5ARtbW2cOHECbdu2lVuIWlpawsLCAk+fPi12lpeopFj4EZHSWFpaonHjxrhx4wbq1q0LOzs7uf1UVFTwyy+/oHfv3igoKEBQUBCysrJkntQdOXIkZs+ejTlz5qBbt24wMTFBVlYWYmNjce3aNcydO7fEcT169Ahr165F69at8fLlS+zevRs1atSQziZaWlrCzMwMO3fuhCAI0NHRwbVr12Tuq/sQbW1t1KtXDwcPHoSuri5MTEzw33//4fTp0wrNBL1P27Ztcf78eSxYsAC9evVCnTp1oKqqipcvX+LOnTto0aKFtOixtbXFhQsXcOHCBZiamkJDQwO2trbo1asXLl++jLlz5+Lzzz+Hra0tBEFAYmIibt26hS+++OKDRWVF09XVxYYNG5CYmAgLCwvcuHEDJ0+eRLdu3WBsbAwAaN++PYKDg7Fq1SrEx8fD1tYWERER2L9/P5o0aSJzf19xtLS0YGJigqtXr8LZ2Rk6OjrSArlp06YICQmBlZUVatasicjISBw8ePC9l2rfR1NTE8OHD8fatWvx888/o3PnztDX10dsbCyePn0qfVp57NixWLBgAX799Ve4ubmhRo0aSE9PR3R0NB4/flzsg01E72LhR0RK1aZNG9y4caPY2T4A6NGjB3Jzc7F582a8evUKNjY2mDlzJurWrSvtY21tDX9/f/zvf//D7t278erVK1SvXh0WFhZFnhL+kPHjx+Ps2bNYvnw5cnNzpev4FV4eVFNTw4wZM7BlyxZs2LABKioqcHZ2xuzZs/HNN9+U+DiTJ0/G5s2bsX37dhQUFMDJyQk//fQTfvvtt1LF+yEqKiqYPn06/v77b5w9exb79++HqqoqjIyMUK9ePZkHIjw9PZGSkoJ169YhKytLuo6fpqYm5s2bhwMHDuCff/5BfHw8NDQ0YGxsDGdnZ5mntqsKAwMDjB49Gtu2bUNUVBR0dHTQt29fmaerNTQ0MHfuXOzatQuHDh1CamoqatSogS+++KLIEkDv8/XXX2P79u1YuHAhcnNzpev4jRw5Empqajhw4ACys7NRq1Yt/PDDD9i9e7fCeXXq1AmGhoYICgrC2rVrAbx5at7NzU3ap2HDhpg/fz7++usvBAQEID09Hbq6urC2tpY+vUxUEhLh3QW5iIjKYPHixXjw4AFWrVpV5JJY4QLOQ4cOhbu7e7nHUrhQ8oIFC+Q+pEIfj8IFnH///ffKDoXoo8YZPyIqs9zcXDx+/BgPHz5EaGgohg8fXub78oiISPn4LzMRlVlycjJ++uknaGlpoUuXLujZs2dlh0RERHLwUi8RERGRSPD7XYiIiIhEgoUfERERkUiw8CMiIiISCRZ+RERERCLBwo+IiIhIJFj4EREREYkECz8iIiIikWDhR0RERCQSLPyIiIiIROL/Abps7E3WiCrZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       163.200000     6.828047\n",
      "1                    TN        87.300000     3.465705\n",
      "2                    FP        26.100000     4.886489\n",
      "3                    FN        20.500000     4.766783\n",
      "4              Accuracy         0.843152     0.026982\n",
      "5             Precision         0.862012     0.026301\n",
      "6           Sensitivity         0.888320     0.026061\n",
      "7           Specificity         0.770690     0.037053\n",
      "8              F1 score         0.874802     0.023023\n",
      "9   F1 score (weighted)         0.842350     0.027035\n",
      "10     F1 score (macro)         0.832284     0.027614\n",
      "11    Balanced Accuracy         0.829503     0.027499\n",
      "12                  MCC         0.665776     0.055001\n",
      "13                  NPV         0.810660     0.037843\n",
      "14              ROC_AUC         0.829503     0.027499\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>324.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>174.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>55.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.838655</td>\n",
       "      <td>0.836975</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.810084</td>\n",
       "      <td>0.838487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.848806</td>\n",
       "      <td>0.865014</td>\n",
       "      <td>0.854497</td>\n",
       "      <td>0.845953</td>\n",
       "      <td>0.830380</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.879357</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.824427</td>\n",
       "      <td>0.854401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.882514</td>\n",
       "      <td>0.907563</td>\n",
       "      <td>0.918768</td>\n",
       "      <td>0.891967</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>0.893733</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.888647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.748900</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.759800</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>0.718500</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>0.793400</td>\n",
       "      <td>0.802600</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.759390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.859060</td>\n",
       "      <td>0.871012</td>\n",
       "      <td>0.868280</td>\n",
       "      <td>0.875676</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.869096</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.874494</td>\n",
       "      <td>0.851511</td>\n",
       "      <td>0.870985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.822811</td>\n",
       "      <td>0.843406</td>\n",
       "      <td>0.834407</td>\n",
       "      <td>0.843628</td>\n",
       "      <td>0.835733</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.848894</td>\n",
       "      <td>0.858455</td>\n",
       "      <td>0.843080</td>\n",
       "      <td>0.807670</td>\n",
       "      <td>0.837370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.811553</td>\n",
       "      <td>0.836359</td>\n",
       "      <td>0.824274</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.826581</td>\n",
       "      <td>0.826530</td>\n",
       "      <td>0.835805</td>\n",
       "      <td>0.849910</td>\n",
       "      <td>0.833683</td>\n",
       "      <td>0.794054</td>\n",
       "      <td>0.827437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.809232</td>\n",
       "      <td>0.835172</td>\n",
       "      <td>0.821169</td>\n",
       "      <td>0.829832</td>\n",
       "      <td>0.818627</td>\n",
       "      <td>0.822052</td>\n",
       "      <td>0.836504</td>\n",
       "      <td>0.848182</td>\n",
       "      <td>0.831269</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.824028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.623523</td>\n",
       "      <td>0.672847</td>\n",
       "      <td>0.649302</td>\n",
       "      <td>0.674804</td>\n",
       "      <td>0.660879</td>\n",
       "      <td>0.654947</td>\n",
       "      <td>0.671631</td>\n",
       "      <td>0.700013</td>\n",
       "      <td>0.667794</td>\n",
       "      <td>0.591346</td>\n",
       "      <td>0.656709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.779800</td>\n",
       "      <td>0.810300</td>\n",
       "      <td>0.801800</td>\n",
       "      <td>0.844300</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.824300</td>\n",
       "      <td>0.809100</td>\n",
       "      <td>0.782200</td>\n",
       "      <td>0.811140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.809232</td>\n",
       "      <td>0.835172</td>\n",
       "      <td>0.821169</td>\n",
       "      <td>0.829832</td>\n",
       "      <td>0.818627</td>\n",
       "      <td>0.822052</td>\n",
       "      <td>0.836504</td>\n",
       "      <td>0.848182</td>\n",
       "      <td>0.831269</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.824028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  320.000000  314.000000  323.000000  324.000000   \n",
       "1                    TN  170.000000  188.000000  174.000000  179.000000   \n",
       "2                    FP   57.000000   49.000000   55.000000   59.000000   \n",
       "3                    FN   48.000000   44.000000   43.000000   33.000000   \n",
       "4              Accuracy    0.823529    0.843697    0.835294    0.845378   \n",
       "5             Precision    0.848806    0.865014    0.854497    0.845953   \n",
       "6           Sensitivity    0.869565    0.877095    0.882514    0.907563   \n",
       "7           Specificity    0.748900    0.793200    0.759800    0.752100   \n",
       "8              F1 score    0.859060    0.871012    0.868280    0.875676   \n",
       "9   F1 score (weighted)    0.822811    0.843406    0.834407    0.843628   \n",
       "10     F1 score (macro)    0.811553    0.836359    0.824274    0.835616   \n",
       "11    Balanced Accuracy    0.809232    0.835172    0.821169    0.829832   \n",
       "12                  MCC    0.623523    0.672847    0.649302    0.674804   \n",
       "13                  NPV    0.779800    0.810300    0.801800    0.844300   \n",
       "14              ROC_AUC    0.809232    0.835172    0.821169    0.829832   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   328.000000  322.000000  336.000000  328.000000  324.000000  324.000000   \n",
       "1   171.000000  176.000000  169.000000  183.000000  178.000000  158.000000   \n",
       "2    67.000000   58.000000   44.000000   45.000000   51.000000   69.000000   \n",
       "3    29.000000   39.000000   46.000000   39.000000   42.000000   44.000000   \n",
       "4     0.838655    0.836975    0.848739    0.858824    0.843697    0.810084   \n",
       "5     0.830380    0.847368    0.884211    0.879357    0.864000    0.824427   \n",
       "6     0.918768    0.891967    0.879581    0.893733    0.885246    0.880435   \n",
       "7     0.718500    0.752100    0.793400    0.802600    0.777300    0.696000   \n",
       "8     0.872340    0.869096    0.881890    0.886486    0.874494    0.851511   \n",
       "9     0.835733    0.835616    0.848894    0.858455    0.843080    0.807670   \n",
       "10    0.826581    0.826530    0.835805    0.849910    0.833683    0.794054   \n",
       "11    0.818627    0.822052    0.836504    0.848182    0.831269    0.788235   \n",
       "12    0.660879    0.654947    0.671631    0.700013    0.667794    0.591346   \n",
       "13    0.855000    0.818600    0.786000    0.824300    0.809100    0.782200   \n",
       "14    0.818627    0.822052    0.836504    0.848182    0.831269    0.788235   \n",
       "\n",
       "           ave  \n",
       "0   324.300000  \n",
       "1   174.600000  \n",
       "2    55.400000  \n",
       "3    40.700000  \n",
       "4     0.838487  \n",
       "5     0.854401  \n",
       "6     0.888647  \n",
       "7     0.759390  \n",
       "8     0.870985  \n",
       "9     0.837370  \n",
       "10    0.827437  \n",
       "11    0.824028  \n",
       "12    0.656709  \n",
       "13    0.811140  \n",
       "14    0.824028  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluations metrics on the test sets\n",
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.846113</td>\n",
       "      <td>0.014913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.862783</td>\n",
       "      <td>0.021994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.893734</td>\n",
       "      <td>0.019724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.769016</td>\n",
       "      <td>0.038851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.877675</td>\n",
       "      <td>0.012538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.845065</td>\n",
       "      <td>0.015234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.834757</td>\n",
       "      <td>0.016173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.831377</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.671513</td>\n",
       "      <td>0.032332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.817710</td>\n",
       "      <td>0.026888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.831377</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.846113     0.014913\n",
       "1             Precision         0.862783     0.021994\n",
       "2           Sensitivity         0.893734     0.019724\n",
       "3           Specificity         0.769016     0.038851\n",
       "4              F1 score         0.877675     0.012538\n",
       "5   F1 score (weighted)         0.845065     0.015234\n",
       "6      F1 score (macro)         0.834757     0.016173\n",
       "7     Balanced Accuracy         0.831377     0.017700\n",
       "8                   MCC         0.671513     0.032332\n",
       "9                   NPV         0.817710     0.026888\n",
       "10              ROC_AUC         0.831377     0.017700"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_lgbm)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "       \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_lgbm))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_lgbm))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_lgbm))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_lgbm))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_lgbm, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_lgbm, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_lgbm))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_lgbm))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_lgbm))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "mat_met_optimized_lgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model f1_score 0.8342 with a standard deviation of 0.0184\n",
      "LightGBM optimized model f1_score 0.8315 with a standard deviation of 0.0214\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#f1_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "f1_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"f1_macro\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)f1_score %0.4f with a standard deviation of %0.4f\" % (f1_cv_lgbm_opt_testSet.mean(), f1_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(f1_cv_lgbm_opt), np.std(f1_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm_clf.joblib']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the modesls, both the one with optimized hyperparameters and the initial one\n",
    "joblib.dump(lgbm_clf, \"OUTPUT/lgbm_clf.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm_clf.joblib\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       162.100000     6.674162\n",
      "1                    TN        87.200000     3.047768\n",
      "2                    FP        26.200000     5.094660\n",
      "3                    FN        21.600000     4.857983\n",
      "4              Accuracy         0.839115     0.023233\n",
      "5             Precision         0.860942     0.026062\n",
      "6           Sensitivity         0.882369     0.026451\n",
      "7           Specificity         0.769950     0.037469\n",
      "8              F1 score         0.871226     0.020480\n",
      "9   F1 score (weighted)         0.838413     0.023360\n",
      "10     F1 score (macro)         0.828254     0.023352\n",
      "11    Balanced Accuracy         0.826161     0.023457\n",
      "12                  MCC         0.657889     0.045830\n",
      "13                  NPV         0.802640     0.033501\n",
      "14              ROC_AUC         0.826161     0.023457\n",
      "CPU times: user 24.5 s, sys: 164 ms, total: 24.6 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    n_jobs=16,\n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_clf.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"logloss\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_clf.predict(X_test) \n",
    "    \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=16, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"logloss\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=16, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"logloss\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        \n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:51:36,643] A new study created in memory with name: XGBClassifier\n",
      "[I 2023-12-04 16:51:42,489] Trial 0 finished with value: 0.8355992710495699 and parameters: {'n_estimators': 465, 'eta': 0.06677184097099524, 'max_depth': 12, 'alpha': 0.3049, 'lambda': 16.28975340097403, 'max_bin': 471}. Best is trial 0 with value: 0.8355992710495699.\n",
      "[I 2023-12-04 16:51:43,611] Trial 1 finished with value: 0.7645871223893292 and parameters: {'n_estimators': 53, 'eta': 0.01908258743303956, 'max_depth': 7, 'alpha': 0.3084, 'lambda': 33.89111737772557, 'max_bin': 452}. Best is trial 0 with value: 0.8355992710495699.\n",
      "[I 2023-12-04 16:51:49,952] Trial 2 finished with value: 0.8344427182646628 and parameters: {'n_estimators': 254, 'eta': 0.02742122305200364, 'max_depth': 12, 'alpha': 0.031100000000000003, 'lambda': 21.31845262874199, 'max_bin': 458}. Best is trial 0 with value: 0.8355992710495699.\n",
      "[I 2023-12-04 16:51:57,064] Trial 3 finished with value: 0.8160250315022062 and parameters: {'n_estimators': 510, 'eta': 0.04375085433630816, 'max_depth': 5, 'alpha': 0.9419000000000001, 'lambda': 32.003303101999514, 'max_bin': 276}. Best is trial 0 with value: 0.8355992710495699.\n",
      "[I 2023-12-04 16:51:58,752] Trial 4 finished with value: 0.7930867963491729 and parameters: {'n_estimators': 77, 'eta': 0.03997741859304366, 'max_depth': 7, 'alpha': 0.8982, 'lambda': 25.439314264563606, 'max_bin': 443}. Best is trial 0 with value: 0.8355992710495699.\n",
      "[I 2023-12-04 16:52:04,457] Trial 5 finished with value: 0.829286187690391 and parameters: {'n_estimators': 497, 'eta': 0.08195957758029686, 'max_depth': 5, 'alpha': 0.8112, 'lambda': 16.08665482905612, 'max_bin': 360}. Best is trial 0 with value: 0.8355992710495699.\n",
      "[I 2023-12-04 16:52:07,557] Trial 6 finished with value: 0.7883144721589267 and parameters: {'n_estimators': 196, 'eta': 0.02567784906072195, 'max_depth': 5, 'alpha': 0.36660000000000004, 'lambda': 16.621806187467094, 'max_bin': 397}. Best is trial 0 with value: 0.8355992710495699.\n",
      "[I 2023-12-04 16:52:16,835] Trial 7 finished with value: 0.8314895417018107 and parameters: {'n_estimators': 711, 'eta': 0.06011640543199083, 'max_depth': 5, 'alpha': 0.7689, 'lambda': 31.13758545203848, 'max_bin': 255}. Best is trial 0 with value: 0.8355992710495699.\n",
      "[I 2023-12-04 16:52:28,559] Trial 8 finished with value: 0.8368621550189916 and parameters: {'n_estimators': 875, 'eta': 0.03731059942407972, 'max_depth': 10, 'alpha': 0.9839, 'lambda': 29.980650708152297, 'max_bin': 373}. Best is trial 8 with value: 0.8368621550189916.\n",
      "[I 2023-12-04 16:52:33,141] Trial 9 finished with value: 0.8076451901926681 and parameters: {'n_estimators': 223, 'eta': 0.020712120415842673, 'max_depth': 8, 'alpha': 0.22010000000000002, 'lambda': 26.39851459997611, 'max_bin': 401}. Best is trial 8 with value: 0.8368621550189916.\n",
      "[I 2023-12-04 16:52:51,597] Trial 10 finished with value: 0.7877437947456845 and parameters: {'n_estimators': 849, 'eta': 0.00011265719582961853, 'max_depth': 10, 'alpha': 0.5964, 'lambda': 6.165290181498779, 'max_bin': 315}. Best is trial 8 with value: 0.8368621550189916.\n",
      "[I 2023-12-04 16:52:59,589] Trial 11 finished with value: 0.8395375929530757 and parameters: {'n_estimators': 665, 'eta': 0.06518207356742978, 'max_depth': 12, 'alpha': 0.5674, 'lambda': 37.880043401527985, 'max_bin': 500}. Best is trial 11 with value: 0.8395375929530757.\n",
      "[I 2023-12-04 16:53:06,194] Trial 12 finished with value: 0.8365993055563685 and parameters: {'n_estimators': 899, 'eta': 0.09235852126899434, 'max_depth': 10, 'alpha': 0.5892000000000001, 'lambda': 39.276589000844126, 'max_bin': 350}. Best is trial 11 with value: 0.8395375929530757.\n",
      "[I 2023-12-04 16:53:14,644] Trial 13 finished with value: 0.8360830778275542 and parameters: {'n_estimators': 707, 'eta': 0.05854616629946531, 'max_depth': 10, 'alpha': 0.7017, 'lambda': 38.310911142284844, 'max_bin': 499}. Best is trial 11 with value: 0.8395375929530757.\n",
      "[I 2023-12-04 16:53:22,103] Trial 14 finished with value: 0.8373762524441954 and parameters: {'n_estimators': 705, 'eta': 0.07369105264961996, 'max_depth': 11, 'alpha': 0.4786, 'lambda': 39.906457943185174, 'max_bin': 409}. Best is trial 11 with value: 0.8395375929530757.\n",
      "[I 2023-12-04 16:53:30,306] Trial 15 finished with value: 0.839270903709185 and parameters: {'n_estimators': 662, 'eta': 0.07017890092207792, 'max_depth': 11, 'alpha': 0.48410000000000003, 'lambda': 39.27282239675393, 'max_bin': 416}. Best is trial 11 with value: 0.8395375929530757.\n",
      "[I 2023-12-04 16:53:35,999] Trial 16 finished with value: 0.8393925071164681 and parameters: {'n_estimators': 641, 'eta': 0.09885168920199397, 'max_depth': 12, 'alpha': 0.4867, 'lambda': 35.65933560147215, 'max_bin': 498}. Best is trial 11 with value: 0.8395375929530757.\n",
      "[I 2023-12-04 16:53:42,021] Trial 17 finished with value: 0.8393344845303661 and parameters: {'n_estimators': 572, 'eta': 0.09976533517294688, 'max_depth': 12, 'alpha': 0.6457, 'lambda': 34.975332822074535, 'max_bin': 499}. Best is trial 11 with value: 0.8395375929530757.\n",
      "[I 2023-12-04 16:53:48,819] Trial 18 finished with value: 0.8423923708081489 and parameters: {'n_estimators': 372, 'eta': 0.08568977610320658, 'max_depth': 11, 'alpha': 0.3996, 'lambda': 35.0642653150748, 'max_bin': 484}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:53:54,267] Trial 19 finished with value: 0.8402090077061833 and parameters: {'n_estimators': 361, 'eta': 0.08558309443540715, 'max_depth': 9, 'alpha': 0.1346, 'lambda': 28.79601803333294, 'max_bin': 437}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:53:59,698] Trial 20 finished with value: 0.8337439287253922 and parameters: {'n_estimators': 365, 'eta': 0.08448929694393965, 'max_depth': 9, 'alpha': 0.0881, 'lambda': 28.130541349324726, 'max_bin': 429}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:54:06,058] Trial 21 finished with value: 0.8412504012695875 and parameters: {'n_estimators': 387, 'eta': 0.08226146376955015, 'max_depth': 11, 'alpha': 0.1312, 'lambda': 34.978877513065086, 'max_bin': 474}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:54:12,823] Trial 22 finished with value: 0.8384319408211394 and parameters: {'n_estimators': 367, 'eta': 0.08083837145321016, 'max_depth': 9, 'alpha': 0.14350000000000002, 'lambda': 32.9366418311634, 'max_bin': 470}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:54:19,364] Trial 23 finished with value: 0.8403044330594364 and parameters: {'n_estimators': 352, 'eta': 0.08970125666089078, 'max_depth': 11, 'alpha': 0.1757, 'lambda': 35.32812213915303, 'max_bin': 474}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:54:25,031] Trial 24 finished with value: 0.8392421506976563 and parameters: {'n_estimators': 311, 'eta': 0.09105247650201823, 'max_depth': 11, 'alpha': 0.2028, 'lambda': 35.89417852453088, 'max_bin': 471}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:54:31,652] Trial 25 finished with value: 0.8365236567526043 and parameters: {'n_estimators': 446, 'eta': 0.07658285023777045, 'max_depth': 11, 'alpha': 0.384, 'lambda': 33.57731474099809, 'max_bin': 474}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:54:35,236] Trial 26 finished with value: 0.8392177212059551 and parameters: {'n_estimators': 161, 'eta': 0.09068043533728479, 'max_depth': 11, 'alpha': 0.231, 'lambda': 35.47327835679016, 'max_bin': 482}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:54:40,584] Trial 27 finished with value: 0.8327563156474926 and parameters: {'n_estimators': 291, 'eta': 0.07186839898124413, 'max_depth': 8, 'alpha': 0.0304, 'lambda': 30.7280288938805, 'max_bin': 431}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:54:47,526] Trial 28 finished with value: 0.8380489077591783 and parameters: {'n_estimators': 441, 'eta': 0.07770797797034086, 'max_depth': 10, 'alpha': 0.3854, 'lambda': 37.09868386013816, 'max_bin': 389}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:54:55,276] Trial 29 finished with value: 0.836301961257246 and parameters: {'n_estimators': 404, 'eta': 0.06641042196201417, 'max_depth': 11, 'alpha': 0.2913, 'lambda': 32.95164678885235, 'max_bin': 457}. Best is trial 18 with value: 0.8423923708081489.\n",
      "[I 2023-12-04 16:55:01,982] Trial 30 finished with value: 0.8425128924158303 and parameters: {'n_estimators': 548, 'eta': 0.09018534818954298, 'max_depth': 12, 'alpha': 0.1328, 'lambda': 36.36011307197028, 'max_bin': 335}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:55:09,205] Trial 31 finished with value: 0.8363659908116491 and parameters: {'n_estimators': 556, 'eta': 0.08768609594373658, 'max_depth': 12, 'alpha': 0.12480000000000001, 'lambda': 36.50601392975394, 'max_bin': 328}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:55:15,657] Trial 32 finished with value: 0.8372829478721812 and parameters: {'n_estimators': 556, 'eta': 0.09579655104541517, 'max_depth': 12, 'alpha': 0.2904, 'lambda': 34.18110978238371, 'max_bin': 309}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:55:21,572] Trial 33 finished with value: 0.8369789135276486 and parameters: {'n_estimators': 311, 'eta': 0.09381281817298125, 'max_depth': 11, 'alpha': 0.0058000000000000005, 'lambda': 32.56596608651925, 'max_bin': 342}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:55:28,405] Trial 34 finished with value: 0.8391374688875942 and parameters: {'n_estimators': 406, 'eta': 0.0849063644595108, 'max_depth': 12, 'alpha': 0.0727, 'lambda': 36.91905543004737, 'max_bin': 454}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:55:35,272] Trial 35 finished with value: 0.8373689226250072 and parameters: {'n_estimators': 503, 'eta': 0.07787441228869207, 'max_depth': 10, 'alpha': 0.1874, 'lambda': 30.573835719058813, 'max_bin': 288}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:55:38,033] Trial 36 finished with value: 0.8199878816166191 and parameters: {'n_estimators': 155, 'eta': 0.08784120027143613, 'max_depth': 6, 'alpha': 0.33190000000000003, 'lambda': 24.573569427338725, 'max_bin': 483}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:55:43,586] Trial 37 finished with value: 0.8407499280513975 and parameters: {'n_estimators': 262, 'eta': 0.08128378368943565, 'max_depth': 11, 'alpha': 0.25620000000000004, 'lambda': 34.711447713959416, 'max_bin': 371}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:55:49,361] Trial 38 finished with value: 0.8377361161913923 and parameters: {'n_estimators': 258, 'eta': 0.0808669077731573, 'max_depth': 12, 'alpha': 0.4202, 'lambda': 39.955539402275036, 'max_bin': 376}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:55:51,706] Trial 39 finished with value: 0.8304469936169923 and parameters: {'n_estimators': 95, 'eta': 0.09557476165737694, 'max_depth': 11, 'alpha': 0.26130000000000003, 'lambda': 32.03292987617937, 'max_bin': 366}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:55:58,174] Trial 40 finished with value: 0.8405362087022186 and parameters: {'n_estimators': 470, 'eta': 0.08226348063536136, 'max_depth': 10, 'alpha': 0.0673, 'lambda': 28.278201297402383, 'max_bin': 339}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:56:04,122] Trial 41 finished with value: 0.8357012749808715 and parameters: {'n_estimators': 475, 'eta': 0.08217197183190797, 'max_depth': 10, 'alpha': 0.07680000000000001, 'lambda': 28.338686899598617, 'max_bin': 336}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:56:10,550] Trial 42 finished with value: 0.8399617431161186 and parameters: {'n_estimators': 414, 'eta': 0.07771385200211849, 'max_depth': 11, 'alpha': 0.252, 'lambda': 33.73314134053854, 'max_bin': 320}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:56:17,908] Trial 43 finished with value: 0.8374509713602389 and parameters: {'n_estimators': 599, 'eta': 0.07237945808507093, 'max_depth': 12, 'alpha': 0.0791, 'lambda': 37.3056797630355, 'max_bin': 303}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:56:24,014] Trial 44 finished with value: 0.8379125959963638 and parameters: {'n_estimators': 515, 'eta': 0.08614049152438857, 'max_depth': 10, 'alpha': 0.33080000000000004, 'lambda': 31.342560724325914, 'max_bin': 356}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:56:29,121] Trial 45 finished with value: 0.8388208409906011 and parameters: {'n_estimators': 276, 'eta': 0.09320964921037138, 'max_depth': 9, 'alpha': 0.1685, 'lambda': 29.92479225617179, 'max_bin': 381}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:56:35,919] Trial 46 finished with value: 0.8389811906144073 and parameters: {'n_estimators': 323, 'eta': 0.060193096651676097, 'max_depth': 11, 'alpha': 0.1125, 'lambda': 34.35442961086303, 'max_bin': 346}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:56:40,574] Trial 47 finished with value: 0.8366171236326163 and parameters: {'n_estimators': 189, 'eta': 0.08190281723469411, 'max_depth': 12, 'alpha': 0.038200000000000005, 'lambda': 37.54250531308986, 'max_bin': 297}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:56:45,507] Trial 48 finished with value: 0.833647460806246 and parameters: {'n_estimators': 222, 'eta': 0.06902253652427233, 'max_depth': 10, 'alpha': 0.4213, 'lambda': 26.48556138376619, 'max_bin': 332}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:56:53,571] Trial 49 finished with value: 0.8377487032067392 and parameters: {'n_estimators': 473, 'eta': 0.0740185702502072, 'max_depth': 10, 'alpha': 0.2323, 'lambda': 38.508089608962614, 'max_bin': 262}. Best is trial 30 with value: 0.8425128924158303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8425\n",
      "\tBest params:\n",
      "\t\tn_estimators: 548\n",
      "\t\teta: 0.09018534818954298\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.1328\n",
      "\t\tlambda: 36.36011307197028\n",
      "\t\tmax_bin: 335\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBClassifier\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  326.000000\n",
      "1                    TN  169.000000\n",
      "2                    FP   58.000000\n",
      "3                    FN   42.000000\n",
      "4              Accuracy    0.831933\n",
      "5             Precision    0.848958\n",
      "6           Sensitivity    0.885870\n",
      "7           Specificity    0.744500\n",
      "8              F1 score    0.867021\n",
      "9   F1 score (weighted)    0.830651\n",
      "10     F1 score (macro)    0.819355\n",
      "11    Balanced Accuracy    0.815181\n",
      "12                  MCC    0.640060\n",
      "13                  NPV    0.800900\n",
      "14              ROC_AUC    0.815181\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "   \n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_xgb_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_xgb_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_xgb_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_xgb_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_xgb_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_xgb_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_xgb_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_xgb_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_xgb_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_xgb_0)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:56:59,688] Trial 50 finished with value: 0.831626580178011 and parameters: {'n_estimators': 749, 'eta': 0.08829276781754136, 'max_depth': 12, 'alpha': 0.0054, 'lambda': 32.18680846636148, 'max_bin': 364}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:57:05,376] Trial 51 finished with value: 0.8313287419481007 and parameters: {'n_estimators': 339, 'eta': 0.08890751398515609, 'max_depth': 11, 'alpha': 0.1585, 'lambda': 35.357120158605674, 'max_bin': 489}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:57:10,684] Trial 52 finished with value: 0.831718226081899 and parameters: {'n_estimators': 385, 'eta': 0.0987183778766157, 'max_depth': 11, 'alpha': 0.20070000000000002, 'lambda': 35.173815580137344, 'max_bin': 419}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:57:16,982] Trial 53 finished with value: 0.8326132916086368 and parameters: {'n_estimators': 522, 'eta': 0.09103284635495493, 'max_depth': 11, 'alpha': 0.5301, 'lambda': 38.27324988097498, 'max_bin': 444}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:57:23,078] Trial 54 finished with value: 0.8317133527397456 and parameters: {'n_estimators': 432, 'eta': 0.08128836090515688, 'max_depth': 8, 'alpha': 0.1095, 'lambda': 34.39474871420841, 'max_bin': 466}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:57:28,857] Trial 55 finished with value: 0.8308938636523575 and parameters: {'n_estimators': 354, 'eta': 0.0952355471887138, 'max_depth': 12, 'alpha': 0.1776, 'lambda': 36.13481935680106, 'max_bin': 355}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:57:34,934] Trial 56 finished with value: 0.8317104598169969 and parameters: {'n_estimators': 593, 'eta': 0.08408203888585465, 'max_depth': 11, 'alpha': 0.0582, 'lambda': 31.668465856153563, 'max_bin': 398}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:57:39,178] Trial 57 finished with value: 0.8263456101677036 and parameters: {'n_estimators': 233, 'eta': 0.07465957591659862, 'max_depth': 7, 'alpha': 0.2674, 'lambda': 33.7725157731208, 'max_bin': 445}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:57:45,006] Trial 58 finished with value: 0.8292716422310358 and parameters: {'n_estimators': 456, 'eta': 0.09049139094787936, 'max_depth': 10, 'alpha': 0.44110000000000005, 'lambda': 38.63709631263243, 'max_bin': 463}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:57:50,934] Trial 59 finished with value: 0.8332673483878155 and parameters: {'n_estimators': 391, 'eta': 0.08560262263756473, 'max_depth': 9, 'alpha': 0.3366, 'lambda': 29.72508440999855, 'max_bin': 320}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:57:56,662] Trial 60 finished with value: 0.8310011966564905 and parameters: {'n_estimators': 285, 'eta': 0.07734586806141994, 'max_depth': 11, 'alpha': 0.8721, 'lambda': 36.655604163343334, 'max_bin': 488}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:01,978] Trial 61 finished with value: 0.8307481040462843 and parameters: {'n_estimators': 346, 'eta': 0.08477920334919803, 'max_depth': 9, 'alpha': 0.1471, 'lambda': 32.457285883287916, 'max_bin': 437}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:07,347] Trial 62 finished with value: 0.8324091301654386 and parameters: {'n_estimators': 376, 'eta': 0.08000179805435764, 'max_depth': 9, 'alpha': 0.1262, 'lambda': 28.91638749010866, 'max_bin': 477}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:12,671] Trial 63 finished with value: 0.8292516614390554 and parameters: {'n_estimators': 426, 'eta': 0.0920072646340848, 'max_depth': 8, 'alpha': 0.10200000000000001, 'lambda': 35.518832494135815, 'max_bin': 492}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:18,097] Trial 64 finished with value: 0.8316186702846048 and parameters: {'n_estimators': 479, 'eta': 0.08779362008438059, 'max_depth': 10, 'alpha': 0.2177, 'lambda': 33.645952433693, 'max_bin': 385}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:22,968] Trial 65 finished with value: 0.8342989839700014 and parameters: {'n_estimators': 531, 'eta': 0.098594460370144, 'max_depth': 11, 'alpha': 0.0455, 'lambda': 31.02272106782113, 'max_bin': 407}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:27,539] Trial 66 finished with value: 0.832658352227055 and parameters: {'n_estimators': 322, 'eta': 0.083184205331928, 'max_depth': 9, 'alpha': 0.14450000000000002, 'lambda': 24.08172332442345, 'max_bin': 461}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:32,446] Trial 67 finished with value: 0.8323689681299106 and parameters: {'n_estimators': 267, 'eta': 0.07519763605481461, 'max_depth': 10, 'alpha': 0.1847, 'lambda': 27.748084706557705, 'max_bin': 476}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:38,312] Trial 68 finished with value: 0.8325209082363385 and parameters: {'n_estimators': 308, 'eta': 0.06944056393672685, 'max_depth': 11, 'alpha': 0.2444, 'lambda': 39.39179134961118, 'max_bin': 449}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:44,616] Trial 69 finished with value: 0.8322660455028525 and parameters: {'n_estimators': 620, 'eta': 0.07987631725578118, 'max_depth': 12, 'alpha': 0.3018, 'lambda': 36.67308060300244, 'max_bin': 431}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:49,029] Trial 70 finished with value: 0.8322246396034341 and parameters: {'n_estimators': 234, 'eta': 0.08926256907836, 'max_depth': 10, 'alpha': 0.6504, 'lambda': 32.64297219109792, 'max_bin': 340}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:58:54,732] Trial 71 finished with value: 0.8365340854291713 and parameters: {'n_estimators': 408, 'eta': 0.07935109119556692, 'max_depth': 11, 'alpha': 0.2753, 'lambda': 34.3832302838886, 'max_bin': 313}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:00,461] Trial 72 finished with value: 0.8336192323697087 and parameters: {'n_estimators': 422, 'eta': 0.08599156085722917, 'max_depth': 11, 'alpha': 0.35550000000000004, 'lambda': 32.868918331674, 'max_bin': 327}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:06,760] Trial 73 finished with value: 0.832738426600742 and parameters: {'n_estimators': 363, 'eta': 0.07751747360801359, 'max_depth': 12, 'alpha': 0.1991, 'lambda': 37.775948103718896, 'max_bin': 371}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:12,082] Trial 74 finished with value: 0.833888027997089 and parameters: {'n_estimators': 452, 'eta': 0.08298378442022407, 'max_depth': 11, 'alpha': 0.23970000000000002, 'lambda': 33.72649929851652, 'max_bin': 325}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:16,928] Trial 75 finished with value: 0.8318987460053051 and parameters: {'n_estimators': 336, 'eta': 0.09337490080716956, 'max_depth': 10, 'alpha': 0.15, 'lambda': 30.75236527262549, 'max_bin': 347}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:23,269] Trial 76 finished with value: 0.8329704645853369 and parameters: {'n_estimators': 497, 'eta': 0.07516550016379085, 'max_depth': 11, 'alpha': 0.0852, 'lambda': 35.07017158643329, 'max_bin': 297}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:28,868] Trial 77 finished with value: 0.8355083536936091 and parameters: {'n_estimators': 394, 'eta': 0.08700162289451879, 'max_depth': 12, 'alpha': 0.031200000000000002, 'lambda': 35.76248527144872, 'max_bin': 320}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:33,652] Trial 78 finished with value: 0.8308158958828994 and parameters: {'n_estimators': 541, 'eta': 0.09658174547005169, 'max_depth': 11, 'alpha': 0.1302, 'lambda': 37.21410368480585, 'max_bin': 283}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:38,468] Trial 79 finished with value: 0.8366337952779634 and parameters: {'n_estimators': 305, 'eta': 0.09159009823403937, 'max_depth': 10, 'alpha': 0.3103, 'lambda': 31.52502082751266, 'max_bin': 423}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:43,772] Trial 80 finished with value: 0.8312781552256145 and parameters: {'n_estimators': 485, 'eta': 0.08345293165812308, 'max_depth': 12, 'alpha': 0.2147, 'lambda': 29.33857056867027, 'max_bin': 455}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:50,643] Trial 81 finished with value: 0.8351519016834461 and parameters: {'n_estimators': 796, 'eta': 0.07255643533259777, 'max_depth': 12, 'alpha': 0.5714, 'lambda': 38.69126262285737, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 16:59:56,508] Trial 82 finished with value: 0.8339050702604375 and parameters: {'n_estimators': 673, 'eta': 0.07894030864087347, 'max_depth': 12, 'alpha': 0.5318, 'lambda': 34.50773914718354, 'max_bin': 488}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:02,350] Trial 83 finished with value: 0.8289446468385069 and parameters: {'n_estimators': 773, 'eta': 0.08886232440359275, 'max_depth': 6, 'alpha': 0.7646000000000001, 'lambda': 36.094027517379935, 'max_bin': 470}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:09,125] Trial 84 finished with value: 0.8359139741802869 and parameters: {'n_estimators': 411, 'eta': 0.06650936030797089, 'max_depth': 11, 'alpha': 0.4575, 'lambda': 33.4262613580401, 'max_bin': 482}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:14,972] Trial 85 finished with value: 0.8303030199203313 and parameters: {'n_estimators': 669, 'eta': 0.08155699514561546, 'max_depth': 12, 'alpha': 0.5972000000000001, 'lambda': 37.788237233770715, 'max_bin': 392}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:20,615] Trial 86 finished with value: 0.8338071115995603 and parameters: {'n_estimators': 574, 'eta': 0.08603557208197132, 'max_depth': 11, 'alpha': 0.3806, 'lambda': 39.975275825586934, 'max_bin': 495}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:26,561] Trial 87 finished with value: 0.829988252999752 and parameters: {'n_estimators': 371, 'eta': 0.07611858612639488, 'max_depth': 11, 'alpha': 0.1695, 'lambda': 35.04248172200173, 'max_bin': 334}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:33,066] Trial 88 finished with value: 0.8282539033631486 and parameters: {'n_estimators': 443, 'eta': 0.07180998496991371, 'max_depth': 12, 'alpha': 0.52, 'lambda': 36.47994787720749, 'max_bin': 479}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:38,448] Trial 89 finished with value: 0.8318526415477914 and parameters: {'n_estimators': 637, 'eta': 0.07831502401661597, 'max_depth': 12, 'alpha': 0.10110000000000001, 'lambda': 30.402660340309136, 'max_bin': 485}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:43,851] Trial 90 finished with value: 0.8293388131710108 and parameters: {'n_estimators': 343, 'eta': 0.08297548030439116, 'max_depth': 10, 'alpha': 0.0603, 'lambda': 31.82472735845935, 'max_bin': 355}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:49,397] Trial 91 finished with value: 0.832664788114452 and parameters: {'n_estimators': 720, 'eta': 0.08909822324523323, 'max_depth': 12, 'alpha': 0.4864, 'lambda': 33.3045126091898, 'max_bin': 496}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:54,851] Trial 92 finished with value: 0.8298640284614172 and parameters: {'n_estimators': 688, 'eta': 0.09452068684071355, 'max_depth': 12, 'alpha': 0.615, 'lambda': 36.02479597400579, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:00:59,774] Trial 93 finished with value: 0.8312573146743552 and parameters: {'n_estimators': 644, 'eta': 0.09985860537919312, 'max_depth': 11, 'alpha': 0.6464000000000001, 'lambda': 37.713067980981904, 'max_bin': 466}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:01:05,274] Trial 94 finished with value: 0.8252891840524585 and parameters: {'n_estimators': 611, 'eta': 0.09308368801010884, 'max_depth': 8, 'alpha': 0.5124000000000001, 'lambda': 34.681718891329645, 'max_bin': 476}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:01:10,673] Trial 95 finished with value: 0.8324183080001385 and parameters: {'n_estimators': 563, 'eta': 0.09075974557614451, 'max_depth': 11, 'alpha': 0.2539, 'lambda': 39.016123557385235, 'max_bin': 493}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:01:15,894] Trial 96 finished with value: 0.8344197522395917 and parameters: {'n_estimators': 470, 'eta': 0.09702509145267253, 'max_depth': 12, 'alpha': 0.458, 'lambda': 36.761303124858344, 'max_bin': 310}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:01:21,313] Trial 97 finished with value: 0.8332777547504728 and parameters: {'n_estimators': 386, 'eta': 0.08628594078063465, 'max_depth': 11, 'alpha': 0.41290000000000004, 'lambda': 35.14665929958303, 'max_bin': 469}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:01:26,900] Trial 98 finished with value: 0.832073999006329 and parameters: {'n_estimators': 729, 'eta': 0.08436304237275954, 'max_depth': 9, 'alpha': 0.5579000000000001, 'lambda': 34.0530845234543, 'max_bin': 361}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:01:29,308] Trial 99 finished with value: 0.8381935873858621 and parameters: {'n_estimators': 104, 'eta': 0.09416416276440615, 'max_depth': 12, 'alpha': 0.1262, 'lambda': 32.67080566935745, 'max_bin': 484}. Best is trial 30 with value: 0.8425128924158303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8425\n",
      "\tBest params:\n",
      "\t\tn_estimators: 548\n",
      "\t\teta: 0.09018534818954298\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.1328\n",
      "\t\tlambda: 36.36011307197028\n",
      "\t\tmax_bin: 335\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  326.000000  318.000000\n",
      "1                    TN  169.000000  186.000000\n",
      "2                    FP   58.000000   51.000000\n",
      "3                    FN   42.000000   40.000000\n",
      "4              Accuracy    0.831933    0.847059\n",
      "5             Precision    0.848958    0.861789\n",
      "6           Sensitivity    0.885870    0.888268\n",
      "7           Specificity    0.744500    0.784800\n",
      "8              F1 score    0.867021    0.874828\n",
      "9   F1 score (weighted)    0.830651    0.846399\n",
      "10     F1 score (macro)    0.819355    0.839142\n",
      "11    Balanced Accuracy    0.815181    0.836539\n",
      "12                  MCC    0.640060    0.678913\n",
      "13                  NPV    0.800900    0.823000\n",
      "14              ROC_AUC    0.815181    0.836539\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_xgb_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_xgb_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_xgb_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_xgb_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_xgb_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_xgb_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_xgb_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_xgb_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_xgb_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_xgb_1)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:01:35,297] Trial 100 finished with value: 0.8339846722786323 and parameters: {'n_estimators': 292, 'eta': 0.08131056651110546, 'max_depth': 11, 'alpha': 0.6858000000000001, 'lambda': 38.029362754973256, 'max_bin': 339}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:01:40,782] Trial 101 finished with value: 0.8337827363692204 and parameters: {'n_estimators': 507, 'eta': 0.09744034964565274, 'max_depth': 12, 'alpha': 0.7166, 'lambda': 35.538607945477004, 'max_bin': 492}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:01:45,900] Trial 102 finished with value: 0.8319923878352364 and parameters: {'n_estimators': 579, 'eta': 0.09999813719585379, 'max_depth': 12, 'alpha': 0.5564, 'lambda': 36.760414132629634, 'max_bin': 487}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:01:50,867] Trial 103 finished with value: 0.8356147966413291 and parameters: {'n_estimators': 547, 'eta': 0.09597917652776401, 'max_depth': 12, 'alpha': 0.1923, 'lambda': 33.34137512233972, 'max_bin': 461}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:01:56,125] Trial 104 finished with value: 0.8345577268026247 and parameters: {'n_estimators': 632, 'eta': 0.09043424155139451, 'max_depth': 11, 'alpha': 0.6507000000000001, 'lambda': 34.346670580878566, 'max_bin': 473}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:01,659] Trial 105 finished with value: 0.8407604885236324 and parameters: {'n_estimators': 427, 'eta': 0.08732492011557849, 'max_depth': 12, 'alpha': 0.6162000000000001, 'lambda': 32.0246114882969, 'max_bin': 437}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:07,263] Trial 106 finished with value: 0.8349704161184466 and parameters: {'n_estimators': 428, 'eta': 0.08751131297859385, 'max_depth': 12, 'alpha': 0.6139, 'lambda': 32.02770968353806, 'max_bin': 450}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:12,736] Trial 107 finished with value: 0.8394085558881652 and parameters: {'n_estimators': 458, 'eta': 0.0847138271268918, 'max_depth': 11, 'alpha': 0.09290000000000001, 'lambda': 31.283968247176762, 'max_bin': 437}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:17,876] Trial 108 finished with value: 0.8358680013617675 and parameters: {'n_estimators': 360, 'eta': 0.08000600851864306, 'max_depth': 10, 'alpha': 0.021500000000000002, 'lambda': 29.807533887237202, 'max_bin': 444}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:23,004] Trial 109 finished with value: 0.8352916554280455 and parameters: {'n_estimators': 451, 'eta': 0.08417812513760269, 'max_depth': 11, 'alpha': 0.0668, 'lambda': 31.08995497932718, 'max_bin': 440}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:28,605] Trial 110 finished with value: 0.8341442844880754 and parameters: {'n_estimators': 409, 'eta': 0.07706342657030184, 'max_depth': 11, 'alpha': 0.0876, 'lambda': 32.14186434175922, 'max_bin': 406}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:33,967] Trial 111 finished with value: 0.8372758229321453 and parameters: {'n_estimators': 494, 'eta': 0.0858043168167775, 'max_depth': 11, 'alpha': 0.10540000000000001, 'lambda': 35.81620747695646, 'max_bin': 413}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:39,287] Trial 112 finished with value: 0.8320244068497431 and parameters: {'n_estimators': 465, 'eta': 0.09195834338114656, 'max_depth': 11, 'alpha': 0.154, 'lambda': 37.249702815066, 'max_bin': 430}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:44,657] Trial 113 finished with value: 0.829355139372635 and parameters: {'n_estimators': 434, 'eta': 0.081862003424512, 'max_depth': 10, 'alpha': 0.1688, 'lambda': 33.18051955287523, 'max_bin': 480}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:49,871] Trial 114 finished with value: 0.8325732781308537 and parameters: {'n_estimators': 395, 'eta': 0.08922890845730365, 'max_depth': 12, 'alpha': 0.22060000000000002, 'lambda': 34.031231317749736, 'max_bin': 321}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:02:54,691] Trial 115 finished with value: 0.8333736199845152 and parameters: {'n_estimators': 328, 'eta': 0.08770247806671806, 'max_depth': 11, 'alpha': 0.1317, 'lambda': 31.306881300776162, 'max_bin': 457}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:02,147] Trial 116 finished with value: 0.8330624050854724 and parameters: {'n_estimators': 380, 'eta': 0.07960448415174332, 'max_depth': 11, 'alpha': 0.403, 'lambda': 32.7288860225384, 'max_bin': 438}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:08,765] Trial 117 finished with value: 0.835456856324658 and parameters: {'n_estimators': 462, 'eta': 0.08366354551021166, 'max_depth': 12, 'alpha': 0.2747, 'lambda': 28.869663532999162, 'max_bin': 426}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:14,731] Trial 118 finished with value: 0.8302466298774123 and parameters: {'n_estimators': 419, 'eta': 0.08592714147641747, 'max_depth': 12, 'alpha': 0.18030000000000002, 'lambda': 30.47262188208885, 'max_bin': 304}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:20,167] Trial 119 finished with value: 0.8319890891167295 and parameters: {'n_estimators': 531, 'eta': 0.09291718561700517, 'max_depth': 10, 'alpha': 0.31620000000000004, 'lambda': 35.041500487892094, 'max_bin': 347}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:23,711] Trial 120 finished with value: 0.8295058568715723 and parameters: {'n_estimators': 193, 'eta': 0.07577094206885074, 'max_depth': 9, 'alpha': 0.1165, 'lambda': 36.27771867991171, 'max_bin': 369}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:28,801] Trial 121 finished with value: 0.8343817274670065 and parameters: {'n_estimators': 584, 'eta': 0.0978603240879747, 'max_depth': 12, 'alpha': 0.6272, 'lambda': 35.053473913832335, 'max_bin': 496}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:33,785] Trial 122 finished with value: 0.8332772952655301 and parameters: {'n_estimators': 698, 'eta': 0.09525233743329005, 'max_depth': 12, 'alpha': 0.5905, 'lambda': 34.158474144488295, 'max_bin': 375}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:38,856] Trial 123 finished with value: 0.8337674668637554 and parameters: {'n_estimators': 521, 'eta': 0.0906391522826673, 'max_depth': 12, 'alpha': 0.044700000000000004, 'lambda': 27.776027500506203, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:44,688] Trial 124 finished with value: 0.8338254194574738 and parameters: {'n_estimators': 659, 'eta': 0.08769573584010991, 'max_depth': 11, 'alpha': 0.9695, 'lambda': 35.916524568092115, 'max_bin': 490}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:50,323] Trial 125 finished with value: 0.8397196510233067 and parameters: {'n_estimators': 361, 'eta': 0.08295319556213791, 'max_depth': 11, 'alpha': 0.6753, 'lambda': 37.33449006318144, 'max_bin': 330}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:03:55,089] Trial 126 finished with value: 0.8363676082093143 and parameters: {'n_estimators': 251, 'eta': 0.08219547385656012, 'max_depth': 11, 'alpha': 0.6833, 'lambda': 38.64296669983386, 'max_bin': 333}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:00,537] Trial 127 finished with value: 0.8372304155394998 and parameters: {'n_estimators': 346, 'eta': 0.08436508089443731, 'max_depth': 11, 'alpha': 0.7113, 'lambda': 37.250126462008794, 'max_bin': 325}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:05,464] Trial 128 finished with value: 0.8249595426535914 and parameters: {'n_estimators': 356, 'eta': 0.07808789358230246, 'max_depth': 7, 'alpha': 0.3534, 'lambda': 37.684586231648126, 'max_bin': 315}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:10,962] Trial 129 finished with value: 0.8358067789700527 and parameters: {'n_estimators': 371, 'eta': 0.08082735314844594, 'max_depth': 11, 'alpha': 0.5517000000000001, 'lambda': 33.61826501797404, 'max_bin': 417}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:16,219] Trial 130 finished with value: 0.8340386395202918 and parameters: {'n_estimators': 440, 'eta': 0.08597520369006961, 'max_depth': 10, 'alpha': 0.6779000000000001, 'lambda': 32.15085874789206, 'max_bin': 381}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:21,488] Trial 131 finished with value: 0.8344268947439882 and parameters: {'n_estimators': 618, 'eta': 0.09401954367285045, 'max_depth': 12, 'alpha': 0.6424000000000001, 'lambda': 34.830146899416576, 'max_bin': 352}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:26,795] Trial 132 finished with value: 0.8306426372384503 and parameters: {'n_estimators': 405, 'eta': 0.08974576047508394, 'max_depth': 11, 'alpha': 0.5773, 'lambda': 36.40092969297229, 'max_bin': 483}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:32,766] Trial 133 finished with value: 0.8380246278422134 and parameters: {'n_estimators': 481, 'eta': 0.08313462497078636, 'max_depth': 12, 'alpha': 0.08510000000000001, 'lambda': 35.53162092652254, 'max_bin': 343}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:37,570] Trial 134 finished with value: 0.8316906353736989 and parameters: {'n_estimators': 319, 'eta': 0.09263852611133247, 'max_depth': 11, 'alpha': 0.2051, 'lambda': 38.33147274429844, 'max_bin': 332}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:43,788] Trial 135 finished with value: 0.8357462391559766 and parameters: {'n_estimators': 390, 'eta': 0.06382193108112927, 'max_depth': 12, 'alpha': 0.6694, 'lambda': 37.01267706748343, 'max_bin': 435}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:48,145] Trial 136 finished with value: 0.8317510628589094 and parameters: {'n_estimators': 287, 'eta': 0.08757458719865735, 'max_depth': 8, 'alpha': 0.6069, 'lambda': 33.1243782066438, 'max_bin': 402}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:53,960] Trial 137 finished with value: 0.8357773457873294 and parameters: {'n_estimators': 596, 'eta': 0.07891665854944246, 'max_depth': 11, 'alpha': 0.0007, 'lambda': 34.52322285725021, 'max_bin': 475}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:04:58,674] Trial 138 finished with value: 0.8384012784315914 and parameters: {'n_estimators': 373, 'eta': 0.09684739375799374, 'max_depth': 12, 'alpha': 0.7559, 'lambda': 27.02134653024212, 'max_bin': 391}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:05:04,210] Trial 139 finished with value: 0.8297826845449044 and parameters: {'n_estimators': 419, 'eta': 0.08976112970688468, 'max_depth': 11, 'alpha': 0.7334, 'lambda': 39.26444733940313, 'max_bin': 494}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:05:07,559] Trial 140 finished with value: 0.8331338672199194 and parameters: {'n_estimators': 154, 'eta': 0.07441651013647321, 'max_depth': 12, 'alpha': 0.4898, 'lambda': 29.909702195465467, 'max_bin': 489}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:05:14,201] Trial 141 finished with value: 0.8367883358429948 and parameters: {'n_estimators': 677, 'eta': 0.06997955681647985, 'max_depth': 11, 'alpha': 0.5284, 'lambda': 39.62543723282163, 'max_bin': 329}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:05:19,662] Trial 142 finished with value: 0.8354813559816214 and parameters: {'n_estimators': 335, 'eta': 0.08116040550578078, 'max_depth': 11, 'alpha': 0.43320000000000003, 'lambda': 38.334274727591506, 'max_bin': 418}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:05:25,955] Trial 143 finished with value: 0.8373331845300239 and parameters: {'n_estimators': 650, 'eta': 0.07659272079446572, 'max_depth': 11, 'alpha': 0.47900000000000004, 'lambda': 35.731883155405164, 'max_bin': 466}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:05:31,792] Trial 144 finished with value: 0.8335811232573123 and parameters: {'n_estimators': 732, 'eta': 0.08371235780384242, 'max_depth': 11, 'alpha': 0.4519, 'lambda': 36.67874266347699, 'max_bin': 340}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:05:38,232] Trial 145 finished with value: 0.8347217391023337 and parameters: {'n_estimators': 629, 'eta': 0.07315862642455462, 'max_depth': 10, 'alpha': 0.502, 'lambda': 37.734976167869505, 'max_bin': 450}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:05:43,786] Trial 146 finished with value: 0.8337406610723128 and parameters: {'n_estimators': 453, 'eta': 0.08522264527225736, 'max_depth': 12, 'alpha': 0.10590000000000001, 'lambda': 39.010758071678666, 'max_bin': 426}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:05:49,544] Trial 147 finished with value: 0.8356168997781358 and parameters: {'n_estimators': 611, 'eta': 0.07941623589564055, 'max_depth': 11, 'alpha': 0.8014, 'lambda': 28.68300085885236, 'max_bin': 316}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:05:57,133] Trial 148 finished with value: 0.8330996481186608 and parameters: {'n_estimators': 558, 'eta': 0.052576712563523684, 'max_depth': 11, 'alpha': 0.1443, 'lambda': 34.20613691949161, 'max_bin': 496}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:02,203] Trial 149 finished with value: 0.8368171791011514 and parameters: {'n_estimators': 689, 'eta': 0.09181085222902878, 'max_depth': 9, 'alpha': 0.060500000000000005, 'lambda': 31.771483386844032, 'max_bin': 481}. Best is trial 30 with value: 0.8425128924158303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8425\n",
      "\tBest params:\n",
      "\t\tn_estimators: 548\n",
      "\t\teta: 0.09018534818954298\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.1328\n",
      "\t\tlambda: 36.36011307197028\n",
      "\t\tmax_bin: 335\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  326.000000  318.000000  325.000000\n",
      "1                    TN  169.000000  186.000000  175.000000\n",
      "2                    FP   58.000000   51.000000   54.000000\n",
      "3                    FN   42.000000   40.000000   41.000000\n",
      "4              Accuracy    0.831933    0.847059    0.840336\n",
      "5             Precision    0.848958    0.861789    0.857520\n",
      "6           Sensitivity    0.885870    0.888268    0.887978\n",
      "7           Specificity    0.744500    0.784800    0.764200\n",
      "8              F1 score    0.867021    0.874828    0.872483\n",
      "9   F1 score (weighted)    0.830651    0.846399    0.839397\n",
      "10     F1 score (macro)    0.819355    0.839142    0.829500\n",
      "11    Balanced Accuracy    0.815181    0.836539    0.826085\n",
      "12                  MCC    0.640060    0.678913    0.659892\n",
      "13                  NPV    0.800900    0.823000    0.810200\n",
      "14              ROC_AUC    0.815181    0.836539    0.826085\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_xgb_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_xgb_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_xgb_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_xgb_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_xgb_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_xgb_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_xgb_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_xgb_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_xgb_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_xgb_2)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:06:08,170] Trial 150 finished with value: 0.8260513980993751 and parameters: {'n_estimators': 502, 'eta': 0.08746031460470927, 'max_depth': 5, 'alpha': 0.538, 'lambda': 33.63258702149681, 'max_bin': 471}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:12,790] Trial 151 finished with value: 0.8318863758140163 and parameters: {'n_estimators': 303, 'eta': 0.09877630514590498, 'max_depth': 11, 'alpha': 0.1689, 'lambda': 36.06638541265984, 'max_bin': 486}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:17,795] Trial 152 finished with value: 0.8344938819277997 and parameters: {'n_estimators': 360, 'eta': 0.08931221061152136, 'max_depth': 11, 'alpha': 0.2214, 'lambda': 34.8582488325943, 'max_bin': 462}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:22,588] Trial 153 finished with value: 0.8308429087849589 and parameters: {'n_estimators': 262, 'eta': 0.08192413697646124, 'max_depth': 12, 'alpha': 0.137, 'lambda': 36.752540854098314, 'max_bin': 435}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:27,345] Trial 154 finished with value: 0.8311032756567943 and parameters: {'n_estimators': 315, 'eta': 0.09608680408421827, 'max_depth': 11, 'alpha': 0.23720000000000002, 'lambda': 35.451818993681805, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:32,043] Trial 155 finished with value: 0.8266254948136138 and parameters: {'n_estimators': 391, 'eta': 0.09476043400468495, 'max_depth': 10, 'alpha': 0.6251, 'lambda': 32.94741287745101, 'max_bin': 453}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:37,183] Trial 156 finished with value: 0.8286711573812079 and parameters: {'n_estimators': 659, 'eta': 0.08611230089992887, 'max_depth': 11, 'alpha': 0.5798, 'lambda': 37.89681401470706, 'max_bin': 472}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:41,364] Trial 157 finished with value: 0.8337831400152995 and parameters: {'n_estimators': 215, 'eta': 0.09129871471956087, 'max_depth': 12, 'alpha': 0.257, 'lambda': 25.5982837188478, 'max_bin': 443}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:46,125] Trial 158 finished with value: 0.8329306273399866 and parameters: {'n_estimators': 347, 'eta': 0.08484932337574415, 'max_depth': 11, 'alpha': 0.1869, 'lambda': 30.996128112431837, 'max_bin': 478}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:51,274] Trial 159 finished with value: 0.8302779125623985 and parameters: {'n_estimators': 276, 'eta': 0.07719343935111064, 'max_depth': 12, 'alpha': 0.1526, 'lambda': 37.20499074601822, 'max_bin': 321}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:06:57,006] Trial 160 finished with value: 0.8304386866048411 and parameters: {'n_estimators': 432, 'eta': 0.07170151185211136, 'max_depth': 11, 'alpha': 0.11910000000000001, 'lambda': 29.203424312843197, 'max_bin': 493}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:00,479] Trial 161 finished with value: 0.8309208079557948 and parameters: {'n_estimators': 170, 'eta': 0.0879883844955047, 'max_depth': 11, 'alpha': 0.2889, 'lambda': 35.393999620330895, 'max_bin': 486}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:05,973] Trial 162 finished with value: 0.8284745560371359 and parameters: {'n_estimators': 411, 'eta': 0.0826343601305139, 'max_depth': 11, 'alpha': 0.2036, 'lambda': 36.12641932001895, 'max_bin': 480}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:08,008] Trial 163 finished with value: 0.8290922503868401 and parameters: {'n_estimators': 93, 'eta': 0.09256241551678872, 'max_depth': 11, 'alpha': 0.0838, 'lambda': 34.1056609438438, 'max_bin': 467}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:11,018] Trial 164 finished with value: 0.8235945150297294 and parameters: {'n_estimators': 145, 'eta': 0.09994562373481224, 'max_depth': 11, 'alpha': 0.2293, 'lambda': 34.88959295755971, 'max_bin': 490}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:13,835] Trial 165 finished with value: 0.8240131825901675 and parameters: {'n_estimators': 127, 'eta': 0.08057101266401398, 'max_depth': 12, 'alpha': 0.6574, 'lambda': 33.54916261898755, 'max_bin': 336}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:18,657] Trial 166 finished with value: 0.8322334604099844 and parameters: {'n_estimators': 375, 'eta': 0.08937798614789572, 'max_depth': 11, 'alpha': 0.6974, 'lambda': 32.58137637610688, 'max_bin': 460}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:24,682] Trial 167 finished with value: 0.8285422234641795 and parameters: {'n_estimators': 338, 'eta': 0.06773168801153077, 'max_depth': 10, 'alpha': 0.2844, 'lambda': 36.50789695951348, 'max_bin': 485}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:29,413] Trial 168 finished with value: 0.832597806896106 and parameters: {'n_estimators': 392, 'eta': 0.08452618156996507, 'max_depth': 12, 'alpha': 0.1752, 'lambda': 22.340632223862332, 'max_bin': 475}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:33,838] Trial 169 finished with value: 0.8295388755405625 and parameters: {'n_estimators': 243, 'eta': 0.09419536042109672, 'max_depth': 11, 'alpha': 0.0999, 'lambda': 39.936257569577215, 'max_bin': 329}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:39,023] Trial 170 finished with value: 0.831993045515605 and parameters: {'n_estimators': 328, 'eta': 0.08664735290407351, 'max_depth': 12, 'alpha': 0.2466, 'lambda': 37.319025652515926, 'max_bin': 447}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:44,159] Trial 171 finished with value: 0.8330568566377053 and parameters: {'n_estimators': 405, 'eta': 0.09098636796512023, 'max_depth': 12, 'alpha': 0.025, 'lambda': 35.78452024797448, 'max_bin': 455}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:49,472] Trial 172 finished with value: 0.8288660467649569 and parameters: {'n_estimators': 359, 'eta': 0.08272392150103411, 'max_depth': 12, 'alpha': 0.0726, 'lambda': 38.26829323890022, 'max_bin': 440}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:50,732] Trial 173 finished with value: 0.8168188879071552 and parameters: {'n_estimators': 51, 'eta': 0.08493543216802875, 'max_depth': 12, 'alpha': 0.1206, 'lambda': 34.61886508767584, 'max_bin': 465}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:07:55,800] Trial 174 finished with value: 0.8294312170752457 and parameters: {'n_estimators': 423, 'eta': 0.08824749700305093, 'max_depth': 10, 'alpha': 0.16290000000000002, 'lambda': 36.94944401722015, 'max_bin': 431}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:01,465] Trial 175 finished with value: 0.8296845456519788 and parameters: {'n_estimators': 443, 'eta': 0.07910897896896651, 'max_depth': 11, 'alpha': 0.4642, 'lambda': 38.9508400479048, 'max_bin': 362}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:06,565] Trial 176 finished with value: 0.8293441364939236 and parameters: {'n_estimators': 481, 'eta': 0.08655788081490372, 'max_depth': 11, 'alpha': 0.053000000000000005, 'lambda': 35.31467707784056, 'max_bin': 412}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:11,846] Trial 177 finished with value: 0.8326721227886024 and parameters: {'n_estimators': 372, 'eta': 0.07583647448133608, 'max_depth': 12, 'alpha': 0.393, 'lambda': 30.54416412383349, 'max_bin': 496}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:17,111] Trial 178 finished with value: 0.8288467245547249 and parameters: {'n_estimators': 456, 'eta': 0.08081168681197262, 'max_depth': 11, 'alpha': 0.63, 'lambda': 36.32708462273543, 'max_bin': 253}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:23,490] Trial 179 finished with value: 0.8331632183898631 and parameters: {'n_estimators': 390, 'eta': 0.06461148032023795, 'max_depth': 12, 'alpha': 0.1962, 'lambda': 33.92345205651532, 'max_bin': 422}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:28,505] Trial 180 finished with value: 0.8344429042579119 and parameters: {'n_estimators': 295, 'eta': 0.09744004743501501, 'max_depth': 12, 'alpha': 0.5969, 'lambda': 37.46236302102782, 'max_bin': 263}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:34,400] Trial 181 finished with value: 0.8327403765261862 and parameters: {'n_estimators': 319, 'eta': 0.06039313567282033, 'max_depth': 11, 'alpha': 0.09870000000000001, 'lambda': 34.55244825209996, 'max_bin': 346}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:39,782] Trial 182 finished with value: 0.8280650084303474 and parameters: {'n_estimators': 345, 'eta': 0.06956739083568633, 'max_depth': 11, 'alpha': 0.1371, 'lambda': 33.112401327499136, 'max_bin': 357}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:45,175] Trial 183 finished with value: 0.8285367640740242 and parameters: {'n_estimators': 640, 'eta': 0.08323695133290801, 'max_depth': 11, 'alpha': 0.0724, 'lambda': 35.53336454171641, 'max_bin': 325}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:50,577] Trial 184 finished with value: 0.8303784237639334 and parameters: {'n_estimators': 678, 'eta': 0.09050955329592451, 'max_depth': 11, 'alpha': 0.11460000000000001, 'lambda': 31.953584782693284, 'max_bin': 342}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:08:55,977] Trial 185 finished with value: 0.8328642976162832 and parameters: {'n_estimators': 353, 'eta': 0.08903272051118223, 'max_depth': 11, 'alpha': 0.5017, 'lambda': 34.73949467033882, 'max_bin': 471}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:00,267] Trial 186 finished with value: 0.8321601943829826 and parameters: {'n_estimators': 404, 'eta': 0.09297285197035378, 'max_depth': 11, 'alpha': 0.1587, 'lambda': 18.264023580124817, 'max_bin': 336}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:05,104] Trial 187 finished with value: 0.8304596362004618 and parameters: {'n_estimators': 469, 'eta': 0.0850211779644761, 'max_depth': 11, 'alpha': 0.2654, 'lambda': 28.277489443571177, 'max_bin': 478}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:11,626] Trial 188 finished with value: 0.8315218365121669 and parameters: {'n_estimators': 374, 'eta': 0.0616702331328264, 'max_depth': 12, 'alpha': 0.134, 'lambda': 36.247106788595126, 'max_bin': 494}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:17,095] Trial 189 finished with value: 0.8283555891605507 and parameters: {'n_estimators': 431, 'eta': 0.06535805402946564, 'max_depth': 9, 'alpha': 0.0941, 'lambda': 27.196607157402067, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:22,071] Trial 190 finished with value: 0.830298615255207 and parameters: {'n_estimators': 697, 'eta': 0.09566497151371184, 'max_depth': 12, 'alpha': 0.0402, 'lambda': 33.58969424724668, 'max_bin': 349}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:26,903] Trial 191 finished with value: 0.8298303552484045 and parameters: {'n_estimators': 271, 'eta': 0.06716537943228823, 'max_depth': 9, 'alpha': 0.20500000000000002, 'lambda': 30.3496977352476, 'max_bin': 387}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:30,744] Trial 192 finished with value: 0.8283846671464152 and parameters: {'n_estimators': 220, 'eta': 0.09816508903438029, 'max_depth': 9, 'alpha': 0.1855, 'lambda': 29.47107237187828, 'max_bin': 397}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:35,355] Trial 193 finished with value: 0.8274619608556522 and parameters: {'n_estimators': 306, 'eta': 0.08725526257184674, 'max_depth': 9, 'alpha': 0.1481, 'lambda': 32.55420692325072, 'max_bin': 370}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:41,419] Trial 194 finished with value: 0.8337420505930939 and parameters: {'n_estimators': 335, 'eta': 0.0568338375908661, 'max_depth': 10, 'alpha': 0.2225, 'lambda': 31.291408751803075, 'max_bin': 381}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:46,244] Trial 195 finished with value: 0.8310789472548722 and parameters: {'n_estimators': 289, 'eta': 0.09184079260795844, 'max_depth': 11, 'alpha': 0.1658, 'lambda': 35.50566575639976, 'max_bin': 304}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:51,505] Trial 196 finished with value: 0.8301238276700499 and parameters: {'n_estimators': 662, 'eta': 0.09414600298999061, 'max_depth': 8, 'alpha': 0.6667000000000001, 'lambda': 34.105287461645865, 'max_bin': 434}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:09:56,452] Trial 197 finished with value: 0.8280967993271604 and parameters: {'n_estimators': 844, 'eta': 0.0819449521850519, 'max_depth': 11, 'alpha': 0.1194, 'lambda': 29.95402161786558, 'max_bin': 330}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:01,648] Trial 198 finished with value: 0.8281075239185549 and parameters: {'n_estimators': 315, 'eta': 0.08985122316198896, 'max_depth': 11, 'alpha': 0.8914000000000001, 'lambda': 38.337102652746616, 'max_bin': 319}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:07,095] Trial 199 finished with value: 0.8288920709931931 and parameters: {'n_estimators': 360, 'eta': 0.07845298495127924, 'max_depth': 12, 'alpha': 0.6367, 'lambda': 36.87913568642144, 'max_bin': 338}. Best is trial 30 with value: 0.8425128924158303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8425\n",
      "\tBest params:\n",
      "\t\tn_estimators: 548\n",
      "\t\teta: 0.09018534818954298\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.1328\n",
      "\t\tlambda: 36.36011307197028\n",
      "\t\tmax_bin: 335\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  326.000000  318.000000  325.000000  328.000000\n",
      "1                    TN  169.000000  186.000000  175.000000  176.000000\n",
      "2                    FP   58.000000   51.000000   54.000000   62.000000\n",
      "3                    FN   42.000000   40.000000   41.000000   29.000000\n",
      "4              Accuracy    0.831933    0.847059    0.840336    0.847059\n",
      "5             Precision    0.848958    0.861789    0.857520    0.841026\n",
      "6           Sensitivity    0.885870    0.888268    0.887978    0.918768\n",
      "7           Specificity    0.744500    0.784800    0.764200    0.739500\n",
      "8              F1 score    0.867021    0.874828    0.872483    0.878179\n",
      "9   F1 score (weighted)    0.830651    0.846399    0.839397    0.844741\n",
      "10     F1 score (macro)    0.819355    0.839142    0.829500    0.836381\n",
      "11    Balanced Accuracy    0.815181    0.836539    0.826085    0.829132\n",
      "12                  MCC    0.640060    0.678913    0.659892    0.678599\n",
      "13                  NPV    0.800900    0.823000    0.810200    0.858500\n",
      "14              ROC_AUC    0.815181    0.836539    0.826085    0.829132\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_xgb_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_xgb_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_xgb_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_xgb_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_xgb_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_xgb_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_xgb_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_xgb_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_xgb_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_xgb_3)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:10:12,851] Trial 200 finished with value: 0.812661180358982 and parameters: {'n_estimators': 382, 'eta': 0.07367615751006926, 'max_depth': 9, 'alpha': 0.0714, 'lambda': 28.332519790865835, 'max_bin': 483}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:17,628] Trial 201 finished with value: 0.8147522220179513 and parameters: {'n_estimators': 403, 'eta': 0.0838052059468657, 'max_depth': 9, 'alpha': 0.13290000000000002, 'lambda': 32.48877748132963, 'max_bin': 470}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:22,641] Trial 202 finished with value: 0.8137781514723038 and parameters: {'n_estimators': 348, 'eta': 0.08049844103095487, 'max_depth': 9, 'alpha': 0.1817, 'lambda': 34.67873440187455, 'max_bin': 461}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:27,473] Trial 203 finished with value: 0.8175248492391265 and parameters: {'n_estimators': 418, 'eta': 0.08682225262520304, 'max_depth': 9, 'alpha': 0.1501, 'lambda': 33.280099697434935, 'max_bin': 476}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:32,080] Trial 204 finished with value: 0.8174991404369489 and parameters: {'n_estimators': 605, 'eta': 0.08507699668675253, 'max_depth': 9, 'alpha': 0.0947, 'lambda': 31.485595872687014, 'max_bin': 452}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:37,344] Trial 205 finished with value: 0.8174567829338988 and parameters: {'n_estimators': 364, 'eta': 0.08131211663433954, 'max_depth': 11, 'alpha': 0.1071, 'lambda': 35.628625103808204, 'max_bin': 491}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:43,184] Trial 206 finished with value: 0.8154329342180613 and parameters: {'n_estimators': 385, 'eta': 0.06856206872015085, 'max_depth': 11, 'alpha': 0.43720000000000003, 'lambda': 37.66331919542867, 'max_bin': 446}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:49,180] Trial 207 finished with value: 0.8075370129924926 and parameters: {'n_estimators': 515, 'eta': 0.06293051616785247, 'max_depth': 6, 'alpha': 0.47400000000000003, 'lambda': 36.29555867893372, 'max_bin': 458}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:54,683] Trial 208 finished with value: 0.8181612092082078 and parameters: {'n_estimators': 326, 'eta': 0.06596077343387871, 'max_depth': 10, 'alpha': 0.33440000000000003, 'lambda': 33.89118399843108, 'max_bin': 481}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:10:59,910] Trial 209 finished with value: 0.8183562460130558 and parameters: {'n_estimators': 708, 'eta': 0.07023849400327262, 'max_depth': 12, 'alpha': 0.5452, 'lambda': 29.117905894525517, 'max_bin': 468}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:05,442] Trial 210 finished with value: 0.8193150656186537 and parameters: {'n_estimators': 446, 'eta': 0.07741653240613879, 'max_depth': 11, 'alpha': 0.228, 'lambda': 34.92895946917164, 'max_bin': 310}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:09,957] Trial 211 finished with value: 0.8194474626637185 and parameters: {'n_estimators': 378, 'eta': 0.09588016770056892, 'max_depth': 12, 'alpha': 0.7387, 'lambda': 27.83833094051901, 'max_bin': 392}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:14,318] Trial 212 finished with value: 0.8205846690030476 and parameters: {'n_estimators': 370, 'eta': 0.09766249402135707, 'max_depth': 12, 'alpha': 0.5184, 'lambda': 27.04034949536742, 'max_bin': 381}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:18,834] Trial 213 finished with value: 0.820414180828449 and parameters: {'n_estimators': 339, 'eta': 0.09877118423833249, 'max_depth': 12, 'alpha': 0.7882, 'lambda': 29.79294184780947, 'max_bin': 374}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:23,014] Trial 214 finished with value: 0.8234219982424307 and parameters: {'n_estimators': 397, 'eta': 0.09237577328039027, 'max_depth': 12, 'alpha': 0.74, 'lambda': 25.346563798125764, 'max_bin': 395}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:27,156] Trial 215 finished with value: 0.8157441309351086 and parameters: {'n_estimators': 301, 'eta': 0.09600522788608341, 'max_depth': 12, 'alpha': 0.17020000000000002, 'lambda': 30.74315478517743, 'max_bin': 404}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:31,829] Trial 216 finished with value: 0.8192697321240537 and parameters: {'n_estimators': 493, 'eta': 0.08301341795960593, 'max_depth': 11, 'alpha': 0.5695, 'lambda': 28.850760964113974, 'max_bin': 324}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:36,539] Trial 217 finished with value: 0.8158301216426972 and parameters: {'n_estimators': 568, 'eta': 0.08853479139735057, 'max_depth': 12, 'alpha': 0.6094, 'lambda': 27.01960154609756, 'max_bin': 389}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:38,327] Trial 218 finished with value: 0.8119331480885075 and parameters: {'n_estimators': 75, 'eta': 0.07150575115260188, 'max_depth': 11, 'alpha': 0.374, 'lambda': 10.596961790754612, 'max_bin': 380}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:45,959] Trial 219 finished with value: 0.8136551706618205 and parameters: {'n_estimators': 541, 'eta': 0.04382805746943008, 'max_depth': 8, 'alpha': 0.20270000000000002, 'lambda': 35.94663578650222, 'max_bin': 366}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:51,912] Trial 220 finished with value: 0.8146571734314909 and parameters: {'n_estimators': 417, 'eta': 0.057634974896150874, 'max_depth': 9, 'alpha': 0.12940000000000002, 'lambda': 32.234958884609185, 'max_bin': 440}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:11:56,135] Trial 221 finished with value: 0.8087258178783732 and parameters: {'n_estimators': 362, 'eta': 0.0930451197172024, 'max_depth': 7, 'alpha': 0.11710000000000001, 'lambda': 33.093145421955185, 'max_bin': 484}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:00,700] Trial 222 finished with value: 0.8180203427703026 and parameters: {'n_estimators': 650, 'eta': 0.0944548852215433, 'max_depth': 12, 'alpha': 0.8438, 'lambda': 31.499779278048557, 'max_bin': 476}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:03,273] Trial 223 finished with value: 0.8108220406242241 and parameters: {'n_estimators': 115, 'eta': 0.0903386005590114, 'max_depth': 12, 'alpha': 0.15360000000000001, 'lambda': 34.44099317114247, 'max_bin': 488}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:05,063] Trial 224 finished with value: 0.8100816314890771 and parameters: {'n_estimators': 75, 'eta': 0.08519088357573242, 'max_depth': 12, 'alpha': 0.08650000000000001, 'lambda': 35.21562037069732, 'max_bin': 492}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:08,932] Trial 225 finished with value: 0.8182227287668935 and parameters: {'n_estimators': 183, 'eta': 0.09964705272113916, 'max_depth': 12, 'alpha': 0.7705000000000001, 'lambda': 32.76871295451214, 'max_bin': 429}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:11,584] Trial 226 finished with value: 0.819499615668082 and parameters: {'n_estimators': 120, 'eta': 0.09679826435390282, 'max_depth': 12, 'alpha': 0.2525, 'lambda': 33.86667903725147, 'max_bin': 411}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:16,959] Trial 227 finished with value: 0.815486608025035 and parameters: {'n_estimators': 274, 'eta': 0.06377750703057651, 'max_depth': 11, 'alpha': 0.1396, 'lambda': 36.78696937755779, 'max_bin': 332}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:23,639] Trial 228 finished with value: 0.7733531320062681 and parameters: {'n_estimators': 323, 'eta': 0.003766180008242491, 'max_depth': 12, 'alpha': 0.052000000000000005, 'lambda': 38.912104005319534, 'max_bin': 474}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:28,675] Trial 229 finished with value: 0.8171656989737057 and parameters: {'n_estimators': 465, 'eta': 0.07998783176743789, 'max_depth': 11, 'alpha': 0.11080000000000001, 'lambda': 30.54834250159292, 'max_bin': 498}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:34,228] Trial 230 finished with value: 0.8170390449471379 and parameters: {'n_estimators': 392, 'eta': 0.07536044037950433, 'max_depth': 11, 'alpha': 0.30820000000000003, 'lambda': 31.888190732148203, 'max_bin': 423}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:39,931] Trial 231 finished with value: 0.8169271528168073 and parameters: {'n_estimators': 427, 'eta': 0.07763256059228009, 'max_depth': 10, 'alpha': 0.42160000000000003, 'lambda': 37.495191160262124, 'max_bin': 378}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:44,489] Trial 232 finished with value: 0.8175273971455816 and parameters: {'n_estimators': 257, 'eta': 0.08264893093333979, 'max_depth': 11, 'alpha': 0.136, 'lambda': 36.08110361949762, 'max_bin': 386}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:49,827] Trial 233 finished with value: 0.8150127260167925 and parameters: {'n_estimators': 435, 'eta': 0.07895073485909865, 'max_depth': 9, 'alpha': 0.3529, 'lambda': 34.8866460920125, 'max_bin': 399}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:54,060] Trial 234 finished with value: 0.8168293566752671 and parameters: {'n_estimators': 205, 'eta': 0.08677347508986623, 'max_depth': 12, 'alpha': 0.191, 'lambda': 38.205520821133554, 'max_bin': 483}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:12:58,770] Trial 235 finished with value: 0.8191392591797998 and parameters: {'n_estimators': 450, 'eta': 0.09423563847567036, 'max_depth': 10, 'alpha': 0.1618, 'lambda': 36.857032477799315, 'max_bin': 468}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:01,816] Trial 236 finished with value: 0.8100613057406122 and parameters: {'n_estimators': 143, 'eta': 0.07269113805545926, 'max_depth': 11, 'alpha': 0.8438, 'lambda': 35.587495626004106, 'max_bin': 488}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:06,511] Trial 237 finished with value: 0.8175029793726833 and parameters: {'n_estimators': 404, 'eta': 0.07494280625354245, 'max_depth': 9, 'alpha': 0.2114, 'lambda': 23.681396008470415, 'max_bin': 389}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:11,961] Trial 238 finished with value: 0.81598685651807 and parameters: {'n_estimators': 374, 'eta': 0.06729394064734501, 'max_depth': 11, 'alpha': 0.0756, 'lambda': 28.27760187018345, 'max_bin': 473}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:17,917] Trial 239 finished with value: 0.8184028932396508 and parameters: {'n_estimators': 351, 'eta': 0.05944247932200931, 'max_depth': 12, 'alpha': 0.1144, 'lambda': 34.38791549971036, 'max_bin': 354}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:20,117] Trial 240 finished with value: 0.8115131971002107 and parameters: {'n_estimators': 100, 'eta': 0.08853392900823027, 'max_depth': 11, 'alpha': 0.48460000000000003, 'lambda': 33.23407649864931, 'max_bin': 464}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:24,959] Trial 241 finished with value: 0.8142917238307277 and parameters: {'n_estimators': 471, 'eta': 0.0839964023566295, 'max_depth': 12, 'alpha': 0.0886, 'lambda': 35.513976797832804, 'max_bin': 347}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:30,178] Trial 242 finished with value: 0.8195156167918356 and parameters: {'n_estimators': 485, 'eta': 0.08218300186915618, 'max_depth': 12, 'alpha': 0.0637, 'lambda': 36.5150065356484, 'max_bin': 340}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:35,160] Trial 243 finished with value: 0.815404608367691 and parameters: {'n_estimators': 669, 'eta': 0.08125875819810413, 'max_depth': 12, 'alpha': 0.0844, 'lambda': 35.13295670792407, 'max_bin': 343}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:40,157] Trial 244 finished with value: 0.8196964075652513 and parameters: {'n_estimators': 633, 'eta': 0.08417341329827449, 'max_depth': 12, 'alpha': 0.2374, 'lambda': 37.30677145664207, 'max_bin': 335}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:45,114] Trial 245 finished with value: 0.8153958564886216 and parameters: {'n_estimators': 443, 'eta': 0.0806929965362019, 'max_depth': 12, 'alpha': 0.6925, 'lambda': 29.630335231853753, 'max_bin': 328}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:50,083] Trial 246 finished with value: 0.817635814727557 and parameters: {'n_estimators': 413, 'eta': 0.08599444153544501, 'max_depth': 11, 'alpha': 0.1253, 'lambda': 36.06193252572007, 'max_bin': 384}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:54,602] Trial 247 finished with value: 0.8193503660234077 and parameters: {'n_estimators': 239, 'eta': 0.09102876055186103, 'max_depth': 12, 'alpha': 0.6451, 'lambda': 34.30660191424215, 'max_bin': 481}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:13:59,976] Trial 248 finished with value: 0.80872661738668 and parameters: {'n_estimators': 357, 'eta': 0.062238755301836576, 'max_depth': 8, 'alpha': 0.09720000000000001, 'lambda': 39.335891566372226, 'max_bin': 346}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:14:04,665] Trial 249 finished with value: 0.8181642512591101 and parameters: {'n_estimators': 384, 'eta': 0.0792628810417271, 'max_depth': 12, 'alpha': 0.1819, 'lambda': 27.516196406481424, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8425\n",
      "\tBest params:\n",
      "\t\tn_estimators: 548\n",
      "\t\teta: 0.09018534818954298\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.1328\n",
      "\t\tlambda: 36.36011307197028\n",
      "\t\tmax_bin: 335\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  326.000000  318.000000  325.000000  328.000000   \n",
      "1                    TN  169.000000  186.000000  175.000000  176.000000   \n",
      "2                    FP   58.000000   51.000000   54.000000   62.000000   \n",
      "3                    FN   42.000000   40.000000   41.000000   29.000000   \n",
      "4              Accuracy    0.831933    0.847059    0.840336    0.847059   \n",
      "5             Precision    0.848958    0.861789    0.857520    0.841026   \n",
      "6           Sensitivity    0.885870    0.888268    0.887978    0.918768   \n",
      "7           Specificity    0.744500    0.784800    0.764200    0.739500   \n",
      "8              F1 score    0.867021    0.874828    0.872483    0.878179   \n",
      "9   F1 score (weighted)    0.830651    0.846399    0.839397    0.844741   \n",
      "10     F1 score (macro)    0.819355    0.839142    0.829500    0.836381   \n",
      "11    Balanced Accuracy    0.815181    0.836539    0.826085    0.829132   \n",
      "12                  MCC    0.640060    0.678913    0.659892    0.678599   \n",
      "13                  NPV    0.800900    0.823000    0.810200    0.858500   \n",
      "14              ROC_AUC    0.815181    0.836539    0.826085    0.829132   \n",
      "\n",
      "          Set4  \n",
      "0   333.000000  \n",
      "1   180.000000  \n",
      "2    58.000000  \n",
      "3    24.000000  \n",
      "4     0.862185  \n",
      "5     0.851662  \n",
      "6     0.932773  \n",
      "7     0.756300  \n",
      "8     0.890374  \n",
      "9     0.860016  \n",
      "10    0.852427  \n",
      "11    0.844538  \n",
      "12    0.711191  \n",
      "13    0.882400  \n",
      "14    0.844538  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_xgb_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_xgb_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_xgb_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_xgb_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_xgb_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_xgb_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_xgb_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_xgb_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_xgb_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_xgb_4)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:14:10,528] Trial 250 finished with value: 0.8154908024130257 and parameters: {'n_estimators': 343, 'eta': 0.09685802939580654, 'max_depth': 11, 'alpha': 0.498, 'lambda': 33.83122111399256, 'max_bin': 341}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:14:15,115] Trial 251 finished with value: 0.8158073080527253 and parameters: {'n_estimators': 529, 'eta': 0.08337449612489119, 'max_depth': 11, 'alpha': 0.1495, 'lambda': 20.51920400827302, 'max_bin': 478}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:14:20,629] Trial 252 finished with value: 0.8153582000047148 and parameters: {'n_estimators': 478, 'eta': 0.08619797432359255, 'max_depth': 9, 'alpha': 0.4434, 'lambda': 35.67290226035475, 'max_bin': 493}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:14:27,275] Trial 253 finished with value: 0.8133943793076754 and parameters: {'n_estimators': 591, 'eta': 0.06520060249009944, 'max_depth': 12, 'alpha': 0.669, 'lambda': 32.24864854811788, 'max_bin': 436}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:14:32,472] Trial 254 finished with value: 0.8170168926055199 and parameters: {'n_estimators': 507, 'eta': 0.09995229608972975, 'max_depth': 11, 'alpha': 0.3991, 'lambda': 37.77718096294756, 'max_bin': 374}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:14:38,036] Trial 255 finished with value: 0.8164854446563646 and parameters: {'n_estimators': 367, 'eta': 0.07654626326525331, 'max_depth': 12, 'alpha': 0.0329, 'lambda': 34.9921702444603, 'max_bin': 417}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:14:42,462] Trial 256 finished with value: 0.8165607646309134 and parameters: {'n_estimators': 685, 'eta': 0.08788586507005858, 'max_depth': 11, 'alpha': 0.1136, 'lambda': 31.21226525974739, 'max_bin': 442}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:14:47,725] Trial 257 finished with value: 0.8139821235113847 and parameters: {'n_estimators': 329, 'eta': 0.091787922423473, 'max_depth': 11, 'alpha': 0.17300000000000001, 'lambda': 36.733595810204385, 'max_bin': 358}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:14:53,668] Trial 258 finished with value: 0.8136808797599002 and parameters: {'n_estimators': 404, 'eta': 0.06995618858377103, 'max_depth': 10, 'alpha': 0.2805, 'lambda': 28.837000266859146, 'max_bin': 395}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:14:58,768] Trial 259 finished with value: 0.8159251394805977 and parameters: {'n_estimators': 429, 'eta': 0.09345585075931158, 'max_depth': 12, 'alpha': 0.13920000000000002, 'lambda': 32.971956923677816, 'max_bin': 335}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:15:03,868] Trial 260 finished with value: 0.8156663804970712 and parameters: {'n_estimators': 307, 'eta': 0.0824789581335377, 'max_depth': 11, 'alpha': 0.0692, 'lambda': 33.895938604264906, 'max_bin': 487}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:15:09,449] Trial 261 finished with value: 0.8203204802970682 and parameters: {'n_estimators': 454, 'eta': 0.09004942031109028, 'max_depth': 12, 'alpha': 0.6175, 'lambda': 39.9952807152568, 'max_bin': 449}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:15:15,666] Trial 262 finished with value: 0.8168815713399008 and parameters: {'n_estimators': 383, 'eta': 0.06077442314335142, 'max_depth': 11, 'alpha': 0.5596, 'lambda': 30.324119648996675, 'max_bin': 350}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:15:21,143] Trial 263 finished with value: 0.8117904287238037 and parameters: {'n_estimators': 348, 'eta': 0.08452987998609149, 'max_depth': 9, 'alpha': 0.0898, 'lambda': 38.10481798981063, 'max_bin': 496}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:15:26,845] Trial 264 finished with value: 0.8218679840307974 and parameters: {'n_estimators': 417, 'eta': 0.07820287417560289, 'max_depth': 12, 'alpha': 0.5903, 'lambda': 35.9818049131703, 'max_bin': 469}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:15:31,747] Trial 265 finished with value: 0.8192922967037545 and parameters: {'n_estimators': 281, 'eta': 0.08836415564971703, 'max_depth': 12, 'alpha': 0.2645, 'lambda': 34.99483222106431, 'max_bin': 473}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:15:35,583] Trial 266 finished with value: 0.8138688726782654 and parameters: {'n_estimators': 393, 'eta': 0.0956806912130918, 'max_depth': 11, 'alpha': 0.1237, 'lambda': 26.851518939583254, 'max_bin': 457}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:15:41,826] Trial 267 finished with value: 0.814732807151801 and parameters: {'n_estimators': 463, 'eta': 0.073850549430102, 'max_depth': 11, 'alpha': 0.1563, 'lambda': 37.191810304356174, 'max_bin': 428}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:15:49,225] Trial 268 finished with value: 0.8107608984000712 and parameters: {'n_estimators': 651, 'eta': 0.05718786195629021, 'max_depth': 10, 'alpha': 0.2151, 'lambda': 38.7861208935711, 'max_bin': 392}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:15:55,161] Trial 269 finished with value: 0.8177571137265826 and parameters: {'n_estimators': 374, 'eta': 0.0673540371349571, 'max_depth': 12, 'alpha': 0.46580000000000005, 'lambda': 25.992881147465415, 'max_bin': 479}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:01,755] Trial 270 finished with value: 0.8137543214975722 and parameters: {'n_estimators': 438, 'eta': 0.05399225998708933, 'max_depth': 11, 'alpha': 0.1052, 'lambda': 28.11571620716932, 'max_bin': 320}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:05,435] Trial 271 finished with value: 0.8182277205743184 and parameters: {'n_estimators': 172, 'eta': 0.080984339780439, 'max_depth': 12, 'alpha': 0.196, 'lambda': 36.376774035866305, 'max_bin': 491}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:12,124] Trial 272 finished with value: 0.8100131119705946 and parameters: {'n_estimators': 626, 'eta': 0.06340942209940921, 'max_depth': 9, 'alpha': 0.7113, 'lambda': 34.21589076587606, 'max_bin': 314}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:17,202] Trial 273 finished with value: 0.8138705475042272 and parameters: {'n_estimators': 327, 'eta': 0.08566202308676846, 'max_depth': 10, 'alpha': 0.5209, 'lambda': 32.62331654502993, 'max_bin': 330}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:22,402] Trial 274 finished with value: 0.8201865291952745 and parameters: {'n_estimators': 493, 'eta': 0.08290412335649049, 'max_depth': 11, 'alpha': 0.13290000000000002, 'lambda': 35.177932785822605, 'max_bin': 485}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:27,201] Trial 275 finished with value: 0.81491845320249 and parameters: {'n_estimators': 405, 'eta': 0.09747901208056127, 'max_depth': 12, 'alpha': 0.0597, 'lambda': 31.187917125852735, 'max_bin': 433}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:31,791] Trial 276 finished with value: 0.8164957809403021 and parameters: {'n_estimators': 357, 'eta': 0.09028614791896084, 'max_depth': 11, 'alpha': 0.2456, 'lambda': 29.41964279678526, 'max_bin': 462}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:36,908] Trial 277 finished with value: 0.8167949729152157 and parameters: {'n_estimators': 574, 'eta': 0.08001590715347232, 'max_depth': 12, 'alpha': 0.1768, 'lambda': 26.284750207099506, 'max_bin': 367}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:41,784] Trial 278 finished with value: 0.8169836619859494 and parameters: {'n_estimators': 301, 'eta': 0.0936955109868259, 'max_depth': 11, 'alpha': 0.32320000000000004, 'lambda': 33.50128648091368, 'max_bin': 407}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:44,404] Trial 279 finished with value: 0.8239325853724171 and parameters: {'n_estimators': 424, 'eta': 0.08720702882959404, 'max_depth': 11, 'alpha': 0.1511, 'lambda': 1.3336467683972622, 'max_bin': 340}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:50,064] Trial 280 finished with value: 0.8180743318440491 and parameters: {'n_estimators': 338, 'eta': 0.07131312688796178, 'max_depth': 12, 'alpha': 0.0502, 'lambda': 35.77692198474617, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:16:56,515] Trial 281 finished with value: 0.8192353595250313 and parameters: {'n_estimators': 553, 'eta': 0.07647105302428757, 'max_depth': 11, 'alpha': 0.6566000000000001, 'lambda': 37.161508487085904, 'max_bin': 476}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:01,619] Trial 282 finished with value: 0.8188548993996644 and parameters: {'n_estimators': 383, 'eta': 0.08475845847742433, 'max_depth': 12, 'alpha': 0.08170000000000001, 'lambda': 34.524105642583336, 'max_bin': 383}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:11,084] Trial 283 finished with value: 0.8096332261794617 and parameters: {'n_estimators': 666, 'eta': 0.03528031819744997, 'max_depth': 8, 'alpha': 0.1042, 'lambda': 31.66300943079816, 'max_bin': 344}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:16,188] Trial 284 finished with value: 0.8159262238169847 and parameters: {'n_estimators': 360, 'eta': 0.0921185166523887, 'max_depth': 9, 'alpha': 0.369, 'lambda': 36.35107420387588, 'max_bin': 326}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:21,407] Trial 285 finished with value: 0.814447981249265 and parameters: {'n_estimators': 458, 'eta': 0.08912100824410632, 'max_depth': 11, 'alpha': 0.1257, 'lambda': 37.957535405790445, 'max_bin': 466}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:26,482] Trial 286 finished with value: 0.8184232388386683 and parameters: {'n_estimators': 479, 'eta': 0.0983508295238341, 'max_depth': 12, 'alpha': 0.16640000000000002, 'lambda': 33.381080727297785, 'max_bin': 484}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:27,893] Trial 287 finished with value: 0.8097119385344168 and parameters: {'n_estimators': 62, 'eta': 0.09515439521397319, 'max_depth': 10, 'alpha': 0.22940000000000002, 'lambda': 35.438653735505724, 'max_bin': 494}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:30,126] Trial 288 finished with value: 0.8158952800142311 and parameters: {'n_estimators': 100, 'eta': 0.08219783961761637, 'max_depth': 11, 'alpha': 0.0167, 'lambda': 18.249560244184075, 'max_bin': 445}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:35,670] Trial 289 finished with value: 0.8201441004055278 and parameters: {'n_estimators': 395, 'eta': 0.0787594616430792, 'max_depth': 12, 'alpha': 0.29000000000000004, 'lambda': 34.606915943427914, 'max_bin': 377}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:41,044] Trial 290 finished with value: 0.8145120403223667 and parameters: {'n_estimators': 744, 'eta': 0.0858486141480902, 'max_depth': 9, 'alpha': 0.5388000000000001, 'lambda': 30.136700898180532, 'max_bin': 336}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:47,492] Trial 291 finished with value: 0.8140598932515012 and parameters: {'n_estimators': 611, 'eta': 0.08325277374234832, 'max_depth': 11, 'alpha': 0.9382, 'lambda': 38.548873879524415, 'max_bin': 454}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:52,275] Trial 292 finished with value: 0.8185848239876428 and parameters: {'n_estimators': 254, 'eta': 0.07339049363819096, 'max_depth': 12, 'alpha': 0.4994, 'lambda': 24.141098230226245, 'max_bin': 480}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:17:58,508] Trial 293 finished with value: 0.8129142381138479 and parameters: {'n_estimators': 321, 'eta': 0.04774550249305056, 'max_depth': 11, 'alpha': 0.6086, 'lambda': 27.56870733776658, 'max_bin': 489}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:18:01,567] Trial 294 finished with value: 0.8159158698942386 and parameters: {'n_estimators': 136, 'eta': 0.06523335229566685, 'max_depth': 12, 'alpha': 0.14200000000000002, 'lambda': 22.548623199514708, 'max_bin': 401}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:18:07,810] Trial 295 finished with value: 0.8181439416116151 and parameters: {'n_estimators': 365, 'eta': 0.06976611079174143, 'max_depth': 11, 'alpha': 0.8247, 'lambda': 32.31437855112469, 'max_bin': 470}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:18:13,590] Trial 296 finished with value: 0.8137962845588668 and parameters: {'n_estimators': 430, 'eta': 0.06859129741220743, 'max_depth': 11, 'alpha': 0.6332, 'lambda': 25.36294761236766, 'max_bin': 333}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:18:20,481] Trial 297 finished with value: 0.8158514075717214 and parameters: {'n_estimators': 709, 'eta': 0.06082196403060997, 'max_depth': 12, 'alpha': 0.1932, 'lambda': 35.91610040692432, 'max_bin': 439}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:18:23,552] Trial 298 finished with value: 0.8156572456780099 and parameters: {'n_estimators': 162, 'eta': 0.08779935648058261, 'max_depth': 9, 'alpha': 0.09230000000000001, 'lambda': 28.69731241906346, 'max_bin': 351}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:18:28,542] Trial 299 finished with value: 0.8159345666923535 and parameters: {'n_estimators': 291, 'eta': 0.08047666749294735, 'max_depth': 10, 'alpha': 0.4212, 'lambda': 37.00625022416848, 'max_bin': 420}. Best is trial 30 with value: 0.8425128924158303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8425\n",
      "\tBest params:\n",
      "\t\tn_estimators: 548\n",
      "\t\teta: 0.09018534818954298\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.1328\n",
      "\t\tlambda: 36.36011307197028\n",
      "\t\tmax_bin: 335\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  326.000000  318.000000  325.000000  328.000000   \n",
      "1                    TN  169.000000  186.000000  175.000000  176.000000   \n",
      "2                    FP   58.000000   51.000000   54.000000   62.000000   \n",
      "3                    FN   42.000000   40.000000   41.000000   29.000000   \n",
      "4              Accuracy    0.831933    0.847059    0.840336    0.847059   \n",
      "5             Precision    0.848958    0.861789    0.857520    0.841026   \n",
      "6           Sensitivity    0.885870    0.888268    0.887978    0.918768   \n",
      "7           Specificity    0.744500    0.784800    0.764200    0.739500   \n",
      "8              F1 score    0.867021    0.874828    0.872483    0.878179   \n",
      "9   F1 score (weighted)    0.830651    0.846399    0.839397    0.844741   \n",
      "10     F1 score (macro)    0.819355    0.839142    0.829500    0.836381   \n",
      "11    Balanced Accuracy    0.815181    0.836539    0.826085    0.829132   \n",
      "12                  MCC    0.640060    0.678913    0.659892    0.678599   \n",
      "13                  NPV    0.800900    0.823000    0.810200    0.858500   \n",
      "14              ROC_AUC    0.815181    0.836539    0.826085    0.829132   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   333.000000  324.000000  \n",
      "1   180.000000  183.000000  \n",
      "2    58.000000   51.000000  \n",
      "3    24.000000   37.000000  \n",
      "4     0.862185    0.852101  \n",
      "5     0.851662    0.864000  \n",
      "6     0.932773    0.897507  \n",
      "7     0.756300    0.782100  \n",
      "8     0.890374    0.880435  \n",
      "9     0.860016    0.851227  \n",
      "10    0.852427    0.843301  \n",
      "11    0.844538    0.839779  \n",
      "12    0.711191    0.687640  \n",
      "13    0.882400    0.831800  \n",
      "14    0.844538    0.839779  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_xgb_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_xgb_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_xgb_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_xgb_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_xgb_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_xgb_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_xgb_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_xgb_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_xgb_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_xgb_5)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:18:34,496] Trial 300 finished with value: 0.824615994317071 and parameters: {'n_estimators': 344, 'eta': 0.09190914432833001, 'max_depth': 12, 'alpha': 0.5850000000000001, 'lambda': 34.148046047539005, 'max_bin': 388}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:18:41,277] Trial 301 finished with value: 0.8226112475764106 and parameters: {'n_estimators': 378, 'eta': 0.05850394299552108, 'max_depth': 11, 'alpha': 0.1136, 'lambda': 33.06984426641224, 'max_bin': 478}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:18:47,757] Trial 302 finished with value: 0.8200930859920834 and parameters: {'n_estimators': 644, 'eta': 0.07504603217953179, 'max_depth': 11, 'alpha': 0.7541, 'lambda': 36.67485181308939, 'max_bin': 472}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:18:53,113] Trial 303 finished with value: 0.8233962152071905 and parameters: {'n_estimators': 515, 'eta': 0.08432860975807599, 'max_depth': 12, 'alpha': 0.222, 'lambda': 30.79887643613922, 'max_bin': 425}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:18:58,632] Trial 304 finished with value: 0.8196547087704266 and parameters: {'n_estimators': 410, 'eta': 0.0998876474192184, 'max_depth': 10, 'alpha': 0.06960000000000001, 'lambda': 37.57984585767599, 'max_bin': 497}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:19:04,013] Trial 305 finished with value: 0.8241224842243392 and parameters: {'n_estimators': 314, 'eta': 0.0781136283235204, 'max_depth': 11, 'alpha': 0.1617, 'lambda': 35.1996650419207, 'max_bin': 489}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:19:08,884] Trial 306 finished with value: 0.8241556096683095 and parameters: {'n_estimators': 442, 'eta': 0.09594552739290303, 'max_depth': 12, 'alpha': 0.6856, 'lambda': 29.23214456506056, 'max_bin': 325}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:19:14,196] Trial 307 finished with value: 0.82218614096402 and parameters: {'n_estimators': 683, 'eta': 0.08977114186426914, 'max_depth': 11, 'alpha': 0.4768, 'lambda': 33.783571671181036, 'max_bin': 465}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:19:21,297] Trial 308 finished with value: 0.8225780961873428 and parameters: {'n_estimators': 371, 'eta': 0.055094056821363965, 'max_depth': 12, 'alpha': 0.25830000000000003, 'lambda': 31.82760116894357, 'max_bin': 459}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:19:26,782] Trial 309 finished with value: 0.8270670673578889 and parameters: {'n_estimators': 399, 'eta': 0.08130773864782863, 'max_depth': 11, 'alpha': 0.1194, 'lambda': 34.874711602301694, 'max_bin': 360}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:19:32,809] Trial 310 finished with value: 0.8226553609699984 and parameters: {'n_estimators': 458, 'eta': 0.0868768330856824, 'max_depth': 9, 'alpha': 0.13720000000000002, 'lambda': 39.40666274586005, 'max_bin': 373}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:19:37,266] Trial 311 finished with value: 0.8252759789877693 and parameters: {'n_estimators': 227, 'eta': 0.09344790196503563, 'max_depth': 12, 'alpha': 0.037000000000000005, 'lambda': 36.039836776988196, 'max_bin': 343}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:19:41,933] Trial 312 finished with value: 0.8187391106704455 and parameters: {'n_estimators': 341, 'eta': 0.09756996144381859, 'max_depth': 7, 'alpha': 0.35100000000000003, 'lambda': 32.451013723220335, 'max_bin': 414}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:19:46,743] Trial 313 finished with value: 0.8219902865489631 and parameters: {'n_estimators': 267, 'eta': 0.08316519168341804, 'max_depth': 11, 'alpha': 0.2066, 'lambda': 27.782157083399404, 'max_bin': 318}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:19:53,610] Trial 314 finished with value: 0.8272789116510332 and parameters: {'n_estimators': 422, 'eta': 0.061964936413557464, 'max_depth': 12, 'alpha': 0.1834, 'lambda': 35.51070413539932, 'max_bin': 484}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:20:00,213] Trial 315 finished with value: 0.8252310492449556 and parameters: {'n_estimators': 493, 'eta': 0.06606856686500896, 'max_depth': 11, 'alpha': 0.0946, 'lambda': 26.69391095775977, 'max_bin': 493}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:20:04,425] Trial 316 finished with value: 0.8240544795829177 and parameters: {'n_estimators': 203, 'eta': 0.08551110896405639, 'max_depth': 12, 'alpha': 0.4506, 'lambda': 37.4497570910281, 'max_bin': 476}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:20:09,781] Trial 317 finished with value: 0.8272505151552096 and parameters: {'n_estimators': 359, 'eta': 0.09101637626986898, 'max_depth': 11, 'alpha': 0.15080000000000002, 'lambda': 34.409747625478964, 'max_bin': 448}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:20:16,278] Trial 318 finished with value: 0.8249843662100789 and parameters: {'n_estimators': 545, 'eta': 0.07963032101884064, 'max_depth': 9, 'alpha': 0.5698, 'lambda': 38.40204526481064, 'max_bin': 330}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:20:21,872] Trial 319 finished with value: 0.8239831404306639 and parameters: {'n_estimators': 385, 'eta': 0.08815078834775746, 'max_depth': 12, 'alpha': 0.6584, 'lambda': 29.805578623755885, 'max_bin': 394}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:20:27,393] Trial 320 finished with value: 0.8228451478339069 and parameters: {'n_estimators': 328, 'eta': 0.07670954841291808, 'max_depth': 11, 'alpha': 0.24180000000000001, 'lambda': 36.43286689477109, 'max_bin': 432}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:20:32,402] Trial 321 finished with value: 0.8222163390289718 and parameters: {'n_estimators': 445, 'eta': 0.09468954313760082, 'max_depth': 10, 'alpha': 0.7281000000000001, 'lambda': 33.08349662408132, 'max_bin': 384}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:20:38,621] Trial 322 finished with value: 0.8271931559546429 and parameters: {'n_estimators': 589, 'eta': 0.07185785423516519, 'max_depth': 12, 'alpha': 0.1072, 'lambda': 31.019966408202595, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:20:46,338] Trial 323 finished with value: 0.8235381755911039 and parameters: {'n_estimators': 478, 'eta': 0.059398801541583335, 'max_depth': 11, 'alpha': 0.0765, 'lambda': 33.805106169718265, 'max_bin': 482}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:20:51,966] Trial 324 finished with value: 0.8215231586931881 and parameters: {'n_estimators': 398, 'eta': 0.08135057910677487, 'max_depth': 12, 'alpha': 0.5088, 'lambda': 35.377473571547206, 'max_bin': 336}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:01,325] Trial 325 finished with value: 0.8227719688566377 and parameters: {'n_estimators': 664, 'eta': 0.050422429046827554, 'max_depth': 11, 'alpha': 0.6273000000000001, 'lambda': 39.08668055855489, 'max_bin': 473}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:08,912] Trial 326 finished with value: 0.8225475579004172 and parameters: {'n_estimators': 791, 'eta': 0.06325677874069544, 'max_depth': 8, 'alpha': 0.16970000000000002, 'lambda': 36.79441049928821, 'max_bin': 489}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:13,928] Trial 327 finished with value: 0.8270530771452396 and parameters: {'n_estimators': 375, 'eta': 0.08412420223072237, 'max_depth': 11, 'alpha': 0.053500000000000006, 'lambda': 28.38019746059082, 'max_bin': 299}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:19,221] Trial 328 finished with value: 0.8248633671340609 and parameters: {'n_estimators': 349, 'eta': 0.08959859179013067, 'max_depth': 12, 'alpha': 0.13620000000000002, 'lambda': 35.01471538654759, 'max_bin': 323}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:23,726] Trial 329 finished with value: 0.82285628515331 and parameters: {'n_estimators': 289, 'eta': 0.06839454656897564, 'max_depth': 10, 'alpha': 0.40340000000000004, 'lambda': 12.63272367009672, 'max_bin': 440}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:28,404] Trial 330 finished with value: 0.8231129573708257 and parameters: {'n_estimators': 527, 'eta': 0.09733244157960289, 'max_depth': 9, 'alpha': 0.11910000000000001, 'lambda': 22.1524191036411, 'max_bin': 310}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:33,863] Trial 331 finished with value: 0.8214822005493513 and parameters: {'n_estimators': 413, 'eta': 0.08743110284091361, 'max_depth': 11, 'alpha': 0.29710000000000003, 'lambda': 32.60813845294878, 'max_bin': 496}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:39,070] Trial 332 finished with value: 0.828218341342537 and parameters: {'n_estimators': 309, 'eta': 0.0925551992807372, 'max_depth': 12, 'alpha': 0.1024, 'lambda': 30.181420915318533, 'max_bin': 338}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:45,847] Trial 333 finished with value: 0.8241394572391748 and parameters: {'n_estimators': 628, 'eta': 0.07469775320715227, 'max_depth': 12, 'alpha': 0.21030000000000001, 'lambda': 37.93584866957433, 'max_bin': 354}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:51,433] Trial 334 finished with value: 0.8223528402075623 and parameters: {'n_estimators': 439, 'eta': 0.0857815419466519, 'max_depth': 11, 'alpha': 0.18810000000000002, 'lambda': 36.11365735636959, 'max_bin': 468}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:53,789] Trial 335 finished with value: 0.814344090015984 and parameters: {'n_estimators': 115, 'eta': 0.07928767646533487, 'max_depth': 9, 'alpha': 0.0823, 'lambda': 31.714403177974354, 'max_bin': 463}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:21:55,766] Trial 336 finished with value: 0.8130723316638099 and parameters: {'n_estimators': 86, 'eta': 0.08247052563894208, 'max_depth': 11, 'alpha': 0.1562, 'lambda': 34.4604077622881, 'max_bin': 390}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:01,229] Trial 337 finished with value: 0.8219876352270074 and parameters: {'n_estimators': 468, 'eta': 0.09513369329887172, 'max_depth': 12, 'alpha': 0.26630000000000004, 'lambda': 36.95529461601261, 'max_bin': 452}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:06,281] Trial 338 finished with value: 0.8188753181683109 and parameters: {'n_estimators': 363, 'eta': 0.09022457528884002, 'max_depth': 10, 'alpha': 0.548, 'lambda': 33.68594489328168, 'max_bin': 401}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:11,976] Trial 339 finished with value: 0.8227949833186632 and parameters: {'n_estimators': 336, 'eta': 0.07818372816139992, 'max_depth': 11, 'alpha': 0.5959, 'lambda': 35.61638713687209, 'max_bin': 484}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:17,458] Trial 340 finished with value: 0.824046539214511 and parameters: {'n_estimators': 393, 'eta': 0.08373335991073014, 'max_depth': 12, 'alpha': 0.6999000000000001, 'lambda': 21.03063709299466, 'max_bin': 378}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:22,792] Trial 341 finished with value: 0.8214925655615438 and parameters: {'n_estimators': 695, 'eta': 0.08646958829457313, 'max_depth': 11, 'alpha': 0.6423, 'lambda': 29.220232828534947, 'max_bin': 346}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:29,462] Trial 342 finished with value: 0.8263180838140723 and parameters: {'n_estimators': 378, 'eta': 0.06653059068080065, 'max_depth': 12, 'alpha': 0.1406, 'lambda': 39.98336175825147, 'max_bin': 287}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:34,473] Trial 343 finished with value: 0.8087671284984437 and parameters: {'n_estimators': 417, 'eta': 0.0815189228426996, 'max_depth': 5, 'alpha': 0.677, 'lambda': 34.541252090838974, 'max_bin': 479}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:40,221] Trial 344 finished with value: 0.8242468228966897 and parameters: {'n_estimators': 355, 'eta': 0.07224941177655447, 'max_depth': 11, 'alpha': 0.4622, 'lambda': 25.669468989544214, 'max_bin': 492}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:44,810] Trial 345 finished with value: 0.8203556954484954 and parameters: {'n_estimators': 433, 'eta': 0.09882091070809619, 'max_depth': 12, 'alpha': 0.7744000000000001, 'lambda': 23.39047154608699, 'max_bin': 364}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:50,211] Trial 346 finished with value: 0.8226012017511856 and parameters: {'n_estimators': 504, 'eta': 0.09176984617305972, 'max_depth': 11, 'alpha': 0.5288, 'lambda': 30.868142553288646, 'max_bin': 472}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:22:56,385] Trial 347 finished with value: 0.8265091606601495 and parameters: {'n_estimators': 647, 'eta': 0.07689964125635923, 'max_depth': 12, 'alpha': 0.1236, 'lambda': 36.29079141014521, 'max_bin': 369}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:23:03,523] Trial 348 finished with value: 0.8114116723391976 and parameters: {'n_estimators': 331, 'eta': 0.016332722000701703, 'max_depth': 11, 'alpha': 0.07200000000000001, 'lambda': 33.22510287863433, 'max_bin': 331}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:23:10,901] Trial 349 finished with value: 0.827502480155592 and parameters: {'n_estimators': 452, 'eta': 0.06448075852061806, 'max_depth': 11, 'alpha': 0.16190000000000002, 'lambda': 37.53780786883324, 'max_bin': 433}. Best is trial 30 with value: 0.8425128924158303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8425\n",
      "\tBest params:\n",
      "\t\tn_estimators: 548\n",
      "\t\teta: 0.09018534818954298\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.1328\n",
      "\t\tlambda: 36.36011307197028\n",
      "\t\tmax_bin: 335\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  326.000000  318.000000  325.000000  328.000000   \n",
      "1                    TN  169.000000  186.000000  175.000000  176.000000   \n",
      "2                    FP   58.000000   51.000000   54.000000   62.000000   \n",
      "3                    FN   42.000000   40.000000   41.000000   29.000000   \n",
      "4              Accuracy    0.831933    0.847059    0.840336    0.847059   \n",
      "5             Precision    0.848958    0.861789    0.857520    0.841026   \n",
      "6           Sensitivity    0.885870    0.888268    0.887978    0.918768   \n",
      "7           Specificity    0.744500    0.784800    0.764200    0.739500   \n",
      "8              F1 score    0.867021    0.874828    0.872483    0.878179   \n",
      "9   F1 score (weighted)    0.830651    0.846399    0.839397    0.844741   \n",
      "10     F1 score (macro)    0.819355    0.839142    0.829500    0.836381   \n",
      "11    Balanced Accuracy    0.815181    0.836539    0.826085    0.829132   \n",
      "12                  MCC    0.640060    0.678913    0.659892    0.678599   \n",
      "13                  NPV    0.800900    0.823000    0.810200    0.858500   \n",
      "14              ROC_AUC    0.815181    0.836539    0.826085    0.829132   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   333.000000  324.000000  336.000000  \n",
      "1   180.000000  183.000000  173.000000  \n",
      "2    58.000000   51.000000   40.000000  \n",
      "3    24.000000   37.000000   46.000000  \n",
      "4     0.862185    0.852101    0.855462  \n",
      "5     0.851662    0.864000    0.893617  \n",
      "6     0.932773    0.897507    0.879581  \n",
      "7     0.756300    0.782100    0.812200  \n",
      "8     0.890374    0.880435    0.886544  \n",
      "9     0.860016    0.851227    0.855894  \n",
      "10    0.852427    0.843301    0.843735  \n",
      "11    0.844538    0.839779    0.845894  \n",
      "12    0.711191    0.687640    0.687667  \n",
      "13    0.882400    0.831800    0.790000  \n",
      "14    0.844538    0.839779    0.845894  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_xgb_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_xgb_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_xgb_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_xgb_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_xgb_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_xgb_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_xgb_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_xgb_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_xgb_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_xgb_6)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:23:16,261] Trial 350 finished with value: 0.818371376146346 and parameters: {'n_estimators': 403, 'eta': 0.08856213802152178, 'max_depth': 6, 'alpha': 0.2384, 'lambda': 27.54615929087489, 'max_bin': 444}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:23:20,454] Trial 351 finished with value: 0.8302681342873125 and parameters: {'n_estimators': 382, 'eta': 0.09413532217583322, 'max_depth': 12, 'alpha': 0.4839, 'lambda': 20.072029323105752, 'max_bin': 427}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:23:25,100] Trial 352 finished with value: 0.8210285267231434 and parameters: {'n_estimators': 278, 'eta': 0.08476806435762481, 'max_depth': 8, 'alpha': 0.0975, 'lambda': 38.81618781928249, 'max_bin': 486}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:23:30,985] Trial 353 finished with value: 0.8283727229289299 and parameters: {'n_estimators': 310, 'eta': 0.0707694375691166, 'max_depth': 12, 'alpha': 0.18030000000000002, 'lambda': 35.43599125816267, 'max_bin': 477}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:23:36,122] Trial 354 finished with value: 0.8255243252841458 and parameters: {'n_estimators': 600, 'eta': 0.08000587327045791, 'max_depth': 9, 'alpha': 0.8054, 'lambda': 32.07120749891894, 'max_bin': 459}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:23:41,185] Trial 355 finished with value: 0.8306017951323099 and parameters: {'n_estimators': 574, 'eta': 0.09714420026147977, 'max_depth': 11, 'alpha': 0.042800000000000005, 'lambda': 33.99677996985751, 'max_bin': 341}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:23:45,832] Trial 356 finished with value: 0.8233483751085939 and parameters: {'n_estimators': 236, 'eta': 0.05979204780303763, 'max_depth': 10, 'alpha': 0.1307, 'lambda': 34.78374606764656, 'max_bin': 494}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:23:51,063] Trial 357 finished with value: 0.8250065417649347 and parameters: {'n_estimators': 724, 'eta': 0.08670347707170419, 'max_depth': 11, 'alpha': 0.2069, 'lambda': 36.733908776620126, 'max_bin': 382}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:23:54,948] Trial 358 finished with value: 0.8281224144447539 and parameters: {'n_estimators': 182, 'eta': 0.0822416962573966, 'max_depth': 12, 'alpha': 0.4297, 'lambda': 28.827951179839946, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:01,777] Trial 359 finished with value: 0.8326131949051385 and parameters: {'n_estimators': 422, 'eta': 0.057470923473070715, 'max_depth': 11, 'alpha': 0.6114, 'lambda': 26.534363744416666, 'max_bin': 471}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:06,950] Trial 360 finished with value: 0.8302564224580127 and parameters: {'n_estimators': 365, 'eta': 0.08883528345788075, 'max_depth': 12, 'alpha': 0.11220000000000001, 'lambda': 38.32142351685138, 'max_bin': 350}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:14,375] Trial 361 finished with value: 0.8253120687316537 and parameters: {'n_estimators': 476, 'eta': 0.05459689980285801, 'max_depth': 10, 'alpha': 0.14550000000000002, 'lambda': 32.761564583150225, 'max_bin': 490}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:19,271] Trial 362 finished with value: 0.8292274565271616 and parameters: {'n_estimators': 394, 'eta': 0.09990337143845258, 'max_depth': 11, 'alpha': 0.055900000000000005, 'lambda': 35.927042446726276, 'max_bin': 334}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:23,338] Trial 363 finished with value: 0.8250750463330784 and parameters: {'n_estimators': 346, 'eta': 0.09315287498595556, 'max_depth': 9, 'alpha': 0.0976, 'lambda': 15.715702416711503, 'max_bin': 316}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:28,255] Trial 364 finished with value: 0.8281087904031947 and parameters: {'n_estimators': 255, 'eta': 0.08353193989781757, 'max_depth': 12, 'alpha': 0.32130000000000003, 'lambda': 29.902050981031188, 'max_bin': 437}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:34,821] Trial 365 finished with value: 0.8294128040473575 and parameters: {'n_estimators': 679, 'eta': 0.07523915163403931, 'max_depth': 11, 'alpha': 0.37320000000000003, 'lambda': 37.582332096522705, 'max_bin': 482}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:39,122] Trial 366 finished with value: 0.8286838038919152 and parameters: {'n_estimators': 619, 'eta': 0.09142084313788812, 'max_depth': 9, 'alpha': 0.22560000000000002, 'lambda': 24.80207091423948, 'max_bin': 466}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:43,559] Trial 367 finished with value: 0.8328827396221612 and parameters: {'n_estimators': 321, 'eta': 0.0955283836532568, 'max_depth': 11, 'alpha': 0.0804, 'lambda': 24.795371084710453, 'max_bin': 476}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:50,738] Trial 368 finished with value: 0.8303808742094263 and parameters: {'n_estimators': 464, 'eta': 0.06153824237368948, 'max_depth': 12, 'alpha': 0.3886, 'lambda': 35.03197998831624, 'max_bin': 325}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:24:55,939] Trial 369 finished with value: 0.8313423166938583 and parameters: {'n_estimators': 660, 'eta': 0.08494104188469845, 'max_depth': 12, 'alpha': 0.1882, 'lambda': 31.590737572069546, 'max_bin': 411}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:01,629] Trial 370 finished with value: 0.8224467574187454 and parameters: {'n_estimators': 561, 'eta': 0.08000929541373146, 'max_depth': 11, 'alpha': 0.15430000000000002, 'lambda': 33.57839699116174, 'max_bin': 387}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:03,147] Trial 371 finished with value: 0.8060244444753824 and parameters: {'n_estimators': 63, 'eta': 0.06949110103167119, 'max_depth': 11, 'alpha': 0.5719000000000001, 'lambda': 36.40929672913712, 'max_bin': 455}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:08,830] Trial 372 finished with value: 0.8299960519576649 and parameters: {'n_estimators': 443, 'eta': 0.0768428289961221, 'max_depth': 12, 'alpha': 0.013000000000000001, 'lambda': 34.362605631632384, 'max_bin': 421}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:14,262] Trial 373 finished with value: 0.8276422684632768 and parameters: {'n_estimators': 381, 'eta': 0.08762328573563749, 'max_depth': 12, 'alpha': 0.2712, 'lambda': 35.54588937089292, 'max_bin': 496}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:19,435] Trial 374 finished with value: 0.8288078604644221 and parameters: {'n_estimators': 293, 'eta': 0.08141879385641997, 'max_depth': 10, 'alpha': 0.1267, 'lambda': 37.15336593585544, 'max_bin': 395}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:24,570] Trial 375 finished with value: 0.8304752482272655 and parameters: {'n_estimators': 412, 'eta': 0.09048453274885587, 'max_depth': 11, 'alpha': 0.1706, 'lambda': 30.47642116008605, 'max_bin': 487}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:30,133] Trial 376 finished with value: 0.8262873514294975 and parameters: {'n_estimators': 364, 'eta': 0.07366811110379612, 'max_depth': 11, 'alpha': 0.1039, 'lambda': 32.80894570283768, 'max_bin': 405}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:35,227] Trial 377 finished with value: 0.8336133188343826 and parameters: {'n_estimators': 489, 'eta': 0.08501874543356697, 'max_depth': 12, 'alpha': 0.6528, 'lambda': 27.317954512076295, 'max_bin': 344}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:39,850] Trial 378 finished with value: 0.8280398794565299 and parameters: {'n_estimators': 343, 'eta': 0.09693752692644847, 'max_depth': 11, 'alpha': 0.5105000000000001, 'lambda': 28.348112177157027, 'max_bin': 480}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:48,168] Trial 379 finished with value: 0.8216833579159127 and parameters: {'n_estimators': 401, 'eta': 0.037224233974775096, 'max_depth': 12, 'alpha': 0.0611, 'lambda': 39.13815260190716, 'max_bin': 449}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:53,823] Trial 380 finished with value: 0.8245683591551535 and parameters: {'n_estimators': 368, 'eta': 0.07803961929530565, 'max_depth': 9, 'alpha': 0.2499, 'lambda': 35.105732090435204, 'max_bin': 377}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:25:58,348] Trial 381 finished with value: 0.8312421501309203 and parameters: {'n_estimators': 428, 'eta': 0.09354052917452771, 'max_depth': 11, 'alpha': 0.1957, 'lambda': 26.053625065291993, 'max_bin': 338}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:06,459] Trial 382 finished with value: 0.8300989433073903 and parameters: {'n_estimators': 454, 'eta': 0.05042585149293563, 'max_depth': 12, 'alpha': 0.6227, 'lambda': 37.984499490894585, 'max_bin': 468}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:12,447] Trial 383 finished with value: 0.8208551713722111 and parameters: {'n_estimators': 349, 'eta': 0.0637131201305157, 'max_depth': 10, 'alpha': 0.1305, 'lambda': 33.785565531105995, 'max_bin': 443}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:19,179] Trial 384 finished with value: 0.8319296584175302 and parameters: {'n_estimators': 531, 'eta': 0.06705124709238058, 'max_depth': 11, 'alpha': 0.079, 'lambda': 36.017058759465556, 'max_bin': 327}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:27,740] Trial 385 finished with value: 0.8280970025194936 and parameters: {'n_estimators': 639, 'eta': 0.045560348792722985, 'max_depth': 12, 'alpha': 0.341, 'lambda': 29.454087345701204, 'max_bin': 461}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:30,750] Trial 386 finished with value: 0.8199572277406784 and parameters: {'n_estimators': 139, 'eta': 0.08326569512852974, 'max_depth': 11, 'alpha': 0.45180000000000003, 'lambda': 31.441934164505017, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:32,920] Trial 387 finished with value: 0.812788959992003 and parameters: {'n_estimators': 107, 'eta': 0.08918677127533076, 'max_depth': 9, 'alpha': 0.1522, 'lambda': 36.86448981768517, 'max_bin': 474}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:38,101] Trial 388 finished with value: 0.8296221164247175 and parameters: {'n_estimators': 875, 'eta': 0.08604831165216026, 'max_depth': 12, 'alpha': 0.22340000000000002, 'lambda': 34.29108443958438, 'max_bin': 493}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:43,696] Trial 389 finished with value: 0.8300470254574053 and parameters: {'n_estimators': 385, 'eta': 0.08073285278422089, 'max_depth': 11, 'alpha': 0.7140000000000001, 'lambda': 32.0905788806708, 'max_bin': 333}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:49,049] Trial 390 finished with value: 0.8327287858691168 and parameters: {'n_estimators': 323, 'eta': 0.08684397037892957, 'max_depth': 12, 'alpha': 0.1116, 'lambda': 35.51316270716261, 'max_bin': 485}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:53,417] Trial 391 finished with value: 0.8236980337378632 and parameters: {'n_estimators': 411, 'eta': 0.0981606082126674, 'max_depth': 11, 'alpha': 0.17070000000000002, 'lambda': 30.59503176963217, 'max_bin': 429}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:26:58,651] Trial 392 finished with value: 0.8263367428904715 and parameters: {'n_estimators': 435, 'eta': 0.09133940649810873, 'max_depth': 8, 'alpha': 0.4889, 'lambda': 33.64478671379663, 'max_bin': 437}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:27:04,662] Trial 393 finished with value: 0.8213370187386726 and parameters: {'n_estimators': 300, 'eta': 0.0415795864029824, 'max_depth': 10, 'alpha': 0.7523000000000001, 'lambda': 34.83663886571098, 'max_bin': 415}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:27:09,935] Trial 394 finished with value: 0.8307088116581806 and parameters: {'n_estimators': 272, 'eta': 0.08272668578947252, 'max_depth': 12, 'alpha': 0.2957, 'lambda': 36.262481595811614, 'max_bin': 321}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:27:16,358] Trial 395 finished with value: 0.8265556642986025 and parameters: {'n_estimators': 392, 'eta': 0.05694474190923373, 'max_depth': 9, 'alpha': 0.5459, 'lambda': 23.705297246593823, 'max_bin': 398}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:27:21,280] Trial 396 finished with value: 0.8280783143488062 and parameters: {'n_estimators': 356, 'eta': 0.09505465128531926, 'max_depth': 11, 'alpha': 0.0897, 'lambda': 32.896917066010296, 'max_bin': 480}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:27:25,905] Trial 397 finished with value: 0.8275317548305766 and parameters: {'n_estimators': 214, 'eta': 0.06860265043507464, 'max_depth': 12, 'alpha': 0.6704, 'lambda': 38.08459613921797, 'max_bin': 359}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:27:33,223] Trial 398 finished with value: 0.8287915235577412 and parameters: {'n_estimators': 507, 'eta': 0.05146850441771459, 'max_depth': 11, 'alpha': 0.0339, 'lambda': 27.873123888916563, 'max_bin': 348}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:27:39,563] Trial 399 finished with value: 0.8280314711209427 and parameters: {'n_estimators': 377, 'eta': 0.07905278030285394, 'max_depth': 12, 'alpha': 0.5910000000000001, 'lambda': 37.286099487627105, 'max_bin': 489}. Best is trial 30 with value: 0.8425128924158303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8425\n",
      "\tBest params:\n",
      "\t\tn_estimators: 548\n",
      "\t\teta: 0.09018534818954298\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.1328\n",
      "\t\tlambda: 36.36011307197028\n",
      "\t\tmax_bin: 335\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  326.000000  318.000000  325.000000  328.000000   \n",
      "1                    TN  169.000000  186.000000  175.000000  176.000000   \n",
      "2                    FP   58.000000   51.000000   54.000000   62.000000   \n",
      "3                    FN   42.000000   40.000000   41.000000   29.000000   \n",
      "4              Accuracy    0.831933    0.847059    0.840336    0.847059   \n",
      "5             Precision    0.848958    0.861789    0.857520    0.841026   \n",
      "6           Sensitivity    0.885870    0.888268    0.887978    0.918768   \n",
      "7           Specificity    0.744500    0.784800    0.764200    0.739500   \n",
      "8              F1 score    0.867021    0.874828    0.872483    0.878179   \n",
      "9   F1 score (weighted)    0.830651    0.846399    0.839397    0.844741   \n",
      "10     F1 score (macro)    0.819355    0.839142    0.829500    0.836381   \n",
      "11    Balanced Accuracy    0.815181    0.836539    0.826085    0.829132   \n",
      "12                  MCC    0.640060    0.678913    0.659892    0.678599   \n",
      "13                  NPV    0.800900    0.823000    0.810200    0.858500   \n",
      "14              ROC_AUC    0.815181    0.836539    0.826085    0.829132   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   333.000000  324.000000  336.000000  324.000000  \n",
      "1   180.000000  183.000000  173.000000  179.000000  \n",
      "2    58.000000   51.000000   40.000000   49.000000  \n",
      "3    24.000000   37.000000   46.000000   43.000000  \n",
      "4     0.862185    0.852101    0.855462    0.845378  \n",
      "5     0.851662    0.864000    0.893617    0.868633  \n",
      "6     0.932773    0.897507    0.879581    0.882834  \n",
      "7     0.756300    0.782100    0.812200    0.785100  \n",
      "8     0.890374    0.880435    0.886544    0.875676  \n",
      "9     0.860016    0.851227    0.855894    0.844974  \n",
      "10    0.852427    0.843301    0.843735    0.835616  \n",
      "11    0.844538    0.839779    0.845894    0.833961  \n",
      "12    0.711191    0.687640    0.687667    0.671421  \n",
      "13    0.882400    0.831800    0.790000    0.806300  \n",
      "14    0.844538    0.839779    0.845894    0.833961  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_xgb_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_xgb_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_xgb_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_xgb_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_xgb_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_xgb_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_xgb_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_xgb_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_xgb_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_xgb_7)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:27:47,791] Trial 400 finished with value: 0.8302138443678148 and parameters: {'n_estimators': 673, 'eta': 0.0481933390592974, 'max_depth': 11, 'alpha': 0.1312, 'lambda': 34.48687187152709, 'max_bin': 339}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:27:53,078] Trial 401 finished with value: 0.8256675874076386 and parameters: {'n_estimators': 331, 'eta': 0.07385805402700639, 'max_depth': 12, 'alpha': 0.1726, 'lambda': 35.742008688367086, 'max_bin': 391}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:27:57,417] Trial 402 finished with value: 0.8259464142672343 and parameters: {'n_estimators': 697, 'eta': 0.08872593856882659, 'max_depth': 11, 'alpha': 0.1981, 'lambda': 29.04675963831382, 'max_bin': 372}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:02,931] Trial 403 finished with value: 0.8295497874020988 and parameters: {'n_estimators': 460, 'eta': 0.06573338087244758, 'max_depth': 11, 'alpha': 0.8714000000000001, 'lambda': 32.41501728265657, 'max_bin': 306}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:06,285] Trial 404 finished with value: 0.8276657338181552 and parameters: {'n_estimators': 152, 'eta': 0.08432684010751788, 'max_depth': 12, 'alpha': 0.4082, 'lambda': 36.75579633663339, 'max_bin': 469}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:10,966] Trial 405 finished with value: 0.8247749760211553 and parameters: {'n_estimators': 421, 'eta': 0.09257853966383767, 'max_depth': 11, 'alpha': 0.6371, 'lambda': 38.813764890353454, 'max_bin': 384}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:15,601] Trial 406 finished with value: 0.8214180008219387 and parameters: {'n_estimators': 366, 'eta': 0.07756455214438877, 'max_depth': 9, 'alpha': 0.1413, 'lambda': 33.410985414995345, 'max_bin': 475}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:21,459] Trial 407 finished with value: 0.8305413127564917 and parameters: {'n_estimators': 311, 'eta': 0.052137856681215164, 'max_depth': 10, 'alpha': 0.1047, 'lambda': 26.713063766353816, 'max_bin': 329}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:26,326] Trial 408 finished with value: 0.8303155884970084 and parameters: {'n_estimators': 400, 'eta': 0.081493078087795, 'max_depth': 12, 'alpha': 0.0606, 'lambda': 31.31086057270871, 'max_bin': 483}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:34,276] Trial 409 finished with value: 0.8338472682982865 and parameters: {'n_estimators': 488, 'eta': 0.024069766896457644, 'max_depth': 12, 'alpha': 0.11380000000000001, 'lambda': 6.714948660730627, 'max_bin': 497}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:38,728] Trial 410 finished with value: 0.8243673557019235 and parameters: {'n_estimators': 344, 'eta': 0.09020469291179053, 'max_depth': 11, 'alpha': 0.1562, 'lambda': 35.07241425638524, 'max_bin': 424}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:43,837] Trial 411 finished with value: 0.8272071018697726 and parameters: {'n_estimators': 654, 'eta': 0.07135030894561839, 'max_depth': 11, 'alpha': 0.2107, 'lambda': 24.26723523884783, 'max_bin': 455}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:50,160] Trial 412 finished with value: 0.830635359833925 and parameters: {'n_estimators': 604, 'eta': 0.05535279756812843, 'max_depth': 12, 'alpha': 0.0772, 'lambda': 30.113504003303486, 'max_bin': 494}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:54,981] Trial 413 finished with value: 0.8268451984753297 and parameters: {'n_estimators': 440, 'eta': 0.08662831413170838, 'max_depth': 11, 'alpha': 0.5205000000000001, 'lambda': 36.314080413795615, 'max_bin': 343}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:28:58,924] Trial 414 finished with value: 0.8329752126690755 and parameters: {'n_estimators': 377, 'eta': 0.09986192362900438, 'max_depth': 11, 'alpha': 0.2533, 'lambda': 22.31963169607735, 'max_bin': 464}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:03,501] Trial 415 finished with value: 0.8283047723042571 and parameters: {'n_estimators': 544, 'eta': 0.09524642819502217, 'max_depth': 12, 'alpha': 0.4393, 'lambda': 39.979456630250176, 'max_bin': 490}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:08,344] Trial 416 finished with value: 0.830269610599381 and parameters: {'n_estimators': 245, 'eta': 0.05915950333187687, 'max_depth': 10, 'alpha': 0.4677, 'lambda': 34.20020952550328, 'max_bin': 274}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:10,882] Trial 417 finished with value: 0.8189520324253937 and parameters: {'n_estimators': 127, 'eta': 0.08396261284953074, 'max_depth': 9, 'alpha': 0.12390000000000001, 'lambda': 37.67025515183031, 'max_bin': 474}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:16,002] Trial 418 finished with value: 0.8306636919397057 and parameters: {'n_estimators': 476, 'eta': 0.07968032859582447, 'max_depth': 12, 'alpha': 0.6936, 'lambda': 35.64021560487443, 'max_bin': 480}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:21,202] Trial 419 finished with value: 0.8325177639293212 and parameters: {'n_estimators': 415, 'eta': 0.06238216180465982, 'max_depth': 11, 'alpha': 0.093, 'lambda': 25.224439885516354, 'max_bin': 336}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:25,162] Trial 420 finished with value: 0.8298365464524664 and parameters: {'n_estimators': 357, 'eta': 0.0964789467242195, 'max_depth': 12, 'alpha': 0.1777, 'lambda': 19.9482784343383, 'max_bin': 314}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:28,467] Trial 421 finished with value: 0.8255526727298752 and parameters: {'n_estimators': 194, 'eta': 0.087903517832874, 'max_depth': 8, 'alpha': 0.2757, 'lambda': 28.283400004667598, 'max_bin': 434}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:32,876] Trial 422 finished with value: 0.829774004987239 and parameters: {'n_estimators': 395, 'eta': 0.07550278071077601, 'max_depth': 11, 'alpha': 0.1432, 'lambda': 21.57275380037267, 'max_bin': 486}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:37,489] Trial 423 finished with value: 0.8245486374220358 and parameters: {'n_estimators': 328, 'eta': 0.08157051459768955, 'max_depth': 9, 'alpha': 0.23620000000000002, 'lambda': 33.140097137659154, 'max_bin': 409}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:41,852] Trial 424 finished with value: 0.8280335053978162 and parameters: {'n_estimators': 284, 'eta': 0.09250209733390037, 'max_depth': 12, 'alpha': 0.6227, 'lambda': 32.26913010652288, 'max_bin': 444}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:46,031] Trial 425 finished with value: 0.8257435181941579 and parameters: {'n_estimators': 435, 'eta': 0.08581351851219271, 'max_depth': 10, 'alpha': 0.1559, 'lambda': 34.808930217927376, 'max_bin': 331}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:50,939] Trial 426 finished with value: 0.8292812672246537 and parameters: {'n_estimators': 639, 'eta': 0.08308418239181088, 'max_depth': 11, 'alpha': 0.12240000000000001, 'lambda': 37.05245927622498, 'max_bin': 355}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:29:55,675] Trial 427 finished with value: 0.8229426671073405 and parameters: {'n_estimators': 342, 'eta': 0.08999715292540125, 'max_depth': 12, 'alpha': 0.3078, 'lambda': 38.45594968174881, 'max_bin': 379}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:00,074] Trial 428 finished with value: 0.8281327313343081 and parameters: {'n_estimators': 461, 'eta': 0.09790392910255852, 'max_depth': 11, 'alpha': 0.09190000000000001, 'lambda': 31.23456517872343, 'max_bin': 469}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:02,159] Trial 429 finished with value: 0.8232399778656827 and parameters: {'n_estimators': 88, 'eta': 0.09395589062060812, 'max_depth': 12, 'alpha': 0.046900000000000004, 'lambda': 29.466407684928637, 'max_bin': 388}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:07,730] Trial 430 finished with value: 0.8325009122415576 and parameters: {'n_estimators': 380, 'eta': 0.07888920079042723, 'max_depth': 11, 'alpha': 0.6619, 'lambda': 36.14460764406589, 'max_bin': 450}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:13,199] Trial 431 finished with value: 0.8335260268812617 and parameters: {'n_estimators': 619, 'eta': 0.0764348512659531, 'max_depth': 11, 'alpha': 0.5637, 'lambda': 33.8883823302796, 'max_bin': 462}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:21,693] Trial 432 finished with value: 0.8269891075949671 and parameters: {'n_estimators': 407, 'eta': 0.034713079062313086, 'max_depth': 12, 'alpha': 0.4864, 'lambda': 35.27008358082608, 'max_bin': 320}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:27,334] Trial 433 finished with value: 0.8271906989297845 and parameters: {'n_estimators': 520, 'eta': 0.0718539877950068, 'max_depth': 10, 'alpha': 0.1892, 'lambda': 36.43519034945895, 'max_bin': 496}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:31,503] Trial 434 finished with value: 0.83252016951571 and parameters: {'n_estimators': 580, 'eta': 0.06441657759368878, 'max_depth': 9, 'alpha': 0.0704, 'lambda': 14.725241501551103, 'max_bin': 366}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:36,204] Trial 435 finished with value: 0.8278501216220777 and parameters: {'n_estimators': 315, 'eta': 0.08534003923998704, 'max_depth': 11, 'alpha': 0.6011000000000001, 'lambda': 34.413822515732576, 'max_bin': 477}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:40,332] Trial 436 finished with value: 0.8323181361805461 and parameters: {'n_estimators': 450, 'eta': 0.08824595923240881, 'max_depth': 12, 'alpha': 0.1394, 'lambda': 19.073700238797414, 'max_bin': 440}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:45,288] Trial 437 finished with value: 0.8296177390784031 and parameters: {'n_estimators': 361, 'eta': 0.06843019369962455, 'max_depth': 11, 'alpha': 0.10450000000000001, 'lambda': 26.29461158148121, 'max_bin': 351}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:51,207] Trial 438 finished with value: 0.8276365656977751 and parameters: {'n_estimators': 681, 'eta': 0.061234647994034955, 'max_depth': 12, 'alpha': 0.22130000000000002, 'lambda': 30.687496973001036, 'max_bin': 500}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:30:56,352] Trial 439 finished with value: 0.8275330788276752 and parameters: {'n_estimators': 422, 'eta': 0.08205738216281228, 'max_depth': 11, 'alpha': 0.5335, 'lambda': 37.54702877963681, 'max_bin': 430}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:00,790] Trial 440 finished with value: 0.826438208816208 and parameters: {'n_estimators': 393, 'eta': 0.09195750812871661, 'max_depth': 12, 'alpha': 0.16570000000000001, 'lambda': 33.04731233181945, 'max_bin': 490}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:05,218] Trial 441 finished with value: 0.8289526594646478 and parameters: {'n_estimators': 262, 'eta': 0.08427088576857013, 'max_depth': 11, 'alpha': 0.5077, 'lambda': 39.31613399430902, 'max_bin': 345}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:10,323] Trial 442 finished with value: 0.825795160093907 and parameters: {'n_estimators': 297, 'eta': 0.07988024285730828, 'max_depth': 12, 'alpha': 0.7268, 'lambda': 35.56343025764612, 'max_bin': 484}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:14,596] Trial 443 finished with value: 0.8242830962441283 and parameters: {'n_estimators': 376, 'eta': 0.09630048245334323, 'max_depth': 6, 'alpha': 0.35900000000000004, 'lambda': 31.785035946554345, 'max_bin': 472}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:20,725] Trial 444 finished with value: 0.8329717177323296 and parameters: {'n_estimators': 336, 'eta': 0.05533718633130385, 'max_depth': 11, 'alpha': 0.6482, 'lambda': 28.479022483069357, 'max_bin': 404}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:25,274] Trial 445 finished with value: 0.8250994047213605 and parameters: {'n_estimators': 475, 'eta': 0.08965306447306154, 'max_depth': 11, 'alpha': 0.1983, 'lambda': 33.78296872936745, 'max_bin': 340}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:30,598] Trial 446 finished with value: 0.8307141096317057 and parameters: {'n_estimators': 709, 'eta': 0.05370194219283477, 'max_depth': 12, 'alpha': 0.11220000000000001, 'lambda': 18.3862275458745, 'max_bin': 383}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:33,964] Trial 447 finished with value: 0.8225055123486573 and parameters: {'n_estimators': 173, 'eta': 0.06627310667446322, 'max_depth': 9, 'alpha': 0.1338, 'lambda': 34.985279354716326, 'max_bin': 419}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:35,281] Trial 448 finished with value: 0.8003121848374299 and parameters: {'n_estimators': 52, 'eta': 0.029807047556777832, 'max_depth': 11, 'alpha': 0.7948000000000001, 'lambda': 16.676556409470017, 'max_bin': 457}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:39,992] Trial 449 finished with value: 0.8273483467655186 and parameters: {'n_estimators': 351, 'eta': 0.08672919800700503, 'max_depth': 12, 'alpha': 0.3865, 'lambda': 36.92858367679421, 'max_bin': 373}. Best is trial 30 with value: 0.8425128924158303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8425\n",
      "\tBest params:\n",
      "\t\tn_estimators: 548\n",
      "\t\teta: 0.09018534818954298\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.1328\n",
      "\t\tlambda: 36.36011307197028\n",
      "\t\tmax_bin: 335\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  326.000000  318.000000  325.000000  328.000000   \n",
      "1                    TN  169.000000  186.000000  175.000000  176.000000   \n",
      "2                    FP   58.000000   51.000000   54.000000   62.000000   \n",
      "3                    FN   42.000000   40.000000   41.000000   29.000000   \n",
      "4              Accuracy    0.831933    0.847059    0.840336    0.847059   \n",
      "5             Precision    0.848958    0.861789    0.857520    0.841026   \n",
      "6           Sensitivity    0.885870    0.888268    0.887978    0.918768   \n",
      "7           Specificity    0.744500    0.784800    0.764200    0.739500   \n",
      "8              F1 score    0.867021    0.874828    0.872483    0.878179   \n",
      "9   F1 score (weighted)    0.830651    0.846399    0.839397    0.844741   \n",
      "10     F1 score (macro)    0.819355    0.839142    0.829500    0.836381   \n",
      "11    Balanced Accuracy    0.815181    0.836539    0.826085    0.829132   \n",
      "12                  MCC    0.640060    0.678913    0.659892    0.678599   \n",
      "13                  NPV    0.800900    0.823000    0.810200    0.858500   \n",
      "14              ROC_AUC    0.815181    0.836539    0.826085    0.829132   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   333.000000  324.000000  336.000000  324.000000  326.000000  \n",
      "1   180.000000  183.000000  173.000000  179.000000  181.000000  \n",
      "2    58.000000   51.000000   40.000000   49.000000   48.000000  \n",
      "3    24.000000   37.000000   46.000000   43.000000   40.000000  \n",
      "4     0.862185    0.852101    0.855462    0.845378    0.852101  \n",
      "5     0.851662    0.864000    0.893617    0.868633    0.871658  \n",
      "6     0.932773    0.897507    0.879581    0.882834    0.890710  \n",
      "7     0.756300    0.782100    0.812200    0.785100    0.790400  \n",
      "8     0.890374    0.880435    0.886544    0.875676    0.881081  \n",
      "9     0.860016    0.851227    0.855894    0.844974    0.851586  \n",
      "10    0.852427    0.843301    0.843735    0.835616    0.842763  \n",
      "11    0.844538    0.839779    0.845894    0.833961    0.840552  \n",
      "12    0.711191    0.687640    0.687667    0.671421    0.685866  \n",
      "13    0.882400    0.831800    0.790000    0.806300    0.819000  \n",
      "14    0.844538    0.839779    0.845894    0.833961    0.840552  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_xgb_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_xgb_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_xgb_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_xgb_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_xgb_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_xgb_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_xgb_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_xgb_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_xgb_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_xgb_8)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:31:48,669] Trial 450 finished with value: 0.8326546272003235 and parameters: {'n_estimators': 661, 'eta': 0.04794789335411084, 'max_depth': 11, 'alpha': 0.0824, 'lambda': 26.966410209197484, 'max_bin': 328}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:31:54,087] Trial 451 finished with value: 0.8316889048569251 and parameters: {'n_estimators': 494, 'eta': 0.09419702308398437, 'max_depth': 12, 'alpha': 0.17070000000000002, 'lambda': 29.771248256568576, 'max_bin': 397}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:00,826] Trial 452 finished with value: 0.8305580674818795 and parameters: {'n_estimators': 432, 'eta': 0.07061218509903801, 'max_depth': 9, 'alpha': 0.24150000000000002, 'lambda': 38.234528541291716, 'max_bin': 479}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:06,943] Trial 453 finished with value: 0.8318038999214654 and parameters: {'n_estimators': 369, 'eta': 0.0827626007651102, 'max_depth': 11, 'alpha': 0.0251, 'lambda': 35.75621315509982, 'max_bin': 334}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:14,574] Trial 454 finished with value: 0.8163485964172719 and parameters: {'n_estimators': 450, 'eta': 0.01458167662806268, 'max_depth': 7, 'alpha': 0.9916, 'lambda': 32.42475818471843, 'max_bin': 466}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:20,244] Trial 455 finished with value: 0.8299318311635547 and parameters: {'n_estimators': 410, 'eta': 0.07757443582577629, 'max_depth': 10, 'alpha': 0.41650000000000004, 'lambda': 27.5821359112806, 'max_bin': 493}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:26,362] Trial 456 finished with value: 0.8339296809582853 and parameters: {'n_estimators': 389, 'eta': 0.07374225001946949, 'max_depth': 12, 'alpha': 0.14780000000000001, 'lambda': 34.616421949167126, 'max_bin': 447}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:31,841] Trial 457 finished with value: 0.8319505499643837 and parameters: {'n_estimators': 552, 'eta': 0.09980028178203458, 'max_depth': 12, 'alpha': 0.5829, 'lambda': 36.29361469940585, 'max_bin': 391}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:37,866] Trial 458 finished with value: 0.8335339873464134 and parameters: {'n_estimators': 324, 'eta': 0.08067922619205212, 'max_depth': 11, 'alpha': 0.0674, 'lambda': 37.36831696399215, 'max_bin': 485}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:41,830] Trial 459 finished with value: 0.8382623090076932 and parameters: {'n_estimators': 359, 'eta': 0.0855105016814076, 'max_depth': 11, 'alpha': 0.2094, 'lambda': 9.542744424315483, 'max_bin': 323}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:47,439] Trial 460 finished with value: 0.8300967249265373 and parameters: {'n_estimators': 354, 'eta': 0.0871148382864659, 'max_depth': 11, 'alpha': 0.2165, 'lambda': 30.30958958961699, 'max_bin': 322}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:52,075] Trial 461 finished with value: 0.8324600978141052 and parameters: {'n_estimators': 370, 'eta': 0.09113022768749046, 'max_depth': 11, 'alpha': 0.2626, 'lambda': 19.662428645748395, 'max_bin': 437}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:32:57,754] Trial 462 finished with value: 0.8401065054896597 and parameters: {'n_estimators': 753, 'eta': 0.0582679600159213, 'max_depth': 11, 'alpha': 0.21980000000000002, 'lambda': 10.165632728549138, 'max_bin': 310}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:02,624] Trial 463 finished with value: 0.8384073453803907 and parameters: {'n_estimators': 763, 'eta': 0.059155985352143024, 'max_depth': 11, 'alpha': 0.2247, 'lambda': 5.963691995761718, 'max_bin': 310}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:08,138] Trial 464 finished with value: 0.834044495918608 and parameters: {'n_estimators': 339, 'eta': 0.05713583928832114, 'max_depth': 11, 'alpha': 0.2127, 'lambda': 8.56546395146054, 'max_bin': 297}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:13,652] Trial 465 finished with value: 0.8406450977800339 and parameters: {'n_estimators': 820, 'eta': 0.05708785333793435, 'max_depth': 11, 'alpha': 0.2414, 'lambda': 6.594251411695939, 'max_bin': 304}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:18,064] Trial 466 finished with value: 0.8379106364825543 and parameters: {'n_estimators': 816, 'eta': 0.06163429720664443, 'max_depth': 11, 'alpha': 0.6739, 'lambda': 4.6165941370992964, 'max_bin': 302}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:22,299] Trial 467 finished with value: 0.8386459227571411 and parameters: {'n_estimators': 754, 'eta': 0.059382212930016366, 'max_depth': 11, 'alpha': 0.24050000000000002, 'lambda': 4.835412993623229, 'max_bin': 298}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:27,668] Trial 468 finished with value: 0.8382555381295023 and parameters: {'n_estimators': 811, 'eta': 0.057333724647783774, 'max_depth': 11, 'alpha': 0.2525, 'lambda': 5.663788920405997, 'max_bin': 295}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:32,192] Trial 469 finished with value: 0.8393386751432486 and parameters: {'n_estimators': 778, 'eta': 0.05719611398272122, 'max_depth': 11, 'alpha': 0.2349, 'lambda': 4.385832390831737, 'max_bin': 311}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:37,358] Trial 470 finished with value: 0.8406994132939675 and parameters: {'n_estimators': 768, 'eta': 0.05656584954249838, 'max_depth': 11, 'alpha': 0.2793, 'lambda': 5.571064444587708, 'max_bin': 305}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:42,078] Trial 471 finished with value: 0.8370219594469956 and parameters: {'n_estimators': 775, 'eta': 0.05625890812752293, 'max_depth': 11, 'alpha': 0.28200000000000003, 'lambda': 3.850085361803645, 'max_bin': 308}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:46,903] Trial 472 finished with value: 0.8369360735638705 and parameters: {'n_estimators': 747, 'eta': 0.058282471914952616, 'max_depth': 11, 'alpha': 0.2414, 'lambda': 5.266974136207588, 'max_bin': 290}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:51,914] Trial 473 finished with value: 0.8387471059441843 and parameters: {'n_estimators': 768, 'eta': 0.053349510867675004, 'max_depth': 11, 'alpha': 0.266, 'lambda': 3.904235621717781, 'max_bin': 301}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:33:56,820] Trial 474 finished with value: 0.8398590499649371 and parameters: {'n_estimators': 777, 'eta': 0.053879876228336884, 'max_depth': 11, 'alpha': 0.2617, 'lambda': 2.9234802024548854, 'max_bin': 305}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:34:01,339] Trial 475 finished with value: 0.8377275210716132 and parameters: {'n_estimators': 842, 'eta': 0.053372006539488245, 'max_depth': 11, 'alpha': 0.2536, 'lambda': 3.544795691056485, 'max_bin': 312}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:34:05,410] Trial 476 finished with value: 0.840025774050196 and parameters: {'n_estimators': 799, 'eta': 0.054704367078075, 'max_depth': 11, 'alpha': 0.2721, 'lambda': 1.9734305785128412, 'max_bin': 315}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:34:09,883] Trial 477 finished with value: 0.8405900465312657 and parameters: {'n_estimators': 793, 'eta': 0.05663223497639471, 'max_depth': 11, 'alpha': 0.275, 'lambda': 3.1442505896660835, 'max_bin': 304}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:34:14,393] Trial 478 finished with value: 0.8388938764995622 and parameters: {'n_estimators': 791, 'eta': 0.05509668145796854, 'max_depth': 11, 'alpha': 0.2805, 'lambda': 3.3566765103819747, 'max_bin': 307}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:34:18,867] Trial 479 finished with value: 0.8369651142061446 and parameters: {'n_estimators': 801, 'eta': 0.05536803129896047, 'max_depth': 11, 'alpha': 0.2896, 'lambda': 2.744150989684649, 'max_bin': 304}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:34:23,633] Trial 480 finished with value: 0.8416426718804416 and parameters: {'n_estimators': 829, 'eta': 0.05239852781555165, 'max_depth': 11, 'alpha': 0.30510000000000004, 'lambda': 2.516240552718939, 'max_bin': 314}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:34:28,294] Trial 481 finished with value: 0.8390908293917642 and parameters: {'n_estimators': 824, 'eta': 0.05399613610832594, 'max_depth': 11, 'alpha': 0.3171, 'lambda': 1.9427116340170334, 'max_bin': 314}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:34:32,724] Trial 482 finished with value: 0.8373363338293031 and parameters: {'n_estimators': 835, 'eta': 0.05000282407891004, 'max_depth': 11, 'alpha': 0.301, 'lambda': 2.087637106876697, 'max_bin': 308}. Best is trial 30 with value: 0.8425128924158303.\n",
      "[I 2023-12-04 17:34:37,377] Trial 483 finished with value: 0.8436387783437759 and parameters: {'n_estimators': 778, 'eta': 0.05746353137910226, 'max_depth': 11, 'alpha': 0.2717, 'lambda': 3.0760728618639215, 'max_bin': 303}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:34:41,917] Trial 484 finished with value: 0.8390538709019133 and parameters: {'n_estimators': 789, 'eta': 0.05320988961691745, 'max_depth': 11, 'alpha': 0.2675, 'lambda': 2.5674967635548462, 'max_bin': 303}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:34:46,234] Trial 485 finished with value: 0.8389809769362605 and parameters: {'n_estimators': 803, 'eta': 0.052290065833319056, 'max_depth': 11, 'alpha': 0.2798, 'lambda': 2.6669105558597854, 'max_bin': 294}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:34:50,848] Trial 486 finished with value: 0.8344458518444651 and parameters: {'n_estimators': 778, 'eta': 0.05638403189877352, 'max_depth': 11, 'alpha': 0.2908, 'lambda': 4.417377630609525, 'max_bin': 314}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:34:54,552] Trial 487 finished with value: 0.8399582795043902 and parameters: {'n_estimators': 735, 'eta': 0.057515174710309744, 'max_depth': 11, 'alpha': 0.31930000000000003, 'lambda': 1.4437280412256093, 'max_bin': 307}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:34:58,528] Trial 488 finished with value: 0.8395711054127991 and parameters: {'n_estimators': 781, 'eta': 0.056501260027205215, 'max_depth': 11, 'alpha': 0.3027, 'lambda': 1.5712420025588423, 'max_bin': 305}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:02,514] Trial 489 finished with value: 0.8426406503549284 and parameters: {'n_estimators': 732, 'eta': 0.057525107593093254, 'max_depth': 11, 'alpha': 0.2821, 'lambda': 1.3901177998993541, 'max_bin': 302}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:06,657] Trial 490 finished with value: 0.8402235539115737 and parameters: {'n_estimators': 736, 'eta': 0.05800632745350658, 'max_depth': 11, 'alpha': 0.3407, 'lambda': 1.4526500512229021, 'max_bin': 305}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:10,972] Trial 491 finished with value: 0.838032794342961 and parameters: {'n_estimators': 865, 'eta': 0.055712037587151, 'max_depth': 11, 'alpha': 0.3341, 'lambda': 2.1925494405618196, 'max_bin': 292}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:14,874] Trial 492 finished with value: 0.8378287260172753 and parameters: {'n_estimators': 724, 'eta': 0.057525983115999074, 'max_depth': 11, 'alpha': 0.2876, 'lambda': 1.14611642127034, 'max_bin': 301}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:18,938] Trial 493 finished with value: 0.842458005867502 and parameters: {'n_estimators': 740, 'eta': 0.0518543522984703, 'max_depth': 11, 'alpha': 0.3148, 'lambda': 1.0311158032615495, 'max_bin': 304}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:23,314] Trial 494 finished with value: 0.837863152703445 and parameters: {'n_estimators': 820, 'eta': 0.051239195437598056, 'max_depth': 11, 'alpha': 0.34440000000000004, 'lambda': 1.792617936800968, 'max_bin': 305}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:27,478] Trial 495 finished with value: 0.8400904384202704 and parameters: {'n_estimators': 740, 'eta': 0.05329689958553705, 'max_depth': 11, 'alpha': 0.31770000000000004, 'lambda': 1.6236755608808688, 'max_bin': 299}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:31,543] Trial 496 finished with value: 0.836210387616877 and parameters: {'n_estimators': 753, 'eta': 0.05220680212725214, 'max_depth': 11, 'alpha': 0.3154, 'lambda': 1.231763319160294, 'max_bin': 283}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:35,610] Trial 497 finished with value: 0.8414742519980422 and parameters: {'n_estimators': 743, 'eta': 0.05530389838556169, 'max_depth': 11, 'alpha': 0.3105, 'lambda': 1.124097826347925, 'max_bin': 297}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:39,765] Trial 498 finished with value: 0.8395549959687271 and parameters: {'n_estimators': 739, 'eta': 0.05442134536523779, 'max_depth': 11, 'alpha': 0.329, 'lambda': 1.2177593485873839, 'max_bin': 298}. Best is trial 483 with value: 0.8436387783437759.\n",
      "[I 2023-12-04 17:35:44,492] Trial 499 finished with value: 0.8415897667092134 and parameters: {'n_estimators': 732, 'eta': 0.05222819940363433, 'max_depth': 11, 'alpha': 0.30820000000000003, 'lambda': 2.962256797058301, 'max_bin': 305}. Best is trial 483 with value: 0.8436387783437759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8436\n",
      "\tBest params:\n",
      "\t\tn_estimators: 778\n",
      "\t\teta: 0.05746353137910226\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.2717\n",
      "\t\tlambda: 3.0760728618639215\n",
      "\t\tmax_bin: 303\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  326.000000  318.000000  325.000000  328.000000   \n",
      "1                    TN  169.000000  186.000000  175.000000  176.000000   \n",
      "2                    FP   58.000000   51.000000   54.000000   62.000000   \n",
      "3                    FN   42.000000   40.000000   41.000000   29.000000   \n",
      "4              Accuracy    0.831933    0.847059    0.840336    0.847059   \n",
      "5             Precision    0.848958    0.861789    0.857520    0.841026   \n",
      "6           Sensitivity    0.885870    0.888268    0.887978    0.918768   \n",
      "7           Specificity    0.744500    0.784800    0.764200    0.739500   \n",
      "8              F1 score    0.867021    0.874828    0.872483    0.878179   \n",
      "9   F1 score (weighted)    0.830651    0.846399    0.839397    0.844741   \n",
      "10     F1 score (macro)    0.819355    0.839142    0.829500    0.836381   \n",
      "11    Balanced Accuracy    0.815181    0.836539    0.826085    0.829132   \n",
      "12                  MCC    0.640060    0.678913    0.659892    0.678599   \n",
      "13                  NPV    0.800900    0.823000    0.810200    0.858500   \n",
      "14              ROC_AUC    0.815181    0.836539    0.826085    0.829132   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   333.000000  324.000000  336.000000  324.000000  326.000000  329.000000  \n",
      "1   180.000000  183.000000  173.000000  179.000000  181.000000  165.000000  \n",
      "2    58.000000   51.000000   40.000000   49.000000   48.000000   62.000000  \n",
      "3    24.000000   37.000000   46.000000   43.000000   40.000000   39.000000  \n",
      "4     0.862185    0.852101    0.855462    0.845378    0.852101    0.830252  \n",
      "5     0.851662    0.864000    0.893617    0.868633    0.871658    0.841432  \n",
      "6     0.932773    0.897507    0.879581    0.882834    0.890710    0.894022  \n",
      "7     0.756300    0.782100    0.812200    0.785100    0.790400    0.726900  \n",
      "8     0.890374    0.880435    0.886544    0.875676    0.881081    0.866930  \n",
      "9     0.860016    0.851227    0.855894    0.844974    0.851586    0.828295  \n",
      "10    0.852427    0.843301    0.843735    0.835616    0.842763    0.816296  \n",
      "11    0.844538    0.839779    0.845894    0.833961    0.840552    0.810447  \n",
      "12    0.711191    0.687640    0.687667    0.671421    0.685866    0.635405  \n",
      "13    0.882400    0.831800    0.790000    0.806300    0.819000    0.808800  \n",
      "14    0.844538    0.839779    0.845894    0.833961    0.840552    0.810447  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=5, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_xgb_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_xgb_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_xgb_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_xgb_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_xgb_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_xgb_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_xgb_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_xgb_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_xgb_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_xgb_9)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>165.400000</td>\n",
       "      <td>6.883152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>88.100000</td>\n",
       "      <td>2.806738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>25.300000</td>\n",
       "      <td>5.034327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>3.591657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.853249</td>\n",
       "      <td>0.025246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.867195</td>\n",
       "      <td>0.027134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.900156</td>\n",
       "      <td>0.020528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.777910</td>\n",
       "      <td>0.036964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.883250</td>\n",
       "      <td>0.021851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.852272</td>\n",
       "      <td>0.025521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.842710</td>\n",
       "      <td>0.025538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.839040</td>\n",
       "      <td>0.025819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.686849</td>\n",
       "      <td>0.050408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.828590</td>\n",
       "      <td>0.028344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.839040</td>\n",
       "      <td>0.025819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       165.400000     6.883152\n",
       "1                    TN        88.100000     2.806738\n",
       "2                    FP        25.300000     5.034327\n",
       "3                    FN        18.300000     3.591657\n",
       "4              Accuracy         0.853249     0.025246\n",
       "5             Precision         0.867195     0.027134\n",
       "6           Sensitivity         0.900156     0.020528\n",
       "7           Specificity         0.777910     0.036964\n",
       "8              F1 score         0.883250     0.021851\n",
       "9   F1 score (weighted)         0.852272     0.025521\n",
       "10     F1 score (macro)         0.842710     0.025538\n",
       "11    Balanced Accuracy         0.839040     0.025819\n",
       "12                  MCC         0.686849     0.050408\n",
       "13                  NPV         0.828590     0.028344\n",
       "14              ROC_AUC         0.839040     0.025819"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>318.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>326.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>176.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>53.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.862185</td>\n",
       "      <td>0.852101</td>\n",
       "      <td>0.855462</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.852101</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.846387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>0.857520</td>\n",
       "      <td>0.841026</td>\n",
       "      <td>0.851662</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.868633</td>\n",
       "      <td>0.871658</td>\n",
       "      <td>0.841432</td>\n",
       "      <td>0.860029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.885870</td>\n",
       "      <td>0.888268</td>\n",
       "      <td>0.887978</td>\n",
       "      <td>0.918768</td>\n",
       "      <td>0.932773</td>\n",
       "      <td>0.897507</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>0.882834</td>\n",
       "      <td>0.890710</td>\n",
       "      <td>0.894022</td>\n",
       "      <td>0.895831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.784800</td>\n",
       "      <td>0.764200</td>\n",
       "      <td>0.739500</td>\n",
       "      <td>0.756300</td>\n",
       "      <td>0.782100</td>\n",
       "      <td>0.812200</td>\n",
       "      <td>0.785100</td>\n",
       "      <td>0.790400</td>\n",
       "      <td>0.726900</td>\n",
       "      <td>0.768600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.867021</td>\n",
       "      <td>0.874828</td>\n",
       "      <td>0.872483</td>\n",
       "      <td>0.878179</td>\n",
       "      <td>0.890374</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.886544</td>\n",
       "      <td>0.875676</td>\n",
       "      <td>0.881081</td>\n",
       "      <td>0.866930</td>\n",
       "      <td>0.877355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.830651</td>\n",
       "      <td>0.846399</td>\n",
       "      <td>0.839397</td>\n",
       "      <td>0.844741</td>\n",
       "      <td>0.860016</td>\n",
       "      <td>0.851227</td>\n",
       "      <td>0.855894</td>\n",
       "      <td>0.844974</td>\n",
       "      <td>0.851586</td>\n",
       "      <td>0.828295</td>\n",
       "      <td>0.845318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>0.839142</td>\n",
       "      <td>0.829500</td>\n",
       "      <td>0.836381</td>\n",
       "      <td>0.852427</td>\n",
       "      <td>0.843301</td>\n",
       "      <td>0.843735</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.842763</td>\n",
       "      <td>0.816296</td>\n",
       "      <td>0.835852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.815181</td>\n",
       "      <td>0.836539</td>\n",
       "      <td>0.826085</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.844538</td>\n",
       "      <td>0.839779</td>\n",
       "      <td>0.845894</td>\n",
       "      <td>0.833961</td>\n",
       "      <td>0.840552</td>\n",
       "      <td>0.810447</td>\n",
       "      <td>0.832211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.640060</td>\n",
       "      <td>0.678913</td>\n",
       "      <td>0.659892</td>\n",
       "      <td>0.678599</td>\n",
       "      <td>0.711191</td>\n",
       "      <td>0.687640</td>\n",
       "      <td>0.687667</td>\n",
       "      <td>0.671421</td>\n",
       "      <td>0.685866</td>\n",
       "      <td>0.635405</td>\n",
       "      <td>0.673665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.810200</td>\n",
       "      <td>0.858500</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>0.831800</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.808800</td>\n",
       "      <td>0.823090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.815181</td>\n",
       "      <td>0.836539</td>\n",
       "      <td>0.826085</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.844538</td>\n",
       "      <td>0.839779</td>\n",
       "      <td>0.845894</td>\n",
       "      <td>0.833961</td>\n",
       "      <td>0.840552</td>\n",
       "      <td>0.810447</td>\n",
       "      <td>0.832211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  326.000000  318.000000  325.000000  328.000000   \n",
       "1                    TN  169.000000  186.000000  175.000000  176.000000   \n",
       "2                    FP   58.000000   51.000000   54.000000   62.000000   \n",
       "3                    FN   42.000000   40.000000   41.000000   29.000000   \n",
       "4              Accuracy    0.831933    0.847059    0.840336    0.847059   \n",
       "5             Precision    0.848958    0.861789    0.857520    0.841026   \n",
       "6           Sensitivity    0.885870    0.888268    0.887978    0.918768   \n",
       "7           Specificity    0.744500    0.784800    0.764200    0.739500   \n",
       "8              F1 score    0.867021    0.874828    0.872483    0.878179   \n",
       "9   F1 score (weighted)    0.830651    0.846399    0.839397    0.844741   \n",
       "10     F1 score (macro)    0.819355    0.839142    0.829500    0.836381   \n",
       "11    Balanced Accuracy    0.815181    0.836539    0.826085    0.829132   \n",
       "12                  MCC    0.640060    0.678913    0.659892    0.678599   \n",
       "13                  NPV    0.800900    0.823000    0.810200    0.858500   \n",
       "14              ROC_AUC    0.815181    0.836539    0.826085    0.829132   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   333.000000  324.000000  336.000000  324.000000  326.000000  329.000000   \n",
       "1   180.000000  183.000000  173.000000  179.000000  181.000000  165.000000   \n",
       "2    58.000000   51.000000   40.000000   49.000000   48.000000   62.000000   \n",
       "3    24.000000   37.000000   46.000000   43.000000   40.000000   39.000000   \n",
       "4     0.862185    0.852101    0.855462    0.845378    0.852101    0.830252   \n",
       "5     0.851662    0.864000    0.893617    0.868633    0.871658    0.841432   \n",
       "6     0.932773    0.897507    0.879581    0.882834    0.890710    0.894022   \n",
       "7     0.756300    0.782100    0.812200    0.785100    0.790400    0.726900   \n",
       "8     0.890374    0.880435    0.886544    0.875676    0.881081    0.866930   \n",
       "9     0.860016    0.851227    0.855894    0.844974    0.851586    0.828295   \n",
       "10    0.852427    0.843301    0.843735    0.835616    0.842763    0.816296   \n",
       "11    0.844538    0.839779    0.845894    0.833961    0.840552    0.810447   \n",
       "12    0.711191    0.687640    0.687667    0.671421    0.685866    0.635405   \n",
       "13    0.882400    0.831800    0.790000    0.806300    0.819000    0.808800   \n",
       "14    0.844538    0.839779    0.845894    0.833961    0.840552    0.810447   \n",
       "\n",
       "           ave  \n",
       "0   326.900000  \n",
       "1   176.700000  \n",
       "2    53.300000  \n",
       "3    38.100000  \n",
       "4     0.846387  \n",
       "5     0.860029  \n",
       "6     0.895831  \n",
       "7     0.768600  \n",
       "8     0.877355  \n",
       "9     0.845318  \n",
       "10    0.835852  \n",
       "11    0.832211  \n",
       "12    0.673665  \n",
       "13    0.823090  \n",
       "14    0.832211  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.852773</td>\n",
       "      <td>0.016934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.865944</td>\n",
       "      <td>0.021339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.902057</td>\n",
       "      <td>0.018833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.772810</td>\n",
       "      <td>0.040438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.883375</td>\n",
       "      <td>0.013261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.851603</td>\n",
       "      <td>0.017357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.841510</td>\n",
       "      <td>0.019134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.837432</td>\n",
       "      <td>0.020408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.685224</td>\n",
       "      <td>0.037852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.829908</td>\n",
       "      <td>0.029534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.837432</td>\n",
       "      <td>0.020408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.852773     0.016934\n",
       "1             Precision         0.865944     0.021339\n",
       "2           Sensitivity         0.902057     0.018833\n",
       "3           Specificity         0.772810     0.040438\n",
       "4              F1 score         0.883375     0.013261\n",
       "5   F1 score (weighted)         0.851603     0.017357\n",
       "6      F1 score (macro)         0.841510     0.019134\n",
       "7     Balanced Accuracy         0.837432     0.020408\n",
       "8                   MCC         0.685224     0.037852\n",
       "9                   NPV         0.829908     0.029534\n",
       "10              ROC_AUC         0.837432     0.020408"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=16,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"logloss\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_xgb)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_xgb))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_xgb))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_xgb))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_xgb))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_xgb, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_xgb, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_xgb))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_xgb))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_xgb))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:36:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost baseline model f1_score 0.8282 with a standard deviation of 0.0133\n",
      "XGBoost optimized model f1_score 0.8371 with a standard deviation of 0.0129\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_clf, X, Y, cv=10, scoring=\"f1_macro\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"f1_macro\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"f1_macro\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb_clf.joblib']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the modesls, both the one with optimized hyperparameters and the initial one\n",
    "joblib.dump(xgb_clf, \"OUTPUT/xgb_clf.joblib\")\n",
    "\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb_clf.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       164.700000     7.689098\n",
      "1                    TN        81.000000     4.988877\n",
      "2                    FP        32.400000     5.581716\n",
      "3                    FN        19.000000     3.711843\n",
      "4              Accuracy         0.827006     0.027216\n",
      "5             Precision         0.835611     0.028376\n",
      "6           Sensitivity         0.896195     0.022185\n",
      "7           Specificity         0.714830     0.044454\n",
      "8              F1 score         0.864720     0.023340\n",
      "9   F1 score (weighted)         0.824530     0.027710\n",
      "10     F1 score (macro)         0.812022     0.027953\n",
      "11    Balanced Accuracy         0.805510     0.028298\n",
      "12                  MCC         0.628369     0.054821\n",
      "13                  NPV         0.810690     0.029191\n",
      "14              ROC_AUC         0.805510     0.028298\n",
      "CPU times: user 3.15 s, sys: 6.81 s, total: 9.96 s\n",
      "Wall time: 321 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    \n",
    "    knn_clf.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_clf.predict(X_test) \n",
    "    \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(**param_grid, n_jobs=16)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average='macro')\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "  \n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(**param_grid, n_jobs=16)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:36:46,907] A new study created in memory with name: KNNClassifier\n",
      "[I 2023-12-04 17:36:47,160] Trial 0 finished with value: 0.8074526085582342 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 55}. Best is trial 0 with value: 0.8074526085582342.\n",
      "[I 2023-12-04 17:36:47,386] Trial 1 finished with value: 0.8020451121620823 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 61}. Best is trial 0 with value: 0.8074526085582342.\n",
      "[I 2023-12-04 17:36:47,881] Trial 2 finished with value: 0.7938822489511856 and parameters: {'n_neighbors': 21, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 0 with value: 0.8074526085582342.\n",
      "[I 2023-12-04 17:36:48,104] Trial 3 finished with value: 0.8111529914152275 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 3 with value: 0.8111529914152275.\n",
      "[I 2023-12-04 17:36:48,369] Trial 4 finished with value: 0.8108562183208493 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 34}. Best is trial 3 with value: 0.8111529914152275.\n",
      "[I 2023-12-04 17:36:48,790] Trial 5 finished with value: 0.824700223949003 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:49,238] Trial 6 finished with value: 0.8024776832214655 and parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:49,730] Trial 7 finished with value: 0.8041782394591616 and parameters: {'n_neighbors': 18, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:50,161] Trial 8 finished with value: 0.8236325244615985 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:50,385] Trial 9 finished with value: 0.8074526085582342 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:50,810] Trial 10 finished with value: 0.8189065841700864 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:51,247] Trial 11 finished with value: 0.8235570383266694 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:51,631] Trial 12 finished with value: 0.8131266346388756 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:52,107] Trial 13 finished with value: 0.8245037744453821 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:52,500] Trial 14 finished with value: 0.8226889943058799 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:52,731] Trial 15 finished with value: 0.818215785836321 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:53,173] Trial 16 finished with value: 0.8226889943058799 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:53,629] Trial 17 finished with value: 0.8236192666893638 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:53,892] Trial 18 finished with value: 0.8127735890878764 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:54,121] Trial 19 finished with value: 0.8055096753196181 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:54,576] Trial 20 finished with value: 0.8200556020804635 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:55,048] Trial 21 finished with value: 0.8202255422411489 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:55,524] Trial 22 finished with value: 0.8217752724894616 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:55,983] Trial 23 finished with value: 0.8131266346388756 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:56,436] Trial 24 finished with value: 0.8245037744453821 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:56,920] Trial 25 finished with value: 0.8245037744453821 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:57,449] Trial 26 finished with value: 0.8130053089353257 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:57,681] Trial 27 finished with value: 0.8186569638806892 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 71}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:57,907] Trial 28 finished with value: 0.820443210548555 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 82}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:58,328] Trial 29 finished with value: 0.8181865348043817 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:58,559] Trial 30 finished with value: 0.8176742802291923 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:59,040] Trial 31 finished with value: 0.8226889943058799 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:59,521] Trial 32 finished with value: 0.8245037744453821 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:36:59,982] Trial 33 finished with value: 0.8236283783848025 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:00,421] Trial 34 finished with value: 0.8191946134525386 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:00,647] Trial 35 finished with value: 0.8176742802291923 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 64}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:01,126] Trial 36 finished with value: 0.8130053089353257 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:01,574] Trial 37 finished with value: 0.8206929381042014 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:02,141] Trial 38 finished with value: 0.8050317537698944 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:02,379] Trial 39 finished with value: 0.802595304946346 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:02,833] Trial 40 finished with value: 0.7831186561998509 and parameters: {'n_neighbors': 27, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:03,273] Trial 41 finished with value: 0.8245037744453821 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:03,749] Trial 42 finished with value: 0.8236283783848025 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:04,240] Trial 43 finished with value: 0.8235570383266694 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:04,735] Trial 44 finished with value: 0.8245037744453821 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:05,189] Trial 45 finished with value: 0.8222036233758507 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:05,423] Trial 46 finished with value: 0.8142532360211474 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:05,870] Trial 47 finished with value: 0.8235570383266694 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:06,315] Trial 48 finished with value: 0.8244536269944588 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:06,712] Trial 49 finished with value: 0.8236283783848025 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 5 with value: 0.824700223949003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8247\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 63\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNClassifier\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  323.000000\n",
      "1                    TN  158.000000\n",
      "2                    FP   69.000000\n",
      "3                    FN   45.000000\n",
      "4              Accuracy    0.808403\n",
      "5             Precision    0.823980\n",
      "6           Sensitivity    0.877717\n",
      "7           Specificity    0.696000\n",
      "8              F1 score    0.850000\n",
      "9   F1 score (weighted)    0.806082\n",
      "10     F1 score (macro)    0.792442\n",
      "11    Balanced Accuracy    0.786876\n",
      "12                  MCC    0.587855\n",
      "13                  NPV    0.778300\n",
      "14              ROC_AUC    0.786876\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_knn_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_knn_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_knn_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_knn_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_knn_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_knn_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_knn_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_knn_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_knn_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_knn_0)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:37:07,252] Trial 50 finished with value: 0.8181343826385923 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:07,731] Trial 51 finished with value: 0.8219105259699331 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:08,239] Trial 52 finished with value: 0.8170770084979084 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:08,743] Trial 53 finished with value: 0.8177501450032008 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:09,210] Trial 54 finished with value: 0.8184074194310451 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:09,663] Trial 55 finished with value: 0.8217564819211696 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:09,939] Trial 56 finished with value: 0.7952478026666367 and parameters: {'n_neighbors': 18, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:10,173] Trial 57 finished with value: 0.8127001398754115 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:10,564] Trial 58 finished with value: 0.8144429144224421 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:10,973] Trial 59 finished with value: 0.8126912766898731 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:11,413] Trial 60 finished with value: 0.8161460130183625 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:11,873] Trial 61 finished with value: 0.8219105259699331 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:12,342] Trial 62 finished with value: 0.8177501450032008 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:12,762] Trial 63 finished with value: 0.8162209203425341 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:13,186] Trial 64 finished with value: 0.8184074194310451 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:13,641] Trial 65 finished with value: 0.8180694874629539 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:14,105] Trial 66 finished with value: 0.8181343826385923 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:14,342] Trial 67 finished with value: 0.8095459825737036 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:14,852] Trial 68 finished with value: 0.8026787790067045 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:15,330] Trial 69 finished with value: 0.8179687555942028 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:15,570] Trial 70 finished with value: 0.8134741485741361 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:16,029] Trial 71 finished with value: 0.8130895715663262 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:16,479] Trial 72 finished with value: 0.8115560087000622 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:16,900] Trial 73 finished with value: 0.8177501450032008 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:17,367] Trial 74 finished with value: 0.8153683315564327 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:17,839] Trial 75 finished with value: 0.8144429144224421 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:18,286] Trial 76 finished with value: 0.8161460130183625 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:18,743] Trial 77 finished with value: 0.8184074194310451 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:19,253] Trial 78 finished with value: 0.8034753817389053 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:19,491] Trial 79 finished with value: 0.8105353031782355 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:19,895] Trial 80 finished with value: 0.8170770084979084 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:20,366] Trial 81 finished with value: 0.8186879696030575 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:20,837] Trial 82 finished with value: 0.8174533677649485 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:21,309] Trial 83 finished with value: 0.8179687555942028 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:21,782] Trial 84 finished with value: 0.8219105259699331 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:22,025] Trial 85 finished with value: 0.8061764717402349 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:22,414] Trial 86 finished with value: 0.8180694874629539 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:22,864] Trial 87 finished with value: 0.8217564819211696 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:23,337] Trial 88 finished with value: 0.8184074194310451 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:23,829] Trial 89 finished with value: 0.8123502917139962 and parameters: {'n_neighbors': 14, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:24,248] Trial 90 finished with value: 0.8186879696030575 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:24,711] Trial 91 finished with value: 0.8174533677649485 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:25,195] Trial 92 finished with value: 0.8179687555942028 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:25,692] Trial 93 finished with value: 0.8170770084979084 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:26,143] Trial 94 finished with value: 0.8219105259699331 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:26,637] Trial 95 finished with value: 0.8144429144224421 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:26,877] Trial 96 finished with value: 0.8090621587610769 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:27,221] Trial 97 finished with value: 0.8154899191279259 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:27,644] Trial 98 finished with value: 0.8180694874629539 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:27,876] Trial 99 finished with value: 0.8055203867937131 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 79}. Best is trial 5 with value: 0.824700223949003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8247\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 63\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  323.000000  310.000000\n",
      "1                    TN  158.000000  175.000000\n",
      "2                    FP   69.000000   62.000000\n",
      "3                    FN   45.000000   48.000000\n",
      "4              Accuracy    0.808403    0.815126\n",
      "5             Precision    0.823980    0.833333\n",
      "6           Sensitivity    0.877717    0.865922\n",
      "7           Specificity    0.696000    0.738400\n",
      "8              F1 score    0.850000    0.849315\n",
      "9   F1 score (weighted)    0.806082    0.814086\n",
      "10     F1 score (macro)    0.792442    0.805092\n",
      "11    Balanced Accuracy    0.786876    0.802159\n",
      "12                  MCC    0.587855    0.611164\n",
      "13                  NPV    0.778300    0.784800\n",
      "14              ROC_AUC    0.786876    0.802159\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_knn_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_knn_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_knn_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_knn_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_knn_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_knn_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_knn_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_knn_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_knn_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_knn_1)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:37:28,455] Trial 100 finished with value: 0.8140281449610083 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:28,930] Trial 101 finished with value: 0.8124414606136501 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:29,432] Trial 102 finished with value: 0.8124414606136501 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:29,947] Trial 103 finished with value: 0.8125690993496825 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:30,428] Trial 104 finished with value: 0.8180675934282856 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:30,869] Trial 105 finished with value: 0.8124809406165667 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:31,298] Trial 106 finished with value: 0.8151093397929046 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:31,749] Trial 107 finished with value: 0.8162699709981085 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:32,224] Trial 108 finished with value: 0.7920830262138513 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:32,647] Trial 109 finished with value: 0.8177172549116722 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:33,022] Trial 110 finished with value: 0.8108675285132891 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:33,462] Trial 111 finished with value: 0.8124414606136501 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:33,916] Trial 112 finished with value: 0.8122104761582696 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:34,385] Trial 113 finished with value: 0.820285673814848 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:34,806] Trial 114 finished with value: 0.8125690993496825 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:35,291] Trial 115 finished with value: 0.821650269438264 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:35,532] Trial 116 finished with value: 0.7999593666581358 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:35,988] Trial 117 finished with value: 0.8124414606136501 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:36,464] Trial 118 finished with value: 0.8124809406165667 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:36,699] Trial 119 finished with value: 0.8032938767404534 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:37,173] Trial 120 finished with value: 0.8157644797267045 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:37,627] Trial 121 finished with value: 0.821650269438264 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:38,026] Trial 122 finished with value: 0.8180675934282856 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:38,473] Trial 123 finished with value: 0.8162699709981085 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:38,931] Trial 124 finished with value: 0.8124809406165667 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:39,416] Trial 125 finished with value: 0.796002016049288 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:39,833] Trial 126 finished with value: 0.821650269438264 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:40,184] Trial 127 finished with value: 0.8142889816079822 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:40,574] Trial 128 finished with value: 0.820285673814848 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:41,014] Trial 129 finished with value: 0.8162699709981085 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:41,405] Trial 130 finished with value: 0.8125690993496825 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:41,784] Trial 131 finished with value: 0.8157644797267045 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:42,104] Trial 132 finished with value: 0.8124809406165667 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:42,475] Trial 133 finished with value: 0.821650269438264 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:42,926] Trial 134 finished with value: 0.8180675934282856 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:43,164] Trial 135 finished with value: 0.8015941170307258 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:43,612] Trial 136 finished with value: 0.821650269438264 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:44,064] Trial 137 finished with value: 0.8144768772037082 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:44,522] Trial 138 finished with value: 0.8124414606136501 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:44,751] Trial 139 finished with value: 0.8014450032658018 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:45,240] Trial 140 finished with value: 0.7955282987718164 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:45,685] Trial 141 finished with value: 0.8157644797267045 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:46,115] Trial 142 finished with value: 0.8108675285132891 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:46,593] Trial 143 finished with value: 0.820285673814848 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:47,011] Trial 144 finished with value: 0.8129041701824737 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:47,454] Trial 145 finished with value: 0.8162699709981085 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:47,869] Trial 146 finished with value: 0.8157644797267045 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:48,286] Trial 147 finished with value: 0.8124809406165667 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:48,729] Trial 148 finished with value: 0.8180675934282856 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:49,167] Trial 149 finished with value: 0.821650269438264 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8247\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 63\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  323.000000  310.000000  326.000000\n",
      "1                    TN  158.000000  175.000000  162.000000\n",
      "2                    FP   69.000000   62.000000   67.000000\n",
      "3                    FN   45.000000   48.000000   40.000000\n",
      "4              Accuracy    0.808403    0.815126    0.820168\n",
      "5             Precision    0.823980    0.833333    0.829517\n",
      "6           Sensitivity    0.877717    0.865922    0.890710\n",
      "7           Specificity    0.696000    0.738400    0.707400\n",
      "8              F1 score    0.850000    0.849315    0.859025\n",
      "9   F1 score (weighted)    0.806082    0.814086    0.817734\n",
      "10     F1 score (macro)    0.792442    0.805092    0.805383\n",
      "11    Balanced Accuracy    0.786876    0.802159    0.799067\n",
      "12                  MCC    0.587855    0.611164    0.614589\n",
      "13                  NPV    0.778300    0.784800    0.802000\n",
      "14              ROC_AUC    0.786876    0.802159    0.799067\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "#predict\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_knn_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_knn_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_knn_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_knn_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_knn_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_knn_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_knn_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_knn_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_knn_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_knn_2)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:37:49,729] Trial 150 finished with value: 0.8051671005428119 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:50,179] Trial 151 finished with value: 0.8147365034742162 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:50,596] Trial 152 finished with value: 0.806121997778978 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:51,033] Trial 153 finished with value: 0.8054845984650513 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:51,455] Trial 154 finished with value: 0.8069680807910936 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:51,854] Trial 155 finished with value: 0.8077268010043437 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:52,229] Trial 156 finished with value: 0.8063964417233429 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:52,696] Trial 157 finished with value: 0.807025158605225 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:52,922] Trial 158 finished with value: 0.8062015307528709 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 57}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:53,350] Trial 159 finished with value: 0.8069680807910936 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:53,744] Trial 160 finished with value: 0.806121997778978 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:54,190] Trial 161 finished with value: 0.8054845984650513 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:54,561] Trial 162 finished with value: 0.8054845984650513 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:54,950] Trial 163 finished with value: 0.8069680807910936 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:55,380] Trial 164 finished with value: 0.8075943748239152 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:55,845] Trial 165 finished with value: 0.8063964417233429 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:56,117] Trial 166 finished with value: 0.7915900133003941 and parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:56,554] Trial 167 finished with value: 0.806121997778978 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:56,937] Trial 168 finished with value: 0.8054845984650513 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:57,320] Trial 169 finished with value: 0.8050665813278263 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:57,725] Trial 170 finished with value: 0.8054962094561755 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:58,164] Trial 171 finished with value: 0.8069680807910936 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:58,607] Trial 172 finished with value: 0.8054845984650513 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:59,050] Trial 173 finished with value: 0.8069680807910936 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:59,512] Trial 174 finished with value: 0.806121997778978 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:37:59,971] Trial 175 finished with value: 0.8063964417233429 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:00,436] Trial 176 finished with value: 0.807025158605225 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:00,843] Trial 177 finished with value: 0.8054845984650513 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:01,250] Trial 178 finished with value: 0.8081179264402938 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:01,636] Trial 179 finished with value: 0.806121997778978 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:02,091] Trial 180 finished with value: 0.807025158605225 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:02,561] Trial 181 finished with value: 0.8069680807910936 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:02,984] Trial 182 finished with value: 0.8054845984650513 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:03,428] Trial 183 finished with value: 0.8077268010043437 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:03,816] Trial 184 finished with value: 0.8069680807910936 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:04,042] Trial 185 finished with value: 0.8049880574603672 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:04,429] Trial 186 finished with value: 0.806121997778978 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:04,770] Trial 187 finished with value: 0.7921851465651727 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:05,159] Trial 188 finished with value: 0.8054845984650513 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:05,544] Trial 189 finished with value: 0.8098215034288743 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:05,771] Trial 190 finished with value: 0.8023537915699395 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:06,189] Trial 191 finished with value: 0.8094481789921103 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:06,633] Trial 192 finished with value: 0.8094481789921103 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:07,081] Trial 193 finished with value: 0.8081179264402938 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:07,434] Trial 194 finished with value: 0.8050665813278263 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:07,824] Trial 195 finished with value: 0.8069680807910936 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:08,238] Trial 196 finished with value: 0.807025158605225 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:08,672] Trial 197 finished with value: 0.8133513564871814 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:09,111] Trial 198 finished with value: 0.8075586974569615 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:09,551] Trial 199 finished with value: 0.8081179264402938 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 5 with value: 0.824700223949003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8247\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 63\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  323.000000  310.000000  326.000000  323.000000\n",
      "1                    TN  158.000000  175.000000  162.000000  173.000000\n",
      "2                    FP   69.000000   62.000000   67.000000   65.000000\n",
      "3                    FN   45.000000   48.000000   40.000000   34.000000\n",
      "4              Accuracy    0.808403    0.815126    0.820168    0.833613\n",
      "5             Precision    0.823980    0.833333    0.829517    0.832474\n",
      "6           Sensitivity    0.877717    0.865922    0.890710    0.904762\n",
      "7           Specificity    0.696000    0.738400    0.707400    0.726900\n",
      "8              F1 score    0.850000    0.849315    0.859025    0.867114\n",
      "9   F1 score (weighted)    0.806082    0.814086    0.817734    0.831280\n",
      "10     F1 score (macro)    0.792442    0.805092    0.805383    0.822321\n",
      "11    Balanced Accuracy    0.786876    0.802159    0.799067    0.815826\n",
      "12                  MCC    0.587855    0.611164    0.614589    0.649681\n",
      "13                  NPV    0.778300    0.784800    0.802000    0.835700\n",
      "14              ROC_AUC    0.786876    0.802159    0.799067    0.815826\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_knn_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_knn_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_knn_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_knn_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_knn_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_knn_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_knn_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_knn_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_knn_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_knn_3)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:38:10,005] Trial 200 finished with value: 0.8072180832778407 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:10,440] Trial 201 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:10,873] Trial 202 finished with value: 0.8080794798840479 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:11,280] Trial 203 finished with value: 0.8078766381544943 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:11,728] Trial 204 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:12,082] Trial 205 finished with value: 0.8080794798840479 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:12,439] Trial 206 finished with value: 0.7954802731033717 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:12,833] Trial 207 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:13,266] Trial 208 finished with value: 0.8078766381544943 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:13,707] Trial 209 finished with value: 0.7966736592362306 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:14,177] Trial 210 finished with value: 0.8103176354303889 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:14,595] Trial 211 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:15,036] Trial 212 finished with value: 0.8080794798840479 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:15,442] Trial 213 finished with value: 0.8035540865961848 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:15,875] Trial 214 finished with value: 0.7977242738828705 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:16,323] Trial 215 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:16,540] Trial 216 finished with value: 0.801124976183495 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:16,980] Trial 217 finished with value: 0.7740001745511474 and parameters: {'n_neighbors': 21, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:17,393] Trial 218 finished with value: 0.8080794798840479 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:17,828] Trial 219 finished with value: 0.8080794798840479 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:18,278] Trial 220 finished with value: 0.8072180832778407 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:18,714] Trial 221 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:19,124] Trial 222 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:19,570] Trial 223 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:20,036] Trial 224 finished with value: 0.8008694514764005 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:20,483] Trial 225 finished with value: 0.8078766381544943 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:20,709] Trial 226 finished with value: 0.7986042501045956 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:21,181] Trial 227 finished with value: 0.8080794798840479 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:21,581] Trial 228 finished with value: 0.8080794798840479 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:22,040] Trial 229 finished with value: 0.8062652745872005 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:22,479] Trial 230 finished with value: 0.8072180832778407 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:22,886] Trial 231 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:23,312] Trial 232 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:23,726] Trial 233 finished with value: 0.8078766381544943 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:24,099] Trial 234 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:24,411] Trial 235 finished with value: 0.8080794798840479 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:24,784] Trial 236 finished with value: 0.8078766381544943 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:25,198] Trial 237 finished with value: 0.8080794798840479 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:25,658] Trial 238 finished with value: 0.8008694514764005 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:26,105] Trial 239 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:26,556] Trial 240 finished with value: 0.8036124341623726 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:26,957] Trial 241 finished with value: 0.8036524341580655 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:27,389] Trial 242 finished with value: 0.8036524341580655 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:27,793] Trial 243 finished with value: 0.8008694514764005 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:28,167] Trial 244 finished with value: 0.8036524341580655 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:28,557] Trial 245 finished with value: 0.8008694514764005 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:28,974] Trial 246 finished with value: 0.8035540865961848 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:29,402] Trial 247 finished with value: 0.8096096372488428 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:29,887] Trial 248 finished with value: 0.7977805856326451 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:30,274] Trial 249 finished with value: 0.8072180832778407 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 5 with value: 0.824700223949003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8247\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 63\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  323.000000  310.000000  326.000000  323.000000   \n",
      "1                    TN  158.000000  175.000000  162.000000  173.000000   \n",
      "2                    FP   69.000000   62.000000   67.000000   65.000000   \n",
      "3                    FN   45.000000   48.000000   40.000000   34.000000   \n",
      "4              Accuracy    0.808403    0.815126    0.820168    0.833613   \n",
      "5             Precision    0.823980    0.833333    0.829517    0.832474   \n",
      "6           Sensitivity    0.877717    0.865922    0.890710    0.904762   \n",
      "7           Specificity    0.696000    0.738400    0.707400    0.726900   \n",
      "8              F1 score    0.850000    0.849315    0.859025    0.867114   \n",
      "9   F1 score (weighted)    0.806082    0.814086    0.817734    0.831280   \n",
      "10     F1 score (macro)    0.792442    0.805092    0.805383    0.822321   \n",
      "11    Balanced Accuracy    0.786876    0.802159    0.799067    0.815826   \n",
      "12                  MCC    0.587855    0.611164    0.614589    0.649681   \n",
      "13                  NPV    0.778300    0.784800    0.802000    0.835700   \n",
      "14              ROC_AUC    0.786876    0.802159    0.799067    0.815826   \n",
      "\n",
      "          Set4  \n",
      "0   332.000000  \n",
      "1   165.000000  \n",
      "2    73.000000  \n",
      "3    25.000000  \n",
      "4     0.835294  \n",
      "5     0.819753  \n",
      "6     0.929972  \n",
      "7     0.693300  \n",
      "8     0.871391  \n",
      "9     0.831246  \n",
      "10    0.821210  \n",
      "11    0.811625  \n",
      "12    0.654908  \n",
      "13    0.868400  \n",
      "14    0.811625  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_knn_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_knn_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_knn_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_knn_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_knn_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_knn_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_knn_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_knn_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_knn_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_knn_4)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:38:30,634] Trial 250 finished with value: 0.7968722199969355 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:30,991] Trial 251 finished with value: 0.8034728180652172 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:31,383] Trial 252 finished with value: 0.7988714312042663 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:31,816] Trial 253 finished with value: 0.8034728180652172 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:32,282] Trial 254 finished with value: 0.8011629205763194 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:32,522] Trial 255 finished with value: 0.798561110687989 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:32,944] Trial 256 finished with value: 0.8011629205763194 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:33,330] Trial 257 finished with value: 0.8039520897741144 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:33,726] Trial 258 finished with value: 0.7993315919313653 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:34,110] Trial 259 finished with value: 0.8064919431024702 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:34,484] Trial 260 finished with value: 0.7988714312042663 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:34,930] Trial 261 finished with value: 0.8052683891710952 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:35,356] Trial 262 finished with value: 0.8028135712214344 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:35,778] Trial 263 finished with value: 0.8034728180652172 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:36,216] Trial 264 finished with value: 0.8011629205763194 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:36,603] Trial 265 finished with value: 0.7999341073308738 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:37,059] Trial 266 finished with value: 0.7988714312042663 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:37,535] Trial 267 finished with value: 0.787663824399697 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:37,900] Trial 268 finished with value: 0.8041473538733099 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:38,280] Trial 269 finished with value: 0.7988714312042663 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:38,685] Trial 270 finished with value: 0.7993315919313653 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:39,080] Trial 271 finished with value: 0.7972352678356737 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:39,308] Trial 272 finished with value: 0.7955698542594597 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:39,721] Trial 273 finished with value: 0.8011629205763194 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:40,091] Trial 274 finished with value: 0.8034728180652172 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:40,494] Trial 275 finished with value: 0.7988714312042663 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:40,861] Trial 276 finished with value: 0.8011629205763194 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:41,086] Trial 277 finished with value: 0.7975505936817642 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 78}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:41,492] Trial 278 finished with value: 0.8064919431024702 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:41,889] Trial 279 finished with value: 0.8028135712214344 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:42,364] Trial 280 finished with value: 0.8011629205763194 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:42,779] Trial 281 finished with value: 0.8040360269786472 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:43,218] Trial 282 finished with value: 0.8034728180652172 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:43,681] Trial 283 finished with value: 0.7993315919313653 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:44,119] Trial 284 finished with value: 0.7975846994555286 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:44,556] Trial 285 finished with value: 0.7988714312042663 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:45,035] Trial 286 finished with value: 0.787663824399697 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:45,492] Trial 287 finished with value: 0.794324824046596 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:45,962] Trial 288 finished with value: 0.8064919431024702 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:46,415] Trial 289 finished with value: 0.8052683891710952 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:46,888] Trial 290 finished with value: 0.804478713507509 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:47,328] Trial 291 finished with value: 0.7993315919313653 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:47,783] Trial 292 finished with value: 0.8034728180652172 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:48,230] Trial 293 finished with value: 0.8006384069203445 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:48,719] Trial 294 finished with value: 0.7988714312042663 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:48,954] Trial 295 finished with value: 0.79553694156596 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:49,357] Trial 296 finished with value: 0.8041473538733099 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:49,777] Trial 297 finished with value: 0.7988714312042663 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:50,154] Trial 298 finished with value: 0.8011629205763194 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:50,480] Trial 299 finished with value: 0.8017650165223017 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 5 with value: 0.824700223949003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8247\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 63\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  323.000000  310.000000  326.000000  323.000000   \n",
      "1                    TN  158.000000  175.000000  162.000000  173.000000   \n",
      "2                    FP   69.000000   62.000000   67.000000   65.000000   \n",
      "3                    FN   45.000000   48.000000   40.000000   34.000000   \n",
      "4              Accuracy    0.808403    0.815126    0.820168    0.833613   \n",
      "5             Precision    0.823980    0.833333    0.829517    0.832474   \n",
      "6           Sensitivity    0.877717    0.865922    0.890710    0.904762   \n",
      "7           Specificity    0.696000    0.738400    0.707400    0.726900   \n",
      "8              F1 score    0.850000    0.849315    0.859025    0.867114   \n",
      "9   F1 score (weighted)    0.806082    0.814086    0.817734    0.831280   \n",
      "10     F1 score (macro)    0.792442    0.805092    0.805383    0.822321   \n",
      "11    Balanced Accuracy    0.786876    0.802159    0.799067    0.815826   \n",
      "12                  MCC    0.587855    0.611164    0.614589    0.649681   \n",
      "13                  NPV    0.778300    0.784800    0.802000    0.835700   \n",
      "14              ROC_AUC    0.786876    0.802159    0.799067    0.815826   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   332.000000  321.000000  \n",
      "1   165.000000  172.000000  \n",
      "2    73.000000   62.000000  \n",
      "3    25.000000   40.000000  \n",
      "4     0.835294    0.828571  \n",
      "5     0.819753    0.838120  \n",
      "6     0.929972    0.889197  \n",
      "7     0.693300    0.735000  \n",
      "8     0.871391    0.862903  \n",
      "9     0.831246    0.826878  \n",
      "10    0.821210    0.817102  \n",
      "11    0.811625    0.812120  \n",
      "12    0.654908    0.636715  \n",
      "13    0.868400    0.811300  \n",
      "14    0.811625    0.812120  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_knn_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_knn_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_knn_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_knn_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_knn_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_knn_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_knn_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_knn_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_knn_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_knn_5)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:38:50,838] Trial 300 finished with value: 0.8096954847671908 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:51,222] Trial 301 finished with value: 0.8148727995507308 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:51,696] Trial 302 finished with value: 0.8156825648453369 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:52,145] Trial 303 finished with value: 0.8124071567804549 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:52,559] Trial 304 finished with value: 0.8111317036357664 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:52,995] Trial 305 finished with value: 0.8126447985134811 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:53,433] Trial 306 finished with value: 0.7926471719900269 and parameters: {'n_neighbors': 18, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:53,909] Trial 307 finished with value: 0.8139164343987592 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:54,344] Trial 308 finished with value: 0.8087608688668588 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:54,794] Trial 309 finished with value: 0.8111317036357664 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:55,280] Trial 310 finished with value: 0.8030297329309617 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:55,708] Trial 311 finished with value: 0.8127642086441117 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:56,051] Trial 312 finished with value: 0.8126447985134811 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:56,472] Trial 313 finished with value: 0.8126447985134811 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:56,906] Trial 314 finished with value: 0.8148727995507308 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:57,269] Trial 315 finished with value: 0.8060899054655103 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:57,503] Trial 316 finished with value: 0.8096954847671908 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:57,958] Trial 317 finished with value: 0.8156825648453369 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:58,408] Trial 318 finished with value: 0.8139164343987592 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:58,972] Trial 319 finished with value: 0.8122256625087024 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:59,425] Trial 320 finished with value: 0.8111317036357664 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:38:59,843] Trial 321 finished with value: 0.8082981440427298 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:00,083] Trial 322 finished with value: 0.8088025767125242 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:00,469] Trial 323 finished with value: 0.8111317036357664 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:00,887] Trial 324 finished with value: 0.8126447985134811 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:01,356] Trial 325 finished with value: 0.8035346992681436 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:01,722] Trial 326 finished with value: 0.8105264541631276 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:02,098] Trial 327 finished with value: 0.8062097259638724 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:02,544] Trial 328 finished with value: 0.8020656107993231 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:02,956] Trial 329 finished with value: 0.8126447985134811 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:03,374] Trial 330 finished with value: 0.8127642086441117 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:03,841] Trial 331 finished with value: 0.811582422759366 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:04,197] Trial 332 finished with value: 0.8111317036357664 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:04,617] Trial 333 finished with value: 0.8139164343987592 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:04,962] Trial 334 finished with value: 0.8126447985134811 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:05,396] Trial 335 finished with value: 0.8127642086441117 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:05,826] Trial 336 finished with value: 0.8156825648453369 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:06,269] Trial 337 finished with value: 0.8111317036357664 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:06,703] Trial 338 finished with value: 0.8126447985134811 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:06,945] Trial 339 finished with value: 0.8051766379568246 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:07,422] Trial 340 finished with value: 0.8111317036357664 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:07,848] Trial 341 finished with value: 0.8139164343987592 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:08,276] Trial 342 finished with value: 0.8127642086441117 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:08,688] Trial 343 finished with value: 0.8126447985134811 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:08,963] Trial 344 finished with value: 0.7857241688536238 and parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:09,373] Trial 345 finished with value: 0.8111317036357664 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:09,752] Trial 346 finished with value: 0.8126447985134811 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:10,195] Trial 347 finished with value: 0.8156825648453369 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:10,656] Trial 348 finished with value: 0.8148727995507308 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:11,109] Trial 349 finished with value: 0.8124071567804549 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 5 with value: 0.824700223949003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8247\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 63\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  323.000000  310.000000  326.000000  323.000000   \n",
      "1                    TN  158.000000  175.000000  162.000000  173.000000   \n",
      "2                    FP   69.000000   62.000000   67.000000   65.000000   \n",
      "3                    FN   45.000000   48.000000   40.000000   34.000000   \n",
      "4              Accuracy    0.808403    0.815126    0.820168    0.833613   \n",
      "5             Precision    0.823980    0.833333    0.829517    0.832474   \n",
      "6           Sensitivity    0.877717    0.865922    0.890710    0.904762   \n",
      "7           Specificity    0.696000    0.738400    0.707400    0.726900   \n",
      "8              F1 score    0.850000    0.849315    0.859025    0.867114   \n",
      "9   F1 score (weighted)    0.806082    0.814086    0.817734    0.831280   \n",
      "10     F1 score (macro)    0.792442    0.805092    0.805383    0.822321   \n",
      "11    Balanced Accuracy    0.786876    0.802159    0.799067    0.815826   \n",
      "12                  MCC    0.587855    0.611164    0.614589    0.649681   \n",
      "13                  NPV    0.778300    0.784800    0.802000    0.835700   \n",
      "14              ROC_AUC    0.786876    0.802159    0.799067    0.815826   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   332.000000  321.000000  330.000000  \n",
      "1   165.000000  172.000000  164.000000  \n",
      "2    73.000000   62.000000   49.000000  \n",
      "3    25.000000   40.000000   52.000000  \n",
      "4     0.835294    0.828571    0.830252  \n",
      "5     0.819753    0.838120    0.870712  \n",
      "6     0.929972    0.889197    0.863874  \n",
      "7     0.693300    0.735000    0.770000  \n",
      "8     0.871391    0.862903    0.867280  \n",
      "9     0.831246    0.826878    0.830511  \n",
      "10    0.821210    0.817102    0.815924  \n",
      "11    0.811625    0.812120    0.816914  \n",
      "12    0.654908    0.636715    0.631897  \n",
      "13    0.868400    0.811300    0.759300  \n",
      "14    0.811625    0.812120    0.816914  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_knn_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_knn_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_knn_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_knn_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_knn_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_knn_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_knn_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_knn_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_knn_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_knn_6)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:39:11,652] Trial 350 finished with value: 0.8186158707694267 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:12,099] Trial 351 finished with value: 0.8132764641223688 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:12,554] Trial 352 finished with value: 0.8146728228104338 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:12,966] Trial 353 finished with value: 0.8196979115661206 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:13,405] Trial 354 finished with value: 0.8215429886126419 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:13,871] Trial 355 finished with value: 0.8128792692526317 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:14,260] Trial 356 finished with value: 0.8195618170446217 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:14,725] Trial 357 finished with value: 0.8146728228104338 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:15,128] Trial 358 finished with value: 0.8186158707694267 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:15,511] Trial 359 finished with value: 0.8142554660985809 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:15,891] Trial 360 finished with value: 0.8146728228104338 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:16,331] Trial 361 finished with value: 0.8059029540648652 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:16,743] Trial 362 finished with value: 0.8215429886126419 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:17,018] Trial 363 finished with value: 0.8022311783810396 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:17,442] Trial 364 finished with value: 0.8161448972117988 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:17,881] Trial 365 finished with value: 0.8082517848451036 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:18,125] Trial 366 finished with value: 0.8127621122470282 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:18,538] Trial 367 finished with value: 0.8146728228104338 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:18,992] Trial 368 finished with value: 0.8186158707694267 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:19,440] Trial 369 finished with value: 0.8209374753981045 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:19,829] Trial 370 finished with value: 0.8186158707694267 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:20,270] Trial 371 finished with value: 0.8215429886126419 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:20,707] Trial 372 finished with value: 0.816267688193389 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:21,151] Trial 373 finished with value: 0.8222598946217017 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:21,569] Trial 374 finished with value: 0.8196979115661206 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:22,019] Trial 375 finished with value: 0.8222598946217017 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 5 with value: 0.824700223949003.\n",
      "[I 2023-12-04 17:39:22,473] Trial 376 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:22,902] Trial 377 finished with value: 0.8222598946217017 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:23,383] Trial 378 finished with value: 0.8222598946217017 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:23,820] Trial 379 finished with value: 0.8222598946217017 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:24,224] Trial 380 finished with value: 0.8222598946217017 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:24,652] Trial 381 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:25,065] Trial 382 finished with value: 0.8222598946217017 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:25,483] Trial 383 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:25,872] Trial 384 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:26,323] Trial 385 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:26,743] Trial 386 finished with value: 0.8209374753981045 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:27,192] Trial 387 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:27,473] Trial 388 finished with value: 0.8022311783810396 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:27,863] Trial 389 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:28,283] Trial 390 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:28,682] Trial 391 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:29,115] Trial 392 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:29,597] Trial 393 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:30,048] Trial 394 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:30,519] Trial 395 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:30,975] Trial 396 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:31,379] Trial 397 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:31,821] Trial 398 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:32,248] Trial 399 finished with value: 0.8292867381176114 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8293\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 10\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 99\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  323.000000  310.000000  326.000000  323.000000   \n",
      "1                    TN  158.000000  175.000000  162.000000  173.000000   \n",
      "2                    FP   69.000000   62.000000   67.000000   65.000000   \n",
      "3                    FN   45.000000   48.000000   40.000000   34.000000   \n",
      "4              Accuracy    0.808403    0.815126    0.820168    0.833613   \n",
      "5             Precision    0.823980    0.833333    0.829517    0.832474   \n",
      "6           Sensitivity    0.877717    0.865922    0.890710    0.904762   \n",
      "7           Specificity    0.696000    0.738400    0.707400    0.726900   \n",
      "8              F1 score    0.850000    0.849315    0.859025    0.867114   \n",
      "9   F1 score (weighted)    0.806082    0.814086    0.817734    0.831280   \n",
      "10     F1 score (macro)    0.792442    0.805092    0.805383    0.822321   \n",
      "11    Balanced Accuracy    0.786876    0.802159    0.799067    0.815826   \n",
      "12                  MCC    0.587855    0.611164    0.614589    0.649681   \n",
      "13                  NPV    0.778300    0.784800    0.802000    0.835700   \n",
      "14              ROC_AUC    0.786876    0.802159    0.799067    0.815826   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   332.000000  321.000000  330.000000  325.000000  \n",
      "1   165.000000  172.000000  164.000000  163.000000  \n",
      "2    73.000000   62.000000   49.000000   65.000000  \n",
      "3    25.000000   40.000000   52.000000   42.000000  \n",
      "4     0.835294    0.828571    0.830252    0.820168  \n",
      "5     0.819753    0.838120    0.870712    0.833333  \n",
      "6     0.929972    0.889197    0.863874    0.885559  \n",
      "7     0.693300    0.735000    0.770000    0.714900  \n",
      "8     0.871391    0.862903    0.867280    0.858653  \n",
      "9     0.831246    0.826878    0.830511    0.818124  \n",
      "10    0.821210    0.817102    0.815924    0.805770  \n",
      "11    0.811625    0.812120    0.816914    0.800235  \n",
      "12    0.654908    0.636715    0.631897    0.614304  \n",
      "13    0.868400    0.811300    0.759300    0.795100  \n",
      "14    0.811625    0.812120    0.816914    0.800235  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_knn_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_knn_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_knn_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_knn_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_knn_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_knn_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_knn_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_knn_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_knn_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_knn_7)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:39:32,785] Trial 400 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:33,226] Trial 401 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:33,650] Trial 402 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:34,080] Trial 403 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:34,494] Trial 404 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:34,901] Trial 405 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:35,135] Trial 406 finished with value: 0.8029410186787336 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:35,496] Trial 407 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:35,899] Trial 408 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:36,323] Trial 409 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:36,771] Trial 410 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:37,222] Trial 411 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:37,632] Trial 412 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:38,012] Trial 413 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:38,396] Trial 414 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:38,778] Trial 415 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:39,202] Trial 416 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:39,556] Trial 417 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:40,044] Trial 418 finished with value: 0.7892855123201705 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:40,505] Trial 419 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:40,895] Trial 420 finished with value: 0.8145398113213818 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:41,319] Trial 421 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:41,559] Trial 422 finished with value: 0.8054342234570525 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:41,941] Trial 423 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:42,344] Trial 424 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:42,794] Trial 425 finished with value: 0.8145398113213818 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:43,215] Trial 426 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:43,581] Trial 427 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:43,997] Trial 428 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:44,226] Trial 429 finished with value: 0.8067140802499582 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:44,591] Trial 430 finished with value: 0.8145398113213818 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:44,984] Trial 431 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:45,401] Trial 432 finished with value: 0.8145398113213818 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:45,856] Trial 433 finished with value: 0.8136004109172719 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:46,288] Trial 434 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:46,710] Trial 435 finished with value: 0.8145398113213818 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:47,060] Trial 436 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:47,542] Trial 437 finished with value: 0.7911550311633313 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:47,984] Trial 438 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:48,407] Trial 439 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:48,821] Trial 440 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:49,252] Trial 441 finished with value: 0.8136004109172719 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:49,625] Trial 442 finished with value: 0.8145398113213818 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:50,070] Trial 443 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:50,540] Trial 444 finished with value: 0.8099403608417738 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:50,775] Trial 445 finished with value: 0.8054342234570525 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:51,163] Trial 446 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:51,517] Trial 447 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:51,879] Trial 448 finished with value: 0.8089632375217019 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:52,328] Trial 449 finished with value: 0.8145398113213818 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8293\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 10\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 99\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  323.000000  310.000000  326.000000  323.000000   \n",
      "1                    TN  158.000000  175.000000  162.000000  173.000000   \n",
      "2                    FP   69.000000   62.000000   67.000000   65.000000   \n",
      "3                    FN   45.000000   48.000000   40.000000   34.000000   \n",
      "4              Accuracy    0.808403    0.815126    0.820168    0.833613   \n",
      "5             Precision    0.823980    0.833333    0.829517    0.832474   \n",
      "6           Sensitivity    0.877717    0.865922    0.890710    0.904762   \n",
      "7           Specificity    0.696000    0.738400    0.707400    0.726900   \n",
      "8              F1 score    0.850000    0.849315    0.859025    0.867114   \n",
      "9   F1 score (weighted)    0.806082    0.814086    0.817734    0.831280   \n",
      "10     F1 score (macro)    0.792442    0.805092    0.805383    0.822321   \n",
      "11    Balanced Accuracy    0.786876    0.802159    0.799067    0.815826   \n",
      "12                  MCC    0.587855    0.611164    0.614589    0.649681   \n",
      "13                  NPV    0.778300    0.784800    0.802000    0.835700   \n",
      "14              ROC_AUC    0.786876    0.802159    0.799067    0.815826   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   332.000000  321.000000  330.000000  325.000000  310.000000  \n",
      "1   165.000000  172.000000  164.000000  163.000000  178.000000  \n",
      "2    73.000000   62.000000   49.000000   65.000000   51.000000  \n",
      "3    25.000000   40.000000   52.000000   42.000000   56.000000  \n",
      "4     0.835294    0.828571    0.830252    0.820168    0.820168  \n",
      "5     0.819753    0.838120    0.870712    0.833333    0.858726  \n",
      "6     0.929972    0.889197    0.863874    0.885559    0.846995  \n",
      "7     0.693300    0.735000    0.770000    0.714900    0.777300  \n",
      "8     0.871391    0.862903    0.867280    0.858653    0.852820  \n",
      "9     0.831246    0.826878    0.830511    0.818124    0.820521  \n",
      "10    0.821210    0.817102    0.815924    0.805770    0.810859  \n",
      "11    0.811625    0.812120    0.816914    0.800235    0.812144  \n",
      "12    0.654908    0.636715    0.631897    0.614304    0.621844  \n",
      "13    0.868400    0.811300    0.759300    0.795100    0.760700  \n",
      "14    0.811625    0.812120    0.816914    0.800235    0.812144  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_knn_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_knn_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_knn_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_knn_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_knn_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_knn_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_knn_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_knn_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_knn_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_knn_8)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:39:52,665] Trial 450 finished with value: 0.8099498573951704 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:53,102] Trial 451 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:53,523] Trial 452 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:53,953] Trial 453 finished with value: 0.8242321543446979 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:54,391] Trial 454 finished with value: 0.8127857811947455 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:54,797] Trial 455 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:55,289] Trial 456 finished with value: 0.8038667771884482 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:55,634] Trial 457 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:56,043] Trial 458 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:56,469] Trial 459 finished with value: 0.8242321543446979 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:56,934] Trial 460 finished with value: 0.8127857811947455 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:57,391] Trial 461 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:57,800] Trial 462 finished with value: 0.8242321543446979 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:58,235] Trial 463 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:58,696] Trial 464 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:59,091] Trial 465 finished with value: 0.8242321543446979 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:59,325] Trial 466 finished with value: 0.8116320232906883 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:39:59,805] Trial 467 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:00,228] Trial 468 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:00,577] Trial 469 finished with value: 0.8242321543446979 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:01,022] Trial 470 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:01,499] Trial 471 finished with value: 0.8127857811947455 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:01,740] Trial 472 finished with value: 0.8116320232906883 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:02,129] Trial 473 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:02,494] Trial 474 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:02,933] Trial 475 finished with value: 0.8008659359117811 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:03,365] Trial 476 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:03,833] Trial 477 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:04,317] Trial 478 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:04,711] Trial 479 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:05,126] Trial 480 finished with value: 0.8242321543446979 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:05,568] Trial 481 finished with value: 0.8127857811947455 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:06,020] Trial 482 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:06,402] Trial 483 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:06,795] Trial 484 finished with value: 0.8242321543446979 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:07,208] Trial 485 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:07,601] Trial 486 finished with value: 0.8203784263506556 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:08,024] Trial 487 finished with value: 0.8242321543446979 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:08,265] Trial 488 finished with value: 0.8099498573951704 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:08,621] Trial 489 finished with value: 0.8142706580177596 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:08,990] Trial 490 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:09,475] Trial 491 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:09,905] Trial 492 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:10,343] Trial 493 finished with value: 0.8242321543446979 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:10,591] Trial 494 finished with value: 0.8116320232906883 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:10,967] Trial 495 finished with value: 0.8152024400484196 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:11,397] Trial 496 finished with value: 0.8173609661181931 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:11,793] Trial 497 finished with value: 0.8242321543446979 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:12,251] Trial 498 finished with value: 0.8127857811947455 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 376 with value: 0.8292867381176114.\n",
      "[I 2023-12-04 17:40:12,719] Trial 499 finished with value: 0.8038667771884482 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 376 with value: 0.8292867381176114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8293\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 10\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 99\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  323.000000  310.000000  326.000000  323.000000   \n",
      "1                    TN  158.000000  175.000000  162.000000  173.000000   \n",
      "2                    FP   69.000000   62.000000   67.000000   65.000000   \n",
      "3                    FN   45.000000   48.000000   40.000000   34.000000   \n",
      "4              Accuracy    0.808403    0.815126    0.820168    0.833613   \n",
      "5             Precision    0.823980    0.833333    0.829517    0.832474   \n",
      "6           Sensitivity    0.877717    0.865922    0.890710    0.904762   \n",
      "7           Specificity    0.696000    0.738400    0.707400    0.726900   \n",
      "8              F1 score    0.850000    0.849315    0.859025    0.867114   \n",
      "9   F1 score (weighted)    0.806082    0.814086    0.817734    0.831280   \n",
      "10     F1 score (macro)    0.792442    0.805092    0.805383    0.822321   \n",
      "11    Balanced Accuracy    0.786876    0.802159    0.799067    0.815826   \n",
      "12                  MCC    0.587855    0.611164    0.614589    0.649681   \n",
      "13                  NPV    0.778300    0.784800    0.802000    0.835700   \n",
      "14              ROC_AUC    0.786876    0.802159    0.799067    0.815826   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   332.000000  321.000000  330.000000  325.000000  310.000000  319.000000  \n",
      "1   165.000000  172.000000  164.000000  163.000000  178.000000  159.000000  \n",
      "2    73.000000   62.000000   49.000000   65.000000   51.000000   68.000000  \n",
      "3    25.000000   40.000000   52.000000   42.000000   56.000000   49.000000  \n",
      "4     0.835294    0.828571    0.830252    0.820168    0.820168    0.803361  \n",
      "5     0.819753    0.838120    0.870712    0.833333    0.858726    0.824289  \n",
      "6     0.929972    0.889197    0.863874    0.885559    0.846995    0.866848  \n",
      "7     0.693300    0.735000    0.770000    0.714900    0.777300    0.700400  \n",
      "8     0.871391    0.862903    0.867280    0.858653    0.852820    0.845033  \n",
      "9     0.831246    0.826878    0.830511    0.818124    0.820521    0.801541  \n",
      "10    0.821210    0.817102    0.815924    0.805770    0.810859    0.788034  \n",
      "11    0.811625    0.812120    0.816914    0.800235    0.812144    0.783644  \n",
      "12    0.654908    0.636715    0.631897    0.614304    0.621844    0.577901  \n",
      "13    0.868400    0.811300    0.759300    0.795100    0.760700    0.764400  \n",
      "14    0.811625    0.812120    0.816914    0.800235    0.812144    0.783644  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_knn_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_knn_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_knn_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_knn_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_knn_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_knn_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_knn_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_knn_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_knn_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_knn_9)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAHJCAYAAADn4h/6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABpY0lEQVR4nO3deVyN6f8/8Ndp3xdKeykqoqKsMTIYOzHITllmMIbBjJnGUBiMZWyDYcyHkl1jN8SYsY6xk5CtBSkVrSqd6v794df5OjpRp6O4vZ6PxzzGue/rvu73fZ1Dr657ORJBEAQQERERkSioVXcBRERERKQ6DHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJw9wGTSCSQSCSvbVO7dm1IJBLEx8dXTVH0zmnbtu0bPydVJSAgABKJBKGhodVdylv3Lo07Eb1fGO6IiIiIRIThjoiIiEhEGO6oQtLT06Gnp4c6depAEASFbbp37w6JRIKLFy8CAOLj4yGRSBAQEICYmBj06tULNWrUgL6+Plq3bo3Dhw+Xub8tW7bg448/hqmpKXR0dFC/fn38+OOPeP78eam2EokEbdu2xaNHjxAYGAgrKyuoq6vLTuGVnNKLjY3F4sWLUa9ePejo6MDW1haTJk1CVlZWqT7/+ecffPbZZ3Bzc4ORkRF0dXXRoEEDBAcHIy8vr1T7kJAQSCQSHDt2DBs2bEDTpk2hr6+P2rVry9qEhoaiT58+cHJygq6uLoyMjNCqVSts2LBB4RiUnJ6TSqWYNWsW6tSpAx0dHbi6umLt2rWyditXrkTDhg2hq6sLW1tbhISEoLi4WGGfZ8+eRd++fWFpaQktLS3Y2dnh888/x6NHj2RtSt6348ePy8a35L+2bdvK9ffw4UOMHz8eTk5O0NbWRs2aNdGzZ0+cP39eqTGqKFWOkbKf1/z8fMybNw/u7u7Q09ODkZERPvroI2zdurVU21f30bdvX5ibm0NNTQ2hoaHlGvfKfDYjIiLQrFkz6OnpoUaNGujfvz8ePnyo8LiePn2KadOmoWHDhtDT04OxsTE8PT3x3Xff4dmzZ6XaBgUFoX79+tDV1YWxsTHat2+vcMyeP3+OJUuWoHHjxjA1NYWenh7s7OzQo0cPHDlyRGEtRFQ+GtVdAL1fTE1NMWDAAKxfvx5//fUXPvnkE7n1Dx48wMGDB+Ht7Q1vb2+5dXFxcWjZsiUaNmyIzz//HElJSdi2bRu6dOmCzZs3o3///nLtR44ciXXr1sHOzg59+vSBsbEx/vvvP0yfPh1Hjx7F4cOHoampKbfNkydP0LJlSxgaGqJv374QBAG1atWSazNp0iScOHEC/v7+8PPzQ2RkJJYuXYqTJ0/i1KlT0NHRkbWdP38+YmJi4OPjg27duiEvLw+nT5/GrFmz8M8//+Dvv/+Ghkbpv0aLFi3CX3/9hR49eqBdu3bIyMiQrRs7dizc3NzQpk0bWFlZIS0tDQcOHMDw4cMRExODuXPnKhz7AQMG4OzZs+jatSs0NTURERGBzz77DFpaWrhw4QI2b96M7t27o0OHDti3bx9mzpwJXV1dfPvtt3L9rF+/HqNHj4aOjg569uwJW1tb3LlzB7///jv27duH//77D/b29jAxMUFwcDBCQ0ORkJCA4OBgWR8vB7FLly6hY8eOePr0KTp16oRPP/0UaWlp2L17N1q3bo1du3aha9euFRojZalqjICKfV4LCgrQsWNHnDx5Em5ubvjiiy+Qm5uLHTt2YODAgbh8+TLmz59fah93795FixYt4OrqiiFDhiAnJwfu7u7lGndlP5urVq3C3r170bNnT/j6+uLs2bPYvn07rly5gqioKGhra8uNwccff4yEhAR4e3tj7NixKC4uxq1bt7BkyRKMGTMG+vr6AICEhAS0bdsW8fHxaNOmDbp06YKcnBzs378fnTt3xurVq/HZZ5/J+h42bBi2b9+Ohg0bYtiwYdDV1cWjR49w6tQpREZGlvq3hYgqQKAPFgABgBAcHFzmf8bGxgIAIS4uTrbdhQsXBABCnz59SvU5ffp0AYDw22+/yZbFxcXJ9vX111/LtT9//rygoaEhmJiYCJmZmbLl69evFwAIffv2FfLy8uS2CQ4OFgAIS5YsUXg8Q4cOFaRSaanahg8fLgAQatasKcTHx8uWFxUVCZ9++qkAQJg1a5bcNvfu3ROKi4tL9RUUFCQAELZs2aKwNj09PeHSpUulthMEQbh7926pZfn5+ULbtm0FDQ0N4cGDB3LrfH19BQBCkyZNhPT0dLnaNDU1BWNjY6F27drCw4cPZesyMjIEMzMzwczMTG4sbt26JWhqagrOzs7Co0eP5PZz9OhRQU1NTfDz81O4f0WkUqlQp04dQUdHRzh58qTcusTERMHa2lqwsLCQew/LM0ZlKXkP169fr7BGVYyRMp/XOXPmCACE7t27y/WVnJws2NnZCQDkxuflfQQFBSk81teNe8mxKfPZNDQ0FKKiouTWDRw4UAAgbN26VW65j4+PAECYO3duqf2kpqbKva++vr6CRCIRtm/fLtcuPT1d8PT0FHR0dISkpCRBEF6MvUQiEby9vYXCwsJSfaelpZV53ET0Zgx3H7CSHy7l+e/lcCcIgtC0aVNBU1NTSE5Oli0rLCwUrK2tBUNDQyEnJ0e2vOQHmbGxsZCVlVWqjpIf2KGhobJljRo1EjQ1NeV+UL+8n5o1awpNmjQpdTxaWlrC48ePFR5vyX5eDXCC8OIHpZqamlC7dm2F274qLS1NACAEBgbKLS/5ATpx4sRy9fOyiIgIAYAQFhYmt7zkh/zRo0dLbfPxxx8LAIT//e9/pdYFBgYKAOSC7FdffSUAEA4cOKCwhl69eglqampyweV1IWP37t0CAOGbb75RuH7p0qUCAGH//v2yZZUZozeFO1WMkTKf1zp16ggSiUS4detWqfa//fZbqc9KyT4sLCyE/Px8hcf6pnBXljd9Nn/44YdS2/z9998CAGHKlCmyZSW/xDVq1EgoKip67T6vXLkiABD69euncH3J52TFihWCIAhCVlaWAEDw8fFRGFCJqHJ4WpbKvHYOeHEaKCEhodTycePGITAwEOvWrUNQUBAAYN++fXj06BHGjh0rO1XzMi8vLxgaGpZa3rZtW4SFheHy5csYPnw4cnNzcfXqVZiZmWHp0qUK69LW1kZMTIzCel89DfsqX1/fUsucnJxgZ2eH+Ph4ZGRkwMTEBADw7NkzLFu2DLt27cLt27eRnZ0tN16JiYkK99G8efMy93///n3Mnz8fR48exf3790tdH1VWn6+e5gYAa2vrN657+PAhHBwcAABnzpwBABw7dgznzp0rtU1KSgqKi4tx584dhX2+qqS/+Ph4hISElFp/584dAEBMTAy6desmt+51Y6QsVYxRifJ+XrOzs3Hv3j3Y2trCxcWlVPsOHToAeHH6+lWenp5yp0ErQtnPZpMmTUots7OzA/DimtoS//33HwCgU6dOUFN7/eXZJZ+DjIwMhZ+D1NRUAJD9nTU0NESPHj2wb98+NG7cGH369EHr1q3RvHlz6OnpvXZfRPRmDHeklP79+2PKlCn4/fff8d1330EikWDNmjUAgDFjxijcxsLCQuFyS0tLAEBmZiaAFz9gBEFAamoqZs6cWaG6Svp6ndfVkZCQgMzMTJiYmEAqlaJdu3Y4d+4cGjZsiP79+8Pc3Fx2nd/MmTMV3tjxujpiY2PRrFkzpKen46OPPkLHjh1hbGwMdXV1xMfHIywsrMw+jY2NSy0ruabqdeukUqls2ZMnTwAACxcuVLiPEjk5Oa9d/2p/O3bsqHB/5XmvKkoVY1SivJ/Xkv+XdTxWVlZy7RT1VVGV+Wy+bhyKiopky0qugbSxsXljPSWfgyNHjrz2ZoiXPwfbtm3D/PnzsXnzZsyYMQMAoKOjA39/fyxatAjm5uZv3C8RKcZwR0rR1dVFQEAAFi9ejCNHjsDFxQWHDx9GixYt4OHhoXCbx48fK1yenJwM4P9+6JT8v3HjxgpnO16nPA99ffz4MVxdXd9Yx549e3Du3DkMHz681ENzk5KSXhs8y6pj8eLFePLkCdavX4+AgAC5dVu2bEFYWNgb66+MkmPLzMyEkZGRyvrbs2cPevbsWaFt3/UH9Fb081qy/FVJSUly7V6m7BhU5rNZXiWz12XNAL6s5NiWLVuGCRMmlKt/XV1dhISEICQkBA8ePMCJEycQGhqKDRs2ID4+Xna3MBFVHB+FQkobO3asbMZu7dq1KC4uxueff15m+0uXLiE7O7vU8mPHjgF4EeYAwMDAAA0aNMD169fx9OlTldet6IdGbGwsHjx4gNq1a8t+qN29excA0KdPn3L1UR5vo8+KaNGiBQDg5MmT5d5GXV0dgPysTmX6e1+U9/NqaGiIOnXqIDExUXYa+mX//PMPgBeneSvideNeFZ+jkvf2yJEjr7104+W2yn4O7OzsMHjwYERGRsLZ2RknTpx4K3/3iT4UDHektLp16+KTTz7B3r178dtvv8HExKTU40xelpmZiVmzZsktu3DhAjZt2gRjY2P07t1btnzy5MkoKCjAiBEjFD4iIz09vcKzeiWWLVsmdx1hcXExvvnmGxQXFyMwMFC2vOSxEyU/nEvExsYqfHRGeZTVZ2RkJH7//Xel+qyI8ePHQ1NTE5MmTcLt27dLrS8oKCj1A7pmzZoAXjzm5lV+fn6oU6cOVq5ciT///FPhPs+cOYPc3FwVVF+1KvJ5HTFiBARBwDfffCMXxtLS0jB79mxZm4p43bi/jc/mq7y9veHj44NLly5h0aJFpdY/efIE+fn5AF5cx/fRRx9h586dWLduncL+rl27hpSUFAAvrsE7e/ZsqTbPnj1DdnY21NXVFT7GhYjKh397qFLGjh2Lw4cPIy0tDRMmTICurm6Zbdu0aYPff/8dZ8+eRatWrWTPDSsuLsaaNWvkThOOGDECFy9exKpVq1CnTh106tQJ9vb2ePr0KeLi4nDixAkEBgZi9erVFa65devWaNSoEfr37w9jY2NERkbi6tWr8Pb2xtSpU2XtevTogbp162LJkiWIjo5G48aNcf/+fezfvx/dunXD/fv3K7zvcePGYf369fD390efPn1gY2OD6OhoHDp0CP7+/ti2bVuF+6yIevXqYd26dRgxYgQaNGiAzp07w8XFBVKpFPfv38fJkydhbm4ud7NK+/btsWPHDnz66afo0qULdHV14eDggKFDh0JTUxM7d+5Ep06d0K1bN/j4+KBRo0bQ09PDgwcPcP78ecTGxiIpKem9u1C+Ip/Xr7/+GgcPHsSePXvg6emJrl27yp5zl5KSgqlTp6J169YV2v/rxv1tfDYV2bhxI9q2bYupU6di+/bt8PX1hSAIuHPnDg4fPoyYmBhZ0Ny8eTPatWuHkSNHYvny5WjevDlMTEzw8OFDREVFITo6GmfOnEGtWrWQmJiIFi1aoH79+vDy8oKdnR2ysrKwf/9+JCcnY/z48Sq5bIDog1WNd+pSNcP/f8zJ6zg4OCh8FEqJwsJCwczMTAAgXL9+XWGbksc+DB8+XLh586bQs2dPwcTERNDV1RV8fHyEQ4cOlbn/ffv2Cd26dRPMzc0FTU1NwcLCQmjatKkwbdo04ebNm6WOx9fXt8y+Sh5hce/ePWHRokWCq6uroK2tLVhbWwsTJ06Ue/xHifv37wuDBg0SrK2tBR0dHcHNzU2YP3++IJVKFe6v5HET//zzT5l1nD59Wvj4448FExMTwcDAQGjVqpWwa9cu4Z9//pE9d/Blr3skRskxKXp/XldLVFSUMHz4cMHe3l7Q0tISTE1NhQYNGgifffZZqceJFBYWCkFBQYKjo6OgoaGh8LgfP34sfPvtt0KDBg0EXV1dQV9fX6hbt67Qp08fITw8XO7Zb+UZo7K86VEor9umvGOk7Oc1Ly9PmDNnjtCgQQNBR0dH9t5u3ry5VNuX91GWN427Kj+br6snLS1NmDp1quDi4iJoa2sLxsbGgqenp/D9998Lz549k2ublZUlzJkzR/Dy8hL09fUFHR0doXbt2kLXrl2FNWvWyB6RlJ6eLsycOVP4+OOPBWtra0FLS0uwtLQUfH19hc2bN/PxKESVJBGEN1xMQfQa9+7dg7OzM1q3bo0TJ04obBMfHw9HR0eFF39XpYCAAISFhSEuLq5SX3VF4vaufF6JiJTFa+6oUhYuXAhBEDB+/PjqLoWIiIjAa+5ICQkJCQgPD8edO3cQHh6Oxo0bo2/fvtVdFhEREYHhjpQQFxeH6dOnQ19fH506dcKvv/76xifYExERUdXgNXdEREREIsLpFiIiIiIRYbgjIiIiEhGGOyIiIiIRYbgjIiIiEhHeLfuBSk9PR2FhYXWX8d4zNzdHampqdZchGhxP1eFYqhbHU3U4lsrR0NCAqalp+dq+5VroHVVYWAipVFrdZbzXJBIJgBdjyZvOK4/jqTocS9XieKoOx7Jq8LQsERERkYgw3BERERGJCMMdERERkYgw3BERERGJCMMdERERkYgw3BERERGJCMMdERERkYgw3BERERGJCMMdERERkYgw3BERERGJCMMdERERkYgw3BERERGJCMMdERERkYgw3BERERGJiEQQBKG6i6CqN2jtOcQk51R3GURERG/F/pH1qrsEldLU1IS5uXm52nLmjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO5U5IsvvsCBAwfK3T4lJQX+/v6Ij49/e0URERERQkND0aJFCzg5OaFz5844e/ZsmW3//fdf2NjYlPrv7t27sjZ//vknunTpgvr166Nu3br45JNPEBERURWHUi4a1V2AWMybNw/a2toq7fPYsWMIDQ1FaGioSvslIiL6UOzZswchISGYO3cumjZtivDwcAwZMgTHjh2DjY1NmdudOHEChoaGstc1a9aU/dnExAQTJkxA3bp1oampib/++guTJ0+GmZkZ2rZt+zYPp1w4c6ciRkZGKg93REREVDlr167FgAEDMGjQIDg7O2PWrFmwtrbGhg0bXrudmZkZatWqJftPXV1dts7HxwddunSBs7MzateujVGjRqF+/fo4d+7c2z6ccvlgw92FCxcQEBCA4uJiAEB8fDz8/f0RHh4ua/Pbb79h6dKlAIBbt24hODgYgwcPxtixY7Fu3Trk5+fL2r56WjYxMRHTp0/H4MGDMWnSJERFRcHf37/UG//48WPMnDkTQ4YMwTfffIPbt28DAK5fv45Vq1YhNzcX/v7+8Pf3x/bt2wEAkZGRmDBhAgYPHozRo0fj559/fitjRERE9D4rKChAVFQUfH195Zb7+vriwoULr922U6dOaNy4Mfz9/XH69Oky2wmCgJMnT+LevXto0aKFSuqurA/2tKybmxvy8vIQHx8PJycn3LhxA4aGhrhx44aszfXr19GtWzfcv38fc+bMQf/+/TFmzBhkZWVh3bp1WLduHcaNG1eq7+LiYixcuBBmZmaYM2cO8vPzy/wNYevWrRg6dCgsLS2xdetWLFu2DMuXL4erqysCAgKwbds2LFu2DACgo6ODe/fuYf369Rg/fjxcXV2Rk5ODmzdvlnmcUqkUUqlU9loikUBXV1fZYSMiInovSCQSpKeno6ioCObm5pBIJLJ15ubmSElJkVtWwsLCAgsXLoS7uzsKCgrwxx9/oH///vjjjz/kwltWVha8vLxQUFAAdXV1zJ07t1SIrC4fbLjT09ND7dq1cf36dTg5OcmCXEREBPLy8vD8+XMkJSWhQYMG2LVrF1q3bo1u3boBAKysrBAYGIjg4GCMGjUKWlpacn1HRUXh8ePHCAkJgYmJCQBgwIAB+PHHH0vV0aNHD3h5eQEA/P39MXnyZCQnJ8PGxgZ6enqQSCSyPgAgLS0N2tra8Pb2hq6uLszNzeHo6Fjmce7atUvuIk9HR0fMnz9f2WEjIiJ6L1hZWUEQBAAvwpyVlZVsnYGBATQ1NeWWvbzdRx99JHvdo0cPPHnyBOvXr0fv3r1lyy0sLHD16lXk5OTg6NGjmDVrFry8vN6Ja+4+2HAHAA0aNMD169fRvXt3xMTEYMCAATh79ixiYmLw7NkzGBsbw8bGBrGxsUhOTsbJkyflthcEASkpKbC1tZVb/ujRI9SsWVMulNWtW1dhDfb29rI/l7TPzMws8yJPDw8PmJubY/z48WjUqBEaNWqEZs2alXm9X+/evdG9e3fZa0W/pRAREYlNUlISpFIp1NXVcfPmTdSuXVu2Li4uDqampkhKSipXXw0aNMAff/xRqr2+vj709fUxaNAgXLx4ESEhIdiyZYsqD0NGQ0MD5ubm5Wv7Vip4T7i5ueHvv/9GQkICJBIJbG1t4ebmhhs3buDZs2dwc3MD8CLEdejQAV27di3Vh5mZWallgiCUO0RpaPzfW1CyTclvGoro6upi/vz5uH79OqKiorB9+3bs2LED8+bNg76+fqn2mpqa0NTULFctREREYiEIAjQ1NeHh4YHjx4+jc+fOsnUnTpxAp06dXvvz9mXXrl1DrVq1XtteEAQUFBSUu8+36YMPd3l5eThw4ADc3NwgkUjg5uaG3bt3IycnRxbmHB0d8fDhQ1haWparXxsbG6SlpSEjI0M2G3fv3r0K16ehoSG74eNl6urq8PDwgIeHB/r27YvAwEBER0ejefPmFd4HERGRmI0ePRoTJ06Ep6cnvL29sXHjRiQmJmLo0KEAXjzKLCkpCcuXLwfw4u5aOzs7uLi4QCqVYufOnfjzzz+xdu1aWZ+//PILPD094eDgAKlUiqNHjyIiIgLz5s2rlmN81Qcd7kquuzt58iQCAgIAAPXr18fixYtRVFSEBg0aAAD8/Pwwbdo0/P777+jQoQO0tbWRmJiIqKgojBgxolS/Hh4esLCwwMqVKzFkyBDk5eVh69atACp2WtTc3Bz5+fm4du0aHBwcoK2tjejoaDx+/Bhubm7Q19fH5cuXUVxcDGtr68oPCBERkcj4+fkhPT0dS5YsQUpKClxdXREeHi67pOrx48d49OiRrL1UKsXs2bORnJwMHR0duLi4YMOGDWjfvr2sTW5uLoKCgmRt6tSpg+XLl8PPz6/Kj0+RDzrcAS/Oo8fFxcmCnIGBAWxtbZGeni677s3BwQEhISHYunUrZsyYAUEQYGlpiZYtWyrsU01NDd988w1Wr16NoKAgWFhYYMiQIZg/f36FTpG6urrik08+wdKlS5GdnY2+ffvCw8MD586dw44dOyCVSmFlZYWJEyfCzs6u8oNBREQkQgEBAbJJnFeVPPKsxLhx4xQ+CeNl3377Lb799lsVVad6EuFdODn8AYiJicGMGTOwfPnycp/efZsGrT2HmOSc6i6DiIjordg/sl51l6BSmpqavKGiup07dw46OjqwtLREcnIyQkND4erq+k4EOyIiIhIvhru3JC8vDxs3bsSTJ09gaGgId3d3DBs2rLrLIiIiIpFjuHtLfH1935knVRMREdGH44P9blkiIiIiMWK4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRiSAIQnUXQVUvNTUVUqm0ust4r0kkElhZWSEpKQn8a1R5HE/V4ViqFsdTdTiWytPU1IS5uXm52nLmjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRESjugug6jFxdxxiknOquwwRuFndBYgMx1N1xDWW+0fWq+4SiN4bnLkjIiIiEhGGOyIiIiIRYbgjIiIiEhGGOyIiIiIRYbgjIiIiEhGGOyIiIiIRYbgjIiIiEhGGOyIiIiIRYbgjIiIiEhGGOyIiIiIRYbgjIiIiEhGGOyIiIiIRUSrcFRQU4K+//sLDhw9VXQ8RERERVYJS4U5LSwvr169HVlaWqushIiIiokpQ+rRsrVq1kJGRocJSiIiIiKiylA53Xbt2xe7du5Gbm6vKeoiIiIioEjSU3fDBgwfIzs7GF198gYYNG8LU1FRuvUQiQWBgYKULJCIiIqLyUzrcRUZGyv587tw5hW0Y7oiIiIiqltLhbtu2baqsg4iIiIhUgM+5IyIiIhIRpWfuSly5cgU3btxAVlYW+vbtCzMzM9y9exe1atWCkZGRKmokIiIionJSOtw9f/4cCxYsQHR0tGxZx44dYWZmhn379qFmzZoYNmyYSookIiIiovJR+rTsli1bEBsbiylTpiAsLExunaenJ65du1bp4oiIiIioYpSeufvvv//Qv39/NGvWDMXFxXLrzMzMkJaWVuniiIiIiKhilJ65y8rKgq2trcJ1EokEBQUFShdFRERERMpROtzVqFED9+/fV7guISEBtWrVUrooIiIiIlKO0uGuWbNm2LVrF+Li4mTLJBIJUlNTceDAAbRs2VIlBRIRERFR+Sl9zV2/fv0QHR2N77//HnZ2dgCAVatW4fHjx7C2tkavXr1UVSMRERERlZPS4U5XVxc//vgj/vzzT1y6dAmWlpbQ1tZGr1690K1bN2hpaamyTiIiIiIqh0o9xFhLSwu9evXiLB0REVWZ0NBQrF69GikpKXBxccHMmTPRvHlzhW3PnTuHOXPm4O7du8jPz4eNjQ2GDBmCzz77TK5dZmYm5s+fj4MHDyIzMxN2dnaYMWMG2rdvXxWHRKRSSl9zN378eMTHxytcd//+fYwfP17ZrqtUSEgIQkNDK7SNv78/zp07V+b669evw9/fH8+ePatkdURE9LI9e/YgJCQEEyZMQGRkJJo1a4YhQ4YgMTFRYXs9PT0EBgZi586dOHbsGCZOnIgFCxZg48aNsjYFBQUYOHAgHjx4gN9++w0nTpzAwoULYWlpWVWHRaRSSs/cpaamorCwUOE6qVSK1NRUpYuqSl9//TXU1dWruwwiIiqHtWvXYsCAARg0aBAAYNasWTh+/Dg2bNiAoKCgUu0bNmyIhg0byl7b2dnh4MGDOHv2LIYMGQIA2Lp1KzIyMrBnzx5oamoCQJmP+iJ6Hyg9c/c6jx8/hq6u7tvoWuUMDAzem1rLCtNERB+CgoICREVFwdfXV265r68vLly4UK4+oqOjceHCBbknOhw5cgTe3t6YNm0aPD090a5dOyxfvhxFRUUqrZ+oqlRo5u7YsWM4fvy47PXvv/9eKhgVFBQgISEBbm5u5eozJCQE9vb20NLSwtGjR6GhoYFPPvkE/v7+b9zW398fn3/+OS5duoSrV6+iRo0aGDZsGJo0aSJr8/DhQ4SHh+PGjRvQ0dGBh4cHhg8fDiMjI9n+a9eujYCAAABAeno6Vq9ejejoaJiYmGDgwIHYsmULunbtim7dusn6zc7OxsKFC8vcLwDcunULW7ZswaNHj+Dg4IAxY8bA3t5etv6///7D9u3bkZycDFNTU3Tu3Bk9evSQrf/iiy/Qrl07JCcn49y5c2jatCnGjBmDsLAwnD17Fs+ePYOJiQk6dOiA3r17l2u8iYjeV0+fPkVRURHMzMzklpuZmSElJeW123p7e+Pp06coLCzE5MmTZTN/wItns54+fRq9e/dGeHg44uLi8P3336OoqAiTJk16K8dC9DZVKNwVFBQgKytL9vrZs2eQSqVybTQ1NeHj41OucFbi+PHj6N69O+bOnYvbt29j1apVqFevHjw8PN64bUREBAYPHoyhQ4fi4MGDWL58OVatWgUDAwOkp6cjODgY7du3x7Bhw1BQUIBNmzZhyZIlCA4OVtjfihUrkJ2djZCQEKirq2PDhg3IzMys0H5LhIeHIzAwECYmJti8eTPmz5+PZcuWQUNDA7GxsViyZAn69esHHx8f3L59G7///jsMDQ3Rtm1bWR979+5Fnz590KdPHwDAn3/+iQsXLmDSpEkwMzPDkydPXvtVb1KpVO49kkgk781MJRFRCYlEAolEAgBQU1OT/VnRekV2796NZ8+e4dKlS5g7dy4cHR1lvxQLgoCaNWti4cKFUFdXh6enJx4/foxff/0VkydPlvX7uv6pfDiWVaNC4a5jx47o2LEjgBezSlOmTEHt2rUrXYSDgwP69esHALCyssKhQ4dw7dq1coU7X19ftG7dGgAwcOBAHDp0CHfv3kWjRo1w+PBhODk5yf2GNnbsWIwdOxaPHj2CtbW1XF+JiYm4du0a5s2bhzp16gAAxowZgwkTJlRovyX69esnO4bx48djzJgxOHfuHHx8fLB//364u7ujb9++AABra2s8fPgQe/fulQt3DRs2RM+ePWWv09LSYGVlhXr16kEikcDc3Py147Nr1y5ERETIXjs6OmL+/Pmv3YaI6F1jZWWFmjVrQl1dHYWFhbCyspKty8vLg42NjdwyRdsDQLt27ZCfn49ly5Zh3LhxAF5cX6epqSl3nV3z5s0xc+ZM1KxZU/ZoL95goTocy7dL6RsqVq5cqbIiXj5VCQCmpqYKZ8sUcXBwkP1ZR0cHOjo6sm1jY2MRHR2NoUOHltqu5GHLL3v06BHU1dXh6OgoW2ZpaQl9ff0K7beEi4uL7M8GBgawtraW3dGVmJhY6jSuq6srDhw4gOLiYqipvbgcsiRklmjbti1+/PFHfPXVV/D09IS3tzc8PT0VjMwLvXv3Rvfu3WWv+dsSEb2PkpKSAAAeHh7Ys2cPWrRoIVt38OBBdOrUSdbmTbKyspCbmytr7+npiV27diExMVH2b++FCxdgYWGBJ0+eQCKRwNLSEsnJyRAEQcVH9mHhWCpPQ0PjjRM6sraV2ZFUKsWxY8dw/fp1ZGdnY9SoUbCyssL58+dhb28PCwuLchf8qvK+6a/e6SqRSGTbCoIAb29v2R1RLzMxMVF6n2/a7+uUhCtBEEoFLUXba2try712cnLCihUrcOXKFURFRWHJkiVwd3fHlClTFO5PU1NTdvcXEdH7quTfx9GjR2PixInw8PCAt7c3Nm7ciMTERAwdOhSCIGDevHlISkrC8uXLAbx4Jp61tTXq1q0LADh//jxWr16NwMBAWZ9Dhw7FunXrMH36dAQGBiIuLg7Lly/HiBEj5P5dFgSBgURFOJZvl9LhLisrCzNnzsTDhw9hYmKCjIwM5OXlAXjxl+fq1asYNWqUygpVhqOjI86ePQtzc/NyPe7ExsYGRUVFiI+Ph5OTEwAgOTlZ6efV3b59W3bhb05ODpKSkmSzhba2toiJiSnV3traWvabY1n09PTg4+MDHx8ftGjRAnPnzkVOTo7c9X5ERGLk5+eH9PR0LFmyBCkpKXB1dUV4eLjslOrjx4/x6NEjWfvi4mL89NNPuH//PjQ0NODg4ICgoCC5Mzo2NjbYvHkzQkJC8Mknn8DS0hIjR47EF198UeXHR6QKSoe7jRs3Ijc3F/PmzYODg4PcdW0NGjTAnj17VFJgZXTq1AlHjx7FsmXL0LNnTxgaGiI5ORmnT5/GmDFjSoUoGxsbuLu7Y82aNRg9erTshgotLS2lTmf+8ccfMDQ0hLGxMbZu3QpDQ0M0a9YMANC9e3cEBQUhIiJCdkPFoUOH3hiI9+/fD1NTU9SuXRsSiQT//fcfTExMoKenV+H6iIjeRwEBAbInHLxq6dKlcq9HjBiBESNGvLHPJk2aYP/+/Sqojqj6KR3uLl26hMGDB8PJyQnFxcVy62rWrIknT55UurjKqlGjBmbPno1NmzZhzpw5kEqlMDc3h6enZ5lhbfz48Vi9ejWCg4Nlj0J5+PChUqc2Bw0ahNDQUCQlJcHBwQFTp06VnYJ2cnLCpEmTsH37dvzxxx8wNTWFv7+/3M0Uiujo6GDPnj1ISkqCmpoa6tati6CgoDfO9hEREdGHQSIoedJ78ODBCAoKQsOGDVFcXIyBAwdi3rx5cHJywpUrV/Dzzz8jPDxc1fVWuSdPnmDs2LGYPn063N3dq7sclRm09hxiknOquwwionLZP7Jete1bIpHAysoKSUlJvE6skjiWytPU1Hz7N1TUqlULt2/flvtalxJ3794tdSfq+yI6Ohr5+fmwt7dHeno6Nm7cCHNzc9SvX7+6SyMiIiJ6I6XDXevWrbFnzx7Y2dnBy8sLwItEfvfuXRw8eLDS35hw8uRJ/PbbbwrXmZubY/HixZXqvyyFhYXYsmWL7CvUXFxcMGHCBIV39BIRERG9a5ROLH5+frh16xYWLVokew7cnDlzkJ2djUaNGqFr166VKqxJkyZwdnZWuK48d74qq1GjRnIPIiYiIiJ6nygd7jQ0NBAUFIR///0Xly5dQmZmJgwNDeHt7Q0fH59KX+Cvq6vLr8kiIiIiqqBKnWuUSCRo1aoVWrVqpap6iIiIiKgS+PwMIiIiIhFReuauuLgYBw8exKlTp5CamgqpVFqqTVhYWKWKIyIiIqKKUTrcbdq0Cfv370ft2rXh4eHBu0mJiIiI3gFKJ7JTp07Bz89P7mvHiIiIiKh6KX3NXUFBATw8PFRZCxERERFVktLhzsPDA3fu3FFlLURERERUSUqflg0MDMRPP/0EbW1teHl5wcDAoFQbRcuIiIiI6O1ROtzp6enB2toaYWFhZd4Vu23bNqULIyIiIqKKUzrc/fbbbzhz5gyaNm0KGxsb3i1LRERE9A5QOpGdP38eAwcORM+ePVVZDxERERFVgtI3VGhoaMDR0VGVtRARERFRJSkd7po1a4arV6+qshYiIiIiqiSlT8u2atUKa9asQWFhYZl3yzo5OVWqOCIiIiKqGKXD3ezZswEABw8exMGDBxW24d2yRERERFVL6XA3duxYVdZBRERERCqgdLhr27atCssgIiIiIlVQ+oYKIiIiInr3VOrJwzk5OTh16hQePnyIgoICuXUSiYSnbomIiIiqmNLhLi0tDUFBQXj+/DmeP38OIyMj5OTkoLi4GPr6+tDT01NlnURERERUDkqflt20aRNsbW2xdu1aAEBQUBDCw8MRGBgITU1NfPfddyorkoiIiIjKR+lwd/v2bXTs2BGampqyZRoaGujcuTPatWuHjRs3qqRAIiIiIio/pcNdZmYmTE1NoaamBjU1NeTm5srWubm5ISYmRiUFEhEREVH5KR3ujI2NkZOTAwAwNzdHbGysbF1qairU1dUrXx0RERERVYjSN1Q4OzsjLi4OTZo0QbNmzRAREQGpVAoNDQ3s3bsXDRo0UGWdpGLLejlCKpVWdxnvNYlEAisrKyQlJUEQhOou573H8VQdjiXRh03pcNezZ0+kpKQAAPr27YvExERs374dAFC/fn0EBgaqpkIiIiIiKjelw52TkxOcnJwAADo6Ovj222+Rm5sLiUQCXV1dlRVIREREROWn1DV3BQUF+Pzzz3HhwgW55Xp6egx2RERERNVIqXCnpaWFgoIC6OjoqLoeIiIiIqoEpe+WdXd3R1RUlCprISIiIqJKUvqau969e+Pnn3+GlpYWmjVrBlNTU0gkErk2BgYGlS6QiIiIiMpP6XBX8vViO3bswI4dOxS22bZtm7LdExEREZESlA53ffr0KTVTR0RERETVS+lw5+/vr8o6iIiIiEgFlL6hgoiIiIjePUrP3AFAcXExLl++jMTERBQUFJRa37dv38p0T0REREQVpHS4y87OxowZM/Do0aMy2zDcEREREVUtpU/LbtmyBVpaWli5ciUAYM6cOVi2bBm6d+8Oa2tr/PrrryorkoiIiIjKR+lwFx0djW7duqFGjRovOlJTg6WlJYYOHQp3d3ds2LBBZUUSERERUfkoHe6ePHmCWrVqQU1NDRKJBPn5+bJ13t7euHbtmkoKJCIiIqLyUzrcGRkZITc3FwBgamqKBw8eyNbl5OSgqKio8tURERERUYUofUOFo6MjHjx4AC8vLzRu3BgRERHQ1dWFhoYGtmzZAmdnZ1XWSURERETloHS469y5Mx4/fgwAGDBgAO7cuSO7ucLCwgKBgYGqqZDeiom74xCTnFPdZbyz9o+sV90lEBERKUXpcOfh4SH7s5GRERYsWCA7NWtjYwN1dfXKV0dEREREFVKphxi/TCKRwN7eXlXdEREREZESKhXucnNzERkZievXryM7OxuGhoZo0KABOnbsCH19fVXVSERERETlpHS4S0lJwcyZM5GWlgYzMzOYmJggKSkJ165dw5EjRxAcHAwLCwtV1kpEREREb6B0uFu/fj0KCgowe/ZsuLi4yJbfunULixYtQmhoKL799luVFElERERE5VOpb6gYOHCgXLADAFdXVwwYMADR0dGVLo6IiIiIKkbpcKepqYmaNWsqXGdmZgZNTU2liyIiIiIi5Sgd7po0aYIzZ84oXHfmzBl4eXkpXRQRERERKUfpa+5at26N1atXY/HixWjdujVMTEyQkZGBkydPIjY2FmPGjEFsbKysvZOTk0oKJiIiIqKyKR3u5syZAwB48uQJzp49W2r9jz/+KPd627Ztyu6KiIiIiMpJ6XA3duxYVdZBRERERCqgVLgrLi6Gi4sLjI2N+bBiIiIioneIUjdUCIKAyZMn4/bt26quh4iIiIgqQalwp66uDhMTEwiCoOp6iIiIiKgSlH4Uio+PD44fP67KWoiIiIiokpS+oaJ27do4c+YMZs6ciebNm8PExAQSiUSuTfPmzStdIBERERGVn9LhbuXKlQCAp0+f4saNGwrb8PEnRERERFVL6XAXHBysyjqIiIiISAWUDndubm6qrIOIiIiIVEDpcFciNzcXt2/fRnZ2Nho3bgwDAwNV1EVERERESqhUuIuIiMCePXtQUFAAAJg3bx4MDAwwa9YseHh4oFevXqqokYiIiIjKSelHoURGRiIiIgIff/wxvvvuO7l1Xl5euHTpUqWLIyIiIqKKUXrm7tChQ+jevTuGDBmC4uJiuXVWVlZISkqqdHFEREREVDFKz9ylpKTA09NT4TpdXV3k5uYqXRQRERERKUfpcKenp4fMzEyF61JSUmBkZKR0UURERESkHKXDXcOGDbFnzx7k5+fLlkkkEhQVFeHIkSNlzuoRERER0duj9DV3/fv3R1BQECZPnoxmzZoBeHEdXnx8PNLS0jBp0iSVFUlERERE5aP0zJ2lpSVmz54NGxsbREZGAgBOnDgBQ0NDzJw5E2ZmZiorkoiIiIjKp1LPubO1tcW0adMglUqRnZ0NAwMDaGlpqao2ondCaGgoVq9ejZSUFLi4uGDmzJlo3rx5me3PnDmDmTNn4vbt27CwsMDYsWMxbNgw2XqpVIoVK1Zgx44dSE5OhpOTE6ZNm4aPP/64Kg6HiIhETumZu5dpaGhAV1cXmpqaquhO5UJCQhAaGqqy/gRBwJo1axAYGAh/f3/Ex8cr3dfKlSuxYMECldVGqrVnzx6EhIRgwoQJiIyMRLNmzTBkyBAkJiYqbH///n0MHToUzZo1Q2RkJL788kvMmDEDBw4ckLVZsGABNm7ciNmzZ+Off/7B0KFDMWrUKERHR1fVYRERkYhVaubuzp072L59O27cuIHCwkJoaGjAzc0N/fr1g4uLi6pqfOdcuXIFx44dQ0hICCwsLGBoaKh0X4GBgRAEQYXVkSqtXbsWAwYMwKBBgwAAs2bNwvHjx7FhwwZ8//33pdqHh4fDxsYGs2bNAgA4Ozvj6tWrWL16Nbp16wYA+OOPPzBhwgS0b98eADB8+HAcP34ca9aswS+//FJFR0ZERGKl9MxddHQ0goODERsbi1atWsHPzw+tWrVCbGwsQkJCcO3aNVXW+U55/PgxTE1N4erqChMTE6irqyvdl56eHvT19VVYHalKQUEBoqKi4OvrK7fc19cXFy5cULjNxYsXS7Vv27YtoqKiIJVKAQDPnz+Htra2XBsdHR2cO3dOhdUTEdGHSumZu02bNsHR0RHTp0+Hjo6ObHleXh5mzZqFzZs3Y968eSopUpUKCwuxdetWnDx5Erm5ubCzs8PgwYPRoEEDAEB2djb+97//ISYmBjk5ObCwsEDv3r3RunVrAC9Oox4/fhwA4O/vD3Nzc6xcufK1+/zvv/9k11dpa2vD0dER33zzDXR0dLBy5Uo8e/YMU6dORUpKCsaPH19qezc3N4SEhAAAbt26hc2bN+Pu3bswMjJC06ZNMWjQILn3gFTj6dOnKCoqKnVzkJmZGVJSUhRuk5KSorB9YWEhnj59CgsLC7Rt2xa//fYbmjdvjtq1a+PUqVOIjIws9U0vREREylA63N2/fx8TJkwoFSp0dXXh5+f3zp5eWrVqFVJTU/HVV1/B1NQU586dw9y5c7Fo0SJYWVlBKpXCyckJvXr1gq6uLi5duoQVK1bAwsICzs7OCAwMhIWFBY4ePYp58+ZBTe31k5/p6elYtmwZBg8ejGbNmiE/Px83b95U2NbMzAy//fab7HVGRgZmz56N+vXrA3gx5nPmzEH//v0xZswYZGVlYd26dVi3bh3GjRunsE+pVCqbMQJePItQV1e3osP2wZFIJJBIJAAANTU12Z8VrX95nUQiUdj+5X5mz56Nr7/+Gr6+vpBIJHBwcED//v2xbds2hdt9KBSNJymHY6laHE/V4VhWDaXDnbGxcZlvjpqa2jv5DRXJyck4ffo0fv31V9SoUQMA0LNnT1y9ehX//PMPBg0ahBo1aqBnz56ybbp06YIrV67gzJkzcHZ2hp6eHnR1daGmpgYTE5M37jM9PR1FRUVo3rw5zM3NAQD29vYK277cZ0FBARYuXAhnZ2f069cPALB37160bt1adu2WlZUVAgMDERwcjFGjRim8U3nXrl2IiIiQvXZ0dMT8+fPfPFgfOCsrK9SsWRPq6uooLCyElZWVbF1eXh5sbGxgaWkJALL/A4CNjQ2ePXsm1764uFh2PaqmpiasrKxw6NAh5Ofn48mTJ7C2tsZ3330HJycnue0+VC+PJ1UOx1K1OJ6qw7F8u5QOdx06dMCBAwfg5eUFDY3/66awsBAHDhxAhw4dVFKgKsXFxUEQBEycOFFueWFhIQwMDAC8+EG8e/du/Pvvv3j69CmkUikKCwtLXSNVXrVr14a7uzu+/vpreHp6wsPDAy1atJDtryyrV69GXl4efvjhB9nsYGxsLJKTk3Hy5Em5toIgICUlBba2tqX66d27N7p37y57zd+WyicpKQkA4OHhgT179qBFixaydQcPHkSnTp2QnJwMS0tLJCcny26KcXd3x8GDB/Hdd9/J2u/evRuenp5IS0srtR81NTU8ePAA27dvR48ePWT7/RBJJJJS40nK4ViqFsdTdTiWytPQ0JBNEr2xbWV2kpqaii+//BLNmjWDiYkJMjIycO7cOaipqUFTUxP79++XtX85YFQXQRCgpqaG+fPnlzqdWnJ6ed++fThw4ACGDx8Oe3t76OjoIDQ0FIWFhUrtU01NDT/88ANu3bqFqKgoHDp0CFu3bsXcuXNRq1Ythdv88ccfuHLlCubOnSt3ClUQBHTo0AFdu3YttU1ZD43W1NR8Zx9R8y4r+Udn9OjRmDhxIjw8PODt7Y2NGzciMTERQ4cOhSAICAoKwt27d7Fs2TIAwNChQ7F+/XoEBwdj8ODBuHjxIrZs2YKVK1fK+rx06RKSk5PRoEEDJCcn4+eff0ZxcTHGjh3Lf+zwYuw5DqrBsVQtjqfqcCzfrkrdUFHi0KFDr10PvBvhrnbt2iguLkZmZqbsOrZX3bx5E02aNEGbNm0AvJjJS0pKgo2NjdL7lUgkqFevHurVq4e+ffti3LhxOHfunMIx+e+//xAREYHvv/++1LS1o6MjHj58yOnsKuTn54f09HQsWbIEKSkpcHV1RXh4uGyWNCkpSe6Zd/b29ggPD0dISAjCwsJgYWGBWbNmyU6lAy/ull2wYAHu378PPT09tGvXDsuXL4exsXGVHx8REYmP0uFuxYoVqqyjSlhbW6N169ZYsWIFhg0bBkdHR2RlZSE6Ohr29vbw8vKCpaUlzp49i1u3bkFfXx/79+9HRkaG0uHuzp07uHbtGjw9PWFsbIw7d+4gKytLYX/379/HypUr4efnBzs7O2RkZAB4MUtqYGAAPz8/TJs2Db///js6dOgAbW1tJCYmIioqCiNGjKjM0NBrBAQEICAgQOG60NBQJCUlyf0G2rJlS9lX8inSsmVLHDt2TMVVEhERvaB0uCvved93zbhx47Bz505s2LABT58+haGhIVxcXODl5QUA6Nu3L1JSUjBnzhxoa2ujffv2aNq0KXJzc5Xan66uLm7evIk///wTeXl5MDMzw7Bhw9C4ceNSbWNjY/H8+XPs3LkTO3fulC0veRSKg4MDQkJCsHXrVsyYMQOCIMDS0hItW7ZUbjCIiIhIdCSCkie9f/rpJ3Tu3BmNGjVScUlUFQatPYeY5JzqLuOdtX9kvTe2kUgksLKyKjVzR8rheKoOx1K1OJ6qw7FUnqam5tu/oSIxMRHz5s2DpaUlOnXqhLZt20JPT0/Z7oiIiIhIBZQOd7/88gsuXbqEyMhIhIWFYevWrWjdujU6d+5c5nPcxCgtLQ2TJk0qc/2SJUvKvJOViIiISNWUDncA4OXlBS8vLyQnJyMyMhLHjh3D0aNHUb9+fXTu3BnNmjV74zc4vO9MTU2xcOHC164nIiIiqiqVCnclLC0tMXz4cPTp0weLFy/G9evXcfPmTdm3PXTu3Fm0D89VV1fno0mIiIjonaGScPfkyRMcOXIER48eRVZWFho1agQfHx+cP38eoaGhePToEUaOHKmKXRERERHRa1Qq3EVHR+PQoUO4ePEitLS04Ovriy5dusi+H9PX1xd//vknduzYwXBHREREVAWUDneTJk3Co0ePUKtWLQwZMgQff/yxwrtl69atq/Qz4oiIiIioYpQOdzVq1MDgwYPh7e392uvpnJyc3stvsyAiIiJ6Hykd7qZPn16+HWhovLffZkFERET0vqlQuBs/fny520okEvzyyy8VLoiIiIiIlFehcGdra1tq2eXLl1GvXj3o6uqqrCgiIiIiUk6Fwt13330n97qoqAiDBg3C8OHD4eTkpNLCiIiIiKjiKvX1EWJ9MDERERHR+0rc3w1GRERE9IFhuCMiIiISEYY7IiIiIhGp0A0VsbGxcq+Li4sBAI8ePVLYnjdZEBEREVWtCoW7oKAghcvLep7dtm3bKl4RERERESmtQuFu7Nixb6sOIiIiIlKBCoW7tm3bvqUyiIiIiEgVeEMFERERkYgw3BERERGJCMMdERERkYgw3BERERGJCMMdERERkYgw3BERERGJCMMdERERkYgw3BERERGJCMMdERERkYhU6BsqSDyW9XKEVCqt7jKIiIhIxThzR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIqJR3QVQ9Zi4Ow4xyTlKbbt/ZD0VV0NERESqwpk7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhFhuHvHbd++Hd988011l1Gm0NBQtGjRAk5OTujcuTPOnj372vZnzpxB586d4eTkhJYtW2LDhg1y6zdt2oTevXvDzc0Nbm5u6N+/Py5fvvw2D4GIiEhUGO6qQUhICEJDQ8vVtmfPnpgxY8bbLUhJe/bsQUhICCZMmIDIyEg0a9YMQ4YMQWJiosL29+/fx9ChQ9GsWTNERkbiyy+/xIwZM3DgwAFZmzNnzsDPzw/bt2/H3r17YWNjg0GDBiEpKamqDouIiOi9xnD3jhIEAUVFRdDR0YGhoWF1l6PQ2rVrMWDAAAwaNAjOzs6YNWsWrK2tS83GlQgPD4eNjQ1mzZoFZ2dnDBo0CP3798fq1atlbVasWIGAgAA0bNgQdevWxcKFC1FcXIxTp05V1WERERG91zSqu4B3XUhICOzt7aGmpobjx49DQ0MD/fv3R+vWrbFu3Tr8999/MDY2xogRI9C4cWMAwMOHDxEeHo4bN25AR0cHHh4eGD58OIyMjLBy5UrcuHEDN27cwJ9//gngRaBJTU3FzJkz8f3332Pr1q1ISEjAtGnTcOPGDZw/fx4LFy6U1fT3339j//79SE5OhoGBAZo3b46RI0dW6bgUFBQgKioKX3zxhdxyX19fXLhwQeE2Fy9ehK+vr9yytm3bYuvWrZBKpdDU1Cy1TV5eHgoLC2FiYqKy2omIiMSM4a4cjh8/jp49e2Lu3Ln4999/sXbtWpw/fx5NmzZF7969ceDAAaxYsQKrVq1Cbm4ugoOD0b59ewwbNgwFBQXYtGkTlixZguDgYAQGBiIpKQl2dnbo378/AMDIyAipqakAXlxzNnToUNSqVQv6+vq4ceOGXC2HDx9GWFgYBg8ejEaNGiE3Nxe3bt2q8jF5+vQpioqKYGZmJrfczMwMKSkpCrdJSUlR2L6wsBBPnz6FhYVFqW3mzp0LS0tLfPTRR6ornoiISMQY7srBwcEBffr0AQD07t0bu3fvhqGhITp06AAA6Nu3Lw4fPoyEhARcvnwZTk5OGDRokGz7sWPHYuzYsXj06BGsra2hoaEBbW1thbNR/v7+8PDwKLOWP/74Az169EDXrl1ly+rWrVtme6lUCqlUKnstkUigq6tb7mNXRCKRQCKRAADU1NRkf1a0/tXlitqX1c/KlSuxZ88eREREVLrmt6GkXkXHQxXH8VQdjqVqcTxVh2NZNRjuysHe3l72ZzU1NRgaGsotMzY2BgBkZWUhNjYW0dHRGDp0aKl+Hj9+DGtr69fuq06dOmWuy8zMRHp6Oho2bFju2nft2oWIiAjZa0dHR8yfP7/c2ytiZWWFmjVrQl1dHYWFhbCyspKty8vLg42NjdyyEjY2Nnj27JncuuLiYmhoaMDNzU3utOyiRYuwYsUK/PXXX2jSpEml6n3bLC0tq7sEUeF4qg7HUrU4nqrDsXy7GO7KQUNDfpgkEgnU1dXlXgMvgoogCPD29saQIUNK9VOe68a0tbXLXKelpVXOiv9P79690b1791K1VkbJnaseHh7Ys2cPWrRoIVt38OBBdOrUSeHdre7u7jh48CC+++472bLdu3fD09MTaWlpsmWrVq3CsmXLsHnzZtjY2Lyzd8pKJBJYWloiOTkZgiBUdznvPY6n6nAsVYvjqTocS+VpaGjA3Ny8fG3fci0fHEdHR5w9exbm5uZyAfBlGhoaKC4urnDfurq6MDc3R3R0dLln7zQ1NRXeqFAZJX8hR48ejYkTJ8LDwwPe3t7YuHEjEhMTMXToUAiCgHnz5iEpKQnLly8HAAwdOhTr169HcHAwBg8ejIsXL2LLli1YuXKlrM9Vq1Zh4cKFWLFiBWxtbfH48WMAgL6+PvT19VV6HKoiCAL/kVIhjqfqcCxVi+OpOhzLt4vhTsU6deqEo0ePYtmyZejZsycMDQ2RnJyM06dPY8yYMVBTU4O5uTnu3LmDlJQU6OjowMDAoNz99+vXD2vXroWRkREaN26MvLw83Lp1C126dHmLR6WYn58f0tPTsWTJEqSkpMDV1RXh4eGwtbUF8OI09KNHj2Tt7e3tER4ejpCQEISFhcHCwgKzZs1Ct27dZG3CwsJQUFCAzz77TG5fkydPxpQpU6rmwIiIiN5jDHcqVqNGDcyePRubNm3CnDlzIJVKYW5uDk9PT9kp0R49emDlypWYPHkyCgoKsGLFinL337ZtW0ilUhw4cADh4eEwMjJC8+bN39bhvFFAQAACAgIUrlu6dGmpZS1btkRkZGSZ/b3pGy6IiIjo9SQC50U/SIPWnkNMco5S2+4fWU/F1byfJBIJrKyskJSUxNMLKsDxVB2OpWpxPFWHY6k8TU3Ncl9zx2+oICIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEWG4IyIiIhIRhjsiIiIiEdGo7gKIiIjeRYIgICcnB4IgIC8vDwUFBdVdkihwLMumra0NbW3tSvfDcEdERKRATk4OtLW1oaWlBU1NTUil0uouSRQ4loqV/BLx7Nkz6OvrV6ovnpYlIiJSQBAEaGlpVXcZ9IGQSCTQ09NDYWFhpftiuCMiIiJ6R0gkkkr3wXBHREREJCIMd0RERB+g5s2bY+3atZVuU1nbtm1D/fr13+o+VOF9qRNguCMiIhKVxMRETJkyBV5eXqhduzaaNWuGGTNm4OnTpxXu688//8SQIUNUVlvz5s2xZs0auWU9e/bEyZMnVbaPVx04cAB2dnZITExUuL5NmzaYPn36W9t/deDdskREROXU/X8xVbq//SPrVah9QkICevbsCScnJ6xcuRL29va4desWfvzxR/z999/Yt28fTE1Ny91fzZo1K1pyhenq6kJXV/et9d+xY0eYmppi+/btmDRpkty68+fP4969e/j111/f2v6rA2fuiIiIRGLatGnQ1NTE5s2b0bJlS9jY2KBdu3bYunUrkpOTMX/+fLn2OTk5+OKLL+Ds7AwvLy+sW7dObv2rp2WzsrIwdepUeHh4wNXVFf369cP169fltjl8+DC6dOkCJycnNGzYEKNGjQIA9O3bFw8fPsT06dNhY2MDGxsbAPKnO+/evQsbGxvcvXtXrs81a9agefPmEAQBAHD79m0MHToUzs7O8PT0xJdfflnmzKSmpib69OmDHTt2yLYvsXXrVnh4eKBBgwZYs2YN2rdvj7p166JJkyYICgrCs2fPyhzrr776CiNGjJBbNmPGDPTt21f2WhAErFq1Ci1btkSdOnXQoUMH7N+/v8w+VYXhjoiISATS09Nx7NgxDB8+vNRMWK1atfDpp59i3759cgFn9erVqF+/Pg4dOoTx48cjJCQEJ06cUNi/IAgYNmwYUlJSEB4ejoMHD8Ld3R39+/dHeno6AOCvv/7CqFGj0L59e0RGRmLbtm3w8PAAAKxduxZWVlb49ttvcfnyZVy+fLnUPurWrQsPDw/s3LlTbvnu3bvRq1cvSCQSPH78GH369IGbmxsOHjyITZs2IS0tDZ9//nmZYzNw4EAkJCTgzJkzsmW5ubnYt28fBgwYAABQU1PDrFmz8Pfff2Pp0qU4ffo0fvzxx9cN+RvNnz8f27Ztw7x58/D3339j9OjRmDBhglwdbwNPyxIREYlAXFwcBEGAs7OzwvV169ZFRkYGnjx5AjMzMwBA06ZNMX78eABAnTp1cP78eaxduxZt2rQptf3p06cRExODq1evyr5FYcaMGYiMjMSBAwcwZMgQLF++HH5+fvj6669l2zVo0AAAYGpqCnV1dRgYGKBWrVplHkfv3r0RGhqKqVOnAgDu3buHqKgoLFu2DACwYcMGuLu7IygoSLbNzz//jKZNm+LevXuoU6dOqT5dXFzQuHFjbNu2DT4+PgCAffv2oaioCL169QIAjB49Wtbe3t4e33zzDYKCgjBv3rwya32d3NxcrF27Ftu2bUOTJk0AAA4ODjh//jw2btyIli1bKtVveTDcERERfQBKZuxefo6at7e3XBtvb2/8/vvvCre/du0anj17hoYNG8otz8/PR0JCAgDg+vXrGDx4cKXq9PPzw48//oiLFy/C29sbu3btQoMGDeDi4gIAiIqKwr///qswxCYkJCgMd8CL2bvg4GDMmTMHBgYG2Lp1K7p27QpjY2MAL8LrL7/8gjt37iA7OxtFRUXIz89Hbm4u9PT0Knwct2/fRn5+PgYOHCi3XCqVlhpDVWO4IyIiEoHatWtDIpHg9u3b6Ny5c6n19+7dg4mJCWrUqPHafsp6iG5xcTFq1aqFiIiIUutKApKOjo4SlcuzsLCAj48Pdu/eDW9vb+zevVvujl1BEPDJJ5/g+++/V7htWfz8/BASEoK9e/eiZcuWOHfunGyG8eHDhxg2bBiGDBmCb775BiYmJjh//jymTJlS5lelqamplbqG7+VvlyguLgbwYqbR0tJSrt3b/uYThjsiIiIRqFGjBtq0aYOwsDCMHj1a7rq7lJQU7Ny5E3379pULb5cuXZLr49KlS6hbt67C/t3d3ZGamgoNDQ3Y2dkpbFO/fn2cOnUK/fv3V7heU1MTRUVFbzyW3r17Y+7cufDz80NCQgL8/Pxk6xo2bIg///wTdnZ20NAof4wxMDBA9+7dsW3bNiQkJMDBwUF2ivbq1asoLCxEcHAw1NRe3I6wb9++1/ZXs2ZN3Lp1S27Z9evXoampCeDFqWBtbW0kJia+1VOwivCGCiIiIpH48ccfUVBQgMGDB+O///5DYmIi/vnnHwwcOBCWlpb49ttv5dqfP38eq1atwr179xAaGor9+/dj5MiRCvv+6KOP4O3tjREjRuDYsWN48OABzp8/j/nz5+Pq1asAgMmTJ2P37t1YtGgR7ty5g5s3b2LVqlWyPuzs7PDff/8hKSnptc/d69q1K3JychAUFAQfHx9YWVnJ1gUEBCAjIwPjxo3D5cuXkZCQgOPHj2Py5MlvDI4DBw7EhQsXEB4ejv79+8uCroODAwoLC7Fu3TokJCQgIiIC4eHhr+2rVatWuHr1Knbs2IHY2FgsWrRILuwZGBjg888/R0hICLZv3474+HhER0cjNDQU27dvf23flcWZuw/Usl6OZU41ExHR+8nJyQkHDx7Ezz//jLFjxyI9PR3m5ubo3LkzJk2aVOoZd59//jmioqKwePFiGBgYYMaMGWjbtq3CviUSCcLDwzF//nxMmTIFT548gbm5OVq0aCG7QcPHxwdr1qzB0qVLsXLlShgYGKBFixayPr7++mt89913aNWqFZ4/f17mg4UNDQ1ljw1ZvHix3DpLS0vs3r0bc+fOxeDBg/H8+XPY2tqibdu2slm3sjRr1gx16tRBXFwc+vXrJ1vesGFDBAcHY9WqVZg3bx5atGiBoKAgTJw4scy+2rZti6+++gpz5szB8+fP0b9/f/Tt2xcxMf/3LMSpU6fCzMwMK1aswP3792FkZAR3d3d8+eWXr62zsiTCqyeM6YOQmprKcFdJEokEVlZWSEpKKnXdBVUcx1N1OJaqkZWVBSMjIwAvTid+iP9mNm7cGN988w0GDRqksj4/1LEsr5c/dy/T1NSEubl5ufrgzB0RERHJycvLw/nz55Gamiq7S5XeH7zmjoiIiORs3LgRY8eOxahRo2TPaKP3B2fuiIiISM7o0aPlHupL7xfO3BERERGJCMMdERERkYgw3BERERGJCMMdERFRGfgoGapKJV9ZVlkMd0RERApoa2sjLy+vusugD0RxcTGys7Ohp6dX6b54tywREZEC2traePbsGTIzM6GtrY2CgoLqLkkUtLS0OJZl0NfXr9D35ZaF4Y6IiKgM+vr6/MYPFeJYVg2eliUiIiISEYY7IiIiIhFhuCMiIiISEYY7IiIiIhHhDRUfKFXcjUMvcCxVi+OpOhxL1eJ4qg7HsuIqMmYSgberfFCkUik0NTWruwwiIiJ6S3ha9gMjlUqxbNkyPphTBfLy8vDtt99yLFWE46k6HEvV4niqDseyajDcfYBOnz7N5wupgCAIiIuL41iqCMdTdTiWqsXxVB2OZdVguCMiIiISEYY7IiIiIhFhuPvAaGpqom/fvrypQgU4lqrF8VQdjqVqcTxVh2NZNXi3LBEREZGIcOaOiIiISEQY7oiIiIhEhOGOiIiISEQY7oiIiIhEhF/uJkKRkZHYu3cvMjIyYGtri4CAANSvX7/M9jdu3EBYWBgePnwIU1NT9OzZEx07dqzCit9dFRnL9PR0bNiwAbGxsUhOTkaXLl0QEBBQtQW/4yoynmfPnsXhw4cRHx+PwsJC2Nraol+/fmjUqFHVFv2OqshYxsTEYNOmTUhMTMTz589hbm6ODh06oHv37lVc9burov9uloiJiUFISAjs7OywcOHCKqj03VeRsbx+/TpmzpxZavmSJUtgY2PztksVLYY7kfn3338RGhqKUaNGwdXVFX/99Rfmzp2LJUuWwMzMrFT7lJQUzJs3D+3bt8eXX36JW7du4ffff4eRkRFatGhRDUfw7qjoWEqlUhgZGeHTTz/FgQMHqqHid1tFx/PmzZvw8PDAwIEDoa+vj3/++Qfz58/H3Llz4ejoWA1H8O6o6Fhqa2ujU6dOcHBwgLa2NmJiYrB27Vro6OigQ4cO1XAE75aKjmeJ3NxcrFy5Eu7u7sjIyKi6gt9hyo7l0qVLoaenJ3ttZGRUFeWKFk/Lisz+/fvRrl07tG/fXvYbk5mZGQ4fPqyw/eHDh2FmZoaAgADY2tqiffv2+Pjjj7Fv374qrvzdU9GxrFWrFgIDA+Hr6yv3jxS9UNHxDAgIgJ+fH+rWrQsrKysMGjQIVlZWuHjxYhVX/u6p6Fg6OjqidevWsLOzQ61atdCmTRt4enri5s2bVVz5u6mi41nit99+Q6tWreDs7FxFlb77lB1LY2NjmJiYyP5TU2M8qQyOnogUFhYiNjYWnp6ecss9PDxw69YthdvcuXMHHh4ecssaNWqE2NhYFBYWvrVa33XKjCWVTRXjWVxcjLy8PBgYGLyNEt8bqhjLuLg43Lp1C25ubm+jxPeKsuP5zz//4PHjx+jXr9/bLvG9UZnP5tSpU/HZZ59h1qxZiI6OfptlfhB4WlZEsrKyUFxcDGNjY7nlxsbGZZ4yyMjIUNi+qKgI2dnZMDU1fVvlvtOUGUsqmyrGc//+/Xj+/Dlatmz5Fip8f1RmLMeMGYOsrCwUFRWhX79+aN++/Vus9P2gzHgmJSVh8+bNmDlzJtTV1augyveDMmNpamqKzz77DE5OTigsLMSJEycwe/ZsBAcH85ePSmC4EyGJRFKuZWWtK/nSktdt86Go6FjS6yk7nqdOncKOHTvwzTfflPrB8aFSZixnzZqF/Px83L59G5s3b4alpSVat279tkp8r5R3PIuLi7F8+XL069cP1tbWVVHae6cin01ra2u5cXRxcUFaWhr27dvHcFcJDHciYmRkBDU1tVK/IWVmZpb5A9HExKRU+6ysLKirq3/Qp7+UGUsqW2XG899//8Xq1asxefLkUpcQfIgqM5a1atUCANjb2yMzMxM7duz44MNdRcczLy8P9+7dQ1xcHNatWwfgxS/EgiBgwIAB+OGHH9CwYcOqKP2do6p/N11cXHDy5EkVV/dh4TV3IqKhoQEnJydERUXJLY+KioKrq6vCbZydnUu1v3r1KpycnKCh8eFmf2XGksqm7HieOnUKK1euxIQJE+Dl5fW2y3wvqOqzKQjCB31dbYmKjqeuri4WLVqEBQsWyP775JNPYG1tjQULFqBu3bpVVfo7R1Wfzbi4OJiYmKi4ug/Lh/vTW6S6d++OX375BU5OTnBxccFff/2FtLQ0fPLJJwCAzZs34+nTpxg/fjwAoGPHjoiMjERYWBjat2+P27dv4++//8bEiROr8zDeCRUdSwCIj48HAOTn5yMrKwvx8fHQ0NCAra1tdRzCO6Wi41kS7AICAuDi4iKbDdDS0vrg70au6FgeOnQIZmZmsueGxcTEYN++fejSpUu1HcO7pCLjqaamBnt7e7ntjYyMoKmpWWr5h6iin80DBw7A3NwcdnZ2KCwsxMmTJ3H27FlMmTKlOg/jvcdwJzI+Pj7Izs7GH3/8gfT0dNjZ2SEoKAjm5uYAXjxoNy0tTda+Vq1aCAoKQlhYGCIjI2FqaorAwMAP/hl3QMXHEnhxx1eJ2NhYnDp1Cubm5li5cmWV1v4uquh4/vXXXygqKsL//vc//O9//5Mt9/X1xRdffFHl9b9LKjqWgiBgy5YtSElJgZqaGiwtLTF48GA+4+7/U+bvOilW0bEsLCxEeHg4nj59Ci0tLdjZ2eG7777jTH0lSYSSq+eJiIiI6L3Ha+6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjoiIiEhEGO6IiIiIRIThjugDdOzYMfj7++PevXsK1//0008f/IOC3xeRkZE4duxYle4zJCTkvf4GgefPn2P79u24fv16dZdC9FYw3BERvccOHz5c5eHufff8+XNEREQw3JFoMdwR0XunsLAQRUVFVba/58+fV9m+3gWCIKCgoKC6y1A5sR4X0av43bJE9EazZs3C06dPsWTJEkgkEtlyQRAwYcIEWFtbIygoCCkpKRg/fjwGDx6MoqIiHDlyBFlZWbCzs8PgwYPh7u4u129SUhK2b9+Oa9euITc3FxYWFujUqRM6d+4sa3P9+nXMnDkT48ePR3x8PE6fPo2MjAwsXrwYd+7cwapVq/DDDz/g1KlTOH/+PAoLC9GgQQMEBgbCwsJC1k9UVBQOHTqE2NhYZGdno0aNGnB3d8eAAQNgZGQka7d9+3ZERETgp59+wq5duxAdHQ1NTU389ttvuHfvHvbt24c7d+4gIyMDJiYmcHZ2xuDBg2XfnQm8OO29atUqzJgxA6dOncK5c+dQVFSEpk2bYtSoUcjPz8e6desQFRUFLS0ttG7dGoMGDYKGxv/9k1xYWIg9e/bg5MmTSElJga6uLry9vTFkyBBZvV988QVSU1MBAP7+/gAg913Gubm5iIiIwNmzZ/H06VMYGRmhZcuWGDBgAHR0dGT78vf3R6dOnWBnZ4eDBw8iOTkZgYGB6NixY7k/IyV9ODk5Yffu3UhLS4OdnR1GjBgBZ2dn7Nu3D5GRkcjKykLdunXx+eefw9LSUrZ9SEgIsrOzMWrUKGzcuBHx8fEwMDDAxx9/DH9/f6ip/d9cRE5ODrZu3Yrz588jKysLNWvWRKtWrdC3b19oamq+8bh+//13AEBERAQiIiIA/N93FicnJ2Pnzp2IiYnB06dPoa+vD0dHRwwaNAj29valPpcTJkzAgwcPcOzYMeTn56Nu3boYOXIkrK2t5cbnypUr2Lt3L+7du4eioiKYm5ujTZs26N27t6zNvXv3EBERgZiYGBQUFMDGxga9evWCj49Pud8HIoDhjuiDVlxcrHAG7NWvnO7atSsWLFiAa9euwcPDQ7b88uXLePz4MQIDA+XaHzp0CObm5ggICIAgCNizZw/mzp2LmTNnwsXFBQDw8OFD/PDDDzAzM8OwYcNgYmKCK1euYP369cjOzka/fv3k+ty8eTNcXFwwevRoqKmpwdjYWLbu119/hYeHByZOnIi0tDRs27YNISEhWLRoEfT19QEAycnJcHFxQbt27aCnp4fU1FTs378fM2bMwKJFi+SCFQD8/PPP8PHxwSeffCKbuUtNTYW1tTV8fHxgYGCAjIwMHD58GEFBQVi8eLFcSASA1atXo1mzZvjqq68QFxeHLVu2oKioCI8ePULz5s3RoUMHXLt2DXv27EGNGjXQvXt32fuyYMEC3Lx5E35+fnBxcUFaWhq2b9+OkJAQ/PTTT9DS0sLXX3+NxYsXQ09PDyNHjgQAWbh5/vw5QkJC8OTJE/Tu3RsODg548OABtm/fjvv372P69OlyQf38+fOIiYlBnz59YGJiIje+5XXp0iXEx8dj8ODBAIBNmzbhp59+gq+vLx4/foyRI0ciNzcXYWFh+Pnnn7FgwQK5GjIyMrB06VL06tUL/v7+uHTpEnbu3Ilnz57Jjq+goAAzZ85EcnIy/P394eDggJs3b2L37t2Ij49HUFCQXE2vHpeBgQG+//57zJ07F+3atUO7du0AQPbePX36FAYGBhg0aBCMjIyQk5OD48eP4/vvv8eCBQtKhbYtW7bA1dUVn3/+OfLy8rBp0ybMnz8fS5YskQXSv//+G2vWrIGbmxtGjx4NY2NjJCUl4f79+7J+oqOjMXfuXDg7O2P06NHQ09PDv//+i6VLl6KgoABt27at8PtBHy6GO6IP2LRp08pc9/JMlJeXFywsLHDo0CG5cBcZGQkLCws0btxYbtvi4mL88MMP0NLSAgB4enriiy++wLZt2zB9+nQAQFhYGHR1dTFr1izo6ekBADw8PFBYWIjdu3ejS5cuMDAwkPVpYWGByZMnK6y1Tp06GDt2rOy1nZ0dpk+fjsjISHz66acAIDcLJQgCXF1d0aBBA4wbNw5XrlxBkyZN5Pr09fWVzYaVaNGiBVq0aCF3nF5eXhg9ejROnTqFrl27yrX38vLCsGHDZMd2+/ZtnD59GsOGDZMFOQ8PD1y9ehUnT56ULTtz5gyuXLmCKVOmoHnz5rL+HBwcEBQUhGPHjqFjx45wdHSElpYWdHV1ZaG5xMGDB5GQkIC5c+eiTp06AAB3d3fUqFEDixcvxpUrV+Tet/z8fCxatEhuzCtKKpVi2rRpsllBiUSChQsX4vr165g/f74syGVlZSE0NBQPHjyQmw3Lzs7G1KlTZe+Fp6cnCgoKcPjwYfj5+cHMzAzHjx9HQkICJk2ahJYtW8rGUEdHB5s2bUJUVJTcZ1TRcWVlZQEAatSoUWrc3Nzc4ObmJntd8h5PmTIFR44cwfDhw+Xa29raYsKECbLXampqWLJkCe7evQsXFxfk5+cjLCwMrq6umDFjhmwMXp3F/t///gc7OzvMmDED6urqAIBGjRohKysLW7ZsQZs2beRmL4leh+GO6AM2fvx42NjYlFoeFhaGJ0+eyF6rqamhU6dO2LhxI9LS0mBmZobk5GRcuXIFQ4cOlZt9AYDmzZvLgh0A2SnF06dPo7i4GIWFhYiOjsYnn3wCbW1tudnDxo0b49ChQ7hz545c+Hg55LyqdevWcq9dXV1hbm6O69evy8JdZmYmtm3bhsuXL+Pp06dys5MPHz4sFe4U7S8/P192mjM1NRXFxcWydYmJiaXae3t7y722sbHB+fPn4eXlVWp5VFSU7PXFixehr68Pb29vubGpXbs2TExMcP369TeeMr148SLs7e1Ru3ZtuT4aNWoEiUSC69evy41vw4YNKxXsAKBBgwZyp3tLPlsl+3x1eWpqqly409XVLfU+tG7dGkePHsWNGzfQpk0bREdHQ1tbWy5kA0Dbtm2xadOmUrPLFT2uoqIi2enw5ORkubFT9B6/Wq+DgwMAIC0tDS4uLrh16xby8vLQsWPHUn9PSiQnJyMxMRFDhw6V1VDCy8sLly5dwqNHj2Bra1vu46APG8Md0QfMxsZGNqvzMj09PblwBwDt2rXD9u3bcfjwYQwaNAiRkZHQ0tLCxx9/XGp7ExMThcsKCwuRn5+P/Px8FBUV4dChQzh06JDC2rKzs+Vem5qalnkcZe2vpI/i4mL8+OOPSE9PR58+fWBvbw9tbW0IgoBp06YpvMhe0f6WLVuG6Oho9OnTB3Xq1IGuri4kEgnmzZunsI9XQ0XJqV9Fy1/ePjMzE8+ePcOgQYMUHu+rY6NIZmYmkpOTMXDgwHL1oWgMK6oixwu8mOl7maJTwSV15eTkyP5vYmJSKigZGxtDXV290scVFhaGyMhI+Pn5wc3NDQYGBpBIJFi9erXC99jQ0FDhsZW0LZklrFmzZpn7zMjIAACEh4cjPDxcYZvyvOdEJRjuiKhc9PT04Ovri7///hs9e/bEsWPH0KpVK9k1bS8r+WH16jINDQ3o6OhAXV0dampqaNOmDTp16qRwf7Vq1ZJ7Xdasx+v2V3LB/oMHD5CQkIBx48bJXbuUnJxcZp+vys3NxaVLl9C3b1/06tVLtlwqlcqCh6oYGhrC0NAQ33//vcL1urq65epDS0tL7nT1q+tf9rrxrSqZmZmllpW8tyUB0cDAAHfu3IEgCHI1Z2ZmoqioqNR1jxU9rpMnT8LX17dUsM7Ozlb4WX+Tknpe/WVJUZtevXqVOUP96rV+RK/DcEdE5dalSxccPnwYP//8M549eyZ3V+vLzp49iyFDhshOzebl5eHixYuoX78+1NTUoK2tjQYNGiAuLg4ODg6lbmaoqFOnTsmdprt16xZSU1NlF8uX/IB/+U5KADhy5EiF9iMIQqk+jh49Knd6VhW8vb3x77//ori4GM7Ozq9t++qs38t97Nq1C4aGhqWC8rsqLy8PFy5ckDvVeerUKUgkEtl1cO7u7jhz5gzOnz+PZs2aydodP34cwIvTsG9S8h4qGjeJRFLq83jp0iU8ffpU7u7e8nJ1dYWenh6OHDmCVq1aKQyb1tbWsLKyQkJCQpmztUQVwXBHROVmbW2NRo0a4fLly6hXrx5q166tsJ2amhp+/PFHdO/eHcXFxdizZw/y8vLk7oANDAzE9OnTMWPGDHTs2BHm5ubIy8tDcnIyLl68iODg4HLXde/ePaxevRotWrTAkydPsHXrVtSoUUM2K2htbQ0LCwts3rwZgiDAwMAAFy9elLvO7U309PRQv3597N27F4aGhjA3N8eNGzfwzz//KDWj8zqtWrXCqVOnMG/ePHTt2hV169aFuro6njx5guvXr6Np06ayYGNvb49///0X//77L2rVqgUtLS3Y29uja9euOHv2LIKDg9GtWzfY29tDEASkpaXh6tWr6NGjxxuDY1UzNDTE2rVrkZaWBisrK1y+fBlHjx5Fx44dYWZmBgBo06YNIiMjsXLlSqSkpMDe3h4xMTHYtWsXGjduLHe9XVl0dXVhbm6OCxcuwN3dHQYGBrIQ7OXlhePHj8PGxgYODg6IjY3F3r17X3ta9XV0dHQwbNgwrF69GrNnz0b79u1hbGyM5ORkJCQkyO4CHj16NObNm4c5c+bA19cXNWrUQE5ODhITExEXF1fmzUREijDcEVGFtGzZEpcvXy5z1g4AOnfuDKlUivXr1yMzMxN2dnb47rvvUK9ePVkbW1tbzJ8/H3/88Qe2bt2KzMxM6Ovrw8rKqtTdt28yduxYnDhxAsuWLYNUKpU9567kVJ6Ghga+/fZbhIaGYu3atVBTU4O7uzumT5+OcePGlXs/EydOxPr167Fx40YUFxfD1dUVP/zwA3766acK1fsmampqmDp1Kv7880+cOHECu3btgrq6OmrWrIn69evL3YTg7++PjIwMrFmzBnl5ebLn3Ono6GDmzJnYvXs3/vrrL6SkpEBLSwtmZmZwd3eXuxv6XWFiYoKRI0ciPDwc9+/fh4GBAXr37i1317KWlhaCg4OxZcsW7Nu3D1lZWahRowZ69OhR6vE5rzNmzBhs3LgRCxYsgFQqlT3nLjAwEBoaGti9ezfy8/Ph6OiIr7/+Glu3blX6uNq1awdTU1Ps2bMHq1evBvDibnRfX19Zm4YNG2Lu3LnYuXMnwsLCkJOTA0NDQ9ja2sruCiYqL4nw6gOtiIheY9GiRbhz5w5WrlxZ6vRVyUOMhwwZgp49e771WkoeFjxv3jyFN4bQ+6PkIcY///xzdZdC9N7jzB0RvZFUKkVcXBzu3r2L8+fPY9iwYZW+To6IiN4O/utMRG+Unp6OH374Abq6uujQoQO6dOlS3SUREVEZeFqWiIiISET4XSZEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIsJwR0RERCQiDHdEREREIvL/AGWD60f3GURtAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>164.500000</td>\n",
       "      <td>6.553201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>4.351245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>5.270463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>3.155243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.831040</td>\n",
       "      <td>0.023095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.841504</td>\n",
       "      <td>0.026411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.895271</td>\n",
       "      <td>0.018282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.727280</td>\n",
       "      <td>0.040644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.867403</td>\n",
       "      <td>0.019681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.828968</td>\n",
       "      <td>0.023642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.817014</td>\n",
       "      <td>0.023858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.811277</td>\n",
       "      <td>0.024452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.637531</td>\n",
       "      <td>0.046488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.811440</td>\n",
       "      <td>0.025146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.811277</td>\n",
       "      <td>0.024452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       164.500000     6.553201\n",
       "1                    TN        82.400000     4.351245\n",
       "2                    FP        31.000000     5.270463\n",
       "3                    FN        19.200000     3.155243\n",
       "4              Accuracy         0.831040     0.023095\n",
       "5             Precision         0.841504     0.026411\n",
       "6           Sensitivity         0.895271     0.018282\n",
       "7           Specificity         0.727280     0.040644\n",
       "8              F1 score         0.867403     0.019681\n",
       "9   F1 score (weighted)         0.828968     0.023642\n",
       "10     F1 score (macro)         0.817014     0.023858\n",
       "11    Balanced Accuracy         0.811277     0.024452\n",
       "12                  MCC         0.637531     0.046488\n",
       "13                  NPV         0.811440     0.025146\n",
       "14              ROC_AUC         0.811277     0.024452"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>321.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>166.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>63.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>43.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.808403</td>\n",
       "      <td>0.815126</td>\n",
       "      <td>0.820168</td>\n",
       "      <td>0.833613</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.820168</td>\n",
       "      <td>0.820168</td>\n",
       "      <td>0.803361</td>\n",
       "      <td>0.821513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.823980</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.829517</td>\n",
       "      <td>0.832474</td>\n",
       "      <td>0.819753</td>\n",
       "      <td>0.838120</td>\n",
       "      <td>0.870712</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.858726</td>\n",
       "      <td>0.824289</td>\n",
       "      <td>0.836424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.877717</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.890710</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.929972</td>\n",
       "      <td>0.889197</td>\n",
       "      <td>0.863874</td>\n",
       "      <td>0.885559</td>\n",
       "      <td>0.846995</td>\n",
       "      <td>0.866848</td>\n",
       "      <td>0.882156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.738400</td>\n",
       "      <td>0.707400</td>\n",
       "      <td>0.726900</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.714900</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>0.725960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.849315</td>\n",
       "      <td>0.859025</td>\n",
       "      <td>0.867114</td>\n",
       "      <td>0.871391</td>\n",
       "      <td>0.862903</td>\n",
       "      <td>0.867280</td>\n",
       "      <td>0.858653</td>\n",
       "      <td>0.852820</td>\n",
       "      <td>0.845033</td>\n",
       "      <td>0.858353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.806082</td>\n",
       "      <td>0.814086</td>\n",
       "      <td>0.817734</td>\n",
       "      <td>0.831280</td>\n",
       "      <td>0.831246</td>\n",
       "      <td>0.826878</td>\n",
       "      <td>0.830511</td>\n",
       "      <td>0.818124</td>\n",
       "      <td>0.820521</td>\n",
       "      <td>0.801541</td>\n",
       "      <td>0.819800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.792442</td>\n",
       "      <td>0.805092</td>\n",
       "      <td>0.805383</td>\n",
       "      <td>0.822321</td>\n",
       "      <td>0.821210</td>\n",
       "      <td>0.817102</td>\n",
       "      <td>0.815924</td>\n",
       "      <td>0.805770</td>\n",
       "      <td>0.810859</td>\n",
       "      <td>0.788034</td>\n",
       "      <td>0.808414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.786876</td>\n",
       "      <td>0.802159</td>\n",
       "      <td>0.799067</td>\n",
       "      <td>0.815826</td>\n",
       "      <td>0.811625</td>\n",
       "      <td>0.812120</td>\n",
       "      <td>0.816914</td>\n",
       "      <td>0.800235</td>\n",
       "      <td>0.812144</td>\n",
       "      <td>0.783644</td>\n",
       "      <td>0.804061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.587855</td>\n",
       "      <td>0.611164</td>\n",
       "      <td>0.614589</td>\n",
       "      <td>0.649681</td>\n",
       "      <td>0.654908</td>\n",
       "      <td>0.636715</td>\n",
       "      <td>0.631897</td>\n",
       "      <td>0.614304</td>\n",
       "      <td>0.621844</td>\n",
       "      <td>0.577901</td>\n",
       "      <td>0.620086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.778300</td>\n",
       "      <td>0.784800</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.835700</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.811300</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>0.795100</td>\n",
       "      <td>0.760700</td>\n",
       "      <td>0.764400</td>\n",
       "      <td>0.796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.786876</td>\n",
       "      <td>0.802159</td>\n",
       "      <td>0.799067</td>\n",
       "      <td>0.815826</td>\n",
       "      <td>0.811625</td>\n",
       "      <td>0.812120</td>\n",
       "      <td>0.816914</td>\n",
       "      <td>0.800235</td>\n",
       "      <td>0.812144</td>\n",
       "      <td>0.783644</td>\n",
       "      <td>0.804061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  323.000000  310.000000  326.000000  323.000000   \n",
       "1                    TN  158.000000  175.000000  162.000000  173.000000   \n",
       "2                    FP   69.000000   62.000000   67.000000   65.000000   \n",
       "3                    FN   45.000000   48.000000   40.000000   34.000000   \n",
       "4              Accuracy    0.808403    0.815126    0.820168    0.833613   \n",
       "5             Precision    0.823980    0.833333    0.829517    0.832474   \n",
       "6           Sensitivity    0.877717    0.865922    0.890710    0.904762   \n",
       "7           Specificity    0.696000    0.738400    0.707400    0.726900   \n",
       "8              F1 score    0.850000    0.849315    0.859025    0.867114   \n",
       "9   F1 score (weighted)    0.806082    0.814086    0.817734    0.831280   \n",
       "10     F1 score (macro)    0.792442    0.805092    0.805383    0.822321   \n",
       "11    Balanced Accuracy    0.786876    0.802159    0.799067    0.815826   \n",
       "12                  MCC    0.587855    0.611164    0.614589    0.649681   \n",
       "13                  NPV    0.778300    0.784800    0.802000    0.835700   \n",
       "14              ROC_AUC    0.786876    0.802159    0.799067    0.815826   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   332.000000  321.000000  330.000000  325.000000  310.000000  319.000000   \n",
       "1   165.000000  172.000000  164.000000  163.000000  178.000000  159.000000   \n",
       "2    73.000000   62.000000   49.000000   65.000000   51.000000   68.000000   \n",
       "3    25.000000   40.000000   52.000000   42.000000   56.000000   49.000000   \n",
       "4     0.835294    0.828571    0.830252    0.820168    0.820168    0.803361   \n",
       "5     0.819753    0.838120    0.870712    0.833333    0.858726    0.824289   \n",
       "6     0.929972    0.889197    0.863874    0.885559    0.846995    0.866848   \n",
       "7     0.693300    0.735000    0.770000    0.714900    0.777300    0.700400   \n",
       "8     0.871391    0.862903    0.867280    0.858653    0.852820    0.845033   \n",
       "9     0.831246    0.826878    0.830511    0.818124    0.820521    0.801541   \n",
       "10    0.821210    0.817102    0.815924    0.805770    0.810859    0.788034   \n",
       "11    0.811625    0.812120    0.816914    0.800235    0.812144    0.783644   \n",
       "12    0.654908    0.636715    0.631897    0.614304    0.621844    0.577901   \n",
       "13    0.868400    0.811300    0.759300    0.795100    0.760700    0.764400   \n",
       "14    0.811625    0.812120    0.816914    0.800235    0.812144    0.783644   \n",
       "\n",
       "           ave  \n",
       "0   321.900000  \n",
       "1   166.900000  \n",
       "2    63.100000  \n",
       "3    43.100000  \n",
       "4     0.821513  \n",
       "5     0.836424  \n",
       "6     0.882156  \n",
       "7     0.725960  \n",
       "8     0.858353  \n",
       "9     0.819800  \n",
       "10    0.808414  \n",
       "11    0.804061  \n",
       "12    0.620086  \n",
       "13    0.796000  \n",
       "14    0.804061  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.830832</td>\n",
       "      <td>0.016548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.841773</td>\n",
       "      <td>0.023193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.895303</td>\n",
       "      <td>0.017305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.726504</td>\n",
       "      <td>0.044106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.867415</td>\n",
       "      <td>0.012829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.828694</td>\n",
       "      <td>0.017377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.816475</td>\n",
       "      <td>0.018995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.810904</td>\n",
       "      <td>0.020665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.637011</td>\n",
       "      <td>0.036576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.810974</td>\n",
       "      <td>0.027156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.810904</td>\n",
       "      <td>0.020665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.830832     0.016548\n",
       "1             Precision         0.841773     0.023193\n",
       "2           Sensitivity         0.895303     0.017305\n",
       "3           Specificity         0.726504     0.044106\n",
       "4              F1 score         0.867415     0.012829\n",
       "5   F1 score (weighted)         0.828694     0.017377\n",
       "6      F1 score (macro)         0.816475     0.018995\n",
       "7     Balanced Accuracy         0.810904     0.020665\n",
       "8                   MCC         0.637011     0.036576\n",
       "9                   NPV         0.810974     0.027156\n",
       "10              ROC_AUC         0.810904     0.020665"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=16,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_knn)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_knn))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_knn))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_knn))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_knn))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_knn, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_knn, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_knn))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_knn))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_knn))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model f1_score 0.8136 with a standard deviation of 0.0172\n",
      "KNN optimized model f1_score 0.8182 with a standard deviation of 0.0203\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"KNN baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn_clf.joblib']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the modesls, both the one with optimized hyperparameters and the initial one\n",
    "joblib.dump(knn_clf, \"OUTPUT/knn_clf.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn_clf.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP       165.100000     6.573516\n",
      "1                    TN        85.500000     4.527693\n",
      "2                    FP        27.900000     5.043147\n",
      "3                    FN        18.600000     4.325634\n",
      "4              Accuracy         0.843487     0.022288\n",
      "5             Precision         0.855581     0.024932\n",
      "6           Sensitivity         0.898649     0.024036\n",
      "7           Specificity         0.754520     0.039571\n",
      "8              F1 score         0.876347     0.019208\n",
      "9   F1 score (weighted)         0.842020     0.022527\n",
      "10     F1 score (macro)         0.831307     0.023024\n",
      "11    Balanced Accuracy         0.826585     0.023399\n",
      "12                  MCC         0.665372     0.045622\n",
      "13                  NPV         0.822370     0.034394\n",
      "14              ROC_AUC         0.826585     0.023399\n",
      "CPU times: user 7.26 s, sys: 3.99 ms, total: 7.26 s\n",
      "Wall time: 7.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_clf = SVC()\n",
    "    \n",
    "    svm_clf.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_clf.predict(X_test) \n",
    "   \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggestegorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVC(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average='macro')\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggestegorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "  \n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVC(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:40:27,582] A new study created in memory with name: SVM_classifier\n",
      "[I 2023-12-04 17:40:35,410] Trial 0 finished with value: 0.38192489052100653 and parameters: {'C': 0.125, 'gamma': 8.0}. Best is trial 0 with value: 0.38192489052100653.\n",
      "[I 2023-12-04 17:40:40,963] Trial 1 finished with value: 0.671028531184511 and parameters: {'C': 0.125, 'gamma': 0.0078125}. Best is trial 1 with value: 0.671028531184511.\n",
      "[I 2023-12-04 17:40:47,528] Trial 2 finished with value: 0.38192489052100653 and parameters: {'C': 0.0078125, 'gamma': 2.0}. Best is trial 1 with value: 0.671028531184511.\n",
      "[I 2023-12-04 17:40:51,591] Trial 3 finished with value: 0.8095756119165864 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 3 with value: 0.8095756119165864.\n",
      "[I 2023-12-04 17:40:57,203] Trial 4 finished with value: 0.4813467579549332 and parameters: {'C': 0.0625, 'gamma': 0.0078125}. Best is trial 3 with value: 0.8095756119165864.\n",
      "[I 2023-12-04 17:41:04,002] Trial 5 finished with value: 0.4021621806440433 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 3 with value: 0.8095756119165864.\n",
      "[I 2023-12-04 17:41:09,536] Trial 6 finished with value: 0.671028531184511 and parameters: {'C': 0.125, 'gamma': 0.0078125}. Best is trial 3 with value: 0.8095756119165864.\n",
      "[I 2023-12-04 17:41:15,435] Trial 7 finished with value: 0.6625589986993007 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 3 with value: 0.8095756119165864.\n",
      "[I 2023-12-04 17:41:22,013] Trial 8 finished with value: 0.38192489052100653 and parameters: {'C': 0.03125, 'gamma': 4.0}. Best is trial 3 with value: 0.8095756119165864.\n",
      "[I 2023-12-04 17:41:28,421] Trial 9 finished with value: 0.8428953030073378 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:41:34,629] Trial 10 finished with value: 0.38192489052100653 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:41:38,681] Trial 11 finished with value: 0.8095756119165864 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:41:42,621] Trial 12 finished with value: 0.8121189415347609 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:41:46,557] Trial 13 finished with value: 0.8121189415347609 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:41:53,289] Trial 14 finished with value: 0.824689064603217 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:41:59,940] Trial 15 finished with value: 0.8140547933875293 and parameters: {'C': 1.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:42:05,612] Trial 16 finished with value: 0.38192489052100653 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:42:12,211] Trial 17 finished with value: 0.4370318728497578 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:42:16,261] Trial 18 finished with value: 0.8190758703950793 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:42:20,161] Trial 19 finished with value: 0.8216643978978663 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:42:25,836] Trial 20 finished with value: 0.38192489052100653 and parameters: {'C': 0.25, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:42:29,730] Trial 21 finished with value: 0.8216643978978663 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:42:36,453] Trial 22 finished with value: 0.824689064603217 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:42:43,170] Trial 23 finished with value: 0.824689064603217 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:42:48,920] Trial 24 finished with value: 0.8127874045442599 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:42:55,657] Trial 25 finished with value: 0.824689064603217 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:43:02,381] Trial 26 finished with value: 0.4355746349110593 and parameters: {'C': 0.5, 'gamma': 0.125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:43:09,267] Trial 27 finished with value: 0.44499544379917033 and parameters: {'C': 1.0, 'gamma': 0.25}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:43:15,591] Trial 28 finished with value: 0.38192489052100653 and parameters: {'C': 0.015625, 'gamma': 1.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:43:21,367] Trial 29 finished with value: 0.38192489052100653 and parameters: {'C': 0.0625, 'gamma': 0.0001220703125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:43:27,143] Trial 30 finished with value: 0.38192489052100653 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:43:33,725] Trial 31 finished with value: 0.824689064603217 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:43:41,291] Trial 32 finished with value: 0.4021621806440433 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:43:47,849] Trial 33 finished with value: 0.824689064603217 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:43:54,431] Trial 34 finished with value: 0.824689064603217 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:44:00,865] Trial 35 finished with value: 0.38192489052100653 and parameters: {'C': 0.03125, 'gamma': 2.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:44:07,129] Trial 36 finished with value: 0.8404380720209825 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:44:13,402] Trial 37 finished with value: 0.8404380720209825 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:44:19,655] Trial 38 finished with value: 0.8404380720209825 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:44:25,908] Trial 39 finished with value: 0.8404380720209825 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:44:32,164] Trial 40 finished with value: 0.8404380720209825 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:44:38,410] Trial 41 finished with value: 0.8404380720209825 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:44:44,663] Trial 42 finished with value: 0.8404380720209825 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:44:50,917] Trial 43 finished with value: 0.8404380720209825 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:44:57,170] Trial 44 finished with value: 0.8404380720209825 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:45:03,421] Trial 45 finished with value: 0.8399239808831481 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:45:09,067] Trial 46 finished with value: 0.38192489052100653 and parameters: {'C': 0.125, 'gamma': 6.103515625e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:45:15,313] Trial 47 finished with value: 0.8404380720209825 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:45:20,743] Trial 48 finished with value: 0.8267473430088412 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:45:24,808] Trial 49 finished with value: 0.8248452593299248 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8429\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_classifier\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP  328.000000\n",
      "1                    TN  164.000000\n",
      "2                    FP   63.000000\n",
      "3                    FN   40.000000\n",
      "4              Accuracy    0.826891\n",
      "5             Precision    0.838875\n",
      "6           Sensitivity    0.891304\n",
      "7           Specificity    0.722500\n",
      "8              F1 score    0.864295\n",
      "9   F1 score (weighted)    0.824895\n",
      "10     F1 score (macro)    0.812658\n",
      "11    Balanced Accuracy    0.806886\n",
      "12                  MCC    0.628116\n",
      "13                  NPV    0.803900\n",
      "14              ROC_AUC    0.806886\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_svm_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_svm_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_svm_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_svm_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_svm_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_svm_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_svm_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_svm_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_svm_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_svm_0)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:45:32,651] Trial 50 finished with value: 0.40309695827455494 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:45:39,317] Trial 51 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:45:45,981] Trial 52 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:45:51,985] Trial 53 finished with value: 0.7401922402205867 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:45:56,562] Trial 54 finished with value: 0.7635947702693132 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:46:03,183] Trial 55 finished with value: 0.42489235479072374 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:46:08,249] Trial 56 finished with value: 0.7281789545315569 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:46:12,448] Trial 57 finished with value: 0.79851416186831 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:46:18,695] Trial 58 finished with value: 0.38352828236137076 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:46:24,640] Trial 59 finished with value: 0.38352828236137076 and parameters: {'C': 0.0625, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:46:31,209] Trial 60 finished with value: 0.6289720846519209 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:46:37,866] Trial 61 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:46:44,523] Trial 62 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:46:51,131] Trial 63 finished with value: 0.45114501738098556 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:46:57,454] Trial 64 finished with value: 0.38352828236137076 and parameters: {'C': 0.015625, 'gamma': 1.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:47:02,578] Trial 65 finished with value: 0.7293888170216828 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:47:07,481] Trial 66 finished with value: 0.8368102946146848 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:47:14,136] Trial 67 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:47:20,324] Trial 68 finished with value: 0.38352828236137076 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:47:26,834] Trial 69 finished with value: 0.388258558179827 and parameters: {'C': 0.5, 'gamma': 2.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:47:34,194] Trial 70 finished with value: 0.38352828236137076 and parameters: {'C': 0.125, 'gamma': 8.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:47:40,841] Trial 71 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:47:47,496] Trial 72 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:47:54,158] Trial 73 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:00,816] Trial 74 finished with value: 0.8237427292417114 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:05,128] Trial 75 finished with value: 0.7969603419042753 and parameters: {'C': 2.0, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:10,194] Trial 76 finished with value: 0.7293888170216828 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:16,733] Trial 77 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:20,890] Trial 78 finished with value: 0.832950493700524 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:26,237] Trial 79 finished with value: 0.7122302175074982 and parameters: {'C': 0.25, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:33,000] Trial 80 finished with value: 0.40309695827455494 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:39,529] Trial 81 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:46,190] Trial 82 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:52,884] Trial 83 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:48:59,678] Trial 84 finished with value: 0.42489235479072374 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:49:05,654] Trial 85 finished with value: 0.38352828236137076 and parameters: {'C': 0.0625, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:49:11,165] Trial 86 finished with value: 0.38352828236137076 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:49:17,846] Trial 87 finished with value: 0.8242172187744983 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:49:24,538] Trial 88 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:49:30,477] Trial 89 finished with value: 0.38352828236137076 and parameters: {'C': 0.015625, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:49:36,028] Trial 90 finished with value: 0.6598997506646785 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:49:42,585] Trial 91 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:49:49,113] Trial 92 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:49:55,689] Trial 93 finished with value: 0.6289720846519209 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:50:02,221] Trial 94 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:50:08,951] Trial 95 finished with value: 0.4145749625920526 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:50:14,896] Trial 96 finished with value: 0.38352828236137076 and parameters: {'C': 0.03125, 'gamma': 0.0001220703125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:50:21,593] Trial 97 finished with value: 0.8237801584983535 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:50:27,755] Trial 98 finished with value: 0.540893481517252 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:50:32,968] Trial 99 finished with value: 0.8126840924040785 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8429\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP  328.000000  319.000000\n",
      "1                    TN  164.000000  181.000000\n",
      "2                    FP   63.000000   56.000000\n",
      "3                    FN   40.000000   39.000000\n",
      "4              Accuracy    0.826891    0.840336\n",
      "5             Precision    0.838875    0.850667\n",
      "6           Sensitivity    0.891304    0.891061\n",
      "7           Specificity    0.722500    0.763700\n",
      "8              F1 score    0.864295    0.870396\n",
      "9   F1 score (weighted)    0.824895    0.839218\n",
      "10     F1 score (macro)    0.812658    0.831259\n",
      "11    Balanced Accuracy    0.806886    0.827387\n",
      "12                  MCC    0.628116    0.664019\n",
      "13                  NPV    0.803900    0.822700\n",
      "14              ROC_AUC    0.806886    0.827387\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_svm_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_svm_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_svm_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_svm_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_svm_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_svm_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_svm_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_svm_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_svm_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_svm_1)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:50:40,959] Trial 100 finished with value: 0.46012554289283125 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:50:47,394] Trial 101 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:50:53,852] Trial 102 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:51:00,368] Trial 103 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:51:08,494] Trial 104 finished with value: 0.4074396451434915 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:51:15,482] Trial 105 finished with value: 0.41193471330424003 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:51:21,735] Trial 106 finished with value: 0.8299650592102823 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:51:25,578] Trial 107 finished with value: 0.8111690870795965 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:51:32,154] Trial 108 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:51:37,695] Trial 109 finished with value: 0.38228491374828766 and parameters: {'C': 0.25, 'gamma': 6.103515625e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:51:44,245] Trial 110 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:51:50,916] Trial 111 finished with value: 0.8282057358885776 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:51:57,472] Trial 112 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:01,821] Trial 113 finished with value: 0.8194237050267648 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:05,849] Trial 114 finished with value: 0.8200875401562241 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:12,380] Trial 115 finished with value: 0.8289936557419306 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:18,401] Trial 116 finished with value: 0.38228491374828766 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:25,014] Trial 117 finished with value: 0.38228491374828766 and parameters: {'C': 0.0625, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:32,211] Trial 118 finished with value: 0.4074396451434915 and parameters: {'C': 64.0, 'gamma': 4.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:38,784] Trial 119 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:44,349] Trial 120 finished with value: 0.38228491374828766 and parameters: {'C': 0.015625, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:49,124] Trial 121 finished with value: 0.8198670312706637 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:53,899] Trial 122 finished with value: 0.8198670312706637 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:52:59,919] Trial 123 finished with value: 0.822299611649885 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:53:05,429] Trial 124 finished with value: 0.38228491374828766 and parameters: {'C': 1.0, 'gamma': 3.0517578125e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:53:10,778] Trial 125 finished with value: 0.4903068748841631 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:53:17,319] Trial 126 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:53:22,125] Trial 127 finished with value: 0.7612906999012803 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:53:27,969] Trial 128 finished with value: 0.38228491374828766 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:53:34,927] Trial 129 finished with value: 0.6268630363717921 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:53:41,344] Trial 130 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:53:48,255] Trial 131 finished with value: 0.4315231328972482 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:53:52,534] Trial 132 finished with value: 0.8229800889312922 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:53:56,803] Trial 133 finished with value: 0.8229800889312922 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:54:02,085] Trial 134 finished with value: 0.6683556458679505 and parameters: {'C': 0.125, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:54:07,182] Trial 135 finished with value: 0.6573873842545264 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:54:12,756] Trial 136 finished with value: 0.800947183947225 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:54:19,650] Trial 137 finished with value: 0.46012554289283125 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:54:26,091] Trial 138 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:54:32,007] Trial 139 finished with value: 0.821536931019865 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:54:40,128] Trial 140 finished with value: 0.4074396451434915 and parameters: {'C': 2.0, 'gamma': 8.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:54:46,373] Trial 141 finished with value: 0.8299650592102823 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:54:52,042] Trial 142 finished with value: 0.7317439113926871 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:54:58,559] Trial 143 finished with value: 0.829181350283946 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:55:04,822] Trial 144 finished with value: 0.8299650592102823 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:55:11,128] Trial 145 finished with value: 0.8299650592102823 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:55:15,326] Trial 146 finished with value: 0.8002241899353372 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:55:21,797] Trial 147 finished with value: 0.8279262854205752 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:55:28,838] Trial 148 finished with value: 0.41193471330424003 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:55:35,580] Trial 149 finished with value: 0.7881124743477829 and parameters: {'C': 1.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8429\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP  328.000000  319.000000  324.000000\n",
      "1                    TN  164.000000  181.000000  169.000000\n",
      "2                    FP   63.000000   56.000000   60.000000\n",
      "3                    FN   40.000000   39.000000   42.000000\n",
      "4              Accuracy    0.826891    0.840336    0.828571\n",
      "5             Precision    0.838875    0.850667    0.843750\n",
      "6           Sensitivity    0.891304    0.891061    0.885246\n",
      "7           Specificity    0.722500    0.763700    0.738000\n",
      "8              F1 score    0.864295    0.870396    0.864000\n",
      "9   F1 score (weighted)    0.824895    0.839218    0.827122\n",
      "10     F1 score (macro)    0.812658    0.831259    0.816091\n",
      "11    Balanced Accuracy    0.806886    0.827387    0.811619\n",
      "12                  MCC    0.628116    0.664019    0.633877\n",
      "13                  NPV    0.803900    0.822700    0.800900\n",
      "14              ROC_AUC    0.806886    0.827387    0.811619\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_svm_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_svm_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_svm_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_svm_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_svm_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_svm_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_svm_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_svm_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_svm_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_svm_2)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:55:43,218] Trial 150 finished with value: 0.8279641380281199 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:55:49,757] Trial 151 finished with value: 0.8279641380281199 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:55:56,294] Trial 152 finished with value: 0.8279641380281199 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:56:02,064] Trial 153 finished with value: 0.38371792981115166 and parameters: {'C': 2.0, 'gamma': 6.103515625e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:56:08,293] Trial 154 finished with value: 0.38371792981115166 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:56:15,088] Trial 155 finished with value: 0.8226435608932325 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:56:20,047] Trial 156 finished with value: 0.8058601704891025 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:56:23,829] Trial 157 finished with value: 0.8167647645027614 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:56:30,062] Trial 158 finished with value: 0.4423826076590361 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:56:37,401] Trial 159 finished with value: 0.4148925053746321 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:56:44,034] Trial 160 finished with value: 0.8226543216025975 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:56:50,689] Trial 161 finished with value: 0.8226543216025975 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:56:57,244] Trial 162 finished with value: 0.8279641380281199 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:57:03,786] Trial 163 finished with value: 0.8279641380281199 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:57:10,325] Trial 164 finished with value: 0.8279641380281199 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:57:17,451] Trial 165 finished with value: 0.4489517252157646 and parameters: {'C': 2.0, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:57:23,371] Trial 166 finished with value: 0.38371792981115166 and parameters: {'C': 0.015625, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:57:29,043] Trial 167 finished with value: 0.38371792981115166 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:57:35,766] Trial 168 finished with value: 0.8226435608932325 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:57:41,905] Trial 169 finished with value: 0.38371792981115166 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:57:48,564] Trial 170 finished with value: 0.8226435608932325 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:57:55,017] Trial 171 finished with value: 0.8279641380281199 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:57:59,583] Trial 172 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:05,056] Trial 173 finished with value: 0.6620325738137764 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:09,610] Trial 174 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:15,263] Trial 175 finished with value: 0.38371792981115166 and parameters: {'C': 0.125, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:19,877] Trial 176 finished with value: 0.8184418852525985 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:25,407] Trial 177 finished with value: 0.8253655506482194 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:31,026] Trial 178 finished with value: 0.8217367320862884 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:35,663] Trial 179 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:40,710] Trial 180 finished with value: 0.8079006905968308 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:45,353] Trial 181 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:50,011] Trial 182 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:54,667] Trial 183 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:58:59,231] Trial 184 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:04,723] Trial 185 finished with value: 0.8253655506482194 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:10,377] Trial 186 finished with value: 0.8135347927137069 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:15,848] Trial 187 finished with value: 0.8253655506482194 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:21,310] Trial 188 finished with value: 0.8253655506482194 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:26,577] Trial 189 finished with value: 0.7677657085874209 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:32,317] Trial 190 finished with value: 0.8212622215821217 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:36,856] Trial 191 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:41,432] Trial 192 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:45,987] Trial 193 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:50,566] Trial 194 finished with value: 0.8302223490605629 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 17:59:57,686] Trial 195 finished with value: 0.6255429198015736 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:00:04,847] Trial 196 finished with value: 0.4379827035961171 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:00:10,359] Trial 197 finished with value: 0.6622223047672746 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:00:14,489] Trial 198 finished with value: 0.8166995943296784 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:00:19,187] Trial 199 finished with value: 0.8184418852525985 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8429\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP  328.000000  319.000000  324.000000  327.000000\n",
      "1                    TN  164.000000  181.000000  169.000000  172.000000\n",
      "2                    FP   63.000000   56.000000   60.000000   66.000000\n",
      "3                    FN   40.000000   39.000000   42.000000   30.000000\n",
      "4              Accuracy    0.826891    0.840336    0.828571    0.838655\n",
      "5             Precision    0.838875    0.850667    0.843750    0.832061\n",
      "6           Sensitivity    0.891304    0.891061    0.885246    0.915966\n",
      "7           Specificity    0.722500    0.763700    0.738000    0.722700\n",
      "8              F1 score    0.864295    0.870396    0.864000    0.872000\n",
      "9   F1 score (weighted)    0.824895    0.839218    0.827122    0.835927\n",
      "10     F1 score (macro)    0.812658    0.831259    0.816091    0.826909\n",
      "11    Balanced Accuracy    0.806886    0.827387    0.811619    0.819328\n",
      "12                  MCC    0.628116    0.664019    0.633877    0.660720\n",
      "13                  NPV    0.803900    0.822700    0.800900    0.851500\n",
      "14              ROC_AUC    0.806886    0.827387    0.811619    0.819328\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_svm_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_svm_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_svm_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_svm_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_svm_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_svm_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_svm_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_svm_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_svm_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_svm_3)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 18:00:26,117] Trial 200 finished with value: 0.8222868980344817 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:00:30,753] Trial 201 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:00:35,398] Trial 202 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:00:40,032] Trial 203 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:00:48,020] Trial 204 finished with value: 0.4119920735244115 and parameters: {'C': 2.0, 'gamma': 8.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:00:54,029] Trial 205 finished with value: 0.8212934099319298 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:00:59,459] Trial 206 finished with value: 0.3836930519646608 and parameters: {'C': 0.0078125, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:06,062] Trial 207 finished with value: 0.45784852566401985 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:12,823] Trial 208 finished with value: 0.4165910937124463 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:19,352] Trial 209 finished with value: 0.7929719411615848 and parameters: {'C': 2.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:23,673] Trial 210 finished with value: 0.7776453823259691 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:28,297] Trial 211 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:32,923] Trial 212 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:37,547] Trial 213 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:43,107] Trial 214 finished with value: 0.47741195810555687 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:47,755] Trial 215 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:53,472] Trial 216 finished with value: 0.8098446666176612 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:01:57,621] Trial 217 finished with value: 0.8053166844349324 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:03,478] Trial 218 finished with value: 0.8222868980344817 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:10,475] Trial 219 finished with value: 0.4119920735244115 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:17,041] Trial 220 finished with value: 0.3836930519646608 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:21,673] Trial 221 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:26,305] Trial 222 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:30,945] Trial 223 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:35,700] Trial 224 finished with value: 0.7522305392561139 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:42,263] Trial 225 finished with value: 0.823055537262064 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:47,885] Trial 226 finished with value: 0.3836930519646608 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:53,386] Trial 227 finished with value: 0.3836930519646608 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:02:59,636] Trial 228 finished with value: 0.8194800956791044 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:04,954] Trial 229 finished with value: 0.478411925347386 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:10,833] Trial 230 finished with value: 0.8222868980344817 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:15,477] Trial 231 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:20,200] Trial 232 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:24,907] Trial 233 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:30,790] Trial 234 finished with value: 0.5375282117856173 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:35,506] Trial 235 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:42,053] Trial 236 finished with value: 0.8235366153252801 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:47,069] Trial 237 finished with value: 0.7118274771044943 and parameters: {'C': 2.0, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:53,047] Trial 238 finished with value: 0.8222868980344817 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:03:59,854] Trial 239 finished with value: 0.4323470620362408 and parameters: {'C': 0.5, 'gamma': 0.125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:04:06,480] Trial 240 finished with value: 0.8256176138187472 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:04:11,205] Trial 241 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:04:15,916] Trial 242 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:04:20,629] Trial 243 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:04:25,340] Trial 244 finished with value: 0.8206632932622784 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:04:30,605] Trial 245 finished with value: 0.6521539273487409 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:04:37,608] Trial 246 finished with value: 0.43131187801266896 and parameters: {'C': 2.0, 'gamma': 1.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:04:44,162] Trial 247 finished with value: 0.8261230473919243 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:04:49,213] Trial 248 finished with value: 0.7355050198289975 and parameters: {'C': 0.25, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:04:55,178] Trial 249 finished with value: 0.8222868980344817 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8429\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  328.000000  319.000000  324.000000  327.000000   \n",
      "1                    TN  164.000000  181.000000  169.000000  172.000000   \n",
      "2                    FP   63.000000   56.000000   60.000000   66.000000   \n",
      "3                    FN   40.000000   39.000000   42.000000   30.000000   \n",
      "4              Accuracy    0.826891    0.840336    0.828571    0.838655   \n",
      "5             Precision    0.838875    0.850667    0.843750    0.832061   \n",
      "6           Sensitivity    0.891304    0.891061    0.885246    0.915966   \n",
      "7           Specificity    0.722500    0.763700    0.738000    0.722700   \n",
      "8              F1 score    0.864295    0.870396    0.864000    0.872000   \n",
      "9   F1 score (weighted)    0.824895    0.839218    0.827122    0.835927   \n",
      "10     F1 score (macro)    0.812658    0.831259    0.816091    0.826909   \n",
      "11    Balanced Accuracy    0.806886    0.827387    0.811619    0.819328   \n",
      "12                  MCC    0.628116    0.664019    0.633877    0.660720   \n",
      "13                  NPV    0.803900    0.822700    0.800900    0.851500   \n",
      "14              ROC_AUC    0.806886    0.827387    0.811619    0.819328   \n",
      "\n",
      "          Set4  \n",
      "0   336.000000  \n",
      "1   181.000000  \n",
      "2    57.000000  \n",
      "3    21.000000  \n",
      "4     0.868908  \n",
      "5     0.854962  \n",
      "6     0.941176  \n",
      "7     0.760500  \n",
      "8     0.896000  \n",
      "9     0.866691  \n",
      "10    0.859364  \n",
      "11    0.850840  \n",
      "12    0.725922  \n",
      "13    0.896000  \n",
      "14    0.850840  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_svm_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_svm_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_svm_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_svm_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_svm_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_svm_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_svm_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_svm_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_svm_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_svm_4)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 18:05:02,922] Trial 250 finished with value: 0.8076960168192852 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:05:10,061] Trial 251 finished with value: 0.45926588859122763 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:05:14,977] Trial 252 finished with value: 0.8124509387021364 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:05:21,668] Trial 253 finished with value: 0.8034902779357171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:05:29,984] Trial 254 finished with value: 0.41025580891342434 and parameters: {'C': 1.0, 'gamma': 8.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:05:34,896] Trial 255 finished with value: 0.8124509387021364 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:05:41,773] Trial 256 finished with value: 0.3831294690710251 and parameters: {'C': 0.0078125, 'gamma': 2.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:05:48,472] Trial 257 finished with value: 0.8034902779357171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:05:52,283] Trial 258 finished with value: 0.8090338950076227 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:05:59,177] Trial 259 finished with value: 0.3831294690710251 and parameters: {'C': 0.0625, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:04,928] Trial 260 finished with value: 0.8142444899865169 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:11,536] Trial 261 finished with value: 0.8076960168192852 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:17,039] Trial 262 finished with value: 0.8117647433901689 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:21,028] Trial 263 finished with value: 0.7982106686440603 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:27,671] Trial 264 finished with value: 0.8076960168192852 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:33,216] Trial 265 finished with value: 0.8117647433901689 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:37,073] Trial 266 finished with value: 0.8023435199857291 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:43,604] Trial 267 finished with value: 0.3831294690710251 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:47,752] Trial 268 finished with value: 0.8157407938280228 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:54,902] Trial 269 finished with value: 0.41025580891342434 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:06:59,661] Trial 270 finished with value: 0.8124509387021364 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:07:06,130] Trial 271 finished with value: 0.8034902779357171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:07:13,226] Trial 272 finished with value: 0.44056020219601655 and parameters: {'C': 2.0, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:07:19,040] Trial 273 finished with value: 0.3831294690710251 and parameters: {'C': 0.125, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:07:24,631] Trial 274 finished with value: 0.3831294690710251 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:07:31,180] Trial 275 finished with value: 0.8034902779357171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:07:36,069] Trial 276 finished with value: 0.81287676087861 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:07:42,047] Trial 277 finished with value: 0.7984737350418099 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:07:46,528] Trial 278 finished with value: 0.7565101447251881 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:07:51,384] Trial 279 finished with value: 0.8124509387021364 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:07:57,892] Trial 280 finished with value: 0.8034902779357171 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:01,966] Trial 281 finished with value: 0.7859032614682289 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:06,735] Trial 282 finished with value: 0.7189453371136982 and parameters: {'C': 2.0, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:12,108] Trial 283 finished with value: 0.8117647433901689 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:18,543] Trial 284 finished with value: 0.8076960168192852 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:23,917] Trial 285 finished with value: 0.8117647433901689 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:30,975] Trial 286 finished with value: 0.6202933417391074 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:36,920] Trial 287 finished with value: 0.7251618014470717 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:44,120] Trial 288 finished with value: 0.4333141739388613 and parameters: {'C': 2.0, 'gamma': 1.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:49,471] Trial 289 finished with value: 0.8117647433901689 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:54,911] Trial 290 finished with value: 0.49840964358873185 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:08:58,913] Trial 291 finished with value: 0.8176099625820893 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:09:05,351] Trial 292 finished with value: 0.8076960168192852 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:09:12,133] Trial 293 finished with value: 0.45926588859122763 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:09:16,872] Trial 294 finished with value: 0.81287676087861 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:09:23,337] Trial 295 finished with value: 0.8076960168192852 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:09:28,948] Trial 296 finished with value: 0.811325550281965 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:09:35,245] Trial 297 finished with value: 0.3831294690710251 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:09:43,392] Trial 298 finished with value: 0.41025580891342434 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:09:48,957] Trial 299 finished with value: 0.49195662774215815 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8429\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  328.000000  319.000000  324.000000  327.000000   \n",
      "1                    TN  164.000000  181.000000  169.000000  172.000000   \n",
      "2                    FP   63.000000   56.000000   60.000000   66.000000   \n",
      "3                    FN   40.000000   39.000000   42.000000   30.000000   \n",
      "4              Accuracy    0.826891    0.840336    0.828571    0.838655   \n",
      "5             Precision    0.838875    0.850667    0.843750    0.832061   \n",
      "6           Sensitivity    0.891304    0.891061    0.885246    0.915966   \n",
      "7           Specificity    0.722500    0.763700    0.738000    0.722700   \n",
      "8              F1 score    0.864295    0.870396    0.864000    0.872000   \n",
      "9   F1 score (weighted)    0.824895    0.839218    0.827122    0.835927   \n",
      "10     F1 score (macro)    0.812658    0.831259    0.816091    0.826909   \n",
      "11    Balanced Accuracy    0.806886    0.827387    0.811619    0.819328   \n",
      "12                  MCC    0.628116    0.664019    0.633877    0.660720   \n",
      "13                  NPV    0.803900    0.822700    0.800900    0.851500   \n",
      "14              ROC_AUC    0.806886    0.827387    0.811619    0.819328   \n",
      "\n",
      "          Set4        Set5  \n",
      "0   336.000000  326.000000  \n",
      "1   181.000000  181.000000  \n",
      "2    57.000000   53.000000  \n",
      "3    21.000000   35.000000  \n",
      "4     0.868908    0.852101  \n",
      "5     0.854962    0.860158  \n",
      "6     0.941176    0.903047  \n",
      "7     0.760500    0.773500  \n",
      "8     0.896000    0.881081  \n",
      "9     0.866691    0.850942  \n",
      "10    0.859364    0.842763  \n",
      "11    0.850840    0.838276  \n",
      "12    0.725922    0.687252  \n",
      "13    0.896000    0.838000  \n",
      "14    0.850840    0.838276  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_svm_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_svm_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_svm_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_svm_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_svm_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_svm_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_svm_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_svm_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_svm_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_svm_5)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 18:09:56,337] Trial 300 finished with value: 0.8100784746364791 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:02,434] Trial 301 finished with value: 0.8254442748185762 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:08,860] Trial 302 finished with value: 0.4148927703218172 and parameters: {'C': 2.0, 'gamma': 2.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:12,847] Trial 303 finished with value: 0.8077392602893099 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:18,697] Trial 304 finished with value: 0.8201215544999279 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:24,782] Trial 305 finished with value: 0.822654582725594 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:30,092] Trial 306 finished with value: 0.5136075348254189 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:34,872] Trial 307 finished with value: 0.8257229463850978 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:40,947] Trial 308 finished with value: 0.8244582963002905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:46,597] Trial 309 finished with value: 0.3796042985937224 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:53,360] Trial 310 finished with value: 0.4137287829359197 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:10:59,210] Trial 311 finished with value: 0.3796042985937224 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:03,244] Trial 312 finished with value: 0.8183373712056807 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:08,682] Trial 313 finished with value: 0.6661215328892831 and parameters: {'C': 0.125, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:15,304] Trial 314 finished with value: 0.43878410463858886 and parameters: {'C': 2.0, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:19,986] Trial 315 finished with value: 0.8222295369121086 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:26,057] Trial 316 finished with value: 0.8244582963002905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:32,045] Trial 317 finished with value: 0.8227305849476977 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:37,303] Trial 318 finished with value: 0.6570890438491432 and parameters: {'C': 0.5, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:43,403] Trial 319 finished with value: 0.822654582725594 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:48,784] Trial 320 finished with value: 0.3796042985937224 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:54,005] Trial 321 finished with value: 0.6598262095591791 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:11:59,765] Trial 322 finished with value: 0.8201125542495593 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:12:05,828] Trial 323 finished with value: 0.8244582963002905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:12:10,751] Trial 324 finished with value: 0.7147645159687693 and parameters: {'C': 2.0, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:12:15,978] Trial 325 finished with value: 0.7658577061911598 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:12:22,199] Trial 326 finished with value: 0.8244582963002905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:12:28,885] Trial 327 finished with value: 0.6457080771732557 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:12:34,943] Trial 328 finished with value: 0.8212847942047388 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:12:41,221] Trial 329 finished with value: 0.8244582963002905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:12:48,027] Trial 330 finished with value: 0.43112442104974447 and parameters: {'C': 2.0, 'gamma': 1.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:12:53,587] Trial 331 finished with value: 0.8273511778932846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:12:59,673] Trial 332 finished with value: 0.8207791295277111 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:13:05,120] Trial 333 finished with value: 0.5117529839802448 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:13:09,497] Trial 334 finished with value: 0.8217052832777384 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:13:15,344] Trial 335 finished with value: 0.3796042985937224 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:13:21,721] Trial 336 finished with value: 0.8258507380294315 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:13:26,624] Trial 337 finished with value: 0.8257229463850978 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:13:34,163] Trial 338 finished with value: 0.3796042985937224 and parameters: {'C': 0.0625, 'gamma': 8.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:13:40,910] Trial 339 finished with value: 0.463392101456557 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:13:47,116] Trial 340 finished with value: 0.8244582963002905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:13:53,032] Trial 341 finished with value: 0.8201125542495593 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:13:59,395] Trial 342 finished with value: 0.8250336639309381 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:14:06,234] Trial 343 finished with value: 0.4148927703218172 and parameters: {'C': 2.0, 'gamma': 2.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:14:10,448] Trial 344 finished with value: 0.8077392602893099 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:14:16,866] Trial 345 finished with value: 0.3796042985937224 and parameters: {'C': 0.015625, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:14:21,775] Trial 346 finished with value: 0.8257229463850978 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:14:28,005] Trial 347 finished with value: 0.8244582963002905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:14:32,903] Trial 348 finished with value: 0.8257229463850978 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:14:38,323] Trial 349 finished with value: 0.5136075348254189 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 9 with value: 0.8428953030073378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8429\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  328.000000  319.000000  324.000000  327.000000   \n",
      "1                    TN  164.000000  181.000000  169.000000  172.000000   \n",
      "2                    FP   63.000000   56.000000   60.000000   66.000000   \n",
      "3                    FN   40.000000   39.000000   42.000000   30.000000   \n",
      "4              Accuracy    0.826891    0.840336    0.828571    0.838655   \n",
      "5             Precision    0.838875    0.850667    0.843750    0.832061   \n",
      "6           Sensitivity    0.891304    0.891061    0.885246    0.915966   \n",
      "7           Specificity    0.722500    0.763700    0.738000    0.722700   \n",
      "8              F1 score    0.864295    0.870396    0.864000    0.872000   \n",
      "9   F1 score (weighted)    0.824895    0.839218    0.827122    0.835927   \n",
      "10     F1 score (macro)    0.812658    0.831259    0.816091    0.826909   \n",
      "11    Balanced Accuracy    0.806886    0.827387    0.811619    0.819328   \n",
      "12                  MCC    0.628116    0.664019    0.633877    0.660720   \n",
      "13                  NPV    0.803900    0.822700    0.800900    0.851500   \n",
      "14              ROC_AUC    0.806886    0.827387    0.811619    0.819328   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0   336.000000  326.000000  340.000000  \n",
      "1   181.000000  181.000000  163.000000  \n",
      "2    57.000000   53.000000   50.000000  \n",
      "3    21.000000   35.000000   42.000000  \n",
      "4     0.868908    0.852101    0.845378  \n",
      "5     0.854962    0.860158    0.871795  \n",
      "6     0.941176    0.903047    0.890052  \n",
      "7     0.760500    0.773500    0.765300  \n",
      "8     0.896000    0.881081    0.880829  \n",
      "9     0.866691    0.850942    0.844700  \n",
      "10    0.859364    0.842763    0.830367  \n",
      "11    0.850840    0.838276    0.827655  \n",
      "12    0.725922    0.687252    0.661088  \n",
      "13    0.896000    0.838000    0.795100  \n",
      "14    0.850840    0.838276    0.827655  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_svm_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_svm_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_svm_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_svm_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_svm_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_svm_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_svm_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_svm_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_svm_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_svm_6)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 18:14:45,888] Trial 350 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:14:52,089] Trial 351 finished with value: 0.38213913629750335 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:14:58,214] Trial 352 finished with value: 0.5476536463146766 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:15:04,765] Trial 353 finished with value: 0.8279480780144043 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:15:11,209] Trial 354 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:15:17,015] Trial 355 finished with value: 0.8023319824304247 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:15:23,124] Trial 356 finished with value: 0.8265566015836125 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:15:29,663] Trial 357 finished with value: 0.8279480780144043 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:15:36,073] Trial 358 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:15:42,630] Trial 359 finished with value: 0.8266563782789855 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:15:49,144] Trial 360 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:15:55,657] Trial 361 finished with value: 0.8279480780144043 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:16:02,062] Trial 362 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:16:08,580] Trial 363 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:16:15,133] Trial 364 finished with value: 0.8265995287711867 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:16:21,673] Trial 365 finished with value: 0.8279480780144043 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:16:27,551] Trial 366 finished with value: 0.7389500250224909 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:16:33,972] Trial 367 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:16:40,481] Trial 368 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:16:47,005] Trial 369 finished with value: 0.8279480780144043 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:16:53,421] Trial 370 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:16:59,961] Trial 371 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:17:06,479] Trial 372 finished with value: 0.8279480780144043 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:17:12,896] Trial 373 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:17:19,414] Trial 374 finished with value: 0.8271481626446515 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:17:26,000] Trial 375 finished with value: 0.8279480780144043 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:17:32,500] Trial 376 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:17:38,883] Trial 377 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:17:45,370] Trial 378 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:17:51,750] Trial 379 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:17:58,257] Trial 380 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:18:04,639] Trial 381 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:18:10,759] Trial 382 finished with value: 0.38213913629750335 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:18:17,132] Trial 383 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:18:23,091] Trial 384 finished with value: 0.8265566015836125 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:18:28,912] Trial 385 finished with value: 0.45220848701381894 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:18:35,399] Trial 386 finished with value: 0.8265995287711867 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:18:41,821] Trial 387 finished with value: 0.8266563782789855 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:18:48,073] Trial 388 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:18:54,433] Trial 389 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:19:00,866] Trial 390 finished with value: 0.8265995287711867 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:19:06,865] Trial 391 finished with value: 0.38213913629750335 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:19:13,350] Trial 392 finished with value: 0.8265995287711867 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:19:19,717] Trial 393 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:19:25,995] Trial 394 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:19:32,373] Trial 395 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:19:38,413] Trial 396 finished with value: 0.38213913629750335 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:19:44,696] Trial 397 finished with value: 0.8334296579409186 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:19:50,491] Trial 398 finished with value: 0.5476536463146766 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:19:56,167] Trial 399 finished with value: 0.8023319824304247 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8429\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  328.000000  319.000000  324.000000  327.000000   \n",
      "1                    TN  164.000000  181.000000  169.000000  172.000000   \n",
      "2                    FP   63.000000   56.000000   60.000000   66.000000   \n",
      "3                    FN   40.000000   39.000000   42.000000   30.000000   \n",
      "4              Accuracy    0.826891    0.840336    0.828571    0.838655   \n",
      "5             Precision    0.838875    0.850667    0.843750    0.832061   \n",
      "6           Sensitivity    0.891304    0.891061    0.885246    0.915966   \n",
      "7           Specificity    0.722500    0.763700    0.738000    0.722700   \n",
      "8              F1 score    0.864295    0.870396    0.864000    0.872000   \n",
      "9   F1 score (weighted)    0.824895    0.839218    0.827122    0.835927   \n",
      "10     F1 score (macro)    0.812658    0.831259    0.816091    0.826909   \n",
      "11    Balanced Accuracy    0.806886    0.827387    0.811619    0.819328   \n",
      "12                  MCC    0.628116    0.664019    0.633877    0.660720   \n",
      "13                  NPV    0.803900    0.822700    0.800900    0.851500   \n",
      "14              ROC_AUC    0.806886    0.827387    0.811619    0.819328   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0   336.000000  326.000000  340.000000  329.000000  \n",
      "1   181.000000  181.000000  163.000000  172.000000  \n",
      "2    57.000000   53.000000   50.000000   56.000000  \n",
      "3    21.000000   35.000000   42.000000   38.000000  \n",
      "4     0.868908    0.852101    0.845378    0.842017  \n",
      "5     0.854962    0.860158    0.871795    0.854545  \n",
      "6     0.941176    0.903047    0.890052    0.896458  \n",
      "7     0.760500    0.773500    0.765300    0.754400  \n",
      "8     0.896000    0.881081    0.880829    0.875000  \n",
      "9     0.866691    0.850942    0.844700    0.840661  \n",
      "10    0.859364    0.842763    0.830367    0.830194  \n",
      "11    0.850840    0.838276    0.827655    0.825422  \n",
      "12    0.725922    0.687252    0.661088    0.662121  \n",
      "13    0.896000    0.838000    0.795100    0.819000  \n",
      "14    0.850840    0.838276    0.827655    0.825422  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_svm_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_svm_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_svm_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_svm_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_svm_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_svm_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_svm_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_svm_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_svm_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_svm_7)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 18:20:03,597] Trial 400 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:20:09,628] Trial 401 finished with value: 0.8236043688338587 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:20:15,915] Trial 402 finished with value: 0.8269732017671373 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:20:22,393] Trial 403 finished with value: 0.8248690508109924 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:20:28,971] Trial 404 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:20:35,368] Trial 405 finished with value: 0.8269732017671373 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:20:41,866] Trial 406 finished with value: 0.8262276951907269 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:20:48,413] Trial 407 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:20:52,574] Trial 408 finished with value: 0.8027920616444593 and parameters: {'C': 2.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:20:59,150] Trial 409 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:21:05,932] Trial 410 finished with value: 0.38228093278962144 and parameters: {'C': 0.25, 'gamma': 4.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:21:13,064] Trial 411 finished with value: 0.4395705438412209 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:21:19,580] Trial 412 finished with value: 0.8269732017671373 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:21:24,155] Trial 413 finished with value: 0.7811037111378216 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:21:30,676] Trial 414 finished with value: 0.8269732017671373 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:21:36,416] Trial 415 finished with value: 0.38228093278962144 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:21:43,075] Trial 416 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:21:49,425] Trial 417 finished with value: 0.38228093278962144 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:21:55,881] Trial 418 finished with value: 0.8269732017671373 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:22:01,265] Trial 419 finished with value: 0.4810277925034388 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:22:06,965] Trial 420 finished with value: 0.38228093278962144 and parameters: {'C': 0.0625, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:22:13,536] Trial 421 finished with value: 0.8262276951907269 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:22:20,268] Trial 422 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:22:26,878] Trial 423 finished with value: 0.8248690508109924 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:22:33,618] Trial 424 finished with value: 0.6289010661262621 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:22:40,273] Trial 425 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:22:46,806] Trial 426 finished with value: 0.8262276951907269 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:22:53,336] Trial 427 finished with value: 0.8262400003731809 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:23:00,365] Trial 428 finished with value: 0.42990784523660713 and parameters: {'C': 2.0, 'gamma': 1.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:23:06,958] Trial 429 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:23:12,480] Trial 430 finished with value: 0.48478675537225424 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:23:19,093] Trial 431 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:23:25,678] Trial 432 finished with value: 0.38228093278962144 and parameters: {'C': 0.015625, 'gamma': 0.25}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:23:33,883] Trial 433 finished with value: 0.4123785821560709 and parameters: {'C': 2.0, 'gamma': 8.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:23:40,478] Trial 434 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:23:46,920] Trial 435 finished with value: 0.8269732017671373 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:23:53,721] Trial 436 finished with value: 0.41454723998975335 and parameters: {'C': 2.0, 'gamma': 2.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:24:00,381] Trial 437 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:24:05,931] Trial 438 finished with value: 0.38228093278962144 and parameters: {'C': 0.03125, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:24:12,324] Trial 439 finished with value: 0.6625653048543706 and parameters: {'C': 0.5, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:24:17,942] Trial 440 finished with value: 0.5419602000398156 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:24:24,291] Trial 441 finished with value: 0.8248690508109924 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:24:29,535] Trial 442 finished with value: 0.38228093278962144 and parameters: {'C': 2.0, 'gamma': 6.103515625e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:24:35,659] Trial 443 finished with value: 0.8236043688338587 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:24:42,238] Trial 444 finished with value: 0.8277673173209882 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:24:48,637] Trial 445 finished with value: 0.8269732017671373 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:24:55,129] Trial 446 finished with value: 0.8262276951907269 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:25:01,018] Trial 447 finished with value: 0.7318391473442233 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:25:05,244] Trial 448 finished with value: 0.8171754775790987 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:25:12,506] Trial 449 finished with value: 0.4123785821560709 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 9 with value: 0.8428953030073378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8429\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  328.000000  319.000000  324.000000  327.000000   \n",
      "1                    TN  164.000000  181.000000  169.000000  172.000000   \n",
      "2                    FP   63.000000   56.000000   60.000000   66.000000   \n",
      "3                    FN   40.000000   39.000000   42.000000   30.000000   \n",
      "4              Accuracy    0.826891    0.840336    0.828571    0.838655   \n",
      "5             Precision    0.838875    0.850667    0.843750    0.832061   \n",
      "6           Sensitivity    0.891304    0.891061    0.885246    0.915966   \n",
      "7           Specificity    0.722500    0.763700    0.738000    0.722700   \n",
      "8              F1 score    0.864295    0.870396    0.864000    0.872000   \n",
      "9   F1 score (weighted)    0.824895    0.839218    0.827122    0.835927   \n",
      "10     F1 score (macro)    0.812658    0.831259    0.816091    0.826909   \n",
      "11    Balanced Accuracy    0.806886    0.827387    0.811619    0.819328   \n",
      "12                  MCC    0.628116    0.664019    0.633877    0.660720   \n",
      "13                  NPV    0.803900    0.822700    0.800900    0.851500   \n",
      "14              ROC_AUC    0.806886    0.827387    0.811619    0.819328   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0   336.000000  326.000000  340.000000  329.000000  323.000000  \n",
      "1   181.000000  181.000000  163.000000  172.000000  179.000000  \n",
      "2    57.000000   53.000000   50.000000   56.000000   50.000000  \n",
      "3    21.000000   35.000000   42.000000   38.000000   43.000000  \n",
      "4     0.868908    0.852101    0.845378    0.842017    0.843697  \n",
      "5     0.854962    0.860158    0.871795    0.854545    0.865952  \n",
      "6     0.941176    0.903047    0.890052    0.896458    0.882514  \n",
      "7     0.760500    0.773500    0.765300    0.754400    0.781700  \n",
      "8     0.896000    0.881081    0.880829    0.875000    0.874154  \n",
      "9     0.866691    0.850942    0.844700    0.840661    0.843225  \n",
      "10    0.859364    0.842763    0.830367    0.830194    0.833973  \n",
      "11    0.850840    0.838276    0.827655    0.825422    0.832087  \n",
      "12    0.725922    0.687252    0.661088    0.662121    0.668203  \n",
      "13    0.896000    0.838000    0.795100    0.819000    0.806300  \n",
      "14    0.850840    0.838276    0.827655    0.825422    0.832087  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_svm_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_svm_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_svm_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_svm_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_svm_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_svm_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_svm_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_svm_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_svm_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_svm_8)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 18:25:20,298] Trial 450 finished with value: 0.8328215591430148 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:25:26,894] Trial 451 finished with value: 0.8315465135013905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:25:34,018] Trial 452 finished with value: 0.4309968325799904 and parameters: {'C': 2.0, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:25:40,598] Trial 453 finished with value: 0.8315465135013905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:25:46,452] Trial 454 finished with value: 0.3819875388704367 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:25:52,324] Trial 455 finished with value: 0.3819875388704367 and parameters: {'C': 0.0078125, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:25:58,811] Trial 456 finished with value: 0.8315465135013905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:26:04,134] Trial 457 finished with value: 0.680833305971126 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:26:10,215] Trial 458 finished with value: 0.4294949347911623 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:26:16,864] Trial 459 finished with value: 0.8309549170184924 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:26:22,202] Trial 460 finished with value: 0.6780063620437637 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:26:28,769] Trial 461 finished with value: 0.8328215591430148 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:26:35,251] Trial 462 finished with value: 0.8315465135013905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:26:42,042] Trial 463 finished with value: 0.8311627618252446 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:26:49,083] Trial 464 finished with value: 0.6274430082829195 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:26:55,697] Trial 465 finished with value: 0.8328215591430148 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:27:02,230] Trial 466 finished with value: 0.8315465135013905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:27:08,430] Trial 467 finished with value: 0.3819875388704367 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:27:15,727] Trial 468 finished with value: 0.4209166548999989 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:27:21,401] Trial 469 finished with value: 0.4927148167368773 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:27:28,661] Trial 470 finished with value: 0.4494722561434302 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:27:34,867] Trial 471 finished with value: 0.3819875388704367 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:27:41,425] Trial 472 finished with value: 0.8328215591430148 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:27:47,975] Trial 473 finished with value: 0.8315465135013905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:27:56,019] Trial 474 finished with value: 0.3819875388704367 and parameters: {'C': 0.125, 'gamma': 8.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:28:02,566] Trial 475 finished with value: 0.8328215591430148 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:28:06,435] Trial 476 finished with value: 0.8000398966413069 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:28:13,520] Trial 477 finished with value: 0.40208624637230006 and parameters: {'C': 2.0, 'gamma': 2.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:28:19,280] Trial 478 finished with value: 0.8093677006911392 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:28:25,466] Trial 479 finished with value: 0.8252812808679131 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:28:32,051] Trial 480 finished with value: 0.8328215591430148 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:28:39,060] Trial 481 finished with value: 0.8103198982857232 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:28:44,375] Trial 482 finished with value: 0.6834049217035053 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:28:50,995] Trial 483 finished with value: 0.8328215591430148 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:28:57,626] Trial 484 finished with value: 0.8315465135013905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:29:03,446] Trial 485 finished with value: 0.7501141775724207 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:29:10,026] Trial 486 finished with value: 0.8309549170184924 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:29:16,756] Trial 487 finished with value: 0.8328215591430148 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:29:23,419] Trial 488 finished with value: 0.8315465135013905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:29:27,464] Trial 489 finished with value: 0.8043779579048953 and parameters: {'C': 2.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:29:34,786] Trial 490 finished with value: 0.39752901684888553 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:29:41,969] Trial 491 finished with value: 0.4309968325799904 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:29:48,570] Trial 492 finished with value: 0.8328215591430148 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:29:55,073] Trial 493 finished with value: 0.8315465135013905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:30:01,172] Trial 494 finished with value: 0.3819875388704367 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:30:07,772] Trial 495 finished with value: 0.8328215591430148 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:30:11,402] Trial 496 finished with value: 0.8057330068371844 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:30:17,564] Trial 497 finished with value: 0.4294949347911623 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:30:24,053] Trial 498 finished with value: 0.8315465135013905 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 9 with value: 0.8428953030073378.\n",
      "[I 2023-12-04 18:30:28,645] Trial 499 finished with value: 0.767172198843243 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 9 with value: 0.8428953030073378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8429\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP  328.000000  319.000000  324.000000  327.000000   \n",
      "1                    TN  164.000000  181.000000  169.000000  172.000000   \n",
      "2                    FP   63.000000   56.000000   60.000000   66.000000   \n",
      "3                    FN   40.000000   39.000000   42.000000   30.000000   \n",
      "4              Accuracy    0.826891    0.840336    0.828571    0.838655   \n",
      "5             Precision    0.838875    0.850667    0.843750    0.832061   \n",
      "6           Sensitivity    0.891304    0.891061    0.885246    0.915966   \n",
      "7           Specificity    0.722500    0.763700    0.738000    0.722700   \n",
      "8              F1 score    0.864295    0.870396    0.864000    0.872000   \n",
      "9   F1 score (weighted)    0.824895    0.839218    0.827122    0.835927   \n",
      "10     F1 score (macro)    0.812658    0.831259    0.816091    0.826909   \n",
      "11    Balanced Accuracy    0.806886    0.827387    0.811619    0.819328   \n",
      "12                  MCC    0.628116    0.664019    0.633877    0.660720   \n",
      "13                  NPV    0.803900    0.822700    0.800900    0.851500   \n",
      "14              ROC_AUC    0.806886    0.827387    0.811619    0.819328   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0   336.000000  326.000000  340.000000  329.000000  323.000000  325.000000  \n",
      "1   181.000000  181.000000  163.000000  172.000000  179.000000  163.000000  \n",
      "2    57.000000   53.000000   50.000000   56.000000   50.000000   64.000000  \n",
      "3    21.000000   35.000000   42.000000   38.000000   43.000000   43.000000  \n",
      "4     0.868908    0.852101    0.845378    0.842017    0.843697    0.820168  \n",
      "5     0.854962    0.860158    0.871795    0.854545    0.865952    0.835476  \n",
      "6     0.941176    0.903047    0.890052    0.896458    0.882514    0.883152  \n",
      "7     0.760500    0.773500    0.765300    0.754400    0.781700    0.718100  \n",
      "8     0.896000    0.881081    0.880829    0.875000    0.874154    0.858653  \n",
      "9     0.866691    0.850942    0.844700    0.840661    0.843225    0.818302  \n",
      "10    0.859364    0.842763    0.830367    0.830194    0.833973    0.805770  \n",
      "11    0.850840    0.838276    0.827655    0.825422    0.832087    0.800607  \n",
      "12    0.725922    0.687252    0.661088    0.662121    0.668203    0.613843  \n",
      "13    0.896000    0.838000    0.795100    0.819000    0.806300    0.791300  \n",
      "14    0.850840    0.838276    0.827655    0.825422    0.832087    0.800607  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_svm_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_svm_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_svm_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_svm_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_svm_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_svm_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_svm_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_svm_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_svm_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_svm_9)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>5.743595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>86.300000</td>\n",
       "      <td>2.584140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>27.100000</td>\n",
       "      <td>4.677369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>1.988858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.845508</td>\n",
       "      <td>0.018325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.858842</td>\n",
       "      <td>0.024397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.897505</td>\n",
       "      <td>0.011970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.761930</td>\n",
       "      <td>0.032807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.877621</td>\n",
       "      <td>0.016218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.844255</td>\n",
       "      <td>0.018865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.833926</td>\n",
       "      <td>0.018220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.829723</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.669728</td>\n",
       "      <td>0.035236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.821360</td>\n",
       "      <td>0.015057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.829723</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP       164.900000     5.743595\n",
       "1                    TN        86.300000     2.584140\n",
       "2                    FP        27.100000     4.677369\n",
       "3                    FN        18.800000     1.988858\n",
       "4              Accuracy         0.845508     0.018325\n",
       "5             Precision         0.858842     0.024397\n",
       "6           Sensitivity         0.897505     0.011970\n",
       "7           Specificity         0.761930     0.032807\n",
       "8              F1 score         0.877621     0.016218\n",
       "9   F1 score (weighted)         0.844255     0.018865\n",
       "10     F1 score (macro)         0.833926     0.018220\n",
       "11    Balanced Accuracy         0.829723     0.019068\n",
       "12                  MCC         0.669728     0.035236\n",
       "13                  NPV         0.821360     0.015057\n",
       "14              ROC_AUC         0.829723     0.019068"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>327.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>172.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>57.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>37.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.826891</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.838655</td>\n",
       "      <td>0.868908</td>\n",
       "      <td>0.852101</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.842017</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.820168</td>\n",
       "      <td>0.840672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.838875</td>\n",
       "      <td>0.850667</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.832061</td>\n",
       "      <td>0.854962</td>\n",
       "      <td>0.860158</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.865952</td>\n",
       "      <td>0.835476</td>\n",
       "      <td>0.850824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.891061</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.915966</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.903047</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.896458</td>\n",
       "      <td>0.882514</td>\n",
       "      <td>0.883152</td>\n",
       "      <td>0.897998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.722500</td>\n",
       "      <td>0.763700</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.722700</td>\n",
       "      <td>0.760500</td>\n",
       "      <td>0.773500</td>\n",
       "      <td>0.765300</td>\n",
       "      <td>0.754400</td>\n",
       "      <td>0.781700</td>\n",
       "      <td>0.718100</td>\n",
       "      <td>0.750040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.864295</td>\n",
       "      <td>0.870396</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.881081</td>\n",
       "      <td>0.880829</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.874154</td>\n",
       "      <td>0.858653</td>\n",
       "      <td>0.873641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.824895</td>\n",
       "      <td>0.839218</td>\n",
       "      <td>0.827122</td>\n",
       "      <td>0.835927</td>\n",
       "      <td>0.866691</td>\n",
       "      <td>0.850942</td>\n",
       "      <td>0.844700</td>\n",
       "      <td>0.840661</td>\n",
       "      <td>0.843225</td>\n",
       "      <td>0.818302</td>\n",
       "      <td>0.839168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.812658</td>\n",
       "      <td>0.831259</td>\n",
       "      <td>0.816091</td>\n",
       "      <td>0.826909</td>\n",
       "      <td>0.859364</td>\n",
       "      <td>0.842763</td>\n",
       "      <td>0.830367</td>\n",
       "      <td>0.830194</td>\n",
       "      <td>0.833973</td>\n",
       "      <td>0.805770</td>\n",
       "      <td>0.828935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.806886</td>\n",
       "      <td>0.827387</td>\n",
       "      <td>0.811619</td>\n",
       "      <td>0.819328</td>\n",
       "      <td>0.850840</td>\n",
       "      <td>0.838276</td>\n",
       "      <td>0.827655</td>\n",
       "      <td>0.825422</td>\n",
       "      <td>0.832087</td>\n",
       "      <td>0.800607</td>\n",
       "      <td>0.824011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.628116</td>\n",
       "      <td>0.664019</td>\n",
       "      <td>0.633877</td>\n",
       "      <td>0.660720</td>\n",
       "      <td>0.725922</td>\n",
       "      <td>0.687252</td>\n",
       "      <td>0.661088</td>\n",
       "      <td>0.662121</td>\n",
       "      <td>0.668203</td>\n",
       "      <td>0.613843</td>\n",
       "      <td>0.660516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.803900</td>\n",
       "      <td>0.822700</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.851500</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.795100</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.791300</td>\n",
       "      <td>0.822470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.806886</td>\n",
       "      <td>0.827387</td>\n",
       "      <td>0.811619</td>\n",
       "      <td>0.819328</td>\n",
       "      <td>0.850840</td>\n",
       "      <td>0.838276</td>\n",
       "      <td>0.827655</td>\n",
       "      <td>0.825422</td>\n",
       "      <td>0.832087</td>\n",
       "      <td>0.800607</td>\n",
       "      <td>0.824011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP  328.000000  319.000000  324.000000  327.000000   \n",
       "1                    TN  164.000000  181.000000  169.000000  172.000000   \n",
       "2                    FP   63.000000   56.000000   60.000000   66.000000   \n",
       "3                    FN   40.000000   39.000000   42.000000   30.000000   \n",
       "4              Accuracy    0.826891    0.840336    0.828571    0.838655   \n",
       "5             Precision    0.838875    0.850667    0.843750    0.832061   \n",
       "6           Sensitivity    0.891304    0.891061    0.885246    0.915966   \n",
       "7           Specificity    0.722500    0.763700    0.738000    0.722700   \n",
       "8              F1 score    0.864295    0.870396    0.864000    0.872000   \n",
       "9   F1 score (weighted)    0.824895    0.839218    0.827122    0.835927   \n",
       "10     F1 score (macro)    0.812658    0.831259    0.816091    0.826909   \n",
       "11    Balanced Accuracy    0.806886    0.827387    0.811619    0.819328   \n",
       "12                  MCC    0.628116    0.664019    0.633877    0.660720   \n",
       "13                  NPV    0.803900    0.822700    0.800900    0.851500   \n",
       "14              ROC_AUC    0.806886    0.827387    0.811619    0.819328   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0   336.000000  326.000000  340.000000  329.000000  323.000000  325.000000   \n",
       "1   181.000000  181.000000  163.000000  172.000000  179.000000  163.000000   \n",
       "2    57.000000   53.000000   50.000000   56.000000   50.000000   64.000000   \n",
       "3    21.000000   35.000000   42.000000   38.000000   43.000000   43.000000   \n",
       "4     0.868908    0.852101    0.845378    0.842017    0.843697    0.820168   \n",
       "5     0.854962    0.860158    0.871795    0.854545    0.865952    0.835476   \n",
       "6     0.941176    0.903047    0.890052    0.896458    0.882514    0.883152   \n",
       "7     0.760500    0.773500    0.765300    0.754400    0.781700    0.718100   \n",
       "8     0.896000    0.881081    0.880829    0.875000    0.874154    0.858653   \n",
       "9     0.866691    0.850942    0.844700    0.840661    0.843225    0.818302   \n",
       "10    0.859364    0.842763    0.830367    0.830194    0.833973    0.805770   \n",
       "11    0.850840    0.838276    0.827655    0.825422    0.832087    0.800607   \n",
       "12    0.725922    0.687252    0.661088    0.662121    0.668203    0.613843   \n",
       "13    0.896000    0.838000    0.795100    0.819000    0.806300    0.791300   \n",
       "14    0.850840    0.838276    0.827655    0.825422    0.832087    0.800607   \n",
       "\n",
       "           ave  \n",
       "0   327.700000  \n",
       "1   172.500000  \n",
       "2    57.500000  \n",
       "3    37.300000  \n",
       "4     0.840672  \n",
       "5     0.850824  \n",
       "6     0.897998  \n",
       "7     0.750040  \n",
       "8     0.873641  \n",
       "9     0.839168  \n",
       "10    0.828935  \n",
       "11    0.824011  \n",
       "12    0.660516  \n",
       "13    0.822470  \n",
       "14    0.824011  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.851902</td>\n",
       "      <td>0.015053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.863626</td>\n",
       "      <td>0.019244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.903607</td>\n",
       "      <td>0.016634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.768052</td>\n",
       "      <td>0.036698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.882952</td>\n",
       "      <td>0.011668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.850615</td>\n",
       "      <td>0.015471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.840342</td>\n",
       "      <td>0.017244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.835827</td>\n",
       "      <td>0.018408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.683030</td>\n",
       "      <td>0.033972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.831068</td>\n",
       "      <td>0.027410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.835827</td>\n",
       "      <td>0.018408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.851902     0.015053\n",
       "1             Precision         0.863626     0.019244\n",
       "2           Sensitivity         0.903607     0.016634\n",
       "3           Specificity         0.768052     0.036698\n",
       "4              F1 score         0.882952     0.011668\n",
       "5   F1 score (weighted)         0.850615     0.015471\n",
       "6      F1 score (macro)         0.840342     0.017244\n",
       "7     Balanced Accuracy         0.835827     0.018408\n",
       "8                   MCC         0.683030     0.033972\n",
       "9                   NPV         0.831068     0.027410\n",
       "10              ROC_AUC         0.835827     0.018408"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_svm)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_svm))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_svm))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_svm))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_svm))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_svm, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_svm, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_svm))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_svm))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_svm))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model f1_score 0.8302 with a standard deviation of 0.0169\n",
      "SVM optimized model f1_score 0.8372 with a standard deviation of 0.0192\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVC \n",
    "svm_baseline_CVscore = cross_val_score(svm_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"SVM baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm_clf.joblib']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the modesls, both the one with optimized hyperparameters and the initial one\n",
    "joblib.dump(svm_clf, \"OUTPUT/svm_clf.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm_clf.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "484e48eb-6732-43c3-9476-635047d3319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4cdd104c-7c12-406a-9bc8-4cfabb5c45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
