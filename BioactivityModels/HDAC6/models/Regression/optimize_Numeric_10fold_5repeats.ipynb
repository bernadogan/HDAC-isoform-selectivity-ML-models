{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC6 = Path(HERE).resolve().parents[1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2047687</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[73638076, 23625102, 56975613, 11863091, 11250...</td>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1164212</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1583330, 10427556, 9160532, 5710863, 14267828...</td>\n",
       "      <td>5.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2337873</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[14042162, 3913535, 11193268, 3843750, 9589145...</td>\n",
       "      <td>6.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4577419</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5976924, 4613032, 4703614, 11956864, 1293667,...</td>\n",
       "      <td>7.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL515452</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[9197507, 192820, 750563, 5652600, 9038104, 33...</td>\n",
       "      <td>6.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL2047687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL1164212  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL2337873  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4577419  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4       CHEMBL515452  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC6  \n",
       "0  [73638076, 23625102, 56975613, 11863091, 11250...           5.48  \n",
       "1  [1583330, 10427556, 9160532, 5710863, 14267828...           5.76  \n",
       "2  [14042162, 3913535, 11193268, 3843750, 9589145...           6.07  \n",
       "3  [5976924, 4613032, 4703614, 11956864, 1293667,...           7.90  \n",
       "4  [9197507, 192820, 750563, 5652600, 9038104, 33...           6.22  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC6/\"HDAC6_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type</th>\n",
       "      <th>Standard_Value_HDAC6</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4082520</td>\n",
       "      <td>CN1C(=O)C2CN(Cc3c2c2cc(OCc4ccccc4)ccc2n3Cc2ccc...</td>\n",
       "      <td>Ki</td>\n",
       "      <td>0.08</td>\n",
       "      <td>10.10</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9.85</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4100534</td>\n",
       "      <td>COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...</td>\n",
       "      <td>Ki</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9.82</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4101480</td>\n",
       "      <td>COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...</td>\n",
       "      <td>Ki</td>\n",
       "      <td>0.16</td>\n",
       "      <td>9.80</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9.77</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL1798006</td>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)C1CCNCC1)C(=O)N1Cc2cc(OC...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>169.00</td>\n",
       "      <td>6.77</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL470843</td>\n",
       "      <td>O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>175.00</td>\n",
       "      <td>6.76</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL1798004</td>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)[C@@H]1CCCN1)C(=O)N1Cc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>191.00</td>\n",
       "      <td>6.72</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>247.00</td>\n",
       "      <td>6.61</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>293.60</td>\n",
       "      <td>6.53</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4082520  CN1C(=O)C2CN(Cc3c2c2cc(OCc4ccccc4)ccc2n3Cc2ccc...   \n",
       "1         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "2         CHEMBL4100534  COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...   \n",
       "3         CHEMBL4101480  COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...   \n",
       "4         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "...                 ...                                                ...   \n",
       "2966      CHEMBL1798006  CC[C@H](C)[C@H](NC(=O)C1CCNCC1)C(=O)N1Cc2cc(OC...   \n",
       "2967       CHEMBL470843          O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO   \n",
       "2968      CHEMBL1798004  CC[C@H](C)[C@H](NC(=O)[C@@H]1CCCN1)C(=O)N1Cc2c...   \n",
       "2969      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "2970      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "      type  Standard_Value_HDAC6  pChEMBL_HDAC6            label  \n",
       "0       Ki                  0.08          10.10    Single points  \n",
       "1     IC50                  0.14           9.85  HDAC6-selective  \n",
       "2       Ki                  0.15           9.82    Single points  \n",
       "3       Ki                  0.16           9.80    Single points  \n",
       "4     IC50                  0.17           9.77  HDAC6-selective  \n",
       "...    ...                   ...            ...              ...  \n",
       "2966  IC50                169.00           6.77    Single points  \n",
       "2967  IC50                175.00           6.76   Semi-selective  \n",
       "2968  IC50                191.00           6.72    Single points  \n",
       "2969    Ki                247.00           6.61      Dual-binder  \n",
       "2970  IC50                293.60           6.53       Non-binder  \n",
       "\n",
       "[2971 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC6/\"HDAC6_dataset.csv\",)\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2047687</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[73638076, 23625102, 56975613, 11863091, 11250...</td>\n",
       "      <td>5.48</td>\n",
       "      <td>HDAC1-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1164212</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1583330, 10427556, 9160532, 5710863, 14267828...</td>\n",
       "      <td>5.76</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2337873</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[14042162, 3913535, 11193268, 3843750, 9589145...</td>\n",
       "      <td>6.07</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4577419</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5976924, 4613032, 4703614, 11956864, 1293667,...</td>\n",
       "      <td>7.90</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL515452</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[9197507, 192820, 750563, 5652600, 9038104, 33...</td>\n",
       "      <td>6.22</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL2047687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL1164212  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL2337873  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4577419  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4       CHEMBL515452  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC6  \\\n",
       "0  [73638076, 23625102, 56975613, 11863091, 11250...           5.48   \n",
       "1  [1583330, 10427556, 9160532, 5710863, 14267828...           5.76   \n",
       "2  [14042162, 3913535, 11193268, 3843750, 9589145...           6.07   \n",
       "3  [5976924, 4613032, 4703614, 11956864, 1293667,...           7.90   \n",
       "4  [9197507, 192820, 750563, 5652600, 9038104, 33...           6.22   \n",
       "\n",
       "             label  \n",
       "0  HDAC1-selective  \n",
       "1    Single points  \n",
       "2    Single points  \n",
       "3   Semi-selective  \n",
       "4       Non-binder  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2047687</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[73638076, 23625102, 56975613, 11863091, 11250...</td>\n",
       "      <td>5.48</td>\n",
       "      <td>HDAC1-selective</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1164212</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1583330, 10427556, 9160532, 5710863, 14267828...</td>\n",
       "      <td>5.76</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2337873</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[14042162, 3913535, 11193268, 3843750, 9589145...</td>\n",
       "      <td>6.07</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4577419</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5976924, 4613032, 4703614, 11956864, 1293667,...</td>\n",
       "      <td>7.90</td>\n",
       "      <td>Semi-selective</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL2047687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL1164212  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL2337873  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4577419  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC6  \\\n",
       "0  [73638076, 23625102, 56975613, 11863091, 11250...           5.48   \n",
       "1  [1583330, 10427556, 9160532, 5710863, 14267828...           5.76   \n",
       "2  [14042162, 3913535, 11193268, 3843750, 9589145...           6.07   \n",
       "3  [5976924, 4613032, 4703614, 11956864, 1293667,...           7.90   \n",
       "\n",
       "             label  Class  \n",
       "0  HDAC1-selective    0.0  \n",
       "1    Single points    0.0  \n",
       "2    Single points    0.0  \n",
       "3   Semi-selective    5.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for activity\n",
    "df[\"activity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as active if pChEMBL_HDAC6 value is >=6.6 0 otherwise\n",
    "df.loc[df[df.pChEMBL_HDAC6 >= 6.6].index, \"activity\"] = 1.0\n",
    "\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"pChEMBL_HDAC6\"].values\n",
    "Y_cat =  df[\"activity\"].values\n",
    "Y_class = df['Class'].values\n",
    "indices =  np.array(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['pChEMBL_HDAC6'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['pChEMBL_HDAC6'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.693745     0.034027\n",
      "1                    TP       165.400000     5.561774\n",
      "2                    TN        87.400000     5.189733\n",
      "3                    FP        26.000000     4.371626\n",
      "4                    FN        18.300000     5.538752\n",
      "5              Accuracy         0.850893     0.018094\n",
      "6             Precision         0.864597     0.018597\n",
      "7           Sensitivity         0.900529     0.029581\n",
      "8           Specificity         0.770700     0.037892\n",
      "9              F1 score         0.881812     0.014969\n",
      "10  F1 score (weighted)         0.849730     0.018001\n",
      "11     F1 score (macro)         0.839673     0.019400\n",
      "12    Balanced Accuracy         0.835614     0.019034\n",
      "13                  MCC         0.682082     0.038800\n",
      "14                  NPV         0.828710     0.042184\n",
      "15              ROC_AUC         0.835614     0.019034\n",
      "CPU times: user 2min 11s, sys: 0 ns, total: 2min 11s\n",
      "Wall time: 19.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=8,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6) , 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=8, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=8, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 08:12:42,080] A new study created in memory with name: RFRegressor\n",
      "[I 2023-12-11 08:13:48,580] Trial 0 finished with value: 0.662304823591157 and parameters: {'n_estimators': 483}. Best is trial 0 with value: 0.662304823591157.\n",
      "[I 2023-12-11 08:14:38,500] Trial 1 finished with value: 0.6630239899323744 and parameters: {'n_estimators': 353}. Best is trial 1 with value: 0.6630239899323744.\n",
      "[I 2023-12-11 08:16:26,134] Trial 2 finished with value: 0.6621222688732152 and parameters: {'n_estimators': 794}. Best is trial 1 with value: 0.6630239899323744.\n",
      "[I 2023-12-11 08:18:05,082] Trial 3 finished with value: 0.6619531181365675 and parameters: {'n_estimators': 723}. Best is trial 1 with value: 0.6630239899323744.\n",
      "[I 2023-12-11 08:18:56,746] Trial 4 finished with value: 0.6629540255611243 and parameters: {'n_estimators': 366}. Best is trial 1 with value: 0.6630239899323744.\n",
      "[I 2023-12-11 08:21:07,249] Trial 5 finished with value: 0.6622868064958716 and parameters: {'n_estimators': 933}. Best is trial 1 with value: 0.6630239899323744.\n",
      "[I 2023-12-11 08:23:20,803] Trial 6 finished with value: 0.6624791186628834 and parameters: {'n_estimators': 962}. Best is trial 1 with value: 0.6630239899323744.\n",
      "[I 2023-12-11 08:25:23,617] Trial 7 finished with value: 0.6620720903526782 and parameters: {'n_estimators': 885}. Best is trial 1 with value: 0.6630239899323744.\n",
      "[I 2023-12-11 08:26:20,385] Trial 8 finished with value: 0.6627666916897661 and parameters: {'n_estimators': 396}. Best is trial 1 with value: 0.6630239899323744.\n",
      "[I 2023-12-11 08:28:02,309] Trial 9 finished with value: 0.6619215075349952 and parameters: {'n_estimators': 720}. Best is trial 1 with value: 0.6630239899323744.\n",
      "[I 2023-12-11 08:28:18,264] Trial 10 finished with value: 0.6573972575272478 and parameters: {'n_estimators': 106}. Best is trial 1 with value: 0.6630239899323744.\n",
      "[I 2023-12-11 08:28:59,509] Trial 11 finished with value: 0.6632208830270052 and parameters: {'n_estimators': 292}. Best is trial 11 with value: 0.6632208830270052.\n",
      "[I 2023-12-11 08:29:28,253] Trial 12 finished with value: 0.6619980246004733 and parameters: {'n_estimators': 207}. Best is trial 11 with value: 0.6632208830270052.\n",
      "[I 2023-12-11 08:30:08,797] Trial 13 finished with value: 0.6631151121592156 and parameters: {'n_estimators': 283}. Best is trial 11 with value: 0.6632208830270052.\n",
      "[I 2023-12-11 08:30:41,339] Trial 14 finished with value: 0.6624304989756004 and parameters: {'n_estimators': 236}. Best is trial 11 with value: 0.6632208830270052.\n",
      "[I 2023-12-11 08:32:04,434] Trial 15 finished with value: 0.6623332468170717 and parameters: {'n_estimators': 586}. Best is trial 11 with value: 0.6632208830270052.\n",
      "[I 2023-12-11 08:32:39,986] Trial 16 finished with value: 0.662936147845518 and parameters: {'n_estimators': 251}. Best is trial 11 with value: 0.6632208830270052.\n",
      "[I 2023-12-11 08:33:55,544] Trial 17 finished with value: 0.6623465562542915 and parameters: {'n_estimators': 550}. Best is trial 11 with value: 0.6632208830270052.\n",
      "[I 2023-12-11 08:34:18,563] Trial 18 finished with value: 0.6603462701587874 and parameters: {'n_estimators': 160}. Best is trial 11 with value: 0.6632208830270052.\n",
      "[I 2023-12-11 08:35:01,045] Trial 19 finished with value: 0.663315547923655 and parameters: {'n_estimators': 302}. Best is trial 19 with value: 0.663315547923655.\n",
      "[I 2023-12-11 08:36:05,716] Trial 20 finished with value: 0.6626434837561662 and parameters: {'n_estimators': 458}. Best is trial 19 with value: 0.663315547923655.\n",
      "[I 2023-12-11 08:36:47,628] Trial 21 finished with value: 0.6634544505904618 and parameters: {'n_estimators': 310}. Best is trial 21 with value: 0.6634544505904618.\n",
      "[I 2023-12-11 08:37:30,550] Trial 22 finished with value: 0.6633044143186348 and parameters: {'n_estimators': 304}. Best is trial 21 with value: 0.6634544505904618.\n",
      "[I 2023-12-11 08:38:35,070] Trial 23 finished with value: 0.6626227681961734 and parameters: {'n_estimators': 454}. Best is trial 21 with value: 0.6634544505904618.\n",
      "[I 2023-12-11 08:39:24,491] Trial 24 finished with value: 0.6630599526797553 and parameters: {'n_estimators': 346}. Best is trial 21 with value: 0.6634544505904618.\n",
      "[I 2023-12-11 08:39:47,861] Trial 25 finished with value: 0.6604302405300699 and parameters: {'n_estimators': 159}. Best is trial 21 with value: 0.6634544505904618.\n",
      "[I 2023-12-11 08:41:05,014] Trial 26 finished with value: 0.6623465562542915 and parameters: {'n_estimators': 550}. Best is trial 21 with value: 0.6634544505904618.\n",
      "[I 2023-12-11 08:42:31,201] Trial 27 finished with value: 0.6619731782395164 and parameters: {'n_estimators': 630}. Best is trial 21 with value: 0.6634544505904618.\n",
      "[I 2023-12-11 08:43:28,947] Trial 28 finished with value: 0.6628796606235106 and parameters: {'n_estimators': 412}. Best is trial 21 with value: 0.6634544505904618.\n",
      "[I 2023-12-11 08:44:12,507] Trial 29 finished with value: 0.6634581538246236 and parameters: {'n_estimators': 307}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:45:23,265] Trial 30 finished with value: 0.6621525771112584 and parameters: {'n_estimators': 497}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:46:05,381] Trial 31 finished with value: 0.6634446729976695 and parameters: {'n_estimators': 308}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:46:32,792] Trial 32 finished with value: 0.6615185134936159 and parameters: {'n_estimators': 194}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:47:17,700] Trial 33 finished with value: 0.6634231927098206 and parameters: {'n_estimators': 327}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:48:08,033] Trial 34 finished with value: 0.6629006632022041 and parameters: {'n_estimators': 361}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:49:08,992] Trial 35 finished with value: 0.6627557822568665 and parameters: {'n_estimators': 424}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:49:56,247] Trial 36 finished with value: 0.6633915523636296 and parameters: {'n_estimators': 338}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:51:06,102] Trial 37 finished with value: 0.6622267754436487 and parameters: {'n_estimators': 500}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:51:25,574] Trial 38 finished with value: 0.6595250370235157 and parameters: {'n_estimators': 137}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:51:58,734] Trial 39 finished with value: 0.6626207732053148 and parameters: {'n_estimators': 231}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:52:50,718] Trial 40 finished with value: 0.66294518136297 and parameters: {'n_estimators': 372}. Best is trial 29 with value: 0.6634581538246236.\n",
      "[I 2023-12-11 08:53:37,582] Trial 41 finished with value: 0.6634611915013586 and parameters: {'n_estimators': 326}. Best is trial 41 with value: 0.6634611915013586.\n",
      "[I 2023-12-11 08:54:13,885] Trial 42 finished with value: 0.663017659144461 and parameters: {'n_estimators': 258}. Best is trial 41 with value: 0.6634611915013586.\n",
      "[I 2023-12-11 08:55:01,667] Trial 43 finished with value: 0.663465567517712 and parameters: {'n_estimators': 329}. Best is trial 43 with value: 0.663465567517712.\n",
      "[I 2023-12-11 08:55:58,856] Trial 44 finished with value: 0.6628565046294381 and parameters: {'n_estimators': 391}. Best is trial 43 with value: 0.663465567517712.\n",
      "[I 2023-12-11 08:56:27,716] Trial 45 finished with value: 0.6615541110692129 and parameters: {'n_estimators': 193}. Best is trial 43 with value: 0.663465567517712.\n",
      "[I 2023-12-11 08:57:09,717] Trial 46 finished with value: 0.6631030989600581 and parameters: {'n_estimators': 286}. Best is trial 43 with value: 0.663465567517712.\n",
      "[I 2023-12-11 08:58:11,945] Trial 47 finished with value: 0.6628212661238726 and parameters: {'n_estimators': 438}. Best is trial 43 with value: 0.663465567517712.\n",
      "[I 2023-12-11 08:58:49,632] Trial 48 finished with value: 0.6627484185757229 and parameters: {'n_estimators': 264}. Best is trial 43 with value: 0.663465567517712.\n",
      "[I 2023-12-11 09:00:42,193] Trial 49 finished with value: 0.6620787932419822 and parameters: {'n_estimators': 800}. Best is trial 43 with value: 0.663465567517712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6635\n",
      "\tBest params:\n",
      "\t\tn_estimators: 329\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.698078\n",
      "1                    TP  329.000000\n",
      "2                    TN  189.000000\n",
      "3                    FP   38.000000\n",
      "4                    FN   39.000000\n",
      "5              Accuracy    0.870588\n",
      "6             Precision    0.896458\n",
      "7           Sensitivity    0.894022\n",
      "8           Specificity    0.832600\n",
      "9              F1 score    0.895238\n",
      "10  F1 score (weighted)    0.870642\n",
      "11     F1 score (macro)    0.863004\n",
      "12    Balanced Accuracy    0.863310\n",
      "13                  MCC    0.726013\n",
      "14                  NPV    0.828900\n",
      "15              ROC_AUC    0.863310\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_0_cat = np.where((y_pred_rf_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 09:01:34,312] Trial 50 finished with value: 0.6698096865761942 and parameters: {'n_estimators': 326}. Best is trial 50 with value: 0.6698096865761942.\n",
      "[I 2023-12-11 09:02:20,265] Trial 51 finished with value: 0.6697328376254046 and parameters: {'n_estimators': 323}. Best is trial 50 with value: 0.6698096865761942.\n",
      "[I 2023-12-11 09:03:15,587] Trial 52 finished with value: 0.6695890933867397 and parameters: {'n_estimators': 389}. Best is trial 50 with value: 0.6698096865761942.\n",
      "[I 2023-12-11 09:04:10,190] Trial 53 finished with value: 0.6694503315489987 and parameters: {'n_estimators': 384}. Best is trial 50 with value: 0.6698096865761942.\n",
      "[I 2023-12-11 09:05:06,991] Trial 54 finished with value: 0.6696637551370975 and parameters: {'n_estimators': 394}. Best is trial 50 with value: 0.6698096865761942.\n",
      "[I 2023-12-11 09:06:01,817] Trial 55 finished with value: 0.669697893471123 and parameters: {'n_estimators': 378}. Best is trial 50 with value: 0.6698096865761942.\n",
      "[I 2023-12-11 09:07:15,546] Trial 56 finished with value: 0.6704434002364617 and parameters: {'n_estimators': 517}. Best is trial 56 with value: 0.6704434002364617.\n",
      "[I 2023-12-11 09:08:42,890] Trial 57 finished with value: 0.6709149109939789 and parameters: {'n_estimators': 620}. Best is trial 57 with value: 0.6709149109939789.\n",
      "[I 2023-12-11 09:10:12,280] Trial 58 finished with value: 0.6708313448406753 and parameters: {'n_estimators': 644}. Best is trial 57 with value: 0.6709149109939789.\n",
      "[I 2023-12-11 09:11:50,065] Trial 59 finished with value: 0.6706235535836516 and parameters: {'n_estimators': 670}. Best is trial 57 with value: 0.6709149109939789.\n",
      "[I 2023-12-11 09:13:25,262] Trial 60 finished with value: 0.6706776762433251 and parameters: {'n_estimators': 662}. Best is trial 57 with value: 0.6709149109939789.\n",
      "[I 2023-12-11 09:14:54,453] Trial 61 finished with value: 0.6708506286141953 and parameters: {'n_estimators': 643}. Best is trial 57 with value: 0.6709149109939789.\n",
      "[I 2023-12-11 09:16:25,386] Trial 62 finished with value: 0.670650269270405 and parameters: {'n_estimators': 666}. Best is trial 57 with value: 0.6709149109939789.\n",
      "[I 2023-12-11 09:17:58,113] Trial 63 finished with value: 0.6705561867323871 and parameters: {'n_estimators': 675}. Best is trial 57 with value: 0.6709149109939789.\n",
      "[I 2023-12-11 09:19:28,232] Trial 64 finished with value: 0.6707043987804591 and parameters: {'n_estimators': 658}. Best is trial 57 with value: 0.6709149109939789.\n",
      "[I 2023-12-11 09:21:10,632] Trial 65 finished with value: 0.6708697088530429 and parameters: {'n_estimators': 746}. Best is trial 57 with value: 0.6709149109939789.\n",
      "[I 2023-12-11 09:22:59,932] Trial 66 finished with value: 0.670877552443385 and parameters: {'n_estimators': 786}. Best is trial 57 with value: 0.6709149109939789.\n",
      "[I 2023-12-11 09:24:44,914] Trial 67 finished with value: 0.6710266860679663 and parameters: {'n_estimators': 763}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:26:33,580] Trial 68 finished with value: 0.6709178650439698 and parameters: {'n_estimators': 783}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:28:23,467] Trial 69 finished with value: 0.6708316082422513 and parameters: {'n_estimators': 796}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:30:17,492] Trial 70 finished with value: 0.6706247853437376 and parameters: {'n_estimators': 823}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:31:59,658] Trial 71 finished with value: 0.6709311600353348 and parameters: {'n_estimators': 725}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:33:41,838] Trial 72 finished with value: 0.6709478862552057 and parameters: {'n_estimators': 740}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:35:21,861] Trial 73 finished with value: 0.6708284673652247 and parameters: {'n_estimators': 711}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:37:03,625] Trial 74 finished with value: 0.6708487095181644 and parameters: {'n_estimators': 745}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:39:03,237] Trial 75 finished with value: 0.6705566336722437 and parameters: {'n_estimators': 859}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:40:47,693] Trial 76 finished with value: 0.6709586356310294 and parameters: {'n_estimators': 758}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:42:31,469] Trial 77 finished with value: 0.6709656116542992 and parameters: {'n_estimators': 759}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:44:38,112] Trial 78 finished with value: 0.6702092633561942 and parameters: {'n_estimators': 924}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:46:02,972] Trial 79 finished with value: 0.6708832600089005 and parameters: {'n_estimators': 607}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:47:26,693] Trial 80 finished with value: 0.6708797704595277 and parameters: {'n_estimators': 612}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:48:47,193] Trial 81 finished with value: 0.6710255802772754 and parameters: {'n_estimators': 590}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:50:05,648] Trial 82 finished with value: 0.6709283966977583 and parameters: {'n_estimators': 584}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:51:41,531] Trial 83 finished with value: 0.6706373506376313 and parameters: {'n_estimators': 692}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:53:01,837] Trial 84 finished with value: 0.6708586259632059 and parameters: {'n_estimators': 579}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:54:45,585] Trial 85 finished with value: 0.6709586356310294 and parameters: {'n_estimators': 758}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:56:27,324] Trial 86 finished with value: 0.6709028165818763 and parameters: {'n_estimators': 735}. Best is trial 67 with value: 0.6710266860679663.\n",
      "[I 2023-12-11 09:58:12,789] Trial 87 finished with value: 0.671036503110267 and parameters: {'n_estimators': 766}. Best is trial 87 with value: 0.671036503110267.\n",
      "[I 2023-12-11 09:59:56,875] Trial 88 finished with value: 0.67101183857672 and parameters: {'n_estimators': 760}. Best is trial 87 with value: 0.671036503110267.\n",
      "[I 2023-12-11 10:01:43,729] Trial 89 finished with value: 0.6710266860679663 and parameters: {'n_estimators': 763}. Best is trial 87 with value: 0.671036503110267.\n",
      "[I 2023-12-11 10:03:30,115] Trial 90 finished with value: 0.671036503110267 and parameters: {'n_estimators': 766}. Best is trial 87 with value: 0.671036503110267.\n",
      "[I 2023-12-11 10:05:29,286] Trial 91 finished with value: 0.6706211542599789 and parameters: {'n_estimators': 841}. Best is trial 87 with value: 0.671036503110267.\n",
      "[I 2023-12-11 10:07:32,760] Trial 92 finished with value: 0.6706013345948934 and parameters: {'n_estimators': 876}. Best is trial 87 with value: 0.671036503110267.\n",
      "[I 2023-12-11 10:09:22,297] Trial 93 finished with value: 0.6710380496514603 and parameters: {'n_estimators': 768}. Best is trial 93 with value: 0.6710380496514603.\n",
      "[I 2023-12-11 10:11:08,272] Trial 94 finished with value: 0.671036503110267 and parameters: {'n_estimators': 766}. Best is trial 93 with value: 0.6710380496514603.\n",
      "[I 2023-12-11 10:12:55,268] Trial 95 finished with value: 0.6710266860679663 and parameters: {'n_estimators': 763}. Best is trial 93 with value: 0.6710380496514603.\n",
      "[I 2023-12-11 10:14:49,006] Trial 96 finished with value: 0.6706277105364806 and parameters: {'n_estimators': 818}. Best is trial 93 with value: 0.6710380496514603.\n",
      "[I 2023-12-11 10:16:26,726] Trial 97 finished with value: 0.6707686771768435 and parameters: {'n_estimators': 705}. Best is trial 93 with value: 0.6710380496514603.\n",
      "[I 2023-12-11 10:18:13,989] Trial 98 finished with value: 0.6710257241854388 and parameters: {'n_estimators': 770}. Best is trial 93 with value: 0.6710380496514603.\n",
      "[I 2023-12-11 10:20:00,298] Trial 99 finished with value: 0.6709685230085406 and parameters: {'n_estimators': 773}. Best is trial 93 with value: 0.6710380496514603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6710\n",
      "\tBest params:\n",
      "\t\tn_estimators: 768\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.698078    0.713316\n",
      "1                    TP  329.000000  336.000000\n",
      "2                    TN  189.000000  180.000000\n",
      "3                    FP   38.000000   43.000000\n",
      "4                    FN   39.000000   36.000000\n",
      "5              Accuracy    0.870588    0.867227\n",
      "6             Precision    0.896458    0.886544\n",
      "7           Sensitivity    0.894022    0.903226\n",
      "8           Specificity    0.832600    0.807200\n",
      "9              F1 score    0.895238    0.894807\n",
      "10  F1 score (weighted)    0.870642    0.866787\n",
      "11     F1 score (macro)    0.863004    0.857426\n",
      "12    Balanced Accuracy    0.863310    0.855200\n",
      "13                  MCC    0.726013    0.715123\n",
      "14                  NPV    0.828900    0.833300\n",
      "15              ROC_AUC    0.863310    0.855200\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_1_cat = np.where((y_pred_rf_1 >= 6.6), 1, 0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 10:22:05,301] Trial 100 finished with value: 0.6721512488407759 and parameters: {'n_estimators': 810}. Best is trial 100 with value: 0.6721512488407759.\n",
      "[I 2023-12-11 10:24:21,660] Trial 101 finished with value: 0.6722723689917279 and parameters: {'n_estimators': 992}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:26:32,471] Trial 102 finished with value: 0.6721093363799107 and parameters: {'n_estimators': 947}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:28:48,690] Trial 103 finished with value: 0.6721956556557076 and parameters: {'n_estimators': 986}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:31:05,443] Trial 104 finished with value: 0.6722723689917279 and parameters: {'n_estimators': 992}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:33:24,024] Trial 105 finished with value: 0.6722338994981149 and parameters: {'n_estimators': 997}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:35:41,876] Trial 106 finished with value: 0.6722429861837098 and parameters: {'n_estimators': 990}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:38:00,672] Trial 107 finished with value: 0.6722701727548785 and parameters: {'n_estimators': 991}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:40:21,910] Trial 108 finished with value: 0.6722441185937674 and parameters: {'n_estimators': 998}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:42:41,502] Trial 109 finished with value: 0.6722222922919008 and parameters: {'n_estimators': 999}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:44:59,988] Trial 110 finished with value: 0.6722222922919008 and parameters: {'n_estimators': 999}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:47:18,525] Trial 111 finished with value: 0.6722441185937674 and parameters: {'n_estimators': 998}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:49:35,286] Trial 112 finished with value: 0.6722222922919008 and parameters: {'n_estimators': 999}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:51:53,917] Trial 113 finished with value: 0.6722222922919008 and parameters: {'n_estimators': 999}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:54:13,486] Trial 114 finished with value: 0.6722222922919008 and parameters: {'n_estimators': 999}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:56:31,093] Trial 115 finished with value: 0.6721903323386704 and parameters: {'n_estimators': 1000}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 10:58:42,694] Trial 116 finished with value: 0.6722444296647385 and parameters: {'n_estimators': 919}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:00:53,482] Trial 117 finished with value: 0.6722444296647385 and parameters: {'n_estimators': 919}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:03:03,188] Trial 118 finished with value: 0.6722093383232972 and parameters: {'n_estimators': 911}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:05:22,138] Trial 119 finished with value: 0.6722098386177822 and parameters: {'n_estimators': 971}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:07:38,475] Trial 120 finished with value: 0.6721979126833258 and parameters: {'n_estimators': 958}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:09:53,704] Trial 121 finished with value: 0.6722328220708437 and parameters: {'n_estimators': 976}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:11:57,406] Trial 122 finished with value: 0.6722319631337711 and parameters: {'n_estimators': 905}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:14:07,833] Trial 123 finished with value: 0.6721393586664935 and parameters: {'n_estimators': 932}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:16:10,020] Trial 124 finished with value: 0.6721892782678166 and parameters: {'n_estimators': 900}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:18:24,021] Trial 125 finished with value: 0.67220788354096 and parameters: {'n_estimators': 970}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:20:33,059] Trial 126 finished with value: 0.6721093363799108 and parameters: {'n_estimators': 947}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:22:47,947] Trial 127 finished with value: 0.6722077354180553 and parameters: {'n_estimators': 978}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:24:59,714] Trial 128 finished with value: 0.6721250041742455 and parameters: {'n_estimators': 949}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:27:07,484] Trial 129 finished with value: 0.6722224288091248 and parameters: {'n_estimators': 921}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:29:15,376] Trial 130 finished with value: 0.6722395293703625 and parameters: {'n_estimators': 918}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:31:22,119] Trial 131 finished with value: 0.6722444296647385 and parameters: {'n_estimators': 919}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:33:26,129] Trial 132 finished with value: 0.6721977675639506 and parameters: {'n_estimators': 889}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:35:32,912] Trial 133 finished with value: 0.6722641494240735 and parameters: {'n_estimators': 916}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:37:43,623] Trial 134 finished with value: 0.6720450158991247 and parameters: {'n_estimators': 938}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:39:58,070] Trial 135 finished with value: 0.6722159936998674 and parameters: {'n_estimators': 961}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:42:04,465] Trial 136 finished with value: 0.6722052411673819 and parameters: {'n_estimators': 910}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:44:05,590] Trial 137 finished with value: 0.672153919249197 and parameters: {'n_estimators': 871}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:46:21,822] Trial 138 finished with value: 0.6722173284322593 and parameters: {'n_estimators': 977}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:48:32,955] Trial 139 finished with value: 0.6720649866096294 and parameters: {'n_estimators': 936}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:50:35,667] Trial 140 finished with value: 0.6722024695492139 and parameters: {'n_estimators': 893}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:52:44,644] Trial 141 finished with value: 0.6722444296647386 and parameters: {'n_estimators': 919}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:54:43,136] Trial 142 finished with value: 0.6720675011835037 and parameters: {'n_estimators': 853}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:56:56,810] Trial 143 finished with value: 0.6721913534081605 and parameters: {'n_estimators': 962}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 11:59:12,573] Trial 144 finished with value: 0.6722398340673849 and parameters: {'n_estimators': 980}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 12:01:19,923] Trial 145 finished with value: 0.6721824294268108 and parameters: {'n_estimators': 925}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 12:03:35,556] Trial 146 finished with value: 0.6722173284322592 and parameters: {'n_estimators': 977}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 12:05:48,388] Trial 147 finished with value: 0.6721778661340425 and parameters: {'n_estimators': 956}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 12:08:06,060] Trial 148 finished with value: 0.6722476664758417 and parameters: {'n_estimators': 981}. Best is trial 101 with value: 0.6722723689917279.\n",
      "[I 2023-12-11 12:10:18,140] Trial 149 finished with value: 0.6720970744907604 and parameters: {'n_estimators': 942}. Best is trial 101 with value: 0.6722723689917279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6723\n",
      "\tBest params:\n",
      "\t\tn_estimators: 992\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.698078    0.713316    0.703192\n",
      "1                    TP  329.000000  336.000000  308.000000\n",
      "2                    TN  189.000000  180.000000  191.000000\n",
      "3                    FP   38.000000   43.000000   62.000000\n",
      "4                    FN   39.000000   36.000000   34.000000\n",
      "5              Accuracy    0.870588    0.867227    0.838655\n",
      "6             Precision    0.896458    0.886544    0.832432\n",
      "7           Sensitivity    0.894022    0.903226    0.900585\n",
      "8           Specificity    0.832600    0.807200    0.754900\n",
      "9              F1 score    0.895238    0.894807    0.865169\n",
      "10  F1 score (weighted)    0.870642    0.866787    0.837102\n",
      "11     F1 score (macro)    0.863004    0.857426    0.832166\n",
      "12    Balanced Accuracy    0.863310    0.855200    0.827763\n",
      "13                  MCC    0.726013    0.715123    0.668299\n",
      "14                  NPV    0.828900    0.833300    0.848900\n",
      "15              ROC_AUC    0.863310    0.855200    0.827763\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_2_cat = np.where((y_pred_rf_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:12:41,933] Trial 150 finished with value: 0.6764688674528643 and parameters: {'n_estimators': 923}. Best is trial 150 with value: 0.6764688674528643.\n",
      "[I 2023-12-11 12:14:47,137] Trial 151 finished with value: 0.6764884095650934 and parameters: {'n_estimators': 925}. Best is trial 151 with value: 0.6764884095650934.\n",
      "[I 2023-12-11 12:16:54,662] Trial 152 finished with value: 0.6764438325290121 and parameters: {'n_estimators': 922}. Best is trial 151 with value: 0.6764884095650934.\n",
      "[I 2023-12-11 12:19:06,515] Trial 153 finished with value: 0.6766320478180069 and parameters: {'n_estimators': 880}. Best is trial 153 with value: 0.6766320478180069.\n",
      "[I 2023-12-11 12:21:10,529] Trial 154 finished with value: 0.6766726580312307 and parameters: {'n_estimators': 870}. Best is trial 154 with value: 0.6766726580312307.\n",
      "[I 2023-12-11 12:23:24,558] Trial 155 finished with value: 0.6766436009594113 and parameters: {'n_estimators': 879}. Best is trial 154 with value: 0.6766726580312307.\n",
      "[I 2023-12-11 12:25:22,002] Trial 156 finished with value: 0.6766658461549574 and parameters: {'n_estimators': 839}. Best is trial 154 with value: 0.6766726580312307.\n",
      "[I 2023-12-11 12:27:08,871] Trial 157 finished with value: 0.6766726580312307 and parameters: {'n_estimators': 870}. Best is trial 154 with value: 0.6766726580312307.\n",
      "[I 2023-12-11 12:28:55,809] Trial 158 finished with value: 0.6766722944832433 and parameters: {'n_estimators': 874}. Best is trial 154 with value: 0.6766726580312307.\n",
      "[I 2023-12-11 12:30:37,515] Trial 159 finished with value: 0.6767225776349675 and parameters: {'n_estimators': 833}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:32:20,248] Trial 160 finished with value: 0.6766848621729075 and parameters: {'n_estimators': 837}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:34:02,235] Trial 161 finished with value: 0.6767220428631004 and parameters: {'n_estimators': 831}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:35:47,146] Trial 162 finished with value: 0.6766674009549558 and parameters: {'n_estimators': 850}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:37:29,951] Trial 163 finished with value: 0.6766921728941826 and parameters: {'n_estimators': 836}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:39:12,581] Trial 164 finished with value: 0.6766883924061824 and parameters: {'n_estimators': 835}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:40:56,014] Trial 165 finished with value: 0.6766796451041399 and parameters: {'n_estimators': 845}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:42:38,407] Trial 166 finished with value: 0.6766664726280405 and parameters: {'n_estimators': 838}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:44:20,091] Trial 167 finished with value: 0.6767028743321586 and parameters: {'n_estimators': 832}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:46:03,602] Trial 168 finished with value: 0.6766796451041399 and parameters: {'n_estimators': 845}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:47:44,376] Trial 169 finished with value: 0.6766883924061824 and parameters: {'n_estimators': 835}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:49:26,637] Trial 170 finished with value: 0.6767096715804961 and parameters: {'n_estimators': 834}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:51:08,798] Trial 171 finished with value: 0.676709671580496 and parameters: {'n_estimators': 834}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:52:50,552] Trial 172 finished with value: 0.6767220428631003 and parameters: {'n_estimators': 831}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:54:33,958] Trial 173 finished with value: 0.6766664726280405 and parameters: {'n_estimators': 838}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:56:16,482] Trial 174 finished with value: 0.6767012473043515 and parameters: {'n_estimators': 829}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:57:57,623] Trial 175 finished with value: 0.6767140795449628 and parameters: {'n_estimators': 820}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 12:59:37,961] Trial 176 finished with value: 0.6766620242820115 and parameters: {'n_estimators': 813}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:01:23,093] Trial 177 finished with value: 0.6766026044903453 and parameters: {'n_estimators': 853}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:03:02,423] Trial 178 finished with value: 0.6765773160453665 and parameters: {'n_estimators': 801}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:04:44,650] Trial 179 finished with value: 0.6767012473043515 and parameters: {'n_estimators': 829}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:06:26,648] Trial 180 finished with value: 0.6766912220798178 and parameters: {'n_estimators': 825}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:08:08,735] Trial 181 finished with value: 0.6767080211821921 and parameters: {'n_estimators': 827}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:09:50,713] Trial 182 finished with value: 0.6766859261662301 and parameters: {'n_estimators': 828}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:11:31,938] Trial 183 finished with value: 0.676718087784621 and parameters: {'n_estimators': 824}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:13:13,205] Trial 184 finished with value: 0.676718087784621 and parameters: {'n_estimators': 824}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:14:54,202] Trial 185 finished with value: 0.6767046725947259 and parameters: {'n_estimators': 823}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:16:35,622] Trial 186 finished with value: 0.6766912220798178 and parameters: {'n_estimators': 825}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:18:16,082] Trial 187 finished with value: 0.6766933925878441 and parameters: {'n_estimators': 817}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:19:53,891] Trial 188 finished with value: 0.6765660331365864 and parameters: {'n_estimators': 795}. Best is trial 159 with value: 0.6767225776349675.\n",
      "[I 2023-12-11 13:21:34,961] Trial 189 finished with value: 0.6767364107014716 and parameters: {'n_estimators': 822}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:23:15,218] Trial 190 finished with value: 0.6766423247219279 and parameters: {'n_estimators': 814}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:24:56,129] Trial 191 finished with value: 0.6767123178196018 and parameters: {'n_estimators': 821}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:26:33,210] Trial 192 finished with value: 0.6765114709241112 and parameters: {'n_estimators': 788}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:28:13,564] Trial 193 finished with value: 0.6767123178196018 and parameters: {'n_estimators': 821}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:29:53,466] Trial 194 finished with value: 0.6766476912150916 and parameters: {'n_estimators': 816}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:31:32,195] Trial 195 finished with value: 0.6766323312993475 and parameters: {'n_estimators': 810}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:33:08,426] Trial 196 finished with value: 0.676508185499645 and parameters: {'n_estimators': 790}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:34:48,087] Trial 197 finished with value: 0.6767123178196018 and parameters: {'n_estimators': 821}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:36:29,515] Trial 198 finished with value: 0.6766034389960249 and parameters: {'n_estimators': 805}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:38:13,032] Trial 199 finished with value: 0.6767123178196018 and parameters: {'n_estimators': 821}. Best is trial 189 with value: 0.6767364107014716.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6767\n",
      "\tBest params:\n",
      "\t\tn_estimators: 822\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.698078    0.713316    0.703192    0.663824\n",
      "1                    TP  329.000000  336.000000  308.000000  319.000000\n",
      "2                    TN  189.000000  180.000000  191.000000  186.000000\n",
      "3                    FP   38.000000   43.000000   62.000000   56.000000\n",
      "4                    FN   39.000000   36.000000   34.000000   34.000000\n",
      "5              Accuracy    0.870588    0.867227    0.838655    0.848739\n",
      "6             Precision    0.896458    0.886544    0.832432    0.850667\n",
      "7           Sensitivity    0.894022    0.903226    0.900585    0.903683\n",
      "8           Specificity    0.832600    0.807200    0.754900    0.768600\n",
      "9              F1 score    0.895238    0.894807    0.865169    0.876374\n",
      "10  F1 score (weighted)    0.870642    0.866787    0.837102    0.847424\n",
      "11     F1 score (macro)    0.863004    0.857426    0.832166    0.840784\n",
      "12    Balanced Accuracy    0.863310    0.855200    0.827763    0.836139\n",
      "13                  MCC    0.726013    0.715123    0.668299    0.684096\n",
      "14                  NPV    0.828900    0.833300    0.848900    0.845500\n",
      "15              ROC_AUC    0.863310    0.855200    0.827763    0.836139\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_3_cat = np.where((y_pred_rf_3 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 13:40:13,479] Trial 200 finished with value: 0.6731473676542509 and parameters: {'n_estimators': 855}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:41:53,699] Trial 201 finished with value: 0.6731547980366636 and parameters: {'n_estimators': 787}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:43:37,898] Trial 202 finished with value: 0.6731787482015942 and parameters: {'n_estimators': 820}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:45:22,537] Trial 203 finished with value: 0.6731155450304078 and parameters: {'n_estimators': 823}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:47:12,112] Trial 204 finished with value: 0.6731500878896037 and parameters: {'n_estimators': 861}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:48:53,725] Trial 205 finished with value: 0.673169858917194 and parameters: {'n_estimators': 801}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:50:38,429] Trial 206 finished with value: 0.6731318354929196 and parameters: {'n_estimators': 822}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:52:27,422] Trial 207 finished with value: 0.6731483315312438 and parameters: {'n_estimators': 858}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:54:09,559] Trial 208 finished with value: 0.673182947053423 and parameters: {'n_estimators': 803}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:55:48,978] Trial 209 finished with value: 0.6731219694039963 and parameters: {'n_estimators': 780}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:57:34,524] Trial 210 finished with value: 0.6731691722184076 and parameters: {'n_estimators': 832}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 13:59:19,922] Trial 211 finished with value: 0.6731393825595425 and parameters: {'n_estimators': 824}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:01:04,728] Trial 212 finished with value: 0.673159584442078 and parameters: {'n_estimators': 821}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:02:47,938] Trial 213 finished with value: 0.6731904591743115 and parameters: {'n_estimators': 806}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:04:37,175] Trial 214 finished with value: 0.6731218945428034 and parameters: {'n_estimators': 852}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:06:23,666] Trial 215 finished with value: 0.6731691722184077 and parameters: {'n_estimators': 832}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:08:04,742] Trial 216 finished with value: 0.673131851354194 and parameters: {'n_estimators': 789}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:09:53,287] Trial 217 finished with value: 0.6731132814947695 and parameters: {'n_estimators': 854}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:11:35,641] Trial 218 finished with value: 0.6731393825595424 and parameters: {'n_estimators': 824}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:13:16,098] Trial 219 finished with value: 0.6731677665839231 and parameters: {'n_estimators': 805}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:15:04,336] Trial 220 finished with value: 0.6731209852688399 and parameters: {'n_estimators': 866}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:16:48,623] Trial 221 finished with value: 0.6730863621988036 and parameters: {'n_estimators': 837}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:18:32,571] Trial 222 finished with value: 0.6730847430960576 and parameters: {'n_estimators': 836}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:20:14,102] Trial 223 finished with value: 0.6731969262275364 and parameters: {'n_estimators': 817}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:21:58,452] Trial 224 finished with value: 0.6731054279933846 and parameters: {'n_estimators': 839}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:23:36,171] Trial 225 finished with value: 0.6731547980366636 and parameters: {'n_estimators': 787}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:25:22,188] Trial 226 finished with value: 0.673147367654251 and parameters: {'n_estimators': 855}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:27:03,527] Trial 227 finished with value: 0.6731917233633675 and parameters: {'n_estimators': 818}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:28:42,828] Trial 228 finished with value: 0.673169858917194 and parameters: {'n_estimators': 801}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:30:27,156] Trial 229 finished with value: 0.6731524182328756 and parameters: {'n_estimators': 843}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:32:15,809] Trial 230 finished with value: 0.6731383901110657 and parameters: {'n_estimators': 885}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:33:57,035] Trial 231 finished with value: 0.6731446819179873 and parameters: {'n_estimators': 828}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:35:40,860] Trial 232 finished with value: 0.6731153909569505 and parameters: {'n_estimators': 833}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:37:21,682] Trial 233 finished with value: 0.6731915188907618 and parameters: {'n_estimators': 809}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:39:06,894] Trial 234 finished with value: 0.6731608808017766 and parameters: {'n_estimators': 845}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:40:54,270] Trial 235 finished with value: 0.6731461151905425 and parameters: {'n_estimators': 867}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:42:36,330] Trial 236 finished with value: 0.6731617280425486 and parameters: {'n_estimators': 825}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:44:14,856] Trial 237 finished with value: 0.6731282967056521 and parameters: {'n_estimators': 795}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:45:59,435] Trial 238 finished with value: 0.6731638846227508 and parameters: {'n_estimators': 847}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:47:40,295] Trial 239 finished with value: 0.6731721038500076 and parameters: {'n_estimators': 814}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:49:16,830] Trial 240 finished with value: 0.6730948161290398 and parameters: {'n_estimators': 779}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:50:59,364] Trial 241 finished with value: 0.6731647735382292 and parameters: {'n_estimators': 830}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:52:41,743] Trial 242 finished with value: 0.6731578609404029 and parameters: {'n_estimators': 829}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:54:27,471] Trial 243 finished with value: 0.6731503455933823 and parameters: {'n_estimators': 860}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:56:07,392] Trial 244 finished with value: 0.6732088042680416 and parameters: {'n_estimators': 808}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:57:51,355] Trial 245 finished with value: 0.6731608808017766 and parameters: {'n_estimators': 845}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 14:59:34,279] Trial 246 finished with value: 0.6731429925705362 and parameters: {'n_estimators': 826}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:01:13,704] Trial 247 finished with value: 0.6731863763794466 and parameters: {'n_estimators': 800}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:02:57,099] Trial 248 finished with value: 0.6731647735382292 and parameters: {'n_estimators': 830}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:04:44,291] Trial 249 finished with value: 0.6731503455933823 and parameters: {'n_estimators': 860}. Best is trial 189 with value: 0.6767364107014716.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6767\n",
      "\tBest params:\n",
      "\t\tn_estimators: 822\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698078    0.713316    0.703192    0.663824   \n",
      "1                    TP  329.000000  336.000000  308.000000  319.000000   \n",
      "2                    TN  189.000000  180.000000  191.000000  186.000000   \n",
      "3                    FP   38.000000   43.000000   62.000000   56.000000   \n",
      "4                    FN   39.000000   36.000000   34.000000   34.000000   \n",
      "5              Accuracy    0.870588    0.867227    0.838655    0.848739   \n",
      "6             Precision    0.896458    0.886544    0.832432    0.850667   \n",
      "7           Sensitivity    0.894022    0.903226    0.900585    0.903683   \n",
      "8           Specificity    0.832600    0.807200    0.754900    0.768600   \n",
      "9              F1 score    0.895238    0.894807    0.865169    0.876374   \n",
      "10  F1 score (weighted)    0.870642    0.866787    0.837102    0.847424   \n",
      "11     F1 score (macro)    0.863004    0.857426    0.832166    0.840784   \n",
      "12    Balanced Accuracy    0.863310    0.855200    0.827763    0.836139   \n",
      "13                  MCC    0.726013    0.715123    0.668299    0.684096   \n",
      "14                  NPV    0.828900    0.833300    0.848900    0.845500   \n",
      "15              ROC_AUC    0.863310    0.855200    0.827763    0.836139   \n",
      "\n",
      "          Set4  \n",
      "0     0.699816  \n",
      "1   340.000000  \n",
      "2   170.000000  \n",
      "3    54.000000  \n",
      "4    31.000000  \n",
      "5     0.857143  \n",
      "6     0.862944  \n",
      "7     0.916442  \n",
      "8     0.758900  \n",
      "9     0.888889  \n",
      "10    0.855425  \n",
      "11    0.844444  \n",
      "12    0.837685  \n",
      "13    0.691842  \n",
      "14    0.845800  \n",
      "15    0.837685  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_4_cat = np.where((y_pred_rf_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 15:06:37,769] Trial 250 finished with value: 0.6703068190693977 and parameters: {'n_estimators': 816}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:08:22,166] Trial 251 finished with value: 0.6704572832838936 and parameters: {'n_estimators': 836}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:10:00,016] Trial 252 finished with value: 0.6703517662032492 and parameters: {'n_estimators': 784}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:11:41,165] Trial 253 finished with value: 0.6703087492783022 and parameters: {'n_estimators': 810}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:13:28,542] Trial 254 finished with value: 0.6703223311700619 and parameters: {'n_estimators': 874}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:15:12,530] Trial 255 finished with value: 0.6703544122959697 and parameters: {'n_estimators': 848}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:16:53,500] Trial 256 finished with value: 0.6703516457725587 and parameters: {'n_estimators': 822}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:18:31,403] Trial 257 finished with value: 0.6702914295256391 and parameters: {'n_estimators': 796}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:20:14,332] Trial 258 finished with value: 0.6703496474295113 and parameters: {'n_estimators': 845}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:21:59,006] Trial 259 finished with value: 0.6703667668187998 and parameters: {'n_estimators': 862}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:23:39,776] Trial 260 finished with value: 0.670417866373324 and parameters: {'n_estimators': 828}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:25:18,187] Trial 261 finished with value: 0.6702895186862163 and parameters: {'n_estimators': 804}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:27:06,524] Trial 262 finished with value: 0.6704277333940688 and parameters: {'n_estimators': 883}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:28:49,198] Trial 263 finished with value: 0.6704431875037251 and parameters: {'n_estimators': 837}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:30:29,153] Trial 264 finished with value: 0.6703168393049986 and parameters: {'n_estimators': 817}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:32:04,603] Trial 265 finished with value: 0.6703350994077241 and parameters: {'n_estimators': 780}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:33:48,973] Trial 266 finished with value: 0.6703640925358443 and parameters: {'n_estimators': 857}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:35:26,926] Trial 267 finished with value: 0.6703192105585052 and parameters: {'n_estimators': 799}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:37:09,453] Trial 268 finished with value: 0.670435273362582 and parameters: {'n_estimators': 838}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:38:49,068] Trial 269 finished with value: 0.6703068206715974 and parameters: {'n_estimators': 814}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:40:31,734] Trial 270 finished with value: 0.6703496474295113 and parameters: {'n_estimators': 845}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:42:12,563] Trial 271 finished with value: 0.6703778179551253 and parameters: {'n_estimators': 827}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:43:59,153] Trial 272 finished with value: 0.6703317626603243 and parameters: {'n_estimators': 869}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:45:36,067] Trial 273 finished with value: 0.6703212561338476 and parameters: {'n_estimators': 789}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:47:16,020] Trial 274 finished with value: 0.670277001713513 and parameters: {'n_estimators': 813}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:49:00,905] Trial 275 finished with value: 0.670343794818407 and parameters: {'n_estimators': 850}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:50:48,608] Trial 276 finished with value: 0.670451860050659 and parameters: {'n_estimators': 888}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:52:29,455] Trial 277 finished with value: 0.670353387707469 and parameters: {'n_estimators': 824}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:54:07,553] Trial 278 finished with value: 0.6703118995445158 and parameters: {'n_estimators': 800}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:55:50,008] Trial 279 finished with value: 0.6704572832838936 and parameters: {'n_estimators': 836}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:57:36,126] Trial 280 finished with value: 0.6703870069177613 and parameters: {'n_estimators': 865}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 15:59:11,595] Trial 281 finished with value: 0.670319598330918 and parameters: {'n_estimators': 779}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:00:50,063] Trial 282 finished with value: 0.670277001713513 and parameters: {'n_estimators': 813}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:02:33,420] Trial 283 finished with value: 0.6703496474295113 and parameters: {'n_estimators': 845}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:04:13,396] Trial 284 finished with value: 0.6704203656311176 and parameters: {'n_estimators': 830}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:05:53,386] Trial 285 finished with value: 0.6703192105585052 and parameters: {'n_estimators': 799}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:07:40,082] Trial 286 finished with value: 0.6703868767847851 and parameters: {'n_estimators': 856}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:09:22,330] Trial 287 finished with value: 0.6703425239589222 and parameters: {'n_estimators': 820}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:11:06,710] Trial 288 finished with value: 0.6704166711246735 and parameters: {'n_estimators': 839}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:12:42,105] Trial 289 finished with value: 0.6702807804499165 and parameters: {'n_estimators': 774}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:14:21,272] Trial 290 finished with value: 0.6703172354391175 and parameters: {'n_estimators': 805}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:16:07,948] Trial 291 finished with value: 0.6703223311700619 and parameters: {'n_estimators': 874}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:17:39,735] Trial 292 finished with value: 0.6701090590102632 and parameters: {'n_estimators': 745}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:19:19,790] Trial 293 finished with value: 0.670373582858381 and parameters: {'n_estimators': 826}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:21:03,953] Trial 294 finished with value: 0.6703469926002608 and parameters: {'n_estimators': 851}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:22:40,951] Trial 295 finished with value: 0.67034258591755 and parameters: {'n_estimators': 793}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:24:20,718] Trial 296 finished with value: 0.6703068190693979 and parameters: {'n_estimators': 816}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:26:02,539] Trial 297 finished with value: 0.6704572832838936 and parameters: {'n_estimators': 836}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:27:49,974] Trial 298 finished with value: 0.6704638695356805 and parameters: {'n_estimators': 891}. Best is trial 189 with value: 0.6767364107014716.\n",
      "[I 2023-12-11 16:29:35,243] Trial 299 finished with value: 0.6703688732376047 and parameters: {'n_estimators': 863}. Best is trial 189 with value: 0.6767364107014716.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6767\n",
      "\tBest params:\n",
      "\t\tn_estimators: 822\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698078    0.713316    0.703192    0.663824   \n",
      "1                    TP  329.000000  336.000000  308.000000  319.000000   \n",
      "2                    TN  189.000000  180.000000  191.000000  186.000000   \n",
      "3                    FP   38.000000   43.000000   62.000000   56.000000   \n",
      "4                    FN   39.000000   36.000000   34.000000   34.000000   \n",
      "5              Accuracy    0.870588    0.867227    0.838655    0.848739   \n",
      "6             Precision    0.896458    0.886544    0.832432    0.850667   \n",
      "7           Sensitivity    0.894022    0.903226    0.900585    0.903683   \n",
      "8           Specificity    0.832600    0.807200    0.754900    0.768600   \n",
      "9              F1 score    0.895238    0.894807    0.865169    0.876374   \n",
      "10  F1 score (weighted)    0.870642    0.866787    0.837102    0.847424   \n",
      "11     F1 score (macro)    0.863004    0.857426    0.832166    0.840784   \n",
      "12    Balanced Accuracy    0.863310    0.855200    0.827763    0.836139   \n",
      "13                  MCC    0.726013    0.715123    0.668299    0.684096   \n",
      "14                  NPV    0.828900    0.833300    0.848900    0.845500   \n",
      "15              ROC_AUC    0.863310    0.855200    0.827763    0.836139   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.699816    0.702531  \n",
      "1   340.000000  323.000000  \n",
      "2   170.000000  188.000000  \n",
      "3    54.000000   44.000000  \n",
      "4    31.000000   40.000000  \n",
      "5     0.857143    0.858824  \n",
      "6     0.862944    0.880109  \n",
      "7     0.916442    0.889807  \n",
      "8     0.758900    0.810300  \n",
      "9     0.888889    0.884932  \n",
      "10    0.855425    0.858597  \n",
      "11    0.844444    0.851161  \n",
      "12    0.837685    0.850076  \n",
      "13    0.691842    0.702408  \n",
      "14    0.845800    0.824600  \n",
      "15    0.837685    0.850076  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_5_cat = np.where((y_pred_rf_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 16:31:26,492] Trial 300 finished with value: 0.6772434445709213 and parameters: {'n_estimators': 814}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:33:04,659] Trial 301 finished with value: 0.6770419689630036 and parameters: {'n_estimators': 799}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:34:39,790] Trial 302 finished with value: 0.6771235535763107 and parameters: {'n_estimators': 786}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:36:14,140] Trial 303 finished with value: 0.6772071118118635 and parameters: {'n_estimators': 774}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:37:49,196] Trial 304 finished with value: 0.6772071118118634 and parameters: {'n_estimators': 774}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:39:20,231] Trial 305 finished with value: 0.6771864306297536 and parameters: {'n_estimators': 749}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:40:53,885] Trial 306 finished with value: 0.6771744536670927 and parameters: {'n_estimators': 763}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:42:24,733] Trial 307 finished with value: 0.6771761656492726 and parameters: {'n_estimators': 744}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:43:52,998] Trial 308 finished with value: 0.6771129085657461 and parameters: {'n_estimators': 735}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:45:27,347] Trial 309 finished with value: 0.6772277084284689 and parameters: {'n_estimators': 758}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:46:58,276] Trial 310 finished with value: 0.6770680074698279 and parameters: {'n_estimators': 729}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:48:26,756] Trial 311 finished with value: 0.6771351995014712 and parameters: {'n_estimators': 722}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:49:57,186] Trial 312 finished with value: 0.6771213102236253 and parameters: {'n_estimators': 726}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:51:28,019] Trial 313 finished with value: 0.6770680074698278 and parameters: {'n_estimators': 729}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:52:57,509] Trial 314 finished with value: 0.6771351995014712 and parameters: {'n_estimators': 722}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:54:27,047] Trial 315 finished with value: 0.6771011534065534 and parameters: {'n_estimators': 723}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:55:55,174] Trial 316 finished with value: 0.6771038940028877 and parameters: {'n_estimators': 713}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:57:24,531] Trial 317 finished with value: 0.6771280410101888 and parameters: {'n_estimators': 721}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 16:58:53,633] Trial 318 finished with value: 0.6771142450749432 and parameters: {'n_estimators': 719}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 17:00:22,636] Trial 319 finished with value: 0.6771049832941378 and parameters: {'n_estimators': 717}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 17:01:51,914] Trial 320 finished with value: 0.6771280410101888 and parameters: {'n_estimators': 721}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 17:03:21,646] Trial 321 finished with value: 0.6771011534065534 and parameters: {'n_estimators': 723}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 17:04:50,586] Trial 322 finished with value: 0.6771049832941378 and parameters: {'n_estimators': 717}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 17:06:19,857] Trial 323 finished with value: 0.6771142450749432 and parameters: {'n_estimators': 719}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 17:07:46,515] Trial 324 finished with value: 0.6772155416899107 and parameters: {'n_estimators': 694}. Best is trial 300 with value: 0.6772434445709213.\n",
      "[I 2023-12-11 17:09:11,942] Trial 325 finished with value: 0.6772684550713655 and parameters: {'n_estimators': 691}. Best is trial 325 with value: 0.6772684550713655.\n",
      "[I 2023-12-11 17:10:36,950] Trial 326 finished with value: 0.6772155416899107 and parameters: {'n_estimators': 694}. Best is trial 325 with value: 0.6772684550713655.\n",
      "[I 2023-12-11 17:12:02,245] Trial 327 finished with value: 0.6772482456390252 and parameters: {'n_estimators': 687}. Best is trial 325 with value: 0.6772684550713655.\n",
      "[I 2023-12-11 17:13:28,766] Trial 328 finished with value: 0.6771722793292174 and parameters: {'n_estimators': 698}. Best is trial 325 with value: 0.6772684550713655.\n",
      "[I 2023-12-11 17:14:54,686] Trial 329 finished with value: 0.6772306550900268 and parameters: {'n_estimators': 693}. Best is trial 325 with value: 0.6772684550713655.\n",
      "[I 2023-12-11 17:16:19,256] Trial 330 finished with value: 0.6772295754147905 and parameters: {'n_estimators': 692}. Best is trial 325 with value: 0.6772684550713655.\n",
      "[I 2023-12-11 17:17:45,815] Trial 331 finished with value: 0.6771460779281885 and parameters: {'n_estimators': 702}. Best is trial 325 with value: 0.6772684550713655.\n",
      "[I 2023-12-11 17:19:11,443] Trial 332 finished with value: 0.6772306550900268 and parameters: {'n_estimators': 693}. Best is trial 325 with value: 0.6772684550713655.\n",
      "[I 2023-12-11 17:20:35,148] Trial 333 finished with value: 0.6772369314115811 and parameters: {'n_estimators': 684}. Best is trial 325 with value: 0.6772684550713655.\n",
      "[I 2023-12-11 17:21:59,762] Trial 334 finished with value: 0.6772684550713655 and parameters: {'n_estimators': 691}. Best is trial 325 with value: 0.6772684550713655.\n",
      "[I 2023-12-11 17:23:21,960] Trial 335 finished with value: 0.6773117305049643 and parameters: {'n_estimators': 679}. Best is trial 335 with value: 0.6773117305049643.\n",
      "[I 2023-12-11 17:24:46,200] Trial 336 finished with value: 0.6772293771552838 and parameters: {'n_estimators': 685}. Best is trial 335 with value: 0.6773117305049643.\n",
      "[I 2023-12-11 17:26:10,205] Trial 337 finished with value: 0.6772369314115811 and parameters: {'n_estimators': 684}. Best is trial 335 with value: 0.6773117305049643.\n",
      "[I 2023-12-11 17:27:34,782] Trial 338 finished with value: 0.6772331210427184 and parameters: {'n_estimators': 689}. Best is trial 335 with value: 0.6773117305049643.\n",
      "[I 2023-12-11 17:28:59,603] Trial 339 finished with value: 0.6772875890248768 and parameters: {'n_estimators': 683}. Best is trial 335 with value: 0.6773117305049643.\n",
      "[I 2023-12-11 17:30:23,727] Trial 340 finished with value: 0.6772331210427184 and parameters: {'n_estimators': 689}. Best is trial 335 with value: 0.6773117305049643.\n",
      "[I 2023-12-11 17:31:47,725] Trial 341 finished with value: 0.6772293771552838 and parameters: {'n_estimators': 685}. Best is trial 335 with value: 0.6773117305049643.\n",
      "[I 2023-12-11 17:33:11,572] Trial 342 finished with value: 0.6772369314115813 and parameters: {'n_estimators': 684}. Best is trial 335 with value: 0.6773117305049643.\n",
      "[I 2023-12-11 17:34:34,761] Trial 343 finished with value: 0.6773203805658375 and parameters: {'n_estimators': 678}. Best is trial 343 with value: 0.6773203805658375.\n",
      "[I 2023-12-11 17:35:58,006] Trial 344 finished with value: 0.6773117305049645 and parameters: {'n_estimators': 679}. Best is trial 343 with value: 0.6773203805658375.\n",
      "[I 2023-12-11 17:37:21,214] Trial 345 finished with value: 0.6773242003533158 and parameters: {'n_estimators': 680}. Best is trial 345 with value: 0.6773242003533158.\n",
      "[I 2023-12-11 17:38:43,729] Trial 346 finished with value: 0.6773211213813936 and parameters: {'n_estimators': 674}. Best is trial 345 with value: 0.6773242003533158.\n",
      "[I 2023-12-11 17:40:07,282] Trial 347 finished with value: 0.6773117305049645 and parameters: {'n_estimators': 679}. Best is trial 345 with value: 0.6773242003533158.\n",
      "[I 2023-12-11 17:41:30,949] Trial 348 finished with value: 0.6773271590344565 and parameters: {'n_estimators': 682}. Best is trial 348 with value: 0.6773271590344565.\n",
      "[I 2023-12-11 17:42:52,446] Trial 349 finished with value: 0.6773297993127595 and parameters: {'n_estimators': 675}. Best is trial 349 with value: 0.6773297993127595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6773\n",
      "\tBest params:\n",
      "\t\tn_estimators: 675\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698078    0.713316    0.703192    0.663824   \n",
      "1                    TP  329.000000  336.000000  308.000000  319.000000   \n",
      "2                    TN  189.000000  180.000000  191.000000  186.000000   \n",
      "3                    FP   38.000000   43.000000   62.000000   56.000000   \n",
      "4                    FN   39.000000   36.000000   34.000000   34.000000   \n",
      "5              Accuracy    0.870588    0.867227    0.838655    0.848739   \n",
      "6             Precision    0.896458    0.886544    0.832432    0.850667   \n",
      "7           Sensitivity    0.894022    0.903226    0.900585    0.903683   \n",
      "8           Specificity    0.832600    0.807200    0.754900    0.768600   \n",
      "9              F1 score    0.895238    0.894807    0.865169    0.876374   \n",
      "10  F1 score (weighted)    0.870642    0.866787    0.837102    0.847424   \n",
      "11     F1 score (macro)    0.863004    0.857426    0.832166    0.840784   \n",
      "12    Balanced Accuracy    0.863310    0.855200    0.827763    0.836139   \n",
      "13                  MCC    0.726013    0.715123    0.668299    0.684096   \n",
      "14                  NPV    0.828900    0.833300    0.848900    0.845500   \n",
      "15              ROC_AUC    0.863310    0.855200    0.827763    0.836139   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.699816    0.702531    0.706200  \n",
      "1   340.000000  323.000000  332.000000  \n",
      "2   170.000000  188.000000  165.000000  \n",
      "3    54.000000   44.000000   63.000000  \n",
      "4    31.000000   40.000000   35.000000  \n",
      "5     0.857143    0.858824    0.835294  \n",
      "6     0.862944    0.880109    0.840506  \n",
      "7     0.916442    0.889807    0.904632  \n",
      "8     0.758900    0.810300    0.723700  \n",
      "9     0.888889    0.884932    0.871391  \n",
      "10    0.855425    0.858597    0.832933  \n",
      "11    0.844444    0.851161    0.821210  \n",
      "12    0.837685    0.850076    0.814158  \n",
      "13    0.691842    0.702408    0.646644  \n",
      "14    0.845800    0.824600    0.825000  \n",
      "15    0.837685    0.850076    0.814158  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_6_cat = np.where((y_pred_rf_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 17:44:24,247] Trial 350 finished with value: 0.670648891918814 and parameters: {'n_estimators': 679}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:45:42,935] Trial 351 finished with value: 0.6704437496476111 and parameters: {'n_estimators': 646}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:47:04,091] Trial 352 finished with value: 0.6704928309114712 and parameters: {'n_estimators': 673}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:48:24,522] Trial 353 finished with value: 0.6705146208180165 and parameters: {'n_estimators': 659}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:49:46,869] Trial 354 finished with value: 0.6706116705133414 and parameters: {'n_estimators': 683}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:51:04,609] Trial 355 finished with value: 0.6702390804461342 and parameters: {'n_estimators': 631}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:52:25,327] Trial 356 finished with value: 0.6705131769594589 and parameters: {'n_estimators': 660}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:53:49,006] Trial 357 finished with value: 0.6705936306328899 and parameters: {'n_estimators': 684}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:55:06,623] Trial 358 finished with value: 0.6704062845034595 and parameters: {'n_estimators': 642}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:56:26,604] Trial 359 finished with value: 0.6704929456282056 and parameters: {'n_estimators': 670}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:57:50,956] Trial 360 finished with value: 0.6704361834865471 and parameters: {'n_estimators': 694}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 17:59:14,975] Trial 361 finished with value: 0.6705505023344838 and parameters: {'n_estimators': 686}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:00:33,618] Trial 362 finished with value: 0.6704958343945071 and parameters: {'n_estimators': 653}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:01:54,716] Trial 363 finished with value: 0.6704922820184945 and parameters: {'n_estimators': 671}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:03:20,350] Trial 364 finished with value: 0.6704519870475799 and parameters: {'n_estimators': 699}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:04:36,057] Trial 365 finished with value: 0.670375452728058 and parameters: {'n_estimators': 621}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:05:58,319] Trial 366 finished with value: 0.670537903923707 and parameters: {'n_estimators': 674}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:07:22,849] Trial 367 finished with value: 0.6704535294804153 and parameters: {'n_estimators': 692}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:08:44,487] Trial 368 finished with value: 0.670558993228503 and parameters: {'n_estimators': 667}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:10:02,935] Trial 369 finished with value: 0.6704062845034595 and parameters: {'n_estimators': 642}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:11:28,387] Trial 370 finished with value: 0.6704519870475799 and parameters: {'n_estimators': 699}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:12:49,990] Trial 371 finished with value: 0.6706160454808485 and parameters: {'n_estimators': 680}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:14:10,401] Trial 372 finished with value: 0.6705161964079701 and parameters: {'n_estimators': 661}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:15:35,703] Trial 373 finished with value: 0.67046359434876 and parameters: {'n_estimators': 700}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:16:57,958] Trial 374 finished with value: 0.6706325905512697 and parameters: {'n_estimators': 681}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:18:17,120] Trial 375 finished with value: 0.6704496703947498 and parameters: {'n_estimators': 651}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:19:40,030] Trial 376 finished with value: 0.6705538652579474 and parameters: {'n_estimators': 687}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:20:56,129] Trial 377 finished with value: 0.6702474390189757 and parameters: {'n_estimators': 632}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:22:13,687] Trial 378 finished with value: 0.6704954633616805 and parameters: {'n_estimators': 663}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:23:26,320] Trial 379 finished with value: 0.6704004558498925 and parameters: {'n_estimators': 607}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:24:51,994] Trial 380 finished with value: 0.6704340535974463 and parameters: {'n_estimators': 697}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:26:16,030] Trial 381 finished with value: 0.6704656142655884 and parameters: {'n_estimators': 703}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:27:37,626] Trial 382 finished with value: 0.6704928309114712 and parameters: {'n_estimators': 673}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:28:58,583] Trial 383 finished with value: 0.6704958343945071 and parameters: {'n_estimators': 653}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:30:25,583] Trial 384 finished with value: 0.6704656142655884 and parameters: {'n_estimators': 703}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:31:29,617] Trial 385 finished with value: 0.6706651739832321 and parameters: {'n_estimators': 530}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:32:54,091] Trial 386 finished with value: 0.6706116705133414 and parameters: {'n_estimators': 683}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:34:16,063] Trial 387 finished with value: 0.6705032562912858 and parameters: {'n_estimators': 668}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:35:41,338] Trial 388 finished with value: 0.6705929280169104 and parameters: {'n_estimators': 685}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:37:00,378] Trial 389 finished with value: 0.6704086152788686 and parameters: {'n_estimators': 639}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:38:25,479] Trial 390 finished with value: 0.6705420951300629 and parameters: {'n_estimators': 705}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:39:45,747] Trial 391 finished with value: 0.6704504555688027 and parameters: {'n_estimators': 655}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:41:09,098] Trial 392 finished with value: 0.6706116705133414 and parameters: {'n_estimators': 683}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:42:34,443] Trial 393 finished with value: 0.6704656142655884 and parameters: {'n_estimators': 703}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:43:54,563] Trial 394 finished with value: 0.6705146208180164 and parameters: {'n_estimators': 659}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:45:16,624] Trial 395 finished with value: 0.6706070715505497 and parameters: {'n_estimators': 676}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:46:40,798] Trial 396 finished with value: 0.6704535294804153 and parameters: {'n_estimators': 692}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:47:55,561] Trial 397 finished with value: 0.6704789331871922 and parameters: {'n_estimators': 615}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:49:20,043] Trial 398 finished with value: 0.6705130701692471 and parameters: {'n_estimators': 706}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:50:41,881] Trial 399 finished with value: 0.6705023021644003 and parameters: {'n_estimators': 665}. Best is trial 349 with value: 0.6773297993127595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6773\n",
      "\tBest params:\n",
      "\t\tn_estimators: 675\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698078    0.713316    0.703192    0.663824   \n",
      "1                    TP  329.000000  336.000000  308.000000  319.000000   \n",
      "2                    TN  189.000000  180.000000  191.000000  186.000000   \n",
      "3                    FP   38.000000   43.000000   62.000000   56.000000   \n",
      "4                    FN   39.000000   36.000000   34.000000   34.000000   \n",
      "5              Accuracy    0.870588    0.867227    0.838655    0.848739   \n",
      "6             Precision    0.896458    0.886544    0.832432    0.850667   \n",
      "7           Sensitivity    0.894022    0.903226    0.900585    0.903683   \n",
      "8           Specificity    0.832600    0.807200    0.754900    0.768600   \n",
      "9              F1 score    0.895238    0.894807    0.865169    0.876374   \n",
      "10  F1 score (weighted)    0.870642    0.866787    0.837102    0.847424   \n",
      "11     F1 score (macro)    0.863004    0.857426    0.832166    0.840784   \n",
      "12    Balanced Accuracy    0.863310    0.855200    0.827763    0.836139   \n",
      "13                  MCC    0.726013    0.715123    0.668299    0.684096   \n",
      "14                  NPV    0.828900    0.833300    0.848900    0.845500   \n",
      "15              ROC_AUC    0.863310    0.855200    0.827763    0.836139   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.699816    0.702531    0.706200    0.685020  \n",
      "1   340.000000  323.000000  332.000000  317.000000  \n",
      "2   170.000000  188.000000  165.000000  186.000000  \n",
      "3    54.000000   44.000000   63.000000   49.000000  \n",
      "4    31.000000   40.000000   35.000000   43.000000  \n",
      "5     0.857143    0.858824    0.835294    0.845378  \n",
      "6     0.862944    0.880109    0.840506    0.866120  \n",
      "7     0.916442    0.889807    0.904632    0.880556  \n",
      "8     0.758900    0.810300    0.723700    0.791500  \n",
      "9     0.888889    0.884932    0.871391    0.873278  \n",
      "10    0.855425    0.858597    0.832933    0.845017  \n",
      "11    0.844444    0.851161    0.821210    0.837501  \n",
      "12    0.837685    0.850076    0.814158    0.836022  \n",
      "13    0.691842    0.702408    0.646644    0.675189  \n",
      "14    0.845800    0.824600    0.825000    0.812200  \n",
      "15    0.837685    0.850076    0.814158    0.836022  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_7_cat = np.where((y_pred_rf_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 18:52:15,324] Trial 400 finished with value: 0.6744190002336495 and parameters: {'n_estimators': 685}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:53:32,521] Trial 401 finished with value: 0.6739872807009777 and parameters: {'n_estimators': 633}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:54:52,610] Trial 402 finished with value: 0.6739838070575856 and parameters: {'n_estimators': 650}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:56:12,432] Trial 403 finished with value: 0.6743471535324078 and parameters: {'n_estimators': 672}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:57:38,250] Trial 404 finished with value: 0.6744578755869457 and parameters: {'n_estimators': 703}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 18:59:08,390] Trial 405 finished with value: 0.674234068393299 and parameters: {'n_estimators': 737}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:00:31,370] Trial 406 finished with value: 0.6744190002336495 and parameters: {'n_estimators': 685}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:01:58,522] Trial 407 finished with value: 0.6744887851832454 and parameters: {'n_estimators': 709}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:03:11,829] Trial 408 finished with value: 0.6740338176758042 and parameters: {'n_estimators': 593}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:04:33,384] Trial 409 finished with value: 0.6742814609482644 and parameters: {'n_estimators': 664}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:05:57,771] Trial 410 finished with value: 0.6744538925958339 and parameters: {'n_estimators': 690}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:07:18,295] Trial 411 finished with value: 0.6739905306353637 and parameters: {'n_estimators': 643}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:08:46,654] Trial 412 finished with value: 0.6743672513320893 and parameters: {'n_estimators': 671}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:10:18,343] Trial 413 finished with value: 0.6744887851832454 and parameters: {'n_estimators': 709}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:11:43,615] Trial 414 finished with value: 0.674499085020895 and parameters: {'n_estimators': 689}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:13:04,815] Trial 415 finished with value: 0.6740900604092654 and parameters: {'n_estimators': 655}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:14:40,097] Trial 416 finished with value: 0.67437383513868 and parameters: {'n_estimators': 674}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:16:31,933] Trial 417 finished with value: 0.6741970529715442 and parameters: {'n_estimators': 745}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:18:20,824] Trial 418 finished with value: 0.6744887851832454 and parameters: {'n_estimators': 709}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:20:11,230] Trial 419 finished with value: 0.6744974004585584 and parameters: {'n_estimators': 688}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:21:53,945] Trial 420 finished with value: 0.6739906887897305 and parameters: {'n_estimators': 641}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:23:39,141] Trial 421 finished with value: 0.6742432448200286 and parameters: {'n_estimators': 667}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:25:29,487] Trial 422 finished with value: 0.6744713932574179 and parameters: {'n_estimators': 708}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:27:24,938] Trial 423 finished with value: 0.6742565914893868 and parameters: {'n_estimators': 738}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:29:14,411] Trial 424 finished with value: 0.6745069293095458 and parameters: {'n_estimators': 693}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:30:53,005] Trial 425 finished with value: 0.6740095800579999 and parameters: {'n_estimators': 621}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:32:45,246] Trial 426 finished with value: 0.6744165297684163 and parameters: {'n_estimators': 676}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:34:15,984] Trial 427 finished with value: 0.6738620394139911 and parameters: {'n_estimators': 557}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:35:35,233] Trial 428 finished with value: 0.6737864169549762 and parameters: {'n_estimators': 488}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:37:19,579] Trial 429 finished with value: 0.6740251069900107 and parameters: {'n_estimators': 652}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:39:11,584] Trial 430 finished with value: 0.6744591430694109 and parameters: {'n_estimators': 713}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:41:02,940] Trial 431 finished with value: 0.674488156202169 and parameters: {'n_estimators': 692}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:42:49,886] Trial 432 finished with value: 0.6742525688956438 and parameters: {'n_estimators': 668}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:44:39,158] Trial 433 finished with value: 0.6745069293095458 and parameters: {'n_estimators': 693}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:46:25,845] Trial 434 finished with value: 0.6741370421353774 and parameters: {'n_estimators': 660}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:48:21,748] Trial 435 finished with value: 0.6743208301271074 and parameters: {'n_estimators': 730}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:50:13,716] Trial 436 finished with value: 0.6744665702970083 and parameters: {'n_estimators': 712}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:51:54,264] Trial 437 finished with value: 0.674061003702041 and parameters: {'n_estimators': 631}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:53:40,581] Trial 438 finished with value: 0.6744694397787698 and parameters: {'n_estimators': 677}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:55:21,529] Trial 439 finished with value: 0.6739838070575856 and parameters: {'n_estimators': 650}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:57:05,084] Trial 440 finished with value: 0.6744390121369195 and parameters: {'n_estimators': 684}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 19:58:55,705] Trial 441 finished with value: 0.674497677806087 and parameters: {'n_estimators': 701}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 20:00:40,626] Trial 442 finished with value: 0.6743232958061357 and parameters: {'n_estimators': 673}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 20:02:32,874] Trial 443 finished with value: 0.6744711605600602 and parameters: {'n_estimators': 711}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 20:04:30,075] Trial 444 finished with value: 0.6742178531914437 and parameters: {'n_estimators': 740}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 20:06:12,556] Trial 445 finished with value: 0.6741184989592778 and parameters: {'n_estimators': 656}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 20:08:01,227] Trial 446 finished with value: 0.6744538925958338 and parameters: {'n_estimators': 690}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 20:09:55,623] Trial 447 finished with value: 0.6743820962006072 and parameters: {'n_estimators': 725}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 20:10:14,147] Trial 448 finished with value: 0.6725255788347416 and parameters: {'n_estimators': 110}. Best is trial 349 with value: 0.6773297993127595.\n",
      "[I 2023-12-11 20:12:02,977] Trial 449 finished with value: 0.6744694397787698 and parameters: {'n_estimators': 677}. Best is trial 349 with value: 0.6773297993127595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6773\n",
      "\tBest params:\n",
      "\t\tn_estimators: 675\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698078    0.713316    0.703192    0.663824   \n",
      "1                    TP  329.000000  336.000000  308.000000  319.000000   \n",
      "2                    TN  189.000000  180.000000  191.000000  186.000000   \n",
      "3                    FP   38.000000   43.000000   62.000000   56.000000   \n",
      "4                    FN   39.000000   36.000000   34.000000   34.000000   \n",
      "5              Accuracy    0.870588    0.867227    0.838655    0.848739   \n",
      "6             Precision    0.896458    0.886544    0.832432    0.850667   \n",
      "7           Sensitivity    0.894022    0.903226    0.900585    0.903683   \n",
      "8           Specificity    0.832600    0.807200    0.754900    0.768600   \n",
      "9              F1 score    0.895238    0.894807    0.865169    0.876374   \n",
      "10  F1 score (weighted)    0.870642    0.866787    0.837102    0.847424   \n",
      "11     F1 score (macro)    0.863004    0.857426    0.832166    0.840784   \n",
      "12    Balanced Accuracy    0.863310    0.855200    0.827763    0.836139   \n",
      "13                  MCC    0.726013    0.715123    0.668299    0.684096   \n",
      "14                  NPV    0.828900    0.833300    0.848900    0.845500   \n",
      "15              ROC_AUC    0.863310    0.855200    0.827763    0.836139   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.699816    0.702531    0.706200    0.685020    0.680507  \n",
      "1   340.000000  323.000000  332.000000  317.000000  330.000000  \n",
      "2   170.000000  188.000000  165.000000  186.000000  176.000000  \n",
      "3    54.000000   44.000000   63.000000   49.000000   45.000000  \n",
      "4    31.000000   40.000000   35.000000   43.000000   44.000000  \n",
      "5     0.857143    0.858824    0.835294    0.845378    0.850420  \n",
      "6     0.862944    0.880109    0.840506    0.866120    0.880000  \n",
      "7     0.916442    0.889807    0.904632    0.880556    0.882353  \n",
      "8     0.758900    0.810300    0.723700    0.791500    0.796400  \n",
      "9     0.888889    0.884932    0.871391    0.873278    0.881175  \n",
      "10    0.855425    0.858597    0.832933    0.845017    0.850350  \n",
      "11    0.844444    0.851161    0.821210    0.837501    0.839680  \n",
      "12    0.837685    0.850076    0.814158    0.836022    0.839367  \n",
      "13    0.691842    0.702408    0.646644    0.675189    0.679366  \n",
      "14    0.845800    0.824600    0.825000    0.812200    0.800000  \n",
      "15    0.837685    0.850076    0.814158    0.836022    0.839367  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_8_cat = np.where((y_pred_rf_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 20:13:58,066] Trial 450 finished with value: 0.6815013435302195 and parameters: {'n_estimators': 635}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:15:35,009] Trial 451 finished with value: 0.6813225464901776 and parameters: {'n_estimators': 602}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:17:16,043] Trial 452 finished with value: 0.6814441095188137 and parameters: {'n_estimators': 640}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:18:53,086] Trial 453 finished with value: 0.681329250144846 and parameters: {'n_estimators': 605}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:20:31,837] Trial 454 finished with value: 0.6814140429968824 and parameters: {'n_estimators': 623}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:22:06,849] Trial 455 finished with value: 0.681333569377675 and parameters: {'n_estimators': 600}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:23:36,367] Trial 456 finished with value: 0.6814250316888882 and parameters: {'n_estimators': 565}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:25:06,598] Trial 457 finished with value: 0.6814143411407716 and parameters: {'n_estimators': 569}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:26:40,032] Trial 458 finished with value: 0.6814185877578616 and parameters: {'n_estimators': 570}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:28:09,566] Trial 459 finished with value: 0.6813819680755806 and parameters: {'n_estimators': 566}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:29:40,441] Trial 460 finished with value: 0.6814754570436694 and parameters: {'n_estimators': 564}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:31:11,032] Trial 461 finished with value: 0.6814233592648109 and parameters: {'n_estimators': 556}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:32:42,551] Trial 462 finished with value: 0.6813805747418853 and parameters: {'n_estimators': 567}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:34:11,145] Trial 463 finished with value: 0.6815012659146787 and parameters: {'n_estimators': 562}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:35:36,675] Trial 464 finished with value: 0.6814272641594968 and parameters: {'n_estimators': 560}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:37:04,643] Trial 465 finished with value: 0.6814065046128325 and parameters: {'n_estimators': 551}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:38:34,746] Trial 466 finished with value: 0.6814250316888882 and parameters: {'n_estimators': 565}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:40:05,199] Trial 467 finished with value: 0.6815012659146787 and parameters: {'n_estimators': 562}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:41:37,519] Trial 468 finished with value: 0.6814250316888883 and parameters: {'n_estimators': 565}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:43:07,607] Trial 469 finished with value: 0.6813819680755804 and parameters: {'n_estimators': 566}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:44:39,844] Trial 470 finished with value: 0.6814250316888882 and parameters: {'n_estimators': 565}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:46:09,270] Trial 471 finished with value: 0.6813805747418854 and parameters: {'n_estimators': 567}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:47:37,514] Trial 472 finished with value: 0.6814629483965264 and parameters: {'n_estimators': 561}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:49:07,783] Trial 473 finished with value: 0.6813819680755806 and parameters: {'n_estimators': 566}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:50:35,815] Trial 474 finished with value: 0.6815012659146787 and parameters: {'n_estimators': 562}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:52:05,851] Trial 475 finished with value: 0.6814754570436694 and parameters: {'n_estimators': 564}. Best is trial 450 with value: 0.6815013435302195.\n",
      "[I 2023-12-11 20:53:33,714] Trial 476 finished with value: 0.681504545191818 and parameters: {'n_estimators': 563}. Best is trial 476 with value: 0.681504545191818.\n",
      "[I 2023-12-11 20:55:00,286] Trial 477 finished with value: 0.6813805747418854 and parameters: {'n_estimators': 567}. Best is trial 476 with value: 0.681504545191818.\n",
      "[I 2023-12-11 20:56:31,101] Trial 478 finished with value: 0.6814143411407717 and parameters: {'n_estimators': 569}. Best is trial 476 with value: 0.681504545191818.\n",
      "[I 2023-12-11 20:57:55,437] Trial 479 finished with value: 0.6813967899864688 and parameters: {'n_estimators': 532}. Best is trial 476 with value: 0.681504545191818.\n",
      "[I 2023-12-11 20:59:21,226] Trial 480 finished with value: 0.6813465041765506 and parameters: {'n_estimators': 535}. Best is trial 476 with value: 0.681504545191818.\n",
      "[I 2023-12-11 21:00:46,788] Trial 481 finished with value: 0.6812741534716774 and parameters: {'n_estimators': 543}. Best is trial 476 with value: 0.681504545191818.\n",
      "[I 2023-12-11 21:02:07,232] Trial 482 finished with value: 0.6815457123514852 and parameters: {'n_estimators': 513}. Best is trial 482 with value: 0.6815457123514852.\n",
      "[I 2023-12-11 21:03:31,122] Trial 483 finished with value: 0.6815764084981762 and parameters: {'n_estimators': 512}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:04:53,666] Trial 484 finished with value: 0.6815094297668112 and parameters: {'n_estimators': 519}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:06:15,561] Trial 485 finished with value: 0.681522942809376 and parameters: {'n_estimators': 514}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:07:37,952] Trial 486 finished with value: 0.6814677329568039 and parameters: {'n_estimators': 504}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:08:59,940] Trial 487 finished with value: 0.6815764084981762 and parameters: {'n_estimators': 512}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:10:20,828] Trial 488 finished with value: 0.6815094077826945 and parameters: {'n_estimators': 506}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:11:43,110] Trial 489 finished with value: 0.681522942809376 and parameters: {'n_estimators': 514}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:13:06,128] Trial 490 finished with value: 0.6815567121513059 and parameters: {'n_estimators': 515}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:14:27,837] Trial 491 finished with value: 0.6814932573235144 and parameters: {'n_estimators': 505}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:15:41,176] Trial 492 finished with value: 0.6811405568731926 and parameters: {'n_estimators': 462}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:17:03,126] Trial 493 finished with value: 0.6815060824840863 and parameters: {'n_estimators': 508}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:18:24,508] Trial 494 finished with value: 0.6815457123514852 and parameters: {'n_estimators': 513}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:19:44,030] Trial 495 finished with value: 0.6814677329568039 and parameters: {'n_estimators': 504}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:21:05,326] Trial 496 finished with value: 0.6814932573235144 and parameters: {'n_estimators': 505}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:22:24,759] Trial 497 finished with value: 0.6815060824840863 and parameters: {'n_estimators': 508}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:23:45,752] Trial 498 finished with value: 0.6815233433183949 and parameters: {'n_estimators': 509}. Best is trial 483 with value: 0.6815764084981762.\n",
      "[I 2023-12-11 21:25:07,219] Trial 499 finished with value: 0.6815094077826945 and parameters: {'n_estimators': 506}. Best is trial 483 with value: 0.6815764084981762.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6816\n",
      "\tBest params:\n",
      "\t\tn_estimators: 512\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698078    0.713316    0.703192    0.663824   \n",
      "1                    TP  329.000000  336.000000  308.000000  319.000000   \n",
      "2                    TN  189.000000  180.000000  191.000000  186.000000   \n",
      "3                    FP   38.000000   43.000000   62.000000   56.000000   \n",
      "4                    FN   39.000000   36.000000   34.000000   34.000000   \n",
      "5              Accuracy    0.870588    0.867227    0.838655    0.848739   \n",
      "6             Precision    0.896458    0.886544    0.832432    0.850667   \n",
      "7           Sensitivity    0.894022    0.903226    0.900585    0.903683   \n",
      "8           Specificity    0.832600    0.807200    0.754900    0.768600   \n",
      "9              F1 score    0.895238    0.894807    0.865169    0.876374   \n",
      "10  F1 score (weighted)    0.870642    0.866787    0.837102    0.847424   \n",
      "11     F1 score (macro)    0.863004    0.857426    0.832166    0.840784   \n",
      "12    Balanced Accuracy    0.863310    0.855200    0.827763    0.836139   \n",
      "13                  MCC    0.726013    0.715123    0.668299    0.684096   \n",
      "14                  NPV    0.828900    0.833300    0.848900    0.845500   \n",
      "15              ROC_AUC    0.863310    0.855200    0.827763    0.836139   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.699816    0.702531    0.706200    0.685020    0.680507    0.714576  \n",
      "1   340.000000  323.000000  332.000000  317.000000  330.000000  338.000000  \n",
      "2   170.000000  188.000000  165.000000  186.000000  176.000000  179.000000  \n",
      "3    54.000000   44.000000   63.000000   49.000000   45.000000   48.000000  \n",
      "4    31.000000   40.000000   35.000000   43.000000   44.000000   30.000000  \n",
      "5     0.857143    0.858824    0.835294    0.845378    0.850420    0.868908  \n",
      "6     0.862944    0.880109    0.840506    0.866120    0.880000    0.875648  \n",
      "7     0.916442    0.889807    0.904632    0.880556    0.882353    0.918478  \n",
      "8     0.758900    0.810300    0.723700    0.791500    0.796400    0.788500  \n",
      "9     0.888889    0.884932    0.871391    0.873278    0.881175    0.896552  \n",
      "10    0.855425    0.858597    0.832933    0.845017    0.850350    0.867766  \n",
      "11    0.844444    0.851161    0.821210    0.837501    0.839680    0.858826  \n",
      "12    0.837685    0.850076    0.814158    0.836022    0.839367    0.853512  \n",
      "13    0.691842    0.702408    0.646644    0.675189    0.679366    0.719456  \n",
      "14    0.845800    0.824600    0.825000    0.812200    0.800000    0.856500  \n",
      "15    0.837685    0.850076    0.814158    0.836022    0.839367    0.853512  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_9_cat = np.where((y_pred_rf_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6816\n",
      "\tBest params:\n",
      "\t\tn_estimators: 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.695785</td>\n",
       "      <td>0.035062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>165.600000</td>\n",
       "      <td>5.660781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>88.400000</td>\n",
       "      <td>5.834762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.011099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>5.021067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.854931</td>\n",
       "      <td>0.022248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.869284</td>\n",
       "      <td>0.023163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.901530</td>\n",
       "      <td>0.027126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.779470</td>\n",
       "      <td>0.043906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.884805</td>\n",
       "      <td>0.018120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.853901</td>\n",
       "      <td>0.022351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.844215</td>\n",
       "      <td>0.023859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.840512</td>\n",
       "      <td>0.024510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.690643</td>\n",
       "      <td>0.047424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.831280</td>\n",
       "      <td>0.039237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.840512</td>\n",
       "      <td>0.024510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.695785     0.035062\n",
       "1                    TP       165.600000     5.660781\n",
       "2                    TN        88.400000     5.834762\n",
       "3                    FP        25.000000     5.011099\n",
       "4                    FN        18.100000     5.021067\n",
       "5              Accuracy         0.854931     0.022248\n",
       "6             Precision         0.869284     0.023163\n",
       "7           Sensitivity         0.901530     0.027126\n",
       "8           Specificity         0.779470     0.043906\n",
       "9              F1 score         0.884805     0.018120\n",
       "10  F1 score (weighted)         0.853901     0.022351\n",
       "11     F1 score (macro)         0.844215     0.023859\n",
       "12    Balanced Accuracy         0.840512     0.024510\n",
       "13                  MCC         0.690643     0.047424\n",
       "14                  NPV         0.831280     0.039237\n",
       "15              ROC_AUC         0.840512     0.024510"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.698078</td>\n",
       "      <td>0.713316</td>\n",
       "      <td>0.703192</td>\n",
       "      <td>0.663824</td>\n",
       "      <td>0.699816</td>\n",
       "      <td>0.702531</td>\n",
       "      <td>0.706200</td>\n",
       "      <td>0.685020</td>\n",
       "      <td>0.680507</td>\n",
       "      <td>0.714576</td>\n",
       "      <td>0.696706</td>\n",
       "      <td>0.015822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>327.200000</td>\n",
       "      <td>10.271860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>8.628119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>50.200000</td>\n",
       "      <td>8.323995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.765618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.867227</td>\n",
       "      <td>0.838655</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.868908</td>\n",
       "      <td>0.854118</td>\n",
       "      <td>0.012497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.896458</td>\n",
       "      <td>0.886544</td>\n",
       "      <td>0.832432</td>\n",
       "      <td>0.850667</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.840506</td>\n",
       "      <td>0.866120</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.875648</td>\n",
       "      <td>0.867143</td>\n",
       "      <td>0.020672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.894022</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.900585</td>\n",
       "      <td>0.903683</td>\n",
       "      <td>0.916442</td>\n",
       "      <td>0.889807</td>\n",
       "      <td>0.904632</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>0.899378</td>\n",
       "      <td>0.012836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.832600</td>\n",
       "      <td>0.807200</td>\n",
       "      <td>0.754900</td>\n",
       "      <td>0.768600</td>\n",
       "      <td>0.758900</td>\n",
       "      <td>0.810300</td>\n",
       "      <td>0.723700</td>\n",
       "      <td>0.791500</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0.788500</td>\n",
       "      <td>0.783260</td>\n",
       "      <td>0.031914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>0.894807</td>\n",
       "      <td>0.865169</td>\n",
       "      <td>0.876374</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.884932</td>\n",
       "      <td>0.871391</td>\n",
       "      <td>0.873278</td>\n",
       "      <td>0.881175</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.882780</td>\n",
       "      <td>0.011082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.870642</td>\n",
       "      <td>0.866787</td>\n",
       "      <td>0.837102</td>\n",
       "      <td>0.847424</td>\n",
       "      <td>0.855425</td>\n",
       "      <td>0.858597</td>\n",
       "      <td>0.832933</td>\n",
       "      <td>0.845017</td>\n",
       "      <td>0.850350</td>\n",
       "      <td>0.867766</td>\n",
       "      <td>0.853204</td>\n",
       "      <td>0.012968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.863004</td>\n",
       "      <td>0.857426</td>\n",
       "      <td>0.832166</td>\n",
       "      <td>0.840784</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.851161</td>\n",
       "      <td>0.821210</td>\n",
       "      <td>0.837501</td>\n",
       "      <td>0.839680</td>\n",
       "      <td>0.858826</td>\n",
       "      <td>0.844620</td>\n",
       "      <td>0.013079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.863310</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.827763</td>\n",
       "      <td>0.836139</td>\n",
       "      <td>0.837685</td>\n",
       "      <td>0.850076</td>\n",
       "      <td>0.814158</td>\n",
       "      <td>0.836022</td>\n",
       "      <td>0.839367</td>\n",
       "      <td>0.853512</td>\n",
       "      <td>0.841323</td>\n",
       "      <td>0.014511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.726013</td>\n",
       "      <td>0.715123</td>\n",
       "      <td>0.668299</td>\n",
       "      <td>0.684096</td>\n",
       "      <td>0.691842</td>\n",
       "      <td>0.702408</td>\n",
       "      <td>0.646644</td>\n",
       "      <td>0.675189</td>\n",
       "      <td>0.679366</td>\n",
       "      <td>0.719456</td>\n",
       "      <td>0.690844</td>\n",
       "      <td>0.025086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.828900</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.848900</td>\n",
       "      <td>0.845500</td>\n",
       "      <td>0.845800</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.812200</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>0.832070</td>\n",
       "      <td>0.017606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.863310</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.827763</td>\n",
       "      <td>0.836139</td>\n",
       "      <td>0.837685</td>\n",
       "      <td>0.850076</td>\n",
       "      <td>0.814158</td>\n",
       "      <td>0.836022</td>\n",
       "      <td>0.839367</td>\n",
       "      <td>0.853512</td>\n",
       "      <td>0.841323</td>\n",
       "      <td>0.014511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.698078    0.713316    0.703192    0.663824   \n",
       "1                    TP  329.000000  336.000000  308.000000  319.000000   \n",
       "2                    TN  189.000000  180.000000  191.000000  186.000000   \n",
       "3                    FP   38.000000   43.000000   62.000000   56.000000   \n",
       "4                    FN   39.000000   36.000000   34.000000   34.000000   \n",
       "5              Accuracy    0.870588    0.867227    0.838655    0.848739   \n",
       "6             Precision    0.896458    0.886544    0.832432    0.850667   \n",
       "7           Sensitivity    0.894022    0.903226    0.900585    0.903683   \n",
       "8           Specificity    0.832600    0.807200    0.754900    0.768600   \n",
       "9              F1 score    0.895238    0.894807    0.865169    0.876374   \n",
       "10  F1 score (weighted)    0.870642    0.866787    0.837102    0.847424   \n",
       "11     F1 score (macro)    0.863004    0.857426    0.832166    0.840784   \n",
       "12    Balanced Accuracy    0.863310    0.855200    0.827763    0.836139   \n",
       "13                  MCC    0.726013    0.715123    0.668299    0.684096   \n",
       "14                  NPV    0.828900    0.833300    0.848900    0.845500   \n",
       "15              ROC_AUC    0.863310    0.855200    0.827763    0.836139   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.699816    0.702531    0.706200    0.685020    0.680507    0.714576   \n",
       "1   340.000000  323.000000  332.000000  317.000000  330.000000  338.000000   \n",
       "2   170.000000  188.000000  165.000000  186.000000  176.000000  179.000000   \n",
       "3    54.000000   44.000000   63.000000   49.000000   45.000000   48.000000   \n",
       "4    31.000000   40.000000   35.000000   43.000000   44.000000   30.000000   \n",
       "5     0.857143    0.858824    0.835294    0.845378    0.850420    0.868908   \n",
       "6     0.862944    0.880109    0.840506    0.866120    0.880000    0.875648   \n",
       "7     0.916442    0.889807    0.904632    0.880556    0.882353    0.918478   \n",
       "8     0.758900    0.810300    0.723700    0.791500    0.796400    0.788500   \n",
       "9     0.888889    0.884932    0.871391    0.873278    0.881175    0.896552   \n",
       "10    0.855425    0.858597    0.832933    0.845017    0.850350    0.867766   \n",
       "11    0.844444    0.851161    0.821210    0.837501    0.839680    0.858826   \n",
       "12    0.837685    0.850076    0.814158    0.836022    0.839367    0.853512   \n",
       "13    0.691842    0.702408    0.646644    0.675189    0.679366    0.719456   \n",
       "14    0.845800    0.824600    0.825000    0.812200    0.800000    0.856500   \n",
       "15    0.837685    0.850076    0.814158    0.836022    0.839367    0.853512   \n",
       "\n",
       "           ave        std  \n",
       "0     0.696706   0.015822  \n",
       "1   327.200000  10.271860  \n",
       "2   181.000000   8.628119  \n",
       "3    50.200000   8.323995  \n",
       "4    36.600000   4.765618  \n",
       "5     0.854118   0.012497  \n",
       "6     0.867143   0.020672  \n",
       "7     0.899378   0.012836  \n",
       "8     0.783260   0.031914  \n",
       "9     0.882780   0.011082  \n",
       "10    0.853204   0.012968  \n",
       "11    0.844620   0.013079  \n",
       "12    0.841323   0.014511  \n",
       "13    0.690844   0.025086  \n",
       "14    0.832070   0.017606  \n",
       "15    0.841323   0.014511  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_rf0</th>\n",
       "      <th>y_pred_rf1</th>\n",
       "      <th>y_pred_rf2</th>\n",
       "      <th>y_pred_rf3</th>\n",
       "      <th>y_pred_rf4</th>\n",
       "      <th>y_pred_rf_ave</th>\n",
       "      <th>y_pred_rf_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2047687</td>\n",
       "      <td>0</td>\n",
       "      <td>5.48</td>\n",
       "      <td>6.487443</td>\n",
       "      <td>6.172934</td>\n",
       "      <td>6.294066</td>\n",
       "      <td>6.766141</td>\n",
       "      <td>6.212660</td>\n",
       "      <td>6.235541</td>\n",
       "      <td>0.392365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1164212</td>\n",
       "      <td>1</td>\n",
       "      <td>5.76</td>\n",
       "      <td>5.956641</td>\n",
       "      <td>5.793450</td>\n",
       "      <td>5.606055</td>\n",
       "      <td>5.828203</td>\n",
       "      <td>5.530371</td>\n",
       "      <td>5.745787</td>\n",
       "      <td>0.141218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2337873</td>\n",
       "      <td>2</td>\n",
       "      <td>6.07</td>\n",
       "      <td>6.281426</td>\n",
       "      <td>6.237559</td>\n",
       "      <td>6.228262</td>\n",
       "      <td>6.257529</td>\n",
       "      <td>6.300801</td>\n",
       "      <td>6.229263</td>\n",
       "      <td>0.075357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4577419</td>\n",
       "      <td>3</td>\n",
       "      <td>7.90</td>\n",
       "      <td>7.753711</td>\n",
       "      <td>7.721074</td>\n",
       "      <td>7.760254</td>\n",
       "      <td>7.743535</td>\n",
       "      <td>7.759980</td>\n",
       "      <td>7.773092</td>\n",
       "      <td>0.058306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL515452</td>\n",
       "      <td>4</td>\n",
       "      <td>6.22</td>\n",
       "      <td>6.341273</td>\n",
       "      <td>6.478275</td>\n",
       "      <td>6.470288</td>\n",
       "      <td>6.489688</td>\n",
       "      <td>6.474082</td>\n",
       "      <td>6.412268</td>\n",
       "      <td>0.099620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL3693800</td>\n",
       "      <td>2966</td>\n",
       "      <td>8.22</td>\n",
       "      <td>8.190238</td>\n",
       "      <td>8.214844</td>\n",
       "      <td>8.157988</td>\n",
       "      <td>8.180781</td>\n",
       "      <td>8.174941</td>\n",
       "      <td>8.189799</td>\n",
       "      <td>0.021808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL2431917</td>\n",
       "      <td>2967</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.931250</td>\n",
       "      <td>6.940498</td>\n",
       "      <td>6.894951</td>\n",
       "      <td>6.794541</td>\n",
       "      <td>6.865117</td>\n",
       "      <td>6.981059</td>\n",
       "      <td>0.219529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL2413298</td>\n",
       "      <td>2968</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.037363</td>\n",
       "      <td>6.013965</td>\n",
       "      <td>5.999160</td>\n",
       "      <td>6.024043</td>\n",
       "      <td>6.069922</td>\n",
       "      <td>6.067409</td>\n",
       "      <td>0.088877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3656016</td>\n",
       "      <td>2969</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.277852</td>\n",
       "      <td>8.361680</td>\n",
       "      <td>8.319395</td>\n",
       "      <td>8.293730</td>\n",
       "      <td>8.361367</td>\n",
       "      <td>8.355671</td>\n",
       "      <td>0.079876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL4643138</td>\n",
       "      <td>2970</td>\n",
       "      <td>6.38</td>\n",
       "      <td>6.224056</td>\n",
       "      <td>6.328841</td>\n",
       "      <td>6.281445</td>\n",
       "      <td>6.217490</td>\n",
       "      <td>6.365693</td>\n",
       "      <td>6.299588</td>\n",
       "      <td>0.063857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_rf0  y_pred_rf1  \\\n",
       "0         CHEMBL2047687            0     5.48    6.487443    6.172934   \n",
       "1         CHEMBL1164212            1     5.76    5.956641    5.793450   \n",
       "2         CHEMBL2337873            2     6.07    6.281426    6.237559   \n",
       "3         CHEMBL4577419            3     7.90    7.753711    7.721074   \n",
       "4          CHEMBL515452            4     6.22    6.341273    6.478275   \n",
       "...                 ...          ...      ...         ...         ...   \n",
       "2966      CHEMBL3693800         2966     8.22    8.190238    8.214844   \n",
       "2967      CHEMBL2431917         2967     7.46    6.931250    6.940498   \n",
       "2968      CHEMBL2413298         2968     6.26    6.037363    6.013965   \n",
       "2969      CHEMBL3656016         2969     8.52    8.277852    8.361680   \n",
       "2970      CHEMBL4643138         2970     6.38    6.224056    6.328841   \n",
       "\n",
       "      y_pred_rf2  y_pred_rf3  y_pred_rf4  y_pred_rf_ave  y_pred_rf_std  \n",
       "0       6.294066    6.766141    6.212660       6.235541       0.392365  \n",
       "1       5.606055    5.828203    5.530371       5.745787       0.141218  \n",
       "2       6.228262    6.257529    6.300801       6.229263       0.075357  \n",
       "3       7.760254    7.743535    7.759980       7.773092       0.058306  \n",
       "4       6.470288    6.489688    6.474082       6.412268       0.099620  \n",
       "...          ...         ...         ...            ...            ...  \n",
       "2966    8.157988    8.180781    8.174941       8.189799       0.021808  \n",
       "2967    6.894951    6.794541    6.865117       6.981059       0.219529  \n",
       "2968    5.999160    6.024043    6.069922       6.067409       0.088877  \n",
       "2969    8.319395    8.293730    8.361367       8.355671       0.079876  \n",
       "2970    6.281445    6.217490    6.365693       6.299588       0.063857  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where((y_pred_optimized_rf >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "rf_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47203ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.699421</td>\n",
       "      <td>0.032262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.854861</td>\n",
       "      <td>0.019029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.871522</td>\n",
       "      <td>0.021260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.898262</td>\n",
       "      <td>0.019072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.783962</td>\n",
       "      <td>0.043908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.884464</td>\n",
       "      <td>0.014417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.853989</td>\n",
       "      <td>0.019517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.844145</td>\n",
       "      <td>0.021952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.841115</td>\n",
       "      <td>0.023206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.689852</td>\n",
       "      <td>0.043010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.826172</td>\n",
       "      <td>0.031839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.841115</td>\n",
       "      <td>0.023206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.699421     0.032262\n",
       "1              Accuracy         0.854861     0.019029\n",
       "2             Precision         0.871522     0.021260\n",
       "3           Sensitivity         0.898262     0.019072\n",
       "4           Specificity         0.783962     0.043908\n",
       "5              F1 score         0.884464     0.014417\n",
       "6   F1 score (weighted)         0.853989     0.019517\n",
       "7      F1 score (macro)         0.844145     0.021952\n",
       "8     Balanced Accuracy         0.841115     0.023206\n",
       "9                   MCC         0.689852     0.043010\n",
       "10                  NPV         0.826172     0.031839\n",
       "11              ROC_AUC         0.841115     0.023206"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.6883 with a standard deviation of 0.0455\n",
      "RF optimized model r2_score 0.6905 with a standard deviation of 0.0471\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf.joblib']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_reg, \"OUTPUT/rf_reg.joblib\")\n",
    "#joblib.dump(optimized_rf, \"OUTPUT/optimized_rf.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.685840     0.037290\n",
      "1                    TP       162.000000     6.000000\n",
      "2                    TN        89.200000     5.411921\n",
      "3                    FP        24.200000     3.645393\n",
      "4                    FN        21.700000     6.766749\n",
      "5              Accuracy         0.845521     0.026458\n",
      "6             Precision         0.870197     0.017561\n",
      "7           Sensitivity         0.882145     0.035800\n",
      "8           Specificity         0.786310     0.033805\n",
      "9              F1 score         0.875819     0.022021\n",
      "10  F1 score (weighted)         0.845193     0.025908\n",
      "11     F1 score (macro)         0.835528     0.027842\n",
      "12    Balanced Accuracy         0.834225     0.026186\n",
      "13                  MCC         0.672291     0.055871\n",
      "14                  NPV         0.806080     0.052256\n",
      "15              ROC_AUC         0.834225     0.026186\n",
      "CPU times: user 15.9 s, sys: 96.1 ms, total: 15.9 s\n",
      "Wall time: 2.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=8,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:37:35,158] A new study created in memory with name: lgbmRegressor\n",
      "[I 2023-12-11 21:37:36,697] Trial 0 finished with value: 0.6579495630360206 and parameters: {'n_estimators': 461, 'learning_rate': 0.15798534695614164, 'max_depth': 4, 'max_bin': 153, 'num_leaves': 406}. Best is trial 0 with value: 0.6579495630360206.\n",
      "[I 2023-12-11 21:37:39,827] Trial 1 finished with value: 0.6636679704926822 and parameters: {'n_estimators': 759, 'learning_rate': 0.04367121853753636, 'max_depth': 6, 'max_bin': 170, 'num_leaves': 144}. Best is trial 1 with value: 0.6636679704926822.\n",
      "[I 2023-12-11 21:37:40,724] Trial 2 finished with value: 0.6279012675226717 and parameters: {'n_estimators': 61, 'learning_rate': 0.1263439736153897, 'max_depth': 6, 'max_bin': 170, 'num_leaves': 229}. Best is trial 1 with value: 0.6636679704926822.\n",
      "[I 2023-12-11 21:37:43,112] Trial 3 finished with value: 0.6622562783137355 and parameters: {'n_estimators': 877, 'learning_rate': 0.16865451269295004, 'max_depth': 4, 'max_bin': 216, 'num_leaves': 501}. Best is trial 1 with value: 0.6636679704926822.\n",
      "[I 2023-12-11 21:37:46,094] Trial 4 finished with value: 0.6490695420093882 and parameters: {'n_estimators': 257, 'learning_rate': 0.031141377132377353, 'max_depth': 7, 'max_bin': 299, 'num_leaves': 399}. Best is trial 1 with value: 0.6636679704926822.\n",
      "[I 2023-12-11 21:37:47,182] Trial 5 finished with value: 0.6377087169253552 and parameters: {'n_estimators': 119, 'learning_rate': 0.12921975699301413, 'max_depth': 5, 'max_bin': 167, 'num_leaves': 379}. Best is trial 1 with value: 0.6636679704926822.\n",
      "[I 2023-12-11 21:37:48,094] Trial 6 finished with value: 0.5903699516599493 and parameters: {'n_estimators': 137, 'learning_rate': 0.08083153332549564, 'max_depth': 3, 'max_bin': 213, 'num_leaves': 529}. Best is trial 1 with value: 0.6636679704926822.\n",
      "[I 2023-12-11 21:37:51,310] Trial 7 finished with value: 0.6811736932282214 and parameters: {'n_estimators': 218, 'learning_rate': 0.1541321971200782, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 612}. Best is trial 7 with value: 0.6811736932282214.\n",
      "[I 2023-12-11 21:37:55,404] Trial 8 finished with value: 0.6834255405077563 and parameters: {'n_estimators': 385, 'learning_rate': 0.07811050409192884, 'max_depth': 12, 'max_bin': 186, 'num_leaves': 151}. Best is trial 8 with value: 0.6834255405077563.\n",
      "[I 2023-12-11 21:37:58,338] Trial 9 finished with value: 0.6819078281656134 and parameters: {'n_estimators': 493, 'learning_rate': 0.14251344533567575, 'max_depth': 9, 'max_bin': 298, 'num_leaves': 154}. Best is trial 8 with value: 0.6834255405077563.\n",
      "[I 2023-12-11 21:38:08,790] Trial 10 finished with value: 0.6115133590375794 and parameters: {'n_estimators': 452, 'learning_rate': 0.0044419980278009585, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 748}. Best is trial 8 with value: 0.6834255405077563.\n",
      "[I 2023-12-11 21:38:12,542] Trial 11 finished with value: 0.6831363190459111 and parameters: {'n_estimators': 608, 'learning_rate': 0.09333848659445856, 'max_depth': 9, 'max_bin': 295, 'num_leaves': 70}. Best is trial 8 with value: 0.6834255405077563.\n",
      "[I 2023-12-11 21:38:15,841] Trial 12 finished with value: 0.6848681554877483 and parameters: {'n_estimators': 617, 'learning_rate': 0.09531079012372828, 'max_depth': 9, 'max_bin': 264, 'num_leaves': 49}. Best is trial 12 with value: 0.6848681554877483.\n",
      "[I 2023-12-11 21:38:18,718] Trial 13 finished with value: 0.6785919742070885 and parameters: {'n_estimators': 624, 'learning_rate': 0.19133663152027464, 'max_depth': 10, 'max_bin': 261, 'num_leaves': 48}. Best is trial 12 with value: 0.6848681554877483.\n",
      "[I 2023-12-11 21:38:22,912] Trial 14 finished with value: 0.685085308148134 and parameters: {'n_estimators': 316, 'learning_rate': 0.07052215574327984, 'max_depth': 12, 'max_bin': 198, 'num_leaves': 246}. Best is trial 14 with value: 0.685085308148134.\n",
      "[I 2023-12-11 21:38:26,173] Trial 15 finished with value: 0.6780874020734761 and parameters: {'n_estimators': 348, 'learning_rate': 0.10814353802890238, 'max_depth': 9, 'max_bin': 259, 'num_leaves': 273}. Best is trial 14 with value: 0.685085308148134.\n",
      "[I 2023-12-11 21:38:30,230] Trial 16 finished with value: 0.6768579045038396 and parameters: {'n_estimators': 635, 'learning_rate': 0.0642945280310046, 'max_depth': 8, 'max_bin': 236, 'num_leaves': 283}. Best is trial 14 with value: 0.685085308148134.\n",
      "[I 2023-12-11 21:38:33,503] Trial 17 finished with value: 0.6818977602495536 and parameters: {'n_estimators': 741, 'learning_rate': 0.11275308111724162, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 34}. Best is trial 14 with value: 0.685085308148134.\n",
      "[I 2023-12-11 21:38:37,716] Trial 18 finished with value: 0.6819789091468405 and parameters: {'n_estimators': 292, 'learning_rate': 0.05930790141182367, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 219}. Best is trial 14 with value: 0.685085308148134.\n",
      "[I 2023-12-11 21:38:41,942] Trial 19 finished with value: 0.689226132498457 and parameters: {'n_estimators': 552, 'learning_rate': 0.09721863153459188, 'max_depth': 10, 'max_bin': 245, 'num_leaves': 330}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:38:45,537] Trial 20 finished with value: 0.6850632910665502 and parameters: {'n_estimators': 514, 'learning_rate': 0.1092996477194208, 'max_depth': 10, 'max_bin': 245, 'num_leaves': 351}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:38:49,175] Trial 21 finished with value: 0.6850406475078276 and parameters: {'n_estimators': 533, 'learning_rate': 0.11333560582691703, 'max_depth': 10, 'max_bin': 241, 'num_leaves': 322}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:38:52,878] Trial 22 finished with value: 0.6862340026031397 and parameters: {'n_estimators': 383, 'learning_rate': 0.08890501658119596, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 468}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:38:57,164] Trial 23 finished with value: 0.6869221148562298 and parameters: {'n_estimators': 388, 'learning_rate': 0.08463799277298045, 'max_depth': 12, 'max_bin': 195, 'num_leaves': 467}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:39:01,478] Trial 24 finished with value: 0.6848895134571568 and parameters: {'n_estimators': 386, 'learning_rate': 0.09021217111424963, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 473}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:39:05,960] Trial 25 finished with value: 0.6830639436409564 and parameters: {'n_estimators': 423, 'learning_rate': 0.0846249720363786, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 580}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:39:09,226] Trial 26 finished with value: 0.6810762769523022 and parameters: {'n_estimators': 227, 'learning_rate': 0.09685165990696559, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 454}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:39:13,977] Trial 27 finished with value: 0.6814211391813745 and parameters: {'n_estimators': 561, 'learning_rate': 0.05133325469930871, 'max_depth': 10, 'max_bin': 248, 'num_leaves': 551}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:39:18,490] Trial 28 finished with value: 0.6891396362734299 and parameters: {'n_estimators': 373, 'learning_rate': 0.07580247235292956, 'max_depth': 12, 'max_bin': 186, 'num_leaves': 636}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:39:22,043] Trial 29 finished with value: 0.6791078007328818 and parameters: {'n_estimators': 691, 'learning_rate': 0.07328029961137167, 'max_depth': 8, 'max_bin': 155, 'num_leaves': 662}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:39:26,805] Trial 30 finished with value: 0.6859299414981224 and parameters: {'n_estimators': 420, 'learning_rate': 0.06618231524530196, 'max_depth': 11, 'max_bin': 183, 'num_leaves': 682}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:39:31,308] Trial 31 finished with value: 0.6858078133462808 and parameters: {'n_estimators': 477, 'learning_rate': 0.0827161982900021, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 447}. Best is trial 19 with value: 0.689226132498457.\n",
      "[I 2023-12-11 21:39:35,247] Trial 32 finished with value: 0.6893895777854706 and parameters: {'n_estimators': 358, 'learning_rate': 0.0997149229551944, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 425}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:39:39,171] Trial 33 finished with value: 0.6824346217520845 and parameters: {'n_estimators': 339, 'learning_rate': 0.1239373370855775, 'max_depth': 12, 'max_bin': 188, 'num_leaves': 334}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:39:42,523] Trial 34 finished with value: 0.6865846634559964 and parameters: {'n_estimators': 267, 'learning_rate': 0.10269489371870348, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 414}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:39:47,427] Trial 35 finished with value: 0.6797653893686924 and parameters: {'n_estimators': 859, 'learning_rate': 0.047551280172499835, 'max_depth': 11, 'max_bin': 173, 'num_leaves': 627}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:39:50,485] Trial 36 finished with value: 0.6765132386621808 and parameters: {'n_estimators': 561, 'learning_rate': 0.09974915720220384, 'max_depth': 7, 'max_bin': 194, 'num_leaves': 724}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:39:53,209] Trial 37 finished with value: 0.6776337135578784 and parameters: {'n_estimators': 167, 'learning_rate': 0.12041236275255526, 'max_depth': 12, 'max_bin': 180, 'num_leaves': 517}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:39:56,023] Trial 38 finished with value: 0.6654743513503283 and parameters: {'n_estimators': 440, 'learning_rate': 0.13250966687468696, 'max_depth': 5, 'max_bin': 221, 'num_leaves': 400}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:39:57,595] Trial 39 finished with value: 0.6506859797741636 and parameters: {'n_estimators': 70, 'learning_rate': 0.07665327711499709, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 584}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:00,180] Trial 40 finished with value: 0.6807611695836461 and parameters: {'n_estimators': 190, 'learning_rate': 0.10214608102553588, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 367}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:03,002] Trial 41 finished with value: 0.6788192824625723 and parameters: {'n_estimators': 252, 'learning_rate': 0.10277350117988615, 'max_depth': 8, 'max_bin': 207, 'num_leaves': 415}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:06,650] Trial 42 finished with value: 0.6826065945975369 and parameters: {'n_estimators': 286, 'learning_rate': 0.08668706156669706, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 424}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:10,721] Trial 43 finished with value: 0.6809114582064505 and parameters: {'n_estimators': 354, 'learning_rate': 0.07884721249390002, 'max_depth': 12, 'max_bin': 191, 'num_leaves': 322}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:12,892] Trial 44 finished with value: 0.6688398268081468 and parameters: {'n_estimators': 285, 'learning_rate': 0.11608580374288083, 'max_depth': 6, 'max_bin': 176, 'num_leaves': 499}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:16,591] Trial 45 finished with value: 0.6820518922742205 and parameters: {'n_estimators': 403, 'learning_rate': 0.1356774844906583, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 380}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:20,440] Trial 46 finished with value: 0.6849048080573915 and parameters: {'n_estimators': 482, 'learning_rate': 0.10350432965563731, 'max_depth': 9, 'max_bin': 166, 'num_leaves': 544}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:24,214] Trial 47 finished with value: 0.6832173319576065 and parameters: {'n_estimators': 311, 'learning_rate': 0.09307353185165272, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 198}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:25,703] Trial 48 finished with value: 0.6327373001279428 and parameters: {'n_estimators': 238, 'learning_rate': 0.12250728457946816, 'max_depth': 3, 'max_bin': 196, 'num_leaves': 289}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:30,229] Trial 49 finished with value: 0.6810585180851281 and parameters: {'n_estimators': 452, 'learning_rate': 0.07257478486572212, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 495}. Best is trial 32 with value: 0.6893895777854706.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6894\n",
      "\tBest params:\n",
      "\t\tn_estimators: 358\n",
      "\t\tlearning_rate: 0.0997149229551944\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 209\n",
      "\t\tnum_leaves: 425\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.707475\n",
      "1                    TP  326.000000\n",
      "2                    TN  186.000000\n",
      "3                    FP   41.000000\n",
      "4                    FN   42.000000\n",
      "5              Accuracy    0.860504\n",
      "6             Precision    0.888283\n",
      "7           Sensitivity    0.885870\n",
      "8           Specificity    0.819400\n",
      "9              F1 score    0.887075\n",
      "10  F1 score (weighted)    0.860563\n",
      "11     F1 score (macro)    0.852329\n",
      "12    Balanced Accuracy    0.852626\n",
      "13                  MCC    0.704663\n",
      "14                  NPV    0.815800\n",
      "15              ROC_AUC    0.852626\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_0_cat = np.where((y_pred_lgbm_0>= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:40:34,994] Trial 50 finished with value: 0.6818691757249026 and parameters: {'n_estimators': 361, 'learning_rate': 0.05773453601017673, 'max_depth': 11, 'max_bin': 188, 'num_leaves': 110}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:39,474] Trial 51 finished with value: 0.6835466080899035 and parameters: {'n_estimators': 384, 'learning_rate': 0.08741934167516924, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 434}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:43,827] Trial 52 finished with value: 0.6872850881540918 and parameters: {'n_estimators': 347, 'learning_rate': 0.09312248457390916, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 397}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:47,683] Trial 53 finished with value: 0.679958015348143 and parameters: {'n_estimators': 324, 'learning_rate': 0.09759371127012928, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 397}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:50,711] Trial 54 finished with value: 0.673058825689796 and parameters: {'n_estimators': 192, 'learning_rate': 0.11008754174003171, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 362}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:54,620] Trial 55 finished with value: 0.6775572423752793 and parameters: {'n_estimators': 270, 'learning_rate': 0.07979827284231886, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 300}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:40:57,793] Trial 56 finished with value: 0.6719155363924016 and parameters: {'n_estimators': 538, 'learning_rate': 0.10486356246136584, 'max_depth': 9, 'max_bin': 210, 'num_leaves': 476}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:01,638] Trial 57 finished with value: 0.6838727004553625 and parameters: {'n_estimators': 313, 'learning_rate': 0.09291891900251159, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 341}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:04,801] Trial 58 finished with value: 0.6783351972849501 and parameters: {'n_estimators': 661, 'learning_rate': 0.11615732537712474, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 265}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:09,697] Trial 59 finished with value: 0.6792726074826186 and parameters: {'n_estimators': 582, 'learning_rate': 0.07118221057394139, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 389}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:11,698] Trial 60 finished with value: 0.6565086414412196 and parameters: {'n_estimators': 122, 'learning_rate': 0.08393063887579266, 'max_depth': 8, 'max_bin': 235, 'num_leaves': 428}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:16,220] Trial 61 finished with value: 0.6872727891264919 and parameters: {'n_estimators': 371, 'learning_rate': 0.0898066439243433, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 463}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:20,090] Trial 62 finished with value: 0.677037083289949 and parameters: {'n_estimators': 360, 'learning_rate': 0.09444408614816124, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 457}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:23,492] Trial 63 finished with value: 0.6795621935791412 and parameters: {'n_estimators': 500, 'learning_rate': 0.1083855378367729, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 520}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:27,270] Trial 64 finished with value: 0.6829797141130156 and parameters: {'n_estimators': 411, 'learning_rate': 0.08880612340685705, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 481}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:31,439] Trial 65 finished with value: 0.681148223927199 and parameters: {'n_estimators': 462, 'learning_rate': 0.06690888857731987, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 302}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:35,484] Trial 66 finished with value: 0.6810758322153218 and parameters: {'n_estimators': 435, 'learning_rate': 0.0981717293514347, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 576}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:39,729] Trial 67 finished with value: 0.6830351555105482 and parameters: {'n_estimators': 377, 'learning_rate': 0.07799477381683685, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 634}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:43,511] Trial 68 finished with value: 0.6780959997177461 and parameters: {'n_estimators': 335, 'learning_rate': 0.08340217339562435, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 441}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:46,286] Trial 69 finished with value: 0.6684977848696052 and parameters: {'n_estimators': 298, 'learning_rate': 0.10703471139158235, 'max_depth': 9, 'max_bin': 203, 'num_leaves': 357}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:49,013] Trial 70 finished with value: 0.6571385098700411 and parameters: {'n_estimators': 401, 'learning_rate': 0.09146606928905646, 'max_depth': 4, 'max_bin': 259, 'num_leaves': 244}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:53,006] Trial 71 finished with value: 0.6824495023196376 and parameters: {'n_estimators': 378, 'learning_rate': 0.08801876814402645, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 410}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:41:57,161] Trial 72 finished with value: 0.6794348066984623 and parameters: {'n_estimators': 342, 'learning_rate': 0.09984174325966291, 'max_depth': 12, 'max_bin': 184, 'num_leaves': 463}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:01,353] Trial 73 finished with value: 0.6825670755527151 and parameters: {'n_estimators': 429, 'learning_rate': 0.08159088791056085, 'max_depth': 12, 'max_bin': 193, 'num_leaves': 544}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:04,611] Trial 74 finished with value: 0.6789627715662624 and parameters: {'n_estimators': 273, 'learning_rate': 0.096076960589714, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 377}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:09,462] Trial 75 finished with value: 0.6817740509742117 and parameters: {'n_estimators': 509, 'learning_rate': 0.07435368471926142, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 509}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:12,810] Trial 76 finished with value: 0.6765858064005739 and parameters: {'n_estimators': 255, 'learning_rate': 0.11485725446722181, 'max_depth': 10, 'max_bin': 245, 'num_leaves': 749}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:15,105] Trial 77 finished with value: 0.6671079842588598 and parameters: {'n_estimators': 212, 'learning_rate': 0.10300255344794897, 'max_depth': 7, 'max_bin': 179, 'num_leaves': 487}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:18,096] Trial 78 finished with value: 0.6757534638551188 and parameters: {'n_estimators': 459, 'learning_rate': 0.08947946664069374, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 323}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:22,706] Trial 79 finished with value: 0.6846930615339358 and parameters: {'n_estimators': 397, 'learning_rate': 0.06723196532810514, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 704}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:26,778] Trial 80 finished with value: 0.6787782646106815 and parameters: {'n_estimators': 322, 'learning_rate': 0.07579675536234647, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 414}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:31,163] Trial 81 finished with value: 0.6829549107607902 and parameters: {'n_estimators': 365, 'learning_rate': 0.06539750978886055, 'max_depth': 11, 'max_bin': 170, 'num_leaves': 676}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:35,469] Trial 82 finished with value: 0.6828668432403612 and parameters: {'n_estimators': 423, 'learning_rate': 0.0829334875798731, 'max_depth': 10, 'max_bin': 181, 'num_leaves': 698}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:39,401] Trial 83 finished with value: 0.680294446966648 and parameters: {'n_estimators': 342, 'learning_rate': 0.09317006697393336, 'max_depth': 12, 'max_bin': 191, 'num_leaves': 608}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:43,626] Trial 84 finished with value: 0.6808610233460419 and parameters: {'n_estimators': 531, 'learning_rate': 0.0857651382799112, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 641}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:47,278] Trial 85 finished with value: 0.6755994472584638 and parameters: {'n_estimators': 482, 'learning_rate': 0.10126979002297587, 'max_depth': 11, 'max_bin': 186, 'num_leaves': 659}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:52,215] Trial 86 finished with value: 0.682570167815386 and parameters: {'n_estimators': 304, 'learning_rate': 0.06034993522600577, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 562}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:56,260] Trial 87 finished with value: 0.6798343365499733 and parameters: {'n_estimators': 407, 'learning_rate': 0.07785469544792344, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 443}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:42:59,074] Trial 88 finished with value: 0.6750299166632268 and parameters: {'n_estimators': 447, 'learning_rate': 0.1106729234396841, 'max_depth': 11, 'max_bin': 202, 'num_leaves': 465}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:03,623] Trial 89 finished with value: 0.6825696774875982 and parameters: {'n_estimators': 597, 'learning_rate': 0.0715882967581697, 'max_depth': 12, 'max_bin': 195, 'num_leaves': 723}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:07,835] Trial 90 finished with value: 0.6769957726956 and parameters: {'n_estimators': 374, 'learning_rate': 0.09685262557249408, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 609}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:12,275] Trial 91 finished with value: 0.685076270193965 and parameters: {'n_estimators': 480, 'learning_rate': 0.08187546786211208, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 452}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:16,879] Trial 92 finished with value: 0.6824063299509145 and parameters: {'n_estimators': 413, 'learning_rate': 0.08998675714776522, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 395}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:21,464] Trial 93 finished with value: 0.6833139110682982 and parameters: {'n_estimators': 550, 'learning_rate': 0.08613351492241064, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 426}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:25,126] Trial 94 finished with value: 0.6756624724050996 and parameters: {'n_estimators': 466, 'learning_rate': 0.10359194123177869, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 376}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:29,910] Trial 95 finished with value: 0.6859510307631591 and parameters: {'n_estimators': 354, 'learning_rate': 0.06928038614391717, 'max_depth': 12, 'max_bin': 173, 'num_leaves': 348}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:33,937] Trial 96 finished with value: 0.6778822001752318 and parameters: {'n_estimators': 392, 'learning_rate': 0.06974377216872908, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 345}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:38,372] Trial 97 finished with value: 0.6819128370481906 and parameters: {'n_estimators': 330, 'learning_rate': 0.0752068155670941, 'max_depth': 12, 'max_bin': 175, 'num_leaves': 315}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:41,252] Trial 98 finished with value: 0.6606720686736932 and parameters: {'n_estimators': 279, 'learning_rate': 0.06350570436150271, 'max_depth': 6, 'max_bin': 170, 'num_leaves': 405}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:45,238] Trial 99 finished with value: 0.6783758753756086 and parameters: {'n_estimators': 360, 'learning_rate': 0.09372174503289937, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 353}. Best is trial 32 with value: 0.6893895777854706.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6894\n",
      "\tBest params:\n",
      "\t\tn_estimators: 358\n",
      "\t\tlearning_rate: 0.0997149229551944\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 209\n",
      "\t\tnum_leaves: 425\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.707475    0.712588\n",
      "1                    TP  326.000000  329.000000\n",
      "2                    TN  186.000000  182.000000\n",
      "3                    FP   41.000000   41.000000\n",
      "4                    FN   42.000000   43.000000\n",
      "5              Accuracy    0.860504    0.858824\n",
      "6             Precision    0.888283    0.889189\n",
      "7           Sensitivity    0.885870    0.884409\n",
      "8           Specificity    0.819400    0.816100\n",
      "9              F1 score    0.887075    0.886792\n",
      "10  F1 score (weighted)    0.860563    0.858948\n",
      "11     F1 score (macro)    0.852329    0.849646\n",
      "12    Balanced Accuracy    0.852626    0.850276\n",
      "13                  MCC    0.704663    0.699314\n",
      "14                  NPV    0.815800    0.808900\n",
      "15              ROC_AUC    0.852626    0.850276\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_1_cat = np.where((y_pred_lgbm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:43:50,365] Trial 100 finished with value: 0.6729954820005902 and parameters: {'n_estimators': 355, 'learning_rate': 0.05427859456759859, 'max_depth': 11, 'max_bin': 178, 'num_leaves': 500}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:54,508] Trial 101 finished with value: 0.6815314939491786 and parameters: {'n_estimators': 425, 'learning_rate': 0.0823828190516165, 'max_depth': 12, 'max_bin': 189, 'num_leaves': 440}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:43:59,005] Trial 102 finished with value: 0.6798250713448306 and parameters: {'n_estimators': 389, 'learning_rate': 0.07962873679670207, 'max_depth': 12, 'max_bin': 183, 'num_leaves': 534}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:02,600] Trial 103 finished with value: 0.6802714010114901 and parameters: {'n_estimators': 444, 'learning_rate': 0.09877732382388624, 'max_depth': 12, 'max_bin': 151, 'num_leaves': 387}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:07,012] Trial 104 finished with value: 0.680334619641658 and parameters: {'n_estimators': 844, 'learning_rate': 0.0877666662306187, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 368}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:11,445] Trial 105 finished with value: 0.6771103584512462 and parameters: {'n_estimators': 299, 'learning_rate': 0.06908996960036887, 'max_depth': 12, 'max_bin': 173, 'num_leaves': 480}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:15,172] Trial 106 finished with value: 0.6807019555127032 and parameters: {'n_estimators': 647, 'learning_rate': 0.1067517012848317, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 428}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:18,672] Trial 107 finished with value: 0.6773277208740428 and parameters: {'n_estimators': 242, 'learning_rate': 0.0750111987143528, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 451}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:22,010] Trial 108 finished with value: 0.6756807717537674 and parameters: {'n_estimators': 323, 'learning_rate': 0.09103177101252541, 'max_depth': 9, 'max_bin': 226, 'num_leaves': 411}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:25,892] Trial 109 finished with value: 0.6790560288235566 and parameters: {'n_estimators': 420, 'learning_rate': 0.09533657886289329, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 329}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:29,785] Trial 110 finished with value: 0.6757095441428709 and parameters: {'n_estimators': 389, 'learning_rate': 0.0798821892400654, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 469}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:33,975] Trial 111 finished with value: 0.6790292625728254 and parameters: {'n_estimators': 349, 'learning_rate': 0.06233264578822202, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 180}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:38,226] Trial 112 finished with value: 0.6770446043446456 and parameters: {'n_estimators': 261, 'learning_rate': 0.07000665706803962, 'max_depth': 12, 'max_bin': 200, 'num_leaves': 246}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:41,970] Trial 113 finished with value: 0.680006226051216 and parameters: {'n_estimators': 310, 'learning_rate': 0.08473697848632732, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 219}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:46,326] Trial 114 finished with value: 0.6789598157148949 and parameters: {'n_estimators': 371, 'learning_rate': 0.07372599546206275, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 280}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:50,895] Trial 115 finished with value: 0.6756228183023671 and parameters: {'n_estimators': 340, 'learning_rate': 0.057679672889235416, 'max_depth': 12, 'max_bin': 184, 'num_leaves': 129}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:55,352] Trial 116 finished with value: 0.677266857487542 and parameters: {'n_estimators': 497, 'learning_rate': 0.06725414714828405, 'max_depth': 11, 'max_bin': 188, 'num_leaves': 307}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:44:59,301] Trial 117 finished with value: 0.6829304336910221 and parameters: {'n_estimators': 290, 'learning_rate': 0.09131839303324692, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 420}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:02,762] Trial 118 finished with value: 0.6781763327116569 and parameters: {'n_estimators': 322, 'learning_rate': 0.10025524133142148, 'max_depth': 11, 'max_bin': 234, 'num_leaves': 518}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:07,621] Trial 119 finished with value: 0.6777834798929917 and parameters: {'n_estimators': 517, 'learning_rate': 0.07917017964115532, 'max_depth': 10, 'max_bin': 243, 'num_leaves': 689}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:12,478] Trial 120 finished with value: 0.6868399570917431 and parameters: {'n_estimators': 377, 'learning_rate': 0.08571200408208067, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 266}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:16,278] Trial 121 finished with value: 0.6798627111012524 and parameters: {'n_estimators': 377, 'learning_rate': 0.0850848464414438, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 266}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:19,774] Trial 122 finished with value: 0.6782246068495097 and parameters: {'n_estimators': 576, 'learning_rate': 0.09700573926267593, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 248}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:23,498] Trial 123 finished with value: 0.6808089359879793 and parameters: {'n_estimators': 404, 'learning_rate': 0.08934195418121421, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 184}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:27,804] Trial 124 finished with value: 0.681052249545193 and parameters: {'n_estimators': 357, 'learning_rate': 0.07554686858643594, 'max_depth': 12, 'max_bin': 199, 'num_leaves': 234}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:30,881] Trial 125 finished with value: 0.6825668296144431 and parameters: {'n_estimators': 433, 'learning_rate': 0.10640572651033235, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 395}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:33,485] Trial 126 finished with value: 0.6619289691712272 and parameters: {'n_estimators': 378, 'learning_rate': 0.08185461024458322, 'max_depth': 5, 'max_bin': 229, 'num_leaves': 491}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:37,719] Trial 127 finished with value: 0.6747286076141167 and parameters: {'n_estimators': 340, 'learning_rate': 0.06525441820743032, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 338}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:41,216] Trial 128 finished with value: 0.6755767126173715 and parameters: {'n_estimators': 408, 'learning_rate': 0.07190737822061982, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 362}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:44,014] Trial 129 finished with value: 0.6801770104549324 and parameters: {'n_estimators': 313, 'learning_rate': 0.09322130270828086, 'max_depth': 12, 'max_bin': 195, 'num_leaves': 285}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:48,128] Trial 130 finished with value: 0.6830883767689263 and parameters: {'n_estimators': 358, 'learning_rate': 0.08765972158704351, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 732}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:52,076] Trial 131 finished with value: 0.6792127418077312 and parameters: {'n_estimators': 464, 'learning_rate': 0.08220070272088079, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 463}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:55,824] Trial 132 finished with value: 0.6739695837450669 and parameters: {'n_estimators': 491, 'learning_rate': 0.07742559742022359, 'max_depth': 9, 'max_bin': 236, 'num_leaves': 445}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:45:59,750] Trial 133 finished with value: 0.6860711502726013 and parameters: {'n_estimators': 442, 'learning_rate': 0.10187881300144043, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 449}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:02,942] Trial 134 finished with value: 0.6712676625281324 and parameters: {'n_estimators': 445, 'learning_rate': 0.10171926309224041, 'max_depth': 8, 'max_bin': 222, 'num_leaves': 665}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:06,721] Trial 135 finished with value: 0.679344245124617 and parameters: {'n_estimators': 397, 'learning_rate': 0.10960984892602388, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 383}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:10,123] Trial 136 finished with value: 0.6805759955109612 and parameters: {'n_estimators': 370, 'learning_rate': 0.09639641004712818, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 427}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:14,186] Trial 137 finished with value: 0.6792932800583642 and parameters: {'n_estimators': 426, 'learning_rate': 0.104037322556028, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 474}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:18,396] Trial 138 finished with value: 0.6802629727012139 and parameters: {'n_estimators': 332, 'learning_rate': 0.08676902168690577, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 403}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:22,317] Trial 139 finished with value: 0.6780370305478349 and parameters: {'n_estimators': 391, 'learning_rate': 0.10065533375516225, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 297}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:25,786] Trial 140 finished with value: 0.6821684319897607 and parameters: {'n_estimators': 279, 'learning_rate': 0.09234654201512124, 'max_depth': 12, 'max_bin': 180, 'num_leaves': 438}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:30,680] Trial 141 finished with value: 0.6827216993650096 and parameters: {'n_estimators': 526, 'learning_rate': 0.08239902254042356, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 453}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:33,554] Trial 142 finished with value: 0.6757621447360149 and parameters: {'n_estimators': 460, 'learning_rate': 0.11324092264400606, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 457}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:37,707] Trial 143 finished with value: 0.6752974634756421 and parameters: {'n_estimators': 483, 'learning_rate': 0.07708602097705307, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 215}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:42,378] Trial 144 finished with value: 0.6788651111049099 and parameters: {'n_estimators': 413, 'learning_rate': 0.07272624337765668, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 481}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:46,629] Trial 145 finished with value: 0.6811585099296541 and parameters: {'n_estimators': 475, 'learning_rate': 0.08894134484778057, 'max_depth': 12, 'max_bin': 190, 'num_leaves': 415}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:50,756] Trial 146 finished with value: 0.6794881691710288 and parameters: {'n_estimators': 437, 'learning_rate': 0.09626215541051163, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 498}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:55,238] Trial 147 finished with value: 0.6775820695672026 and parameters: {'n_estimators': 353, 'learning_rate': 0.06799786374924205, 'max_depth': 11, 'max_bin': 185, 'num_leaves': 445}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:46:59,268] Trial 148 finished with value: 0.6806621883585035 and parameters: {'n_estimators': 704, 'learning_rate': 0.0852832350397496, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 591}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:47:04,050] Trial 149 finished with value: 0.6794012055291502 and parameters: {'n_estimators': 371, 'learning_rate': 0.06021563825613291, 'max_depth': 12, 'max_bin': 176, 'num_leaves': 262}. Best is trial 32 with value: 0.6893895777854706.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6894\n",
      "\tBest params:\n",
      "\t\tn_estimators: 358\n",
      "\t\tlearning_rate: 0.0997149229551944\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 209\n",
      "\t\tnum_leaves: 425\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.707475    0.712588    0.708269\n",
      "1                    TP  326.000000  329.000000  309.000000\n",
      "2                    TN  186.000000  182.000000  190.000000\n",
      "3                    FP   41.000000   41.000000   63.000000\n",
      "4                    FN   42.000000   43.000000   33.000000\n",
      "5              Accuracy    0.860504    0.858824    0.838655\n",
      "6             Precision    0.888283    0.889189    0.830645\n",
      "7           Sensitivity    0.885870    0.884409    0.903509\n",
      "8           Specificity    0.819400    0.816100    0.751000\n",
      "9              F1 score    0.887075    0.886792    0.865546\n",
      "10  F1 score (weighted)    0.860563    0.858948    0.836961\n",
      "11     F1 score (macro)    0.852329    0.849646    0.831933\n",
      "12    Balanced Accuracy    0.852626    0.850276    0.827248\n",
      "13                  MCC    0.704663    0.699314    0.668432\n",
      "14                  NPV    0.815800    0.808900    0.852000\n",
      "15              ROC_AUC    0.852626    0.850276    0.827248\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_2_cat = np.where((y_pred_lgbm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:47:08,097] Trial 150 finished with value: 0.6851289732484257 and parameters: {'n_estimators': 210, 'learning_rate': 0.08021496471857605, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 373}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:47:11,755] Trial 151 finished with value: 0.6884515051809654 and parameters: {'n_estimators': 235, 'learning_rate': 0.08168102198758859, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 372}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:47:14,336] Trial 152 finished with value: 0.6754872460032126 and parameters: {'n_estimators': 158, 'learning_rate': 0.07914791633125928, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 379}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:47:17,437] Trial 153 finished with value: 0.6845963833389044 and parameters: {'n_estimators': 230, 'learning_rate': 0.09116578612020888, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 351}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:47:20,569] Trial 154 finished with value: 0.6789172686870519 and parameters: {'n_estimators': 198, 'learning_rate': 0.06398231164892164, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 370}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:47:24,237] Trial 155 finished with value: 0.6832567624249297 and parameters: {'n_estimators': 212, 'learning_rate': 0.0728993622795027, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 400}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:47:27,573] Trial 156 finished with value: 0.6866780581907361 and parameters: {'n_estimators': 260, 'learning_rate': 0.098606409668961, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 328}. Best is trial 32 with value: 0.6893895777854706.\n",
      "[I 2023-12-11 21:47:30,959] Trial 157 finished with value: 0.6898640882009115 and parameters: {'n_estimators': 242, 'learning_rate': 0.1048805349156567, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 328}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:47:34,223] Trial 158 finished with value: 0.6826615918388176 and parameters: {'n_estimators': 242, 'learning_rate': 0.09934883171982978, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 317}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:47:36,980] Trial 159 finished with value: 0.6842561930596458 and parameters: {'n_estimators': 186, 'learning_rate': 0.10597013996280821, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 329}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:47:39,882] Trial 160 finished with value: 0.6859197449668 and parameters: {'n_estimators': 265, 'learning_rate': 0.09565655003981743, 'max_depth': 9, 'max_bin': 201, 'num_leaves': 334}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:47:42,835] Trial 161 finished with value: 0.6830071177427526 and parameters: {'n_estimators': 256, 'learning_rate': 0.10334951476029931, 'max_depth': 9, 'max_bin': 202, 'num_leaves': 343}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:47:45,749] Trial 162 finished with value: 0.6849667946648389 and parameters: {'n_estimators': 227, 'learning_rate': 0.09530555208022717, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 310}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:47:48,382] Trial 163 finished with value: 0.6813720685720247 and parameters: {'n_estimators': 268, 'learning_rate': 0.1107430667710274, 'max_depth': 9, 'max_bin': 203, 'num_leaves': 335}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:47:51,022] Trial 164 finished with value: 0.6822017960377711 and parameters: {'n_estimators': 306, 'learning_rate': 0.09932459988365104, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 348}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:47:53,384] Trial 165 finished with value: 0.6885312643588501 and parameters: {'n_estimators': 248, 'learning_rate': 0.11839323798371, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 360}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:47:55,860] Trial 166 finished with value: 0.6871254398248088 and parameters: {'n_estimators': 243, 'learning_rate': 0.11674038941559639, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 355}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:47:58,448] Trial 167 finished with value: 0.6888539477928328 and parameters: {'n_estimators': 235, 'learning_rate': 0.11736454723322906, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 358}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:48:00,912] Trial 168 finished with value: 0.6838040861161048 and parameters: {'n_estimators': 170, 'learning_rate': 0.11909276831297971, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 358}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:48:04,147] Trial 169 finished with value: 0.6865317412948545 and parameters: {'n_estimators': 246, 'learning_rate': 0.11819347768443436, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 362}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:48:07,138] Trial 170 finished with value: 0.6829700649494853 and parameters: {'n_estimators': 247, 'learning_rate': 0.11542922061318635, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 386}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:48:10,272] Trial 171 finished with value: 0.6835819843033968 and parameters: {'n_estimators': 220, 'learning_rate': 0.12165164269211359, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 369}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:48:13,064] Trial 172 finished with value: 0.6875698446948848 and parameters: {'n_estimators': 240, 'learning_rate': 0.11775249536347705, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 358}. Best is trial 157 with value: 0.6898640882009115.\n",
      "[I 2023-12-11 21:48:16,379] Trial 173 finished with value: 0.6912339451393456 and parameters: {'n_estimators': 234, 'learning_rate': 0.11756888337413936, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 322}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:19,210] Trial 174 finished with value: 0.6857948565519101 and parameters: {'n_estimators': 235, 'learning_rate': 0.12597972451187525, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 316}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:22,088] Trial 175 finished with value: 0.686668822573799 and parameters: {'n_estimators': 253, 'learning_rate': 0.11763436579082394, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 297}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:24,825] Trial 176 finished with value: 0.6854943231304014 and parameters: {'n_estimators': 204, 'learning_rate': 0.11793553638766083, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 301}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:27,447] Trial 177 finished with value: 0.6837366350717297 and parameters: {'n_estimators': 248, 'learning_rate': 0.12210324665034655, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 291}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:30,333] Trial 178 finished with value: 0.6885848710822 and parameters: {'n_estimators': 286, 'learning_rate': 0.11398703185330869, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 327}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:33,322] Trial 179 finished with value: 0.6871487725257541 and parameters: {'n_estimators': 287, 'learning_rate': 0.11271268270391534, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 324}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:36,312] Trial 180 finished with value: 0.6876539685815982 and parameters: {'n_estimators': 293, 'learning_rate': 0.11276789474474494, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 330}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:39,438] Trial 181 finished with value: 0.6900769465223249 and parameters: {'n_estimators': 280, 'learning_rate': 0.11329434858811625, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 330}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:42,513] Trial 182 finished with value: 0.6812870267683643 and parameters: {'n_estimators': 289, 'learning_rate': 0.11307433182887593, 'max_depth': 10, 'max_bin': 221, 'num_leaves': 324}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:45,825] Trial 183 finished with value: 0.6904666966636523 and parameters: {'n_estimators': 276, 'learning_rate': 0.11260973104040571, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 328}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:49,037] Trial 184 finished with value: 0.6834002524756791 and parameters: {'n_estimators': 287, 'learning_rate': 0.12577288337053513, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 348}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:52,068] Trial 185 finished with value: 0.6869580820334489 and parameters: {'n_estimators': 274, 'learning_rate': 0.12125735101819378, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 312}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:55,238] Trial 186 finished with value: 0.6884012239524904 and parameters: {'n_estimators': 272, 'learning_rate': 0.11228018810695749, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 337}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:48:58,389] Trial 187 finished with value: 0.6854781044069738 and parameters: {'n_estimators': 276, 'learning_rate': 0.11451151993948828, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 313}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:49:01,396] Trial 188 finished with value: 0.6867831174053916 and parameters: {'n_estimators': 228, 'learning_rate': 0.11039738591914604, 'max_depth': 10, 'max_bin': 222, 'num_leaves': 331}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:49:04,560] Trial 189 finished with value: 0.6878154506399671 and parameters: {'n_estimators': 276, 'learning_rate': 0.12124226050480391, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 340}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:49:07,642] Trial 190 finished with value: 0.6837904645707283 and parameters: {'n_estimators': 298, 'learning_rate': 0.1290559300045876, 'max_depth': 9, 'max_bin': 213, 'num_leaves': 357}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:49:10,747] Trial 191 finished with value: 0.6878182348600627 and parameters: {'n_estimators': 283, 'learning_rate': 0.1244499744856893, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 343}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:49:13,877] Trial 192 finished with value: 0.6877650328442378 and parameters: {'n_estimators': 289, 'learning_rate': 0.11313270485322828, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 346}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:49:17,017] Trial 193 finished with value: 0.6883929538899145 and parameters: {'n_estimators': 294, 'learning_rate': 0.11244023243355296, 'max_depth': 10, 'max_bin': 221, 'num_leaves': 322}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:49:20,802] Trial 194 finished with value: 0.6861613690609383 and parameters: {'n_estimators': 309, 'learning_rate': 0.12261795767111679, 'max_depth': 10, 'max_bin': 220, 'num_leaves': 342}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:49:24,467] Trial 195 finished with value: 0.6887687439414044 and parameters: {'n_estimators': 268, 'learning_rate': 0.1082897859214406, 'max_depth': 10, 'max_bin': 223, 'num_leaves': 340}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:49:27,522] Trial 196 finished with value: 0.6817714177083621 and parameters: {'n_estimators': 264, 'learning_rate': 0.10786152443833943, 'max_depth': 10, 'max_bin': 223, 'num_leaves': 337}. Best is trial 173 with value: 0.6912339451393456.\n",
      "[I 2023-12-11 21:49:30,868] Trial 197 finished with value: 0.6933563112923475 and parameters: {'n_estimators': 279, 'learning_rate': 0.11146307406304887, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 344}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:49:34,447] Trial 198 finished with value: 0.6881270200707345 and parameters: {'n_estimators': 278, 'learning_rate': 0.11220365897114729, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 342}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:49:38,028] Trial 199 finished with value: 0.6885315179761259 and parameters: {'n_estimators': 296, 'learning_rate': 0.11036176522540528, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 305}. Best is trial 197 with value: 0.6933563112923475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6934\n",
      "\tBest params:\n",
      "\t\tn_estimators: 279\n",
      "\t\tlearning_rate: 0.11146307406304887\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 211\n",
      "\t\tnum_leaves: 344\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.707475    0.712588    0.708269    0.684985\n",
      "1                    TP  326.000000  329.000000  309.000000  309.000000\n",
      "2                    TN  186.000000  182.000000  190.000000  183.000000\n",
      "3                    FP   41.000000   41.000000   63.000000   59.000000\n",
      "4                    FN   42.000000   43.000000   33.000000   44.000000\n",
      "5              Accuracy    0.860504    0.858824    0.838655    0.826891\n",
      "6             Precision    0.888283    0.889189    0.830645    0.839674\n",
      "7           Sensitivity    0.885870    0.884409    0.903509    0.875354\n",
      "8           Specificity    0.819400    0.816100    0.751000    0.756200\n",
      "9              F1 score    0.887075    0.886792    0.865546    0.857143\n",
      "10  F1 score (weighted)    0.860563    0.858948    0.836961    0.825923\n",
      "11     F1 score (macro)    0.852329    0.849646    0.831933    0.818763\n",
      "12    Balanced Accuracy    0.852626    0.850276    0.827248    0.815776\n",
      "13                  MCC    0.704663    0.699314    0.668432    0.638657\n",
      "14                  NPV    0.815800    0.808900    0.852000    0.806200\n",
      "15              ROC_AUC    0.852626    0.850276    0.827248    0.815776\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_3_cat = np.where((y_pred_lgbm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:49:41,250] Trial 200 finished with value: 0.672398983467474 and parameters: {'n_estimators': 282, 'learning_rate': 0.10896229371991623, 'max_depth': 9, 'max_bin': 210, 'num_leaves': 281}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:49:44,130] Trial 201 finished with value: 0.6753326284035068 and parameters: {'n_estimators': 297, 'learning_rate': 0.11296523228086457, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 308}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:49:47,219] Trial 202 finished with value: 0.678626145206785 and parameters: {'n_estimators': 268, 'learning_rate': 0.11099341001063845, 'max_depth': 10, 'max_bin': 220, 'num_leaves': 341}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:49:50,221] Trial 203 finished with value: 0.6799052927125464 and parameters: {'n_estimators': 297, 'learning_rate': 0.11495736197661101, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 323}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:49:53,172] Trial 204 finished with value: 0.6750327888035058 and parameters: {'n_estimators': 279, 'learning_rate': 0.107753898793148, 'max_depth': 10, 'max_bin': 224, 'num_leaves': 337}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:49:55,583] Trial 205 finished with value: 0.6719815082373135 and parameters: {'n_estimators': 261, 'learning_rate': 0.12780758917577634, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 302}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:49:58,740] Trial 206 finished with value: 0.6750409847623018 and parameters: {'n_estimators': 317, 'learning_rate': 0.12038676559913952, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 321}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:01,655] Trial 207 finished with value: 0.6752472717779067 and parameters: {'n_estimators': 224, 'learning_rate': 0.10711096494501508, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 349}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:04,341] Trial 208 finished with value: 0.6723858552666162 and parameters: {'n_estimators': 274, 'learning_rate': 0.12488055653858787, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 370}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:07,291] Trial 209 finished with value: 0.6754589356624203 and parameters: {'n_estimators': 306, 'learning_rate': 0.11275718897970957, 'max_depth': 9, 'max_bin': 214, 'num_leaves': 329}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:10,132] Trial 210 finished with value: 0.6764213449510231 and parameters: {'n_estimators': 256, 'learning_rate': 0.13257394678617673, 'max_depth': 10, 'max_bin': 225, 'num_leaves': 344}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:13,449] Trial 211 finished with value: 0.6764271589679157 and parameters: {'n_estimators': 235, 'learning_rate': 0.11858878463429919, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 350}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:16,535] Trial 212 finished with value: 0.6750920290968991 and parameters: {'n_estimators': 292, 'learning_rate': 0.11669108246330417, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 363}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:19,716] Trial 213 finished with value: 0.680079270457898 and parameters: {'n_estimators': 239, 'learning_rate': 0.11489128546722154, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 317}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:22,988] Trial 214 finished with value: 0.680693294674689 and parameters: {'n_estimators': 266, 'learning_rate': 0.1103552537392936, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 359}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:25,854] Trial 215 finished with value: 0.6702628558401906 and parameters: {'n_estimators': 212, 'learning_rate': 0.12037555274389114, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 291}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:29,144] Trial 216 finished with value: 0.6724297043200859 and parameters: {'n_estimators': 286, 'learning_rate': 0.10461049938342916, 'max_depth': 10, 'max_bin': 221, 'num_leaves': 336}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:32,247] Trial 217 finished with value: 0.676609332385526 and parameters: {'n_estimators': 256, 'learning_rate': 0.1233860735390001, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 376}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:35,582] Trial 218 finished with value: 0.6749790099133204 and parameters: {'n_estimators': 274, 'learning_rate': 0.11628110500302008, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 310}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:39,025] Trial 219 finished with value: 0.6784623382348182 and parameters: {'n_estimators': 317, 'learning_rate': 0.11167659905324874, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 328}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:42,011] Trial 220 finished with value: 0.6726369558811089 and parameters: {'n_estimators': 184, 'learning_rate': 0.10646849502108331, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 356}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:45,501] Trial 221 finished with value: 0.6822152120427616 and parameters: {'n_estimators': 328, 'learning_rate': 0.10973439623951864, 'max_depth': 10, 'max_bin': 222, 'num_leaves': 392}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:49,062] Trial 222 finished with value: 0.6773643246382397 and parameters: {'n_estimators': 299, 'learning_rate': 0.1182090567075732, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 340}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:52,348] Trial 223 finished with value: 0.6809141304256349 and parameters: {'n_estimators': 247, 'learning_rate': 0.11385400420922734, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 366}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:55,634] Trial 224 finished with value: 0.6761059064271517 and parameters: {'n_estimators': 228, 'learning_rate': 0.10555497137583164, 'max_depth': 10, 'max_bin': 226, 'num_leaves': 382}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:50:58,670] Trial 225 finished with value: 0.6727006814295585 and parameters: {'n_estimators': 285, 'learning_rate': 0.12383264813907548, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 323}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:01,530] Trial 226 finished with value: 0.6704603484104708 and parameters: {'n_estimators': 257, 'learning_rate': 0.11661914355928132, 'max_depth': 9, 'max_bin': 219, 'num_leaves': 301}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:05,179] Trial 227 finished with value: 0.6776545056057633 and parameters: {'n_estimators': 307, 'learning_rate': 0.10948942710374526, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 350}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:08,229] Trial 228 finished with value: 0.6722655044416233 and parameters: {'n_estimators': 279, 'learning_rate': 0.12002696859521993, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 334}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:11,676] Trial 229 finished with value: 0.6762665755418558 and parameters: {'n_estimators': 237, 'learning_rate': 0.10373965868192758, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 319}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:14,641] Trial 230 finished with value: 0.6786025742460949 and parameters: {'n_estimators': 265, 'learning_rate': 0.11408443795693672, 'max_depth': 10, 'max_bin': 221, 'num_leaves': 366}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:17,832] Trial 231 finished with value: 0.6737662662512578 and parameters: {'n_estimators': 330, 'learning_rate': 0.10871257301441799, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 348}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:21,284] Trial 232 finished with value: 0.6794126798081076 and parameters: {'n_estimators': 291, 'learning_rate': 0.11212277503505048, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 339}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:24,870] Trial 233 finished with value: 0.677226692155548 and parameters: {'n_estimators': 246, 'learning_rate': 0.11936752854045485, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 380}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:27,907] Trial 234 finished with value: 0.6758571689741262 and parameters: {'n_estimators': 219, 'learning_rate': 0.12911479113773544, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 307}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:30,922] Trial 235 finished with value: 0.6688810730758216 and parameters: {'n_estimators': 271, 'learning_rate': 0.12428996764157416, 'max_depth': 9, 'max_bin': 224, 'num_leaves': 328}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:34,241] Trial 236 finished with value: 0.6854344134498871 and parameters: {'n_estimators': 320, 'learning_rate': 0.11533620696954214, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 357}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:37,773] Trial 237 finished with value: 0.6764495178327417 and parameters: {'n_estimators': 302, 'learning_rate': 0.10767339733464919, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 316}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:40,920] Trial 238 finished with value: 0.6779840055864014 and parameters: {'n_estimators': 254, 'learning_rate': 0.11270861574775635, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 340}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:44,359] Trial 239 finished with value: 0.6778378513981121 and parameters: {'n_estimators': 284, 'learning_rate': 0.10311871767595607, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 288}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:47,220] Trial 240 finished with value: 0.6745595339508089 and parameters: {'n_estimators': 226, 'learning_rate': 0.12004799256707857, 'max_depth': 10, 'max_bin': 228, 'num_leaves': 367}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:50,630] Trial 241 finished with value: 0.677692477976692 and parameters: {'n_estimators': 284, 'learning_rate': 0.11289794802750683, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 323}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:54,071] Trial 242 finished with value: 0.6761796904191378 and parameters: {'n_estimators': 293, 'learning_rate': 0.11690692596365179, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 331}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:51:57,802] Trial 243 finished with value: 0.6784666200630796 and parameters: {'n_estimators': 270, 'learning_rate': 0.11119193521078076, 'max_depth': 10, 'max_bin': 222, 'num_leaves': 351}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:01,389] Trial 244 finished with value: 0.6729282881190471 and parameters: {'n_estimators': 312, 'learning_rate': 0.10812222197358329, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 315}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:02,886] Trial 245 finished with value: 0.6401720165639303 and parameters: {'n_estimators': 256, 'learning_rate': 0.12250849098422925, 'max_depth': 3, 'max_bin': 219, 'num_leaves': 332}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:05,944] Trial 246 finished with value: 0.6790725488829107 and parameters: {'n_estimators': 276, 'learning_rate': 0.11498537887554118, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 300}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:08,896] Trial 247 finished with value: 0.6744284749503714 and parameters: {'n_estimators': 244, 'learning_rate': 0.10513538286361135, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 351}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:11,331] Trial 248 finished with value: 0.6728367824680437 and parameters: {'n_estimators': 200, 'learning_rate': 0.11066517737920338, 'max_depth': 7, 'max_bin': 213, 'num_leaves': 90}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:13,523] Trial 249 finished with value: 0.6730202470508002 and parameters: {'n_estimators': 297, 'learning_rate': 0.11828620389503969, 'max_depth': 6, 'max_bin': 209, 'num_leaves': 322}. Best is trial 197 with value: 0.6933563112923475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6934\n",
      "\tBest params:\n",
      "\t\tn_estimators: 279\n",
      "\t\tlearning_rate: 0.11146307406304887\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 211\n",
      "\t\tnum_leaves: 344\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.707475    0.712588    0.708269    0.684985   \n",
      "1                    TP  326.000000  329.000000  309.000000  309.000000   \n",
      "2                    TN  186.000000  182.000000  190.000000  183.000000   \n",
      "3                    FP   41.000000   41.000000   63.000000   59.000000   \n",
      "4                    FN   42.000000   43.000000   33.000000   44.000000   \n",
      "5              Accuracy    0.860504    0.858824    0.838655    0.826891   \n",
      "6             Precision    0.888283    0.889189    0.830645    0.839674   \n",
      "7           Sensitivity    0.885870    0.884409    0.903509    0.875354   \n",
      "8           Specificity    0.819400    0.816100    0.751000    0.756200   \n",
      "9              F1 score    0.887075    0.886792    0.865546    0.857143   \n",
      "10  F1 score (weighted)    0.860563    0.858948    0.836961    0.825923   \n",
      "11     F1 score (macro)    0.852329    0.849646    0.831933    0.818763   \n",
      "12    Balanced Accuracy    0.852626    0.850276    0.827248    0.815776   \n",
      "13                  MCC    0.704663    0.699314    0.668432    0.638657   \n",
      "14                  NPV    0.815800    0.808900    0.852000    0.806200   \n",
      "15              ROC_AUC    0.852626    0.850276    0.827248    0.815776   \n",
      "\n",
      "          Set4  \n",
      "0     0.690156  \n",
      "1   328.000000  \n",
      "2   164.000000  \n",
      "3    60.000000  \n",
      "4    43.000000  \n",
      "5     0.826891  \n",
      "6     0.845361  \n",
      "7     0.884097  \n",
      "8     0.732100  \n",
      "9     0.864295  \n",
      "10    0.825415  \n",
      "11    0.812658  \n",
      "12    0.808120  \n",
      "13    0.626844  \n",
      "14    0.792300  \n",
      "15    0.808120  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_4_cat = np.where((y_pred_lgbm_4 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:52:17,286] Trial 250 finished with value: 0.691993940717144 and parameters: {'n_estimators': 327, 'learning_rate': 0.11275025411186665, 'max_depth': 10, 'max_bin': 223, 'num_leaves': 391}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:21,063] Trial 251 finished with value: 0.6862339650646565 and parameters: {'n_estimators': 334, 'learning_rate': 0.10006404240793174, 'max_depth': 10, 'max_bin': 224, 'num_leaves': 379}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:24,509] Trial 252 finished with value: 0.6874849549733001 and parameters: {'n_estimators': 322, 'learning_rate': 0.12797049655533094, 'max_depth': 10, 'max_bin': 227, 'num_leaves': 365}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:27,648] Trial 253 finished with value: 0.6887627196147245 and parameters: {'n_estimators': 613, 'learning_rate': 0.13248961009600513, 'max_depth': 10, 'max_bin': 224, 'num_leaves': 370}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:30,401] Trial 254 finished with value: 0.6920098367831286 and parameters: {'n_estimators': 626, 'learning_rate': 0.13581557601727504, 'max_depth': 10, 'max_bin': 228, 'num_leaves': 365}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:33,626] Trial 255 finished with value: 0.6870446190841328 and parameters: {'n_estimators': 611, 'learning_rate': 0.1253269015587438, 'max_depth': 10, 'max_bin': 222, 'num_leaves': 395}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:37,050] Trial 256 finished with value: 0.6881731095997645 and parameters: {'n_estimators': 650, 'learning_rate': 0.11592961725803615, 'max_depth': 10, 'max_bin': 225, 'num_leaves': 359}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:40,384] Trial 257 finished with value: 0.6882067421631652 and parameters: {'n_estimators': 630, 'learning_rate': 0.13644840995739999, 'max_depth': 10, 'max_bin': 277, 'num_leaves': 345}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:43,828] Trial 258 finished with value: 0.6884069748437193 and parameters: {'n_estimators': 635, 'learning_rate': 0.13530717368960501, 'max_depth': 9, 'max_bin': 281, 'num_leaves': 376}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:47,284] Trial 259 finished with value: 0.6844195918420395 and parameters: {'n_estimators': 647, 'learning_rate': 0.1367650977975837, 'max_depth': 9, 'max_bin': 282, 'num_leaves': 384}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:50,444] Trial 260 finished with value: 0.6820193962842253 and parameters: {'n_estimators': 626, 'learning_rate': 0.1321271840817923, 'max_depth': 9, 'max_bin': 281, 'num_leaves': 372}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:54,168] Trial 261 finished with value: 0.6870841431798472 and parameters: {'n_estimators': 657, 'learning_rate': 0.14729674809046073, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 360}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:52:57,397] Trial 262 finished with value: 0.6837914274356265 and parameters: {'n_estimators': 665, 'learning_rate': 0.13911017937208908, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 385}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:00,727] Trial 263 finished with value: 0.6876616969335998 and parameters: {'n_estimators': 581, 'learning_rate': 0.1328577735313295, 'max_depth': 10, 'max_bin': 284, 'num_leaves': 349}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:03,503] Trial 264 finished with value: 0.6821417833890245 and parameters: {'n_estimators': 691, 'learning_rate': 0.13913676340730857, 'max_depth': 8, 'max_bin': 227, 'num_leaves': 365}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:06,710] Trial 265 finished with value: 0.6863018229721022 and parameters: {'n_estimators': 595, 'learning_rate': 0.13482249890865594, 'max_depth': 10, 'max_bin': 292, 'num_leaves': 343}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:09,711] Trial 266 finished with value: 0.6890695818047897 and parameters: {'n_estimators': 635, 'learning_rate': 0.1310016769070051, 'max_depth': 9, 'max_bin': 286, 'num_leaves': 391}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:12,809] Trial 267 finished with value: 0.6841093307916539 and parameters: {'n_estimators': 633, 'learning_rate': 0.1311783729894752, 'max_depth': 8, 'max_bin': 225, 'num_leaves': 403}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:15,582] Trial 268 finished with value: 0.6839978987853562 and parameters: {'n_estimators': 670, 'learning_rate': 0.13262599581752627, 'max_depth': 9, 'max_bin': 272, 'num_leaves': 397}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:18,753] Trial 269 finished with value: 0.6856690409505102 and parameters: {'n_estimators': 609, 'learning_rate': 0.13519152115820376, 'max_depth': 9, 'max_bin': 283, 'num_leaves': 378}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:21,739] Trial 270 finished with value: 0.6853521962276662 and parameters: {'n_estimators': 637, 'learning_rate': 0.14196200657016153, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 390}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:25,168] Trial 271 finished with value: 0.6883652103763932 and parameters: {'n_estimators': 672, 'learning_rate': 0.12970136249263176, 'max_depth': 10, 'max_bin': 287, 'num_leaves': 367}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:28,201] Trial 272 finished with value: 0.6880681256016211 and parameters: {'n_estimators': 672, 'learning_rate': 0.13770729756042044, 'max_depth': 9, 'max_bin': 288, 'num_leaves': 373}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:31,601] Trial 273 finished with value: 0.6913710562152348 and parameters: {'n_estimators': 621, 'learning_rate': 0.1310909845204772, 'max_depth': 10, 'max_bin': 286, 'num_leaves': 412}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:34,699] Trial 274 finished with value: 0.6859443813570156 and parameters: {'n_estimators': 643, 'learning_rate': 0.14302632801800375, 'max_depth': 10, 'max_bin': 288, 'num_leaves': 415}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:38,834] Trial 275 finished with value: 0.6880441228766347 and parameters: {'n_estimators': 618, 'learning_rate': 0.13022528554234503, 'max_depth': 10, 'max_bin': 285, 'num_leaves': 398}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:42,079] Trial 276 finished with value: 0.6860636543329814 and parameters: {'n_estimators': 614, 'learning_rate': 0.14514340389028724, 'max_depth': 10, 'max_bin': 279, 'num_leaves': 416}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:45,338] Trial 277 finished with value: 0.6846186401309656 and parameters: {'n_estimators': 596, 'learning_rate': 0.12955802674701494, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 388}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:48,684] Trial 278 finished with value: 0.6872240948183513 and parameters: {'n_estimators': 720, 'learning_rate': 0.13472132480015786, 'max_depth': 10, 'max_bin': 290, 'num_leaves': 378}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:52,263] Trial 279 finished with value: 0.6877700960539092 and parameters: {'n_estimators': 562, 'learning_rate': 0.13755378848571093, 'max_depth': 10, 'max_bin': 299, 'num_leaves': 362}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:55,583] Trial 280 finished with value: 0.6879287245776928 and parameters: {'n_estimators': 651, 'learning_rate': 0.128405288411046, 'max_depth': 10, 'max_bin': 287, 'num_leaves': 405}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:53:58,410] Trial 281 finished with value: 0.6774602758481468 and parameters: {'n_estimators': 631, 'learning_rate': 0.14071970019445654, 'max_depth': 8, 'max_bin': 295, 'num_leaves': 373}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:01,647] Trial 282 finished with value: 0.6882689488279631 and parameters: {'n_estimators': 629, 'learning_rate': 0.12679346256595103, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 307}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:04,870] Trial 283 finished with value: 0.6870174470556993 and parameters: {'n_estimators': 675, 'learning_rate': 0.12917088136362212, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 276}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:08,086] Trial 284 finished with value: 0.6868506630723201 and parameters: {'n_estimators': 618, 'learning_rate': 0.1338263820942913, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 303}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:11,193] Trial 285 finished with value: 0.6852953788023951 and parameters: {'n_estimators': 593, 'learning_rate': 0.1253885703607267, 'max_depth': 9, 'max_bin': 278, 'num_leaves': 304}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:14,612] Trial 286 finished with value: 0.6876692103150919 and parameters: {'n_estimators': 626, 'learning_rate': 0.12811028780953226, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 314}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:17,670] Trial 287 finished with value: 0.6895124511289291 and parameters: {'n_estimators': 637, 'learning_rate': 0.13653224480268156, 'max_depth': 9, 'max_bin': 289, 'num_leaves': 291}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:26,985] Trial 288 finished with value: 0.6427742292278875 and parameters: {'n_estimators': 600, 'learning_rate': 0.0058691768802413, 'max_depth': 9, 'max_bin': 281, 'num_leaves': 297}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:30,119] Trial 289 finished with value: 0.6850962525286995 and parameters: {'n_estimators': 686, 'learning_rate': 0.13219441343977048, 'max_depth': 9, 'max_bin': 288, 'num_leaves': 281}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:33,707] Trial 290 finished with value: 0.686773782863718 and parameters: {'n_estimators': 771, 'learning_rate': 0.10416663926609829, 'max_depth': 9, 'max_bin': 289, 'num_leaves': 275}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:36,764] Trial 291 finished with value: 0.6831527130068599 and parameters: {'n_estimators': 571, 'learning_rate': 0.12650854547665558, 'max_depth': 9, 'max_bin': 293, 'num_leaves': 258}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:39,688] Trial 292 finished with value: 0.6828180964642406 and parameters: {'n_estimators': 542, 'learning_rate': 0.12352834722314149, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 289}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:42,551] Trial 293 finished with value: 0.6854658538526424 and parameters: {'n_estimators': 643, 'learning_rate': 0.1663032667674189, 'max_depth': 10, 'max_bin': 296, 'num_leaves': 309}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:45,598] Trial 294 finished with value: 0.6826744603119482 and parameters: {'n_estimators': 622, 'learning_rate': 0.1397850055217333, 'max_depth': 7, 'max_bin': 291, 'num_leaves': 328}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:47,740] Trial 295 finished with value: 0.6601677329685132 and parameters: {'n_estimators': 705, 'learning_rate': 0.18210733340257795, 'max_depth': 4, 'max_bin': 286, 'num_leaves': 425}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:51,742] Trial 296 finished with value: 0.6916771036680027 and parameters: {'n_estimators': 651, 'learning_rate': 0.10641027190203459, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 403}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:55,345] Trial 297 finished with value: 0.6878524353031721 and parameters: {'n_estimators': 667, 'learning_rate': 0.10687470621303785, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 409}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:54:59,031] Trial 298 finished with value: 0.6843807370016145 and parameters: {'n_estimators': 656, 'learning_rate': 0.10050592435390476, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 394}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:02,575] Trial 299 finished with value: 0.6870695329625253 and parameters: {'n_estimators': 655, 'learning_rate': 0.10379629837029344, 'max_depth': 9, 'max_bin': 204, 'num_leaves': 387}. Best is trial 197 with value: 0.6933563112923475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6934\n",
      "\tBest params:\n",
      "\t\tn_estimators: 279\n",
      "\t\tlearning_rate: 0.11146307406304887\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 211\n",
      "\t\tnum_leaves: 344\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.707475    0.712588    0.708269    0.684985   \n",
      "1                    TP  326.000000  329.000000  309.000000  309.000000   \n",
      "2                    TN  186.000000  182.000000  190.000000  183.000000   \n",
      "3                    FP   41.000000   41.000000   63.000000   59.000000   \n",
      "4                    FN   42.000000   43.000000   33.000000   44.000000   \n",
      "5              Accuracy    0.860504    0.858824    0.838655    0.826891   \n",
      "6             Precision    0.888283    0.889189    0.830645    0.839674   \n",
      "7           Sensitivity    0.885870    0.884409    0.903509    0.875354   \n",
      "8           Specificity    0.819400    0.816100    0.751000    0.756200   \n",
      "9              F1 score    0.887075    0.886792    0.865546    0.857143   \n",
      "10  F1 score (weighted)    0.860563    0.858948    0.836961    0.825923   \n",
      "11     F1 score (macro)    0.852329    0.849646    0.831933    0.818763   \n",
      "12    Balanced Accuracy    0.852626    0.850276    0.827248    0.815776   \n",
      "13                  MCC    0.704663    0.699314    0.668432    0.638657   \n",
      "14                  NPV    0.815800    0.808900    0.852000    0.806200   \n",
      "15              ROC_AUC    0.852626    0.850276    0.827248    0.815776   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.690156    0.693837  \n",
      "1   328.000000  320.000000  \n",
      "2   164.000000  188.000000  \n",
      "3    60.000000   44.000000  \n",
      "4    43.000000   43.000000  \n",
      "5     0.826891    0.853782  \n",
      "6     0.845361    0.879121  \n",
      "7     0.884097    0.881543  \n",
      "8     0.732100    0.810300  \n",
      "9     0.864295    0.880330  \n",
      "10    0.825415    0.853724  \n",
      "11    0.812658    0.846213  \n",
      "12    0.808120    0.845944  \n",
      "13    0.626844    0.692430  \n",
      "14    0.792300    0.813900  \n",
      "15    0.808120    0.845944  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_5_cat = np.where((y_pred_lgbm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:55:06,874] Trial 300 finished with value: 0.692499185949022 and parameters: {'n_estimators': 687, 'learning_rate': 0.10748252240425245, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 410}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:08,245] Trial 301 finished with value: 0.6607806798467613 and parameters: {'n_estimators': 63, 'learning_rate': 0.10630530847915554, 'max_depth': 10, 'max_bin': 194, 'num_leaves': 429}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:11,900] Trial 302 finished with value: 0.6913080586373965 and parameters: {'n_estimators': 689, 'learning_rate': 0.10886331079904914, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 409}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:16,093] Trial 303 finished with value: 0.6926475923239603 and parameters: {'n_estimators': 734, 'learning_rate': 0.10232236016117385, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 428}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:20,080] Trial 304 finished with value: 0.6892381639044586 and parameters: {'n_estimators': 752, 'learning_rate': 0.09893248565080086, 'max_depth': 9, 'max_bin': 198, 'num_leaves': 409}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:23,447] Trial 305 finished with value: 0.6863853389429484 and parameters: {'n_estimators': 766, 'learning_rate': 0.09738169317258681, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 411}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:27,568] Trial 306 finished with value: 0.6891570365967861 and parameters: {'n_estimators': 773, 'learning_rate': 0.10201658987755262, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 407}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:31,027] Trial 307 finished with value: 0.6853779638366598 and parameters: {'n_estimators': 742, 'learning_rate': 0.1014317043426171, 'max_depth': 8, 'max_bin': 194, 'num_leaves': 428}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:34,595] Trial 308 finished with value: 0.6838975696160046 and parameters: {'n_estimators': 723, 'learning_rate': 0.10035255209240355, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 402}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:38,446] Trial 309 finished with value: 0.6880801852541619 and parameters: {'n_estimators': 791, 'learning_rate': 0.09806411833126459, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 433}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:42,362] Trial 310 finished with value: 0.6916828152707052 and parameters: {'n_estimators': 738, 'learning_rate': 0.10542347321537136, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 440}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:46,705] Trial 311 finished with value: 0.6906347996160301 and parameters: {'n_estimators': 744, 'learning_rate': 0.1027927874307991, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 421}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:50,474] Trial 312 finished with value: 0.6859655470102688 and parameters: {'n_estimators': 801, 'learning_rate': 0.10588757703015093, 'max_depth': 10, 'max_bin': 192, 'num_leaves': 416}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:54,214] Trial 313 finished with value: 0.685537325510803 and parameters: {'n_estimators': 779, 'learning_rate': 0.10234662022205736, 'max_depth': 9, 'max_bin': 196, 'num_leaves': 422}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:55:58,355] Trial 314 finished with value: 0.6883528375744383 and parameters: {'n_estimators': 752, 'learning_rate': 0.10253069218148717, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 442}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:56:02,247] Trial 315 finished with value: 0.6893947929763549 and parameters: {'n_estimators': 741, 'learning_rate': 0.0950927204597757, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 433}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:56:06,493] Trial 316 finished with value: 0.6877523034092092 and parameters: {'n_estimators': 714, 'learning_rate': 0.09582359845029288, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 439}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:56:10,187] Trial 317 finished with value: 0.6886925783593528 and parameters: {'n_estimators': 734, 'learning_rate': 0.09581437932562205, 'max_depth': 9, 'max_bin': 200, 'num_leaves': 419}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:56:13,968] Trial 318 finished with value: 0.6893210839323823 and parameters: {'n_estimators': 751, 'learning_rate': 0.0993633466800498, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 436}. Best is trial 197 with value: 0.6933563112923475.\n",
      "[I 2023-12-11 21:56:18,350] Trial 319 finished with value: 0.6934612754698257 and parameters: {'n_estimators': 744, 'learning_rate': 0.09163568258296839, 'max_depth': 10, 'max_bin': 190, 'num_leaves': 436}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:22,401] Trial 320 finished with value: 0.6872652589319086 and parameters: {'n_estimators': 741, 'learning_rate': 0.0938334441013157, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 464}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:25,996] Trial 321 finished with value: 0.6829514008310091 and parameters: {'n_estimators': 763, 'learning_rate': 0.09217668299608561, 'max_depth': 10, 'max_bin': 190, 'num_leaves': 440}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:29,995] Trial 322 finished with value: 0.6883740597601313 and parameters: {'n_estimators': 747, 'learning_rate': 0.09749823933859741, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 429}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:33,226] Trial 323 finished with value: 0.6839730490307645 and parameters: {'n_estimators': 792, 'learning_rate': 0.0996755477725976, 'max_depth': 7, 'max_bin': 197, 'num_leaves': 448}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:37,352] Trial 324 finished with value: 0.6900289859351683 and parameters: {'n_estimators': 736, 'learning_rate': 0.10282439403725871, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 416}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:41,163] Trial 325 finished with value: 0.6932644940748595 and parameters: {'n_estimators': 719, 'learning_rate': 0.10154607182188939, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 412}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:44,909] Trial 326 finished with value: 0.6908406314949451 and parameters: {'n_estimators': 735, 'learning_rate': 0.10272875980099938, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 412}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:48,266] Trial 327 finished with value: 0.6874955445615074 and parameters: {'n_estimators': 728, 'learning_rate': 0.10126539692986841, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 413}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:51,608] Trial 328 finished with value: 0.6870107782829057 and parameters: {'n_estimators': 700, 'learning_rate': 0.10366720839213146, 'max_depth': 10, 'max_bin': 192, 'num_leaves': 432}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:55,281] Trial 329 finished with value: 0.6891509738124734 and parameters: {'n_estimators': 730, 'learning_rate': 0.09784936358167795, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 456}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:56:59,434] Trial 330 finished with value: 0.6911766337260385 and parameters: {'n_estimators': 755, 'learning_rate': 0.09327860603040199, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 413}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:03,611] Trial 331 finished with value: 0.6874599691138032 and parameters: {'n_estimators': 759, 'learning_rate': 0.09214873150427108, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 425}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:07,594] Trial 332 finished with value: 0.6843382666550871 and parameters: {'n_estimators': 715, 'learning_rate': 0.09466410066153752, 'max_depth': 10, 'max_bin': 190, 'num_leaves': 431}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:10,919] Trial 333 finished with value: 0.6860120594098665 and parameters: {'n_estimators': 751, 'learning_rate': 0.09911534257398465, 'max_depth': 10, 'max_bin': 186, 'num_leaves': 447}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:14,864] Trial 334 finished with value: 0.6854716364675764 and parameters: {'n_estimators': 812, 'learning_rate': 0.10608389264469541, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 417}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:19,023] Trial 335 finished with value: 0.6901659790770543 and parameters: {'n_estimators': 734, 'learning_rate': 0.09187917517145663, 'max_depth': 10, 'max_bin': 194, 'num_leaves': 408}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:22,679] Trial 336 finished with value: 0.6870252229470194 and parameters: {'n_estimators': 736, 'learning_rate': 0.08826524984638812, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 407}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:26,630] Trial 337 finished with value: 0.6901284758448025 and parameters: {'n_estimators': 705, 'learning_rate': 0.09208737827123402, 'max_depth': 10, 'max_bin': 194, 'num_leaves': 439}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:31,143] Trial 338 finished with value: 0.6863259347615033 and parameters: {'n_estimators': 705, 'learning_rate': 0.09299489181250797, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 435}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:35,326] Trial 339 finished with value: 0.6888130067725081 and parameters: {'n_estimators': 696, 'learning_rate': 0.08989385177866538, 'max_depth': 10, 'max_bin': 192, 'num_leaves': 455}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:38,672] Trial 340 finished with value: 0.6879757747421555 and parameters: {'n_estimators': 713, 'learning_rate': 0.10539523876031576, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 418}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:42,564] Trial 341 finished with value: 0.6851938208881307 and parameters: {'n_estimators': 722, 'learning_rate': 0.0953146788292899, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 440}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:46,813] Trial 342 finished with value: 0.6873350038994062 and parameters: {'n_estimators': 734, 'learning_rate': 0.0895074415199166, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 467}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:50,566] Trial 343 finished with value: 0.6885685290478535 and parameters: {'n_estimators': 894, 'learning_rate': 0.10602689198395533, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 426}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:54,635] Trial 344 finished with value: 0.6867834782705589 and parameters: {'n_estimators': 685, 'learning_rate': 0.09414293435968239, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 406}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:57:58,683] Trial 345 finished with value: 0.6888685241264808 and parameters: {'n_estimators': 754, 'learning_rate': 0.10181820054315267, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 441}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:02,520] Trial 346 finished with value: 0.6860187775494208 and parameters: {'n_estimators': 722, 'learning_rate': 0.09930996756693274, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 421}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:06,974] Trial 347 finished with value: 0.691526411090239 and parameters: {'n_estimators': 692, 'learning_rate': 0.10778715032344248, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 406}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:10,352] Trial 348 finished with value: 0.688592252402384 and parameters: {'n_estimators': 714, 'learning_rate': 0.14772958753056115, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 400}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:14,380] Trial 349 finished with value: 0.6891950552248061 and parameters: {'n_estimators': 688, 'learning_rate': 0.10835720234916932, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 403}. Best is trial 319 with value: 0.6934612754698257.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.693461\n",
      "\tBest params:\n",
      "\t\tn_estimators: 744\n",
      "\t\tlearning_rate: 0.09163568258296839\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 190\n",
      "\t\tnum_leaves: 436\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.707475    0.712588    0.708269    0.684985   \n",
      "1                    TP  326.000000  329.000000  309.000000  309.000000   \n",
      "2                    TN  186.000000  182.000000  190.000000  183.000000   \n",
      "3                    FP   41.000000   41.000000   63.000000   59.000000   \n",
      "4                    FN   42.000000   43.000000   33.000000   44.000000   \n",
      "5              Accuracy    0.860504    0.858824    0.838655    0.826891   \n",
      "6             Precision    0.888283    0.889189    0.830645    0.839674   \n",
      "7           Sensitivity    0.885870    0.884409    0.903509    0.875354   \n",
      "8           Specificity    0.819400    0.816100    0.751000    0.756200   \n",
      "9              F1 score    0.887075    0.886792    0.865546    0.857143   \n",
      "10  F1 score (weighted)    0.860563    0.858948    0.836961    0.825923   \n",
      "11     F1 score (macro)    0.852329    0.849646    0.831933    0.818763   \n",
      "12    Balanced Accuracy    0.852626    0.850276    0.827248    0.815776   \n",
      "13                  MCC    0.704663    0.699314    0.668432    0.638657   \n",
      "14                  NPV    0.815800    0.808900    0.852000    0.806200   \n",
      "15              ROC_AUC    0.852626    0.850276    0.827248    0.815776   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.690156    0.693837    0.686856  \n",
      "1   328.000000  320.000000  331.000000  \n",
      "2   164.000000  188.000000  170.000000  \n",
      "3    60.000000   44.000000   58.000000  \n",
      "4    43.000000   43.000000   36.000000  \n",
      "5     0.826891    0.853782    0.842017  \n",
      "6     0.845361    0.879121    0.850900  \n",
      "7     0.884097    0.881543    0.901907  \n",
      "8     0.732100    0.810300    0.745600  \n",
      "9     0.864295    0.880330    0.875661  \n",
      "10    0.825415    0.853724    0.840311  \n",
      "11    0.812658    0.846213    0.829536  \n",
      "12    0.808120    0.845944    0.823761  \n",
      "13    0.626844    0.692430    0.661677  \n",
      "14    0.792300    0.813900    0.825200  \n",
      "15    0.808120    0.845944    0.823761  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_6_cat = np.where((y_pred_lgbm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:58:18,443] Trial 350 finished with value: 0.6844960735677386 and parameters: {'n_estimators': 697, 'learning_rate': 0.10429092763374932, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 418}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:22,262] Trial 351 finished with value: 0.6864448013257091 and parameters: {'n_estimators': 738, 'learning_rate': 0.10533849838002188, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 456}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:27,064] Trial 352 finished with value: 0.6839309077231602 and parameters: {'n_estimators': 688, 'learning_rate': 0.09005633784695138, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 400}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:31,496] Trial 353 finished with value: 0.6800769055823412 and parameters: {'n_estimators': 736, 'learning_rate': 0.0953089912888374, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 425}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:35,024] Trial 354 finished with value: 0.6847358222429757 and parameters: {'n_estimators': 716, 'learning_rate': 0.10936690277767738, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 413}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:39,444] Trial 355 finished with value: 0.6793629433083511 and parameters: {'n_estimators': 782, 'learning_rate': 0.10757663419368797, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 395}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:42,322] Trial 356 finished with value: 0.6729368948652897 and parameters: {'n_estimators': 767, 'learning_rate': 0.15208432667785113, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 448}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:45,645] Trial 357 finished with value: 0.6862100584821169 and parameters: {'n_estimators': 705, 'learning_rate': 0.10937487762148934, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 429}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:49,893] Trial 358 finished with value: 0.6860665073153858 and parameters: {'n_estimators': 735, 'learning_rate': 0.10274697188854674, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 415}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:54,067] Trial 359 finished with value: 0.6825122615303172 and parameters: {'n_estimators': 682, 'learning_rate': 0.08516510433024232, 'max_depth': 10, 'max_bin': 194, 'num_leaves': 398}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:58:58,309] Trial 360 finished with value: 0.6867705541213269 and parameters: {'n_estimators': 724, 'learning_rate': 0.10856983346451991, 'max_depth': 10, 'max_bin': 186, 'num_leaves': 438}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:02,595] Trial 361 finished with value: 0.6857471847394068 and parameters: {'n_estimators': 757, 'learning_rate': 0.09375658070910385, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 477}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:06,340] Trial 362 finished with value: 0.6809156730737638 and parameters: {'n_estimators': 821, 'learning_rate': 0.10227382667142458, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 411}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:10,772] Trial 363 finished with value: 0.6869147500619818 and parameters: {'n_estimators': 702, 'learning_rate': 0.09765734610655034, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 422}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:14,451] Trial 364 finished with value: 0.6854346083595045 and parameters: {'n_estimators': 743, 'learning_rate': 0.10453146419777594, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 393}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:18,571] Trial 365 finished with value: 0.6849227354935181 and parameters: {'n_estimators': 772, 'learning_rate': 0.10072199673490308, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 453}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:23,002] Trial 366 finished with value: 0.6853664012178473 and parameters: {'n_estimators': 723, 'learning_rate': 0.10914134451712876, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 435}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:27,578] Trial 367 finished with value: 0.6854623571543333 and parameters: {'n_estimators': 701, 'learning_rate': 0.08677119631583187, 'max_depth': 10, 'max_bin': 190, 'num_leaves': 411}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:30,741] Trial 368 finished with value: 0.682293867069521 and parameters: {'n_estimators': 682, 'learning_rate': 0.14415002028881238, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 426}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:34,581] Trial 369 finished with value: 0.6825512197548551 and parameters: {'n_estimators': 750, 'learning_rate': 0.09159552423932918, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 393}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:41,064] Trial 370 finished with value: 0.6869032811705932 and parameters: {'n_estimators': 725, 'learning_rate': 0.03801490625862311, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 406}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:44,759] Trial 371 finished with value: 0.6868975774850046 and parameters: {'n_estimators': 711, 'learning_rate': 0.11002953343326304, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 424}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:48,921] Trial 372 finished with value: 0.6817981337886226 and parameters: {'n_estimators': 672, 'learning_rate': 0.09547450679960941, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 449}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:53,085] Trial 373 finished with value: 0.6830350172330143 and parameters: {'n_estimators': 751, 'learning_rate': 0.10491333472827127, 'max_depth': 10, 'max_bin': 157, 'num_leaves': 437}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 21:59:56,818] Trial 374 finished with value: 0.6821350264950679 and parameters: {'n_estimators': 737, 'learning_rate': 0.09827816765281297, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 390}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:00,177] Trial 375 finished with value: 0.6830057542748486 and parameters: {'n_estimators': 705, 'learning_rate': 0.11171876180294678, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 464}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:04,263] Trial 376 finished with value: 0.6848330248226777 and parameters: {'n_estimators': 765, 'learning_rate': 0.101309713316867, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 410}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:06,889] Trial 377 finished with value: 0.6755660823293226 and parameters: {'n_estimators': 731, 'learning_rate': 0.19504441973288045, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 435}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:11,450] Trial 378 finished with value: 0.6852934004379672 and parameters: {'n_estimators': 782, 'learning_rate': 0.09200058913694077, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 420}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:15,019] Trial 379 finished with value: 0.6856692534137838 and parameters: {'n_estimators': 684, 'learning_rate': 0.1075553444038327, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 399}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:21,009] Trial 380 finished with value: 0.6844635293320431 and parameters: {'n_estimators': 720, 'learning_rate': 0.048868204278135076, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 386}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:25,380] Trial 381 finished with value: 0.6871040314724934 and parameters: {'n_estimators': 666, 'learning_rate': 0.10443972613393433, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 448}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:29,440] Trial 382 finished with value: 0.6821852383104308 and parameters: {'n_estimators': 761, 'learning_rate': 0.09699856526494706, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 416}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:33,040] Trial 383 finished with value: 0.6875718503188419 and parameters: {'n_estimators': 692, 'learning_rate': 0.11240664072481057, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 403}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:37,565] Trial 384 finished with value: 0.6832075263398238 and parameters: {'n_estimators': 745, 'learning_rate': 0.08644836999511973, 'max_depth': 10, 'max_bin': 194, 'num_leaves': 429}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:41,206] Trial 385 finished with value: 0.6817259897442879 and parameters: {'n_estimators': 707, 'learning_rate': 0.10202490065957191, 'max_depth': 9, 'max_bin': 189, 'num_leaves': 389}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:44,318] Trial 386 finished with value: 0.6796855186172363 and parameters: {'n_estimators': 735, 'learning_rate': 0.15454064596657094, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 442}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:48,536] Trial 387 finished with value: 0.6841216128245925 and parameters: {'n_estimators': 718, 'learning_rate': 0.09027174679967054, 'max_depth': 10, 'max_bin': 192, 'num_leaves': 468}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:52,000] Trial 388 finished with value: 0.6846206225524021 and parameters: {'n_estimators': 658, 'learning_rate': 0.11509462011710593, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 410}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:00:56,488] Trial 389 finished with value: 0.686242946968146 and parameters: {'n_estimators': 765, 'learning_rate': 0.10818594074462907, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 426}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:00,119] Trial 390 finished with value: 0.6827323465626275 and parameters: {'n_estimators': 698, 'learning_rate': 0.09756941307469383, 'max_depth': 10, 'max_bin': 184, 'num_leaves': 403}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:03,897] Trial 391 finished with value: 0.6832207774545795 and parameters: {'n_estimators': 727, 'learning_rate': 0.10533184608393911, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 419}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:12,960] Trial 392 finished with value: 0.6830247565387393 and parameters: {'n_estimators': 742, 'learning_rate': 0.023180749981439208, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 443}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:15,698] Trial 393 finished with value: 0.6778931265612862 and parameters: {'n_estimators': 786, 'learning_rate': 0.1414223907131037, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 385}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:19,601] Trial 394 finished with value: 0.6808142444254197 and parameters: {'n_estimators': 685, 'learning_rate': 0.09383947129150529, 'max_depth': 9, 'max_bin': 201, 'num_leaves': 456}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:22,664] Trial 395 finished with value: 0.6822177507585987 and parameters: {'n_estimators': 708, 'learning_rate': 0.18754453325662046, 'max_depth': 11, 'max_bin': 191, 'num_leaves': 430}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:25,466] Trial 396 finished with value: 0.6777466471490796 and parameters: {'n_estimators': 751, 'learning_rate': 0.1648709440793253, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 404}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:27,932] Trial 397 finished with value: 0.6745803638666109 and parameters: {'n_estimators': 142, 'learning_rate': 0.1009163138343443, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 417}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:31,671] Trial 398 finished with value: 0.6830469204986164 and parameters: {'n_estimators': 669, 'learning_rate': 0.11125920811725507, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 49}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:35,168] Trial 399 finished with value: 0.6830570365701145 and parameters: {'n_estimators': 728, 'learning_rate': 0.11560266623047125, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 384}. Best is trial 319 with value: 0.6934612754698257.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6934613\n",
      "\tBest params:\n",
      "\t\tn_estimators: 744\n",
      "\t\tlearning_rate: 0.09163568258296839\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 190\n",
      "\t\tnum_leaves: 436\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.707475    0.712588    0.708269    0.684985   \n",
      "1                    TP  326.000000  329.000000  309.000000  309.000000   \n",
      "2                    TN  186.000000  182.000000  190.000000  183.000000   \n",
      "3                    FP   41.000000   41.000000   63.000000   59.000000   \n",
      "4                    FN   42.000000   43.000000   33.000000   44.000000   \n",
      "5              Accuracy    0.860504    0.858824    0.838655    0.826891   \n",
      "6             Precision    0.888283    0.889189    0.830645    0.839674   \n",
      "7           Sensitivity    0.885870    0.884409    0.903509    0.875354   \n",
      "8           Specificity    0.819400    0.816100    0.751000    0.756200   \n",
      "9              F1 score    0.887075    0.886792    0.865546    0.857143   \n",
      "10  F1 score (weighted)    0.860563    0.858948    0.836961    0.825923   \n",
      "11     F1 score (macro)    0.852329    0.849646    0.831933    0.818763   \n",
      "12    Balanced Accuracy    0.852626    0.850276    0.827248    0.815776   \n",
      "13                  MCC    0.704663    0.699314    0.668432    0.638657   \n",
      "14                  NPV    0.815800    0.808900    0.852000    0.806200   \n",
      "15              ROC_AUC    0.852626    0.850276    0.827248    0.815776   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.690156    0.693837    0.686856    0.669776  \n",
      "1   328.000000  320.000000  331.000000  315.000000  \n",
      "2   164.000000  188.000000  170.000000  177.000000  \n",
      "3    60.000000   44.000000   58.000000   58.000000  \n",
      "4    43.000000   43.000000   36.000000   45.000000  \n",
      "5     0.826891    0.853782    0.842017    0.826891  \n",
      "6     0.845361    0.879121    0.850900    0.844504  \n",
      "7     0.884097    0.881543    0.901907    0.875000  \n",
      "8     0.732100    0.810300    0.745600    0.753200  \n",
      "9     0.864295    0.880330    0.875661    0.859482  \n",
      "10    0.825415    0.853724    0.840311    0.825964  \n",
      "11    0.812658    0.846213    0.829536    0.817049  \n",
      "12    0.808120    0.845944    0.823761    0.814096  \n",
      "13    0.626844    0.692430    0.661677    0.634960  \n",
      "14    0.792300    0.813900    0.825200    0.797300  \n",
      "15    0.808120    0.845944    0.823761    0.814096  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_7_cat = np.where((y_pred_lgbm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 22:01:38,383] Trial 400 finished with value: 0.677893211847519 and parameters: {'n_estimators': 757, 'learning_rate': 0.1484615405119425, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 402}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:41,652] Trial 401 finished with value: 0.6696519809886892 and parameters: {'n_estimators': 775, 'learning_rate': 0.10670157306192099, 'max_depth': 8, 'max_bin': 201, 'num_leaves': 439}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:44,735] Trial 402 finished with value: 0.6712843268690133 and parameters: {'n_estimators': 695, 'learning_rate': 0.12033162611931754, 'max_depth': 10, 'max_bin': 190, 'num_leaves': 421}. Best is trial 319 with value: 0.6934612754698257.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 22:01:47,825] Trial 403 finished with value: 0.66999372361087 and parameters: {'n_estimators': 738, 'learning_rate': 0.1027038994006237, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 31}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:49,633] Trial 404 finished with value: 0.6632895025553133 and parameters: {'n_estimators': 105, 'learning_rate': 0.09737221829814169, 'max_depth': 9, 'max_bin': 198, 'num_leaves': 395}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:52,575] Trial 405 finished with value: 0.6745626058795097 and parameters: {'n_estimators': 801, 'learning_rate': 0.13812614173128424, 'max_depth': 11, 'max_bin': 181, 'num_leaves': 486}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:01:55,862] Trial 406 finished with value: 0.6718099068131711 and parameters: {'n_estimators': 718, 'learning_rate': 0.11055369242684682, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 457}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:00,245] Trial 407 finished with value: 0.6749079727552153 and parameters: {'n_estimators': 648, 'learning_rate': 0.09336277611828143, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 431}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:03,825] Trial 408 finished with value: 0.6787482856361242 and parameters: {'n_estimators': 681, 'learning_rate': 0.08751501137139443, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 386}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:07,199] Trial 409 finished with value: 0.6702794154430964 and parameters: {'n_estimators': 715, 'learning_rate': 0.10534265928525592, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 412}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:10,180] Trial 410 finished with value: 0.6730454581534191 and parameters: {'n_estimators': 737, 'learning_rate': 0.11499522203119106, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 442}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:12,889] Trial 411 finished with value: 0.6721765080573557 and parameters: {'n_estimators': 757, 'learning_rate': 0.159908178146839, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 415}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:16,303] Trial 412 finished with value: 0.6726507355237861 and parameters: {'n_estimators': 701, 'learning_rate': 0.10098583857637326, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 428}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:19,562] Trial 413 finished with value: 0.6752639623617129 and parameters: {'n_estimators': 668, 'learning_rate': 0.10914215501392902, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 400}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:22,575] Trial 414 finished with value: 0.6730934164016371 and parameters: {'n_estimators': 746, 'learning_rate': 0.09735041128757058, 'max_depth': 9, 'max_bin': 185, 'num_leaves': 380}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:26,620] Trial 415 finished with value: 0.6762687894971539 and parameters: {'n_estimators': 715, 'learning_rate': 0.09130763816390242, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 466}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:30,401] Trial 416 finished with value: 0.6763232808944025 and parameters: {'n_estimators': 728, 'learning_rate': 0.08486742704516677, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 411}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:33,863] Trial 417 finished with value: 0.6761237302670111 and parameters: {'n_estimators': 779, 'learning_rate': 0.10350221190490036, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 449}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:36,781] Trial 418 finished with value: 0.673145944081256 and parameters: {'n_estimators': 655, 'learning_rate': 0.11243122823435642, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 427}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:39,160] Trial 419 finished with value: 0.671796016490357 and parameters: {'n_estimators': 695, 'learning_rate': 0.1440089682165993, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 402}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:42,288] Trial 420 finished with value: 0.6760985112897837 and parameters: {'n_estimators': 760, 'learning_rate': 0.10683588329477384, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 436}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:45,326] Trial 421 finished with value: 0.6756655540278695 and parameters: {'n_estimators': 681, 'learning_rate': 0.118310485855409, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 390}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:48,205] Trial 422 finished with value: 0.6723692932151462 and parameters: {'n_estimators': 213, 'learning_rate': 0.09888797612189035, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 417}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:51,977] Trial 423 finished with value: 0.6753687761149595 and parameters: {'n_estimators': 739, 'learning_rate': 0.09537403047307186, 'max_depth': 9, 'max_bin': 188, 'num_leaves': 378}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:55,023] Trial 424 finished with value: 0.6752598559947448 and parameters: {'n_estimators': 719, 'learning_rate': 0.10911871057729966, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 402}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:02:58,127] Trial 425 finished with value: 0.6785686610677727 and parameters: {'n_estimators': 841, 'learning_rate': 0.1022547736167592, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 451}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:01,329] Trial 426 finished with value: 0.6735244855151847 and parameters: {'n_estimators': 706, 'learning_rate': 0.0899026266941288, 'max_depth': 7, 'max_bin': 199, 'num_leaves': 422}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:03,709] Trial 427 finished with value: 0.6614120170126465 and parameters: {'n_estimators': 773, 'learning_rate': 0.12092401519196008, 'max_depth': 5, 'max_bin': 205, 'num_leaves': 433}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:07,054] Trial 428 finished with value: 0.6738660321637552 and parameters: {'n_estimators': 747, 'learning_rate': 0.11654047752090979, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 409}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:10,229] Trial 429 finished with value: 0.6702143919350775 and parameters: {'n_estimators': 672, 'learning_rate': 0.10593421548764946, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 392}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:12,666] Trial 430 finished with value: 0.6649096564937728 and parameters: {'n_estimators': 728, 'learning_rate': 0.17987668428585835, 'max_depth': 10, 'max_bin': 192, 'num_leaves': 477}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:15,734] Trial 431 finished with value: 0.6729902472770831 and parameters: {'n_estimators': 698, 'learning_rate': 0.11247877716056902, 'max_depth': 9, 'max_bin': 196, 'num_leaves': 441}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:17,918] Trial 432 finished with value: 0.6606541534950905 and parameters: {'n_estimators': 191, 'learning_rate': 0.09973572518773534, 'max_depth': 6, 'max_bin': 202, 'num_leaves': 419}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:20,707] Trial 433 finished with value: 0.6759787119291296 and parameters: {'n_estimators': 757, 'learning_rate': 0.1390994938329086, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 396}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:24,156] Trial 434 finished with value: 0.6769979560802852 and parameters: {'n_estimators': 657, 'learning_rate': 0.09297129104112915, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 378}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:26,261] Trial 435 finished with value: 0.6661238052394913 and parameters: {'n_estimators': 639, 'learning_rate': 0.1979116143036142, 'max_depth': 10, 'max_bin': 194, 'num_leaves': 425}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:31,035] Trial 436 finished with value: 0.6812205843359724 and parameters: {'n_estimators': 729, 'learning_rate': 0.049477922951105514, 'max_depth': 10, 'max_bin': 258, 'num_leaves': 411}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:33,819] Trial 437 finished with value: 0.6734372895915249 and parameters: {'n_estimators': 707, 'learning_rate': 0.10890728381584834, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 453}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:36,901] Trial 438 finished with value: 0.6758602940942751 and parameters: {'n_estimators': 684, 'learning_rate': 0.08263201689940601, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 436}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:41,113] Trial 439 finished with value: 0.6743409817881781 and parameters: {'n_estimators': 605, 'learning_rate': 0.10367420790547795, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 405}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:44,229] Trial 440 finished with value: 0.673810437348671 and parameters: {'n_estimators': 745, 'learning_rate': 0.09671415153548157, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 383}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:46,963] Trial 441 finished with value: 0.6717426545624284 and parameters: {'n_estimators': 725, 'learning_rate': 0.15005421666303645, 'max_depth': 9, 'max_bin': 208, 'num_leaves': 257}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:49,608] Trial 442 finished with value: 0.6691747632181011 and parameters: {'n_estimators': 760, 'learning_rate': 0.14457217754254897, 'max_depth': 11, 'max_bin': 192, 'num_leaves': 424}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:52,510] Trial 443 finished with value: 0.668622158316834 and parameters: {'n_estimators': 713, 'learning_rate': 0.11463211213312066, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 463}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:55,485] Trial 444 finished with value: 0.6781994853836866 and parameters: {'n_estimators': 770, 'learning_rate': 0.13667971139217183, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 394}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:03:58,759] Trial 445 finished with value: 0.6742946181132632 and parameters: {'n_estimators': 797, 'learning_rate': 0.09998915681075499, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 441}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:02,462] Trial 446 finished with value: 0.6722519642666238 and parameters: {'n_estimators': 692, 'learning_rate': 0.08876386532477842, 'max_depth': 10, 'max_bin': 186, 'num_leaves': 370}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:04,957] Trial 447 finished with value: 0.6734240896574865 and parameters: {'n_estimators': 741, 'learning_rate': 0.15600383155522832, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 289}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:08,323] Trial 448 finished with value: 0.674275509642607 and parameters: {'n_estimators': 643, 'learning_rate': 0.10526590737348837, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 409}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:11,122] Trial 449 finished with value: 0.6725303896853155 and parameters: {'n_estimators': 345, 'learning_rate': 0.12216008487868824, 'max_depth': 8, 'max_bin': 190, 'num_leaves': 424}. Best is trial 319 with value: 0.6934612754698257.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.69346128\n",
      "\tBest params:\n",
      "\t\tn_estimators: 744\n",
      "\t\tlearning_rate: 0.09163568258296839\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 190\n",
      "\t\tnum_leaves: 436\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.707475    0.712588    0.708269    0.684985   \n",
      "1                    TP  326.000000  329.000000  309.000000  309.000000   \n",
      "2                    TN  186.000000  182.000000  190.000000  183.000000   \n",
      "3                    FP   41.000000   41.000000   63.000000   59.000000   \n",
      "4                    FN   42.000000   43.000000   33.000000   44.000000   \n",
      "5              Accuracy    0.860504    0.858824    0.838655    0.826891   \n",
      "6             Precision    0.888283    0.889189    0.830645    0.839674   \n",
      "7           Sensitivity    0.885870    0.884409    0.903509    0.875354   \n",
      "8           Specificity    0.819400    0.816100    0.751000    0.756200   \n",
      "9              F1 score    0.887075    0.886792    0.865546    0.857143   \n",
      "10  F1 score (weighted)    0.860563    0.858948    0.836961    0.825923   \n",
      "11     F1 score (macro)    0.852329    0.849646    0.831933    0.818763   \n",
      "12    Balanced Accuracy    0.852626    0.850276    0.827248    0.815776   \n",
      "13                  MCC    0.704663    0.699314    0.668432    0.638657   \n",
      "14                  NPV    0.815800    0.808900    0.852000    0.806200   \n",
      "15              ROC_AUC    0.852626    0.850276    0.827248    0.815776   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.690156    0.693837    0.686856    0.669776    0.693923  \n",
      "1   328.000000  320.000000  331.000000  315.000000  329.000000  \n",
      "2   164.000000  188.000000  170.000000  177.000000  179.000000  \n",
      "3    60.000000   44.000000   58.000000   58.000000   42.000000  \n",
      "4    43.000000   43.000000   36.000000   45.000000   45.000000  \n",
      "5     0.826891    0.853782    0.842017    0.826891    0.853782  \n",
      "6     0.845361    0.879121    0.850900    0.844504    0.886792  \n",
      "7     0.884097    0.881543    0.901907    0.875000    0.879679  \n",
      "8     0.732100    0.810300    0.745600    0.753200    0.810000  \n",
      "9     0.864295    0.880330    0.875661    0.859482    0.883221  \n",
      "10    0.825415    0.853724    0.840311    0.825964    0.853980  \n",
      "11    0.812658    0.846213    0.829536    0.817049    0.843858  \n",
      "12    0.808120    0.845944    0.823761    0.814096    0.844817  \n",
      "13    0.626844    0.692430    0.661677    0.634960    0.687764  \n",
      "14    0.792300    0.813900    0.825200    0.797300    0.799100  \n",
      "15    0.808120    0.845944    0.823761    0.814096    0.844817  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_8_cat = np.where((y_pred_lgbm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 22:04:14,379] Trial 450 finished with value: 0.6794440951240515 and parameters: {'n_estimators': 223, 'learning_rate': 0.1107611852943453, 'max_depth': 10, 'max_bin': 166, 'num_leaves': 448}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:20,044] Trial 451 finished with value: 0.6796442172928703 and parameters: {'n_estimators': 514, 'learning_rate': 0.03133234667752596, 'max_depth': 9, 'max_bin': 210, 'num_leaves': 391}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:23,222] Trial 452 finished with value: 0.6796782084849815 and parameters: {'n_estimators': 585, 'learning_rate': 0.1509623197946436, 'max_depth': 10, 'max_bin': 296, 'num_leaves': 410}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:26,412] Trial 453 finished with value: 0.6836261381477191 and parameters: {'n_estimators': 661, 'learning_rate': 0.10729071601116238, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 433}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:30,136] Trial 454 finished with value: 0.684146943717451 and parameters: {'n_estimators': 713, 'learning_rate': 0.09546405768331791, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 416}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:33,008] Trial 455 finished with value: 0.6881551059423023 and parameters: {'n_estimators': 734, 'learning_rate': 0.14096631770281534, 'max_depth': 10, 'max_bin': 182, 'num_leaves': 373}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:35,443] Trial 456 finished with value: 0.6808305201177608 and parameters: {'n_estimators': 784, 'learning_rate': 0.17148822755071533, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 398}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:38,927] Trial 457 finished with value: 0.6865689102456684 and parameters: {'n_estimators': 678, 'learning_rate': 0.1147884426345362, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 427}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:44,552] Trial 458 finished with value: 0.6876351037860442 and parameters: {'n_estimators': 743, 'learning_rate': 0.04529771826654342, 'max_depth': 9, 'max_bin': 192, 'num_leaves': 457}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:48,280] Trial 459 finished with value: 0.6842893436134562 and parameters: {'n_estimators': 692, 'learning_rate': 0.10040255850225319, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 439}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:52,972] Trial 460 finished with value: 0.6887198081118383 and parameters: {'n_estimators': 619, 'learning_rate': 0.06156510427884385, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 353}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:56,435] Trial 461 finished with value: 0.6846030321672634 and parameters: {'n_estimators': 717, 'learning_rate': 0.11895601247945058, 'max_depth': 11, 'max_bin': 195, 'num_leaves': 319}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:04:59,223] Trial 462 finished with value: 0.6742386209307324 and parameters: {'n_estimators': 765, 'learning_rate': 0.10414078555399368, 'max_depth': 5, 'max_bin': 216, 'num_leaves': 407}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:03,210] Trial 463 finished with value: 0.6867064830797487 and parameters: {'n_estimators': 747, 'learning_rate': 0.09204482653176149, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 382}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:06,007] Trial 464 finished with value: 0.6850012339012197 and parameters: {'n_estimators': 255, 'learning_rate': 0.13430293532321305, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 416}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:08,552] Trial 465 finished with value: 0.6820675774881835 and parameters: {'n_estimators': 206, 'learning_rate': 0.11115049025696842, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 234}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:12,176] Trial 466 finished with value: 0.6853455050591392 and parameters: {'n_estimators': 704, 'learning_rate': 0.08659181796924482, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 393}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:15,687] Trial 467 finished with value: 0.687806296818575 and parameters: {'n_estimators': 716, 'learning_rate': 0.10240876570057694, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 435}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:18,815] Trial 468 finished with value: 0.6831260175830152 and parameters: {'n_estimators': 728, 'learning_rate': 0.10814965498493313, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 422}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:22,453] Trial 469 finished with value: 0.6836847136257722 and parameters: {'n_estimators': 667, 'learning_rate': 0.09760926144965541, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 472}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:26,002] Trial 470 finished with value: 0.6865564620446463 and parameters: {'n_estimators': 756, 'learning_rate': 0.07940683337380648, 'max_depth': 9, 'max_bin': 204, 'num_leaves': 368}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:28,974] Trial 471 finished with value: 0.687128964471135 and parameters: {'n_estimators': 640, 'learning_rate': 0.11590143419639992, 'max_depth': 10, 'max_bin': 185, 'num_leaves': 138}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:32,376] Trial 472 finished with value: 0.6864964785699641 and parameters: {'n_estimators': 819, 'learning_rate': 0.09388287885434486, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 449}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:35,118] Trial 473 finished with value: 0.6817164926421702 and parameters: {'n_estimators': 235, 'learning_rate': 0.12454601113018605, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 272}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:40,392] Trial 474 finished with value: 0.6897811497273845 and parameters: {'n_estimators': 682, 'learning_rate': 0.059105137079478176, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 407}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:45,679] Trial 475 finished with value: 0.6892604019045846 and parameters: {'n_estimators': 677, 'learning_rate': 0.055988108568463396, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 402}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:49,343] Trial 476 finished with value: 0.6797538680708876 and parameters: {'n_estimators': 693, 'learning_rate': 0.08491764648950889, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 384}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:54,826] Trial 477 finished with value: 0.6864428413823171 and parameters: {'n_estimators': 652, 'learning_rate': 0.05349576333554197, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 354}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:05:58,877] Trial 478 finished with value: 0.6833332333748454 and parameters: {'n_estimators': 685, 'learning_rate': 0.07973394611378895, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 399}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:02,119] Trial 479 finished with value: 0.6847850043000204 and parameters: {'n_estimators': 666, 'learning_rate': 0.12227393371665915, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 331}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:06,362] Trial 480 finished with value: 0.6869433957099537 and parameters: {'n_estimators': 704, 'learning_rate': 0.07767303168231471, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 295}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:09,690] Trial 481 finished with value: 0.6893189841265019 and parameters: {'n_estimators': 681, 'learning_rate': 0.11309386146652979, 'max_depth': 9, 'max_bin': 201, 'num_leaves': 413}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:13,844] Trial 482 finished with value: 0.6872654988164159 and parameters: {'n_estimators': 729, 'learning_rate': 0.07180743053838258, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 378}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:15,804] Trial 483 finished with value: 0.6647992990622138 and parameters: {'n_estimators': 96, 'learning_rate': 0.07307344613886722, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 426}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:25,432] Trial 484 finished with value: 0.6800955334452098 and parameters: {'n_estimators': 624, 'learning_rate': 0.013629991913262487, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 396}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:29,404] Trial 485 finished with value: 0.6857074797822119 and parameters: {'n_estimators': 651, 'learning_rate': 0.08141323005302949, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 312}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:33,761] Trial 486 finished with value: 0.687558894992016 and parameters: {'n_estimators': 699, 'learning_rate': 0.06673658767682925, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 363}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:41,219] Trial 487 finished with value: 0.6852748830644361 and parameters: {'n_estimators': 773, 'learning_rate': 0.02864220798475129, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 411}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:48,141] Trial 488 finished with value: 0.6833895598111981 and parameters: {'n_estimators': 738, 'learning_rate': 0.03281249318300665, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 389}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:52,787] Trial 489 finished with value: 0.6854782032396232 and parameters: {'n_estimators': 718, 'learning_rate': 0.07588597273748858, 'max_depth': 10, 'max_bin': 185, 'num_leaves': 437}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:06:57,267] Trial 490 finished with value: 0.6879614898260062 and parameters: {'n_estimators': 665, 'learning_rate': 0.05858991680353986, 'max_depth': 10, 'max_bin': 162, 'num_leaves': 418}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:07:03,141] Trial 491 finished with value: 0.6862370378566943 and parameters: {'n_estimators': 753, 'learning_rate': 0.03983753092961639, 'max_depth': 11, 'max_bin': 202, 'num_leaves': 160}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:07:05,803] Trial 492 finished with value: 0.672754652909263 and parameters: {'n_estimators': 177, 'learning_rate': 0.047949660401396726, 'max_depth': 9, 'max_bin': 188, 'num_leaves': 327}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:07:10,175] Trial 493 finished with value: 0.6855416584743047 and parameters: {'n_estimators': 702, 'learning_rate': 0.06268430247856713, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 404}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:07:14,839] Trial 494 finished with value: 0.6843918890958511 and parameters: {'n_estimators': 730, 'learning_rate': 0.05071715170126901, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 377}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:07:19,755] Trial 495 finished with value: 0.6884546518035858 and parameters: {'n_estimators': 638, 'learning_rate': 0.054388073623393526, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 341}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:07:22,758] Trial 496 finished with value: 0.6405025271123567 and parameters: {'n_estimators': 688, 'learning_rate': 0.04021017543962162, 'max_depth': 3, 'max_bin': 200, 'num_leaves': 428}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:07:25,782] Trial 497 finished with value: 0.6844201625833802 and parameters: {'n_estimators': 604, 'learning_rate': 0.14536928435904645, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 448}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:07:28,783] Trial 498 finished with value: 0.6828943119677863 and parameters: {'n_estimators': 742, 'learning_rate': 0.1275779704311915, 'max_depth': 10, 'max_bin': 229, 'num_leaves': 462}. Best is trial 319 with value: 0.6934612754698257.\n",
      "[I 2023-12-11 22:07:32,488] Trial 499 finished with value: 0.6793690486225357 and parameters: {'n_estimators': 785, 'learning_rate': 0.07161808636493924, 'max_depth': 9, 'max_bin': 195, 'num_leaves': 405}. Best is trial 319 with value: 0.6934612754698257.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.693461275\n",
      "\tBest params:\n",
      "\t\tn_estimators: 744\n",
      "\t\tlearning_rate: 0.09163568258296839\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 190\n",
      "\t\tnum_leaves: 436\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.707475    0.712588    0.708269    0.684985   \n",
      "1                    TP  326.000000  329.000000  309.000000  309.000000   \n",
      "2                    TN  186.000000  182.000000  190.000000  183.000000   \n",
      "3                    FP   41.000000   41.000000   63.000000   59.000000   \n",
      "4                    FN   42.000000   43.000000   33.000000   44.000000   \n",
      "5              Accuracy    0.860504    0.858824    0.838655    0.826891   \n",
      "6             Precision    0.888283    0.889189    0.830645    0.839674   \n",
      "7           Sensitivity    0.885870    0.884409    0.903509    0.875354   \n",
      "8           Specificity    0.819400    0.816100    0.751000    0.756200   \n",
      "9              F1 score    0.887075    0.886792    0.865546    0.857143   \n",
      "10  F1 score (weighted)    0.860563    0.858948    0.836961    0.825923   \n",
      "11     F1 score (macro)    0.852329    0.849646    0.831933    0.818763   \n",
      "12    Balanced Accuracy    0.852626    0.850276    0.827248    0.815776   \n",
      "13                  MCC    0.704663    0.699314    0.668432    0.638657   \n",
      "14                  NPV    0.815800    0.808900    0.852000    0.806200   \n",
      "15              ROC_AUC    0.852626    0.850276    0.827248    0.815776   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.690156    0.693837    0.686856    0.669776    0.693923    0.723982  \n",
      "1   328.000000  320.000000  331.000000  315.000000  329.000000  333.000000  \n",
      "2   164.000000  188.000000  170.000000  177.000000  179.000000  180.000000  \n",
      "3    60.000000   44.000000   58.000000   58.000000   42.000000   47.000000  \n",
      "4    43.000000   43.000000   36.000000   45.000000   45.000000   35.000000  \n",
      "5     0.826891    0.853782    0.842017    0.826891    0.853782    0.862185  \n",
      "6     0.845361    0.879121    0.850900    0.844504    0.886792    0.876316  \n",
      "7     0.884097    0.881543    0.901907    0.875000    0.879679    0.904891  \n",
      "8     0.732100    0.810300    0.745600    0.753200    0.810000    0.793000  \n",
      "9     0.864295    0.880330    0.875661    0.859482    0.883221    0.890374  \n",
      "10    0.825415    0.853724    0.840311    0.825964    0.853980    0.861420  \n",
      "11    0.812658    0.846213    0.829536    0.817049    0.843858    0.852427  \n",
      "12    0.808120    0.845944    0.823761    0.814096    0.844817    0.848921  \n",
      "13    0.626844    0.692430    0.661677    0.634960    0.687764    0.705640  \n",
      "14    0.792300    0.813900    0.825200    0.797300    0.799100    0.837200  \n",
      "15    0.808120    0.845944    0.823761    0.814096    0.844817    0.848921  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_9_cat = np.where((y_pred_lgbm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.702777     0.027579\n",
      "1                    TP       163.300000     4.967673\n",
      "2                    TN        89.300000     5.538752\n",
      "3                    FP        24.100000     4.581363\n",
      "4                    FN        20.400000     5.378971\n",
      "5              Accuracy         0.850220     0.020697\n",
      "6             Precision         0.871846     0.020730\n",
      "7           Sensitivity         0.889152     0.028304\n",
      "8           Specificity         0.787400     0.040307\n",
      "9              F1 score         0.880075     0.016670\n",
      "10  F1 score (weighted)         0.849663     0.020623\n",
      "11     F1 score (macro)         0.840115     0.022301\n",
      "12    Balanced Accuracy         0.838284     0.022368\n",
      "13                  MCC         0.681773     0.044447\n",
      "14                  NPV         0.815350     0.041577\n",
      "15              ROC_AUC         0.838284     0.022368\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.707475</td>\n",
       "      <td>0.712588</td>\n",
       "      <td>0.708269</td>\n",
       "      <td>0.684985</td>\n",
       "      <td>0.690156</td>\n",
       "      <td>0.693837</td>\n",
       "      <td>0.686856</td>\n",
       "      <td>0.669776</td>\n",
       "      <td>0.693923</td>\n",
       "      <td>0.723982</td>\n",
       "      <td>0.697185</td>\n",
       "      <td>0.015854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>322.900000</td>\n",
       "      <td>9.036346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>179.900000</td>\n",
       "      <td>8.020114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>51.300000</td>\n",
       "      <td>9.019116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>40.900000</td>\n",
       "      <td>4.458450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.860504</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.838655</td>\n",
       "      <td>0.826891</td>\n",
       "      <td>0.826891</td>\n",
       "      <td>0.853782</td>\n",
       "      <td>0.842017</td>\n",
       "      <td>0.826891</td>\n",
       "      <td>0.853782</td>\n",
       "      <td>0.862185</td>\n",
       "      <td>0.845042</td>\n",
       "      <td>0.014583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.888283</td>\n",
       "      <td>0.889189</td>\n",
       "      <td>0.830645</td>\n",
       "      <td>0.839674</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>0.844504</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.876316</td>\n",
       "      <td>0.863079</td>\n",
       "      <td>0.022898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.885870</td>\n",
       "      <td>0.884409</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.875354</td>\n",
       "      <td>0.884097</td>\n",
       "      <td>0.881543</td>\n",
       "      <td>0.901907</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.879679</td>\n",
       "      <td>0.904891</td>\n",
       "      <td>0.887626</td>\n",
       "      <td>0.011501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.819400</td>\n",
       "      <td>0.816100</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.756200</td>\n",
       "      <td>0.732100</td>\n",
       "      <td>0.810300</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.753200</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.778690</td>\n",
       "      <td>0.034041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.887075</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.865546</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.864295</td>\n",
       "      <td>0.880330</td>\n",
       "      <td>0.875661</td>\n",
       "      <td>0.859482</td>\n",
       "      <td>0.883221</td>\n",
       "      <td>0.890374</td>\n",
       "      <td>0.874992</td>\n",
       "      <td>0.012389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.860563</td>\n",
       "      <td>0.858948</td>\n",
       "      <td>0.836961</td>\n",
       "      <td>0.825923</td>\n",
       "      <td>0.825415</td>\n",
       "      <td>0.853724</td>\n",
       "      <td>0.840311</td>\n",
       "      <td>0.825964</td>\n",
       "      <td>0.853980</td>\n",
       "      <td>0.861420</td>\n",
       "      <td>0.844321</td>\n",
       "      <td>0.015111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.852329</td>\n",
       "      <td>0.849646</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.818763</td>\n",
       "      <td>0.812658</td>\n",
       "      <td>0.846213</td>\n",
       "      <td>0.829536</td>\n",
       "      <td>0.817049</td>\n",
       "      <td>0.843858</td>\n",
       "      <td>0.852427</td>\n",
       "      <td>0.835441</td>\n",
       "      <td>0.015438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.852626</td>\n",
       "      <td>0.850276</td>\n",
       "      <td>0.827248</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>0.808120</td>\n",
       "      <td>0.845944</td>\n",
       "      <td>0.823761</td>\n",
       "      <td>0.814096</td>\n",
       "      <td>0.844817</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>0.833159</td>\n",
       "      <td>0.017113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.704663</td>\n",
       "      <td>0.699314</td>\n",
       "      <td>0.668432</td>\n",
       "      <td>0.638657</td>\n",
       "      <td>0.626844</td>\n",
       "      <td>0.692430</td>\n",
       "      <td>0.661677</td>\n",
       "      <td>0.634960</td>\n",
       "      <td>0.687764</td>\n",
       "      <td>0.705640</td>\n",
       "      <td>0.672038</td>\n",
       "      <td>0.030281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.808900</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>0.792300</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.825200</td>\n",
       "      <td>0.797300</td>\n",
       "      <td>0.799100</td>\n",
       "      <td>0.837200</td>\n",
       "      <td>0.814790</td>\n",
       "      <td>0.018748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.852626</td>\n",
       "      <td>0.850276</td>\n",
       "      <td>0.827248</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>0.808120</td>\n",
       "      <td>0.845944</td>\n",
       "      <td>0.823761</td>\n",
       "      <td>0.814096</td>\n",
       "      <td>0.844817</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>0.833159</td>\n",
       "      <td>0.017113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.707475    0.712588    0.708269    0.684985   \n",
       "1                    TP  326.000000  329.000000  309.000000  309.000000   \n",
       "2                    TN  186.000000  182.000000  190.000000  183.000000   \n",
       "3                    FP   41.000000   41.000000   63.000000   59.000000   \n",
       "4                    FN   42.000000   43.000000   33.000000   44.000000   \n",
       "5              Accuracy    0.860504    0.858824    0.838655    0.826891   \n",
       "6             Precision    0.888283    0.889189    0.830645    0.839674   \n",
       "7           Sensitivity    0.885870    0.884409    0.903509    0.875354   \n",
       "8           Specificity    0.819400    0.816100    0.751000    0.756200   \n",
       "9              F1 score    0.887075    0.886792    0.865546    0.857143   \n",
       "10  F1 score (weighted)    0.860563    0.858948    0.836961    0.825923   \n",
       "11     F1 score (macro)    0.852329    0.849646    0.831933    0.818763   \n",
       "12    Balanced Accuracy    0.852626    0.850276    0.827248    0.815776   \n",
       "13                  MCC    0.704663    0.699314    0.668432    0.638657   \n",
       "14                  NPV    0.815800    0.808900    0.852000    0.806200   \n",
       "15              ROC_AUC    0.852626    0.850276    0.827248    0.815776   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.690156    0.693837    0.686856    0.669776    0.693923    0.723982   \n",
       "1   328.000000  320.000000  331.000000  315.000000  329.000000  333.000000   \n",
       "2   164.000000  188.000000  170.000000  177.000000  179.000000  180.000000   \n",
       "3    60.000000   44.000000   58.000000   58.000000   42.000000   47.000000   \n",
       "4    43.000000   43.000000   36.000000   45.000000   45.000000   35.000000   \n",
       "5     0.826891    0.853782    0.842017    0.826891    0.853782    0.862185   \n",
       "6     0.845361    0.879121    0.850900    0.844504    0.886792    0.876316   \n",
       "7     0.884097    0.881543    0.901907    0.875000    0.879679    0.904891   \n",
       "8     0.732100    0.810300    0.745600    0.753200    0.810000    0.793000   \n",
       "9     0.864295    0.880330    0.875661    0.859482    0.883221    0.890374   \n",
       "10    0.825415    0.853724    0.840311    0.825964    0.853980    0.861420   \n",
       "11    0.812658    0.846213    0.829536    0.817049    0.843858    0.852427   \n",
       "12    0.808120    0.845944    0.823761    0.814096    0.844817    0.848921   \n",
       "13    0.626844    0.692430    0.661677    0.634960    0.687764    0.705640   \n",
       "14    0.792300    0.813900    0.825200    0.797300    0.799100    0.837200   \n",
       "15    0.808120    0.845944    0.823761    0.814096    0.844817    0.848921   \n",
       "\n",
       "           ave       std  \n",
       "0     0.697185  0.015854  \n",
       "1   322.900000  9.036346  \n",
       "2   179.900000  8.020114  \n",
       "3    51.300000  9.019116  \n",
       "4    40.900000  4.458450  \n",
       "5     0.845042  0.014583  \n",
       "6     0.863079  0.022898  \n",
       "7     0.887626  0.011501  \n",
       "8     0.778690  0.034041  \n",
       "9     0.874992  0.012389  \n",
       "10    0.844321  0.015111  \n",
       "11    0.835441  0.015438  \n",
       "12    0.833159  0.017113  \n",
       "13    0.672038  0.030281  \n",
       "14    0.814790  0.018748  \n",
       "15    0.833159  0.017113  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_lgbm0</th>\n",
       "      <th>y_pred_lgbm1</th>\n",
       "      <th>y_pred_lgbm2</th>\n",
       "      <th>y_pred_lgbm3</th>\n",
       "      <th>y_pred_lgbm4</th>\n",
       "      <th>y_pred_lgbm_ave</th>\n",
       "      <th>y_pred_lgbm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2047687</td>\n",
       "      <td>0</td>\n",
       "      <td>5.48</td>\n",
       "      <td>6.528383</td>\n",
       "      <td>6.458908</td>\n",
       "      <td>6.483957</td>\n",
       "      <td>6.955678</td>\n",
       "      <td>6.556552</td>\n",
       "      <td>6.410580</td>\n",
       "      <td>0.448333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1164212</td>\n",
       "      <td>1</td>\n",
       "      <td>5.76</td>\n",
       "      <td>5.040031</td>\n",
       "      <td>5.084255</td>\n",
       "      <td>4.899712</td>\n",
       "      <td>4.885101</td>\n",
       "      <td>5.258132</td>\n",
       "      <td>5.154539</td>\n",
       "      <td>0.298021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2337873</td>\n",
       "      <td>2</td>\n",
       "      <td>6.07</td>\n",
       "      <td>6.218656</td>\n",
       "      <td>6.028848</td>\n",
       "      <td>6.090655</td>\n",
       "      <td>6.119664</td>\n",
       "      <td>6.239675</td>\n",
       "      <td>6.127916</td>\n",
       "      <td>0.076756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4577419</td>\n",
       "      <td>3</td>\n",
       "      <td>7.90</td>\n",
       "      <td>7.520235</td>\n",
       "      <td>7.656090</td>\n",
       "      <td>7.608416</td>\n",
       "      <td>7.657833</td>\n",
       "      <td>7.683077</td>\n",
       "      <td>7.670942</td>\n",
       "      <td>0.115202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL515452</td>\n",
       "      <td>4</td>\n",
       "      <td>6.22</td>\n",
       "      <td>6.634060</td>\n",
       "      <td>6.757312</td>\n",
       "      <td>6.499352</td>\n",
       "      <td>6.670867</td>\n",
       "      <td>6.764183</td>\n",
       "      <td>6.590962</td>\n",
       "      <td>0.187938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL3693800</td>\n",
       "      <td>2966</td>\n",
       "      <td>8.22</td>\n",
       "      <td>8.290476</td>\n",
       "      <td>8.055604</td>\n",
       "      <td>8.171219</td>\n",
       "      <td>8.089339</td>\n",
       "      <td>7.975300</td>\n",
       "      <td>8.133656</td>\n",
       "      <td>0.105350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL2431917</td>\n",
       "      <td>2967</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.768940</td>\n",
       "      <td>6.884417</td>\n",
       "      <td>6.945022</td>\n",
       "      <td>6.847230</td>\n",
       "      <td>6.713712</td>\n",
       "      <td>6.936554</td>\n",
       "      <td>0.245804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL2413298</td>\n",
       "      <td>2968</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.201689</td>\n",
       "      <td>6.112414</td>\n",
       "      <td>6.022732</td>\n",
       "      <td>6.065887</td>\n",
       "      <td>6.101353</td>\n",
       "      <td>6.127346</td>\n",
       "      <td>0.080357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3656016</td>\n",
       "      <td>2969</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.087657</td>\n",
       "      <td>8.186060</td>\n",
       "      <td>8.151152</td>\n",
       "      <td>8.122321</td>\n",
       "      <td>8.204391</td>\n",
       "      <td>8.211930</td>\n",
       "      <td>0.143062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL4643138</td>\n",
       "      <td>2970</td>\n",
       "      <td>6.38</td>\n",
       "      <td>5.967330</td>\n",
       "      <td>6.129734</td>\n",
       "      <td>6.099335</td>\n",
       "      <td>6.299921</td>\n",
       "      <td>6.514398</td>\n",
       "      <td>6.231786</td>\n",
       "      <td>0.184560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_lgbm0  y_pred_lgbm1  \\\n",
       "0         CHEMBL2047687            0     5.48      6.528383      6.458908   \n",
       "1         CHEMBL1164212            1     5.76      5.040031      5.084255   \n",
       "2         CHEMBL2337873            2     6.07      6.218656      6.028848   \n",
       "3         CHEMBL4577419            3     7.90      7.520235      7.656090   \n",
       "4          CHEMBL515452            4     6.22      6.634060      6.757312   \n",
       "...                 ...          ...      ...           ...           ...   \n",
       "2966      CHEMBL3693800         2966     8.22      8.290476      8.055604   \n",
       "2967      CHEMBL2431917         2967     7.46      6.768940      6.884417   \n",
       "2968      CHEMBL2413298         2968     6.26      6.201689      6.112414   \n",
       "2969      CHEMBL3656016         2969     8.52      8.087657      8.186060   \n",
       "2970      CHEMBL4643138         2970     6.38      5.967330      6.129734   \n",
       "\n",
       "      y_pred_lgbm2  y_pred_lgbm3  y_pred_lgbm4  y_pred_lgbm_ave  \\\n",
       "0         6.483957      6.955678      6.556552         6.410580   \n",
       "1         4.899712      4.885101      5.258132         5.154539   \n",
       "2         6.090655      6.119664      6.239675         6.127916   \n",
       "3         7.608416      7.657833      7.683077         7.670942   \n",
       "4         6.499352      6.670867      6.764183         6.590962   \n",
       "...            ...           ...           ...              ...   \n",
       "2966      8.171219      8.089339      7.975300         8.133656   \n",
       "2967      6.945022      6.847230      6.713712         6.936554   \n",
       "2968      6.022732      6.065887      6.101353         6.127346   \n",
       "2969      8.151152      8.122321      8.204391         8.211930   \n",
       "2970      6.099335      6.299921      6.514398         6.231786   \n",
       "\n",
       "      y_pred_lgbm_std  \n",
       "0            0.448333  \n",
       "1            0.298021  \n",
       "2            0.076756  \n",
       "3            0.115202  \n",
       "4            0.187938  \n",
       "...               ...  \n",
       "2966         0.105350  \n",
       "2967         0.245804  \n",
       "2968         0.080357  \n",
       "2969         0.143062  \n",
       "2970         0.184560  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where((y_pred_optimized_lgbm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "lgbm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.6906 with a standard deviation of 0.0521\n",
      "LightGBM optimized model r2_score 0.6968 with a standard deviation of 0.0524\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm.joblib']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"OUTPUT/lgbm_reg.joblib\")\n",
    "#joblib.dump(optimized_lgbm, \"OUTPUT/optimized_lgbm.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm.joblib\") \n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.662598     0.038923\n",
      "1                    TP       161.600000     4.427189\n",
      "2                    TN        86.800000     5.996295\n",
      "3                    FP        26.600000     5.295701\n",
      "4                    FN        22.100000     5.363457\n",
      "5              Accuracy         0.836089     0.025141\n",
      "6             Precision         0.859163     0.024939\n",
      "7           Sensitivity         0.879961     0.027890\n",
      "8           Specificity         0.765400     0.046490\n",
      "9              F1 score         0.869120     0.019589\n",
      "10  F1 score (weighted)         0.835359     0.025155\n",
      "11     F1 score (macro)         0.824767     0.027497\n",
      "12    Balanced Accuracy         0.822684     0.028069\n",
      "13                  MCC         0.651094     0.055165\n",
      "14                  NPV         0.797850     0.044256\n",
      "15              ROC_AUC         0.822684     0.028069\n",
      "CPU times: user 1h 48min 34s, sys: 6.51 s, total: 1h 48min 41s\n",
      "Wall time: 5min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 22:13:34,076] A new study created in memory with name: XGBRegressor\n",
      "[I 2023-12-11 22:13:41,290] Trial 0 finished with value: 0.6449008119095928 and parameters: {'n_estimators': 197, 'eta': 0.03174881352395979, 'max_depth': 10, 'alpha': 0.2625, 'lambda': 21.291875048940536, 'max_bin': 493}. Best is trial 0 with value: 0.6449008119095928.\n",
      "[I 2023-12-11 22:13:50,773] Trial 1 finished with value: 0.6696103902623959 and parameters: {'n_estimators': 291, 'eta': 0.05032193045334962, 'max_depth': 8, 'alpha': 0.5870000000000001, 'lambda': 36.58635931682256, 'max_bin': 387}. Best is trial 1 with value: 0.6696103902623959.\n",
      "[I 2023-12-11 22:14:18,615] Trial 2 finished with value: 0.6570326413347682 and parameters: {'n_estimators': 615, 'eta': 0.01103680977149842, 'max_depth': 12, 'alpha': 0.5929, 'lambda': 23.60465359775388, 'max_bin': 279}. Best is trial 1 with value: 0.6696103902623959.\n",
      "[I 2023-12-11 22:14:22,057] Trial 3 finished with value: -9.900503702392943 and parameters: {'n_estimators': 309, 'eta': 0.0018919252402458712, 'max_depth': 9, 'alpha': 0.27790000000000004, 'lambda': 32.13539429257381, 'max_bin': 324}. Best is trial 1 with value: 0.6696103902623959.\n",
      "[I 2023-12-11 22:14:27,411] Trial 4 finished with value: 0.6521263547233032 and parameters: {'n_estimators': 160, 'eta': 0.04325057352794066, 'max_depth': 8, 'alpha': 0.3176, 'lambda': 7.847675733651195, 'max_bin': 401}. Best is trial 1 with value: 0.6696103902623959.\n",
      "[I 2023-12-11 22:14:50,228] Trial 5 finished with value: 0.6692855457786486 and parameters: {'n_estimators': 478, 'eta': 0.01823050455774488, 'max_depth': 12, 'alpha': 0.025900000000000003, 'lambda': 10.80340156845886, 'max_bin': 302}. Best is trial 1 with value: 0.6696103902623959.\n",
      "[I 2023-12-11 22:15:00,951] Trial 6 finished with value: 0.5883637737507539 and parameters: {'n_estimators': 314, 'eta': 0.01287177706705942, 'max_depth': 10, 'alpha': 0.38570000000000004, 'lambda': 17.925625357960335, 'max_bin': 312}. Best is trial 1 with value: 0.6696103902623959.\n",
      "[I 2023-12-11 22:15:11,382] Trial 7 finished with value: 0.6778795876511977 and parameters: {'n_estimators': 454, 'eta': 0.06806591224333114, 'max_depth': 6, 'alpha': 0.8078000000000001, 'lambda': 1.494505959900326, 'max_bin': 278}. Best is trial 7 with value: 0.6778795876511977.\n",
      "[I 2023-12-11 22:15:24,547] Trial 8 finished with value: 0.6846788813935144 and parameters: {'n_estimators': 457, 'eta': 0.09302585623322479, 'max_depth': 7, 'alpha': 0.7151000000000001, 'lambda': 35.181139123161756, 'max_bin': 345}. Best is trial 8 with value: 0.6846788813935144.\n",
      "[I 2023-12-11 22:15:53,674] Trial 9 finished with value: 0.6777361853786832 and parameters: {'n_estimators': 631, 'eta': 0.02270997620658751, 'max_depth': 12, 'alpha': 0.08710000000000001, 'lambda': 6.947678291594431, 'max_bin': 436}. Best is trial 8 with value: 0.6846788813935144.\n",
      "[I 2023-12-11 22:16:08,251] Trial 10 finished with value: 0.6805413686982404 and parameters: {'n_estimators': 890, 'eta': 0.0949373737375996, 'max_depth': 5, 'alpha': 0.8593000000000001, 'lambda': 39.576848602273344, 'max_bin': 359}. Best is trial 8 with value: 0.6846788813935144.\n",
      "[I 2023-12-11 22:16:23,039] Trial 11 finished with value: 0.6822475686547618 and parameters: {'n_estimators': 882, 'eta': 0.09670058937429818, 'max_depth': 5, 'alpha': 0.9828, 'lambda': 39.284314450885276, 'max_bin': 348}. Best is trial 8 with value: 0.6846788813935144.\n",
      "[I 2023-12-11 22:16:34,936] Trial 12 finished with value: 0.6828053792062475 and parameters: {'n_estimators': 882, 'eta': 0.0998886117496655, 'max_depth': 6, 'alpha': 0.9866, 'lambda': 32.383275704065845, 'max_bin': 354}. Best is trial 8 with value: 0.6846788813935144.\n",
      "[I 2023-12-11 22:16:51,497] Trial 13 finished with value: 0.6881853836296028 and parameters: {'n_estimators': 712, 'eta': 0.084227683406628, 'max_depth': 7, 'alpha': 0.7535000000000001, 'lambda': 30.93522471027289, 'max_bin': 422}. Best is trial 13 with value: 0.6881853836296028.\n",
      "[I 2023-12-11 22:17:08,019] Trial 14 finished with value: 0.6870725726047073 and parameters: {'n_estimators': 691, 'eta': 0.07741337825524569, 'max_depth': 7, 'alpha': 0.7248, 'lambda': 28.36874318438658, 'max_bin': 434}. Best is trial 13 with value: 0.6881853836296028.\n",
      "[I 2023-12-11 22:17:25,249] Trial 15 finished with value: 0.6875350001548297 and parameters: {'n_estimators': 691, 'eta': 0.07680710657124769, 'max_depth': 7, 'alpha': 0.6869000000000001, 'lambda': 28.848697662329492, 'max_bin': 446}. Best is trial 13 with value: 0.6881853836296028.\n",
      "[I 2023-12-11 22:17:41,620] Trial 16 finished with value: 0.6852651733383839 and parameters: {'n_estimators': 748, 'eta': 0.07312865949514855, 'max_depth': 7, 'alpha': 0.49670000000000003, 'lambda': 27.51804899962757, 'max_bin': 487}. Best is trial 13 with value: 0.6881853836296028.\n",
      "[I 2023-12-11 22:17:57,988] Trial 17 finished with value: 0.683719704291046 and parameters: {'n_estimators': 738, 'eta': 0.08159377935397448, 'max_depth': 9, 'alpha': 0.6335000000000001, 'lambda': 26.947310692518016, 'max_bin': 451}. Best is trial 13 with value: 0.6881853836296028.\n",
      "[I 2023-12-11 22:18:14,591] Trial 18 finished with value: 0.687068391714609 and parameters: {'n_estimators': 789, 'eta': 0.06435886611873087, 'max_depth': 6, 'alpha': 0.8406, 'lambda': 17.928389724144985, 'max_bin': 415}. Best is trial 13 with value: 0.6881853836296028.\n",
      "[I 2023-12-11 22:18:31,384] Trial 19 finished with value: 0.6898560992144231 and parameters: {'n_estimators': 547, 'eta': 0.08256083426589007, 'max_depth': 8, 'alpha': 0.48100000000000004, 'lambda': 30.790034104198973, 'max_bin': 464}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:18:48,461] Trial 20 finished with value: 0.6855604724962716 and parameters: {'n_estimators': 548, 'eta': 0.060685539265900916, 'max_depth': 8, 'alpha': 0.4425, 'lambda': 31.94054123297164, 'max_bin': 468}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:19:03,439] Trial 21 finished with value: 0.6878885480923239 and parameters: {'n_estimators': 607, 'eta': 0.08423499406010355, 'max_depth': 7, 'alpha': 0.6787000000000001, 'lambda': 28.742248899166075, 'max_bin': 461}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:19:18,380] Trial 22 finished with value: 0.6825949197959871 and parameters: {'n_estimators': 532, 'eta': 0.0887002329287783, 'max_depth': 9, 'alpha': 0.532, 'lambda': 23.80290708873843, 'max_bin': 463}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:19:35,766] Trial 23 finished with value: 0.6881569514197572 and parameters: {'n_estimators': 626, 'eta': 0.07918932595451159, 'max_depth': 8, 'alpha': 0.7636000000000001, 'lambda': 34.6224812303434, 'max_bin': 417}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:19:38,410] Trial 24 finished with value: 0.6339955072460068 and parameters: {'n_estimators': 69, 'eta': 0.0883448514463577, 'max_depth': 10, 'alpha': 0.7817000000000001, 'lambda': 36.15943494556969, 'max_bin': 409}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:19:51,012] Trial 25 finished with value: 0.686386269348993 and parameters: {'n_estimators': 393, 'eta': 0.08345469916733132, 'max_depth': 8, 'alpha': 0.8965000000000001, 'lambda': 33.02787793625221, 'max_bin': 427}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:20:13,556] Trial 26 finished with value: 0.6826167686987198 and parameters: {'n_estimators': 799, 'eta': 0.06950875452279416, 'max_depth': 11, 'alpha': 0.1559, 'lambda': 35.27432581105728, 'max_bin': 382}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:20:31,109] Trial 27 finished with value: 0.6862277930974532 and parameters: {'n_estimators': 562, 'eta': 0.07851817931393566, 'max_depth': 9, 'alpha': 0.5038, 'lambda': 29.92998552251545, 'max_bin': 476}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:20:47,347] Trial 28 finished with value: 0.6851093105988271 and parameters: {'n_estimators': 671, 'eta': 0.05811870376332539, 'max_depth': 6, 'alpha': 0.7694000000000001, 'lambda': 38.409821075540506, 'max_bin': 251}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:21:00,252] Trial 29 finished with value: 0.6831746535946479 and parameters: {'n_estimators': 382, 'eta': 0.07201707444486403, 'max_depth': 8, 'alpha': 0.3925, 'lambda': 34.737261343163794, 'max_bin': 488}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:21:16,505] Trial 30 finished with value: 0.6869011564156251 and parameters: {'n_estimators': 512, 'eta': 0.08900873708615151, 'max_depth': 10, 'alpha': 0.8995000000000001, 'lambda': 30.44572689283292, 'max_bin': 500}. Best is trial 19 with value: 0.6898560992144231.\n",
      "[I 2023-12-11 22:21:31,053] Trial 31 finished with value: 0.6909394963503777 and parameters: {'n_estimators': 606, 'eta': 0.08140645804057761, 'max_depth': 7, 'alpha': 0.6611, 'lambda': 26.15370889233347, 'max_bin': 451}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:21:45,800] Trial 32 finished with value: 0.6881597977720922 and parameters: {'n_estimators': 617, 'eta': 0.08385843445696887, 'max_depth': 7, 'alpha': 0.6157, 'lambda': 25.0259096437473, 'max_bin': 397}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:21:58,532] Trial 33 finished with value: 0.6874839335569122 and parameters: {'n_estimators': 594, 'eta': 0.09986843398248234, 'max_depth': 7, 'alpha': 0.5693, 'lambda': 24.315744330486524, 'max_bin': 401}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:22:12,479] Trial 34 finished with value: 0.6880594053064597 and parameters: {'n_estimators': 666, 'eta': 0.08490663551424324, 'max_depth': 6, 'alpha': 0.6112000000000001, 'lambda': 24.918515841635436, 'max_bin': 369}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:22:30,718] Trial 35 finished with value: 0.6904693672592829 and parameters: {'n_estimators': 743, 'eta': 0.05483879114486611, 'max_depth': 7, 'alpha': 0.6524, 'lambda': 21.0161077761259, 'max_bin': 451}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:22:51,686] Trial 36 finished with value: 0.6870631196599308 and parameters: {'n_estimators': 801, 'eta': 0.05147679777068484, 'max_depth': 8, 'alpha': 0.443, 'lambda': 21.51012291671043, 'max_bin': 446}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:23:06,745] Trial 37 finished with value: 0.6705610745006518 and parameters: {'n_estimators': 739, 'eta': 0.0374961747174306, 'max_depth': 5, 'alpha': 0.6674, 'lambda': 21.425953707880822, 'max_bin': 478}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:23:24,771] Trial 38 finished with value: 0.6835394683215072 and parameters: {'n_estimators': 849, 'eta': 0.05492383829457515, 'max_depth': 6, 'alpha': 0.552, 'lambda': 26.381902514671435, 'max_bin': 426}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:23:41,059] Trial 39 finished with value: 0.6872568636562303 and parameters: {'n_estimators': 493, 'eta': 0.06483705216639882, 'max_depth': 8, 'alpha': 0.34700000000000003, 'lambda': 30.158821412342036, 'max_bin': 453}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:23:56,325] Trial 40 finished with value: 0.6827250769629363 and parameters: {'n_estimators': 569, 'eta': 0.046435258178746344, 'max_depth': 7, 'alpha': 0.21930000000000002, 'lambda': 19.36829552731244, 'max_bin': 439}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:24:13,068] Trial 41 finished with value: 0.6865227037663785 and parameters: {'n_estimators': 657, 'eta': 0.07212689197533415, 'max_depth': 7, 'alpha': 0.625, 'lambda': 25.403146436958405, 'max_bin': 400}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:24:26,112] Trial 42 finished with value: 0.6798086405379722 and parameters: {'n_estimators': 723, 'eta': 0.09003750961092247, 'max_depth': 7, 'alpha': 0.4484, 'lambda': 22.508791322081123, 'max_bin': 391}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:24:35,644] Trial 43 finished with value: 0.6814619440983295 and parameters: {'n_estimators': 430, 'eta': 0.09290022500701324, 'max_depth': 6, 'alpha': 0.7344, 'lambda': 26.145143377814584, 'max_bin': 419}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:24:50,696] Trial 44 finished with value: 0.6880319398616362 and parameters: {'n_estimators': 592, 'eta': 0.07497903776434992, 'max_depth': 8, 'alpha': 0.5962000000000001, 'lambda': 23.131232250784546, 'max_bin': 465}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:24:57,163] Trial 45 finished with value: 0.6738768813398471 and parameters: {'n_estimators': 245, 'eta': 0.08176778985682673, 'max_depth': 7, 'alpha': 0.6519, 'lambda': 19.73303733614236, 'max_bin': 390}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:25:10,411] Trial 46 finished with value: 0.6850505237007463 and parameters: {'n_estimators': 646, 'eta': 0.07559990724869417, 'max_depth': 6, 'alpha': 0.7191000000000001, 'lambda': 15.139470298446739, 'max_bin': 432}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:25:28,676] Trial 47 finished with value: 0.6854714789719598 and parameters: {'n_estimators': 703, 'eta': 0.06904343753273999, 'max_depth': 8, 'alpha': 0.5074000000000001, 'lambda': 26.686020357204008, 'max_bin': 334}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:25:43,246] Trial 48 finished with value: 0.6839290892132462 and parameters: {'n_estimators': 824, 'eta': 0.08526914435515591, 'max_depth': 7, 'alpha': 0.8088000000000001, 'lambda': 24.820822680527737, 'max_bin': 369}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:25:58,671] Trial 49 finished with value: 0.688536418214698 and parameters: {'n_estimators': 765, 'eta': 0.09599341093740518, 'max_depth': 9, 'alpha': 0.5817, 'lambda': 28.015068024175946, 'max_bin': 475}. Best is trial 31 with value: 0.6909394963503777.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6909\n",
      "\tBest params:\n",
      "\t\tn_estimators: 606\n",
      "\t\teta: 0.08140645804057761\n",
      "\t\tmax_depth: 7\n",
      "\t\talpha: 0.6611\n",
      "\t\tlambda: 26.15370889233347\n",
      "\t\tmax_bin: 451\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.714082\n",
      "1                    TP  328.000000\n",
      "2                    TN  186.000000\n",
      "3                    FP   41.000000\n",
      "4                    FN   40.000000\n",
      "5              Accuracy    0.863866\n",
      "6             Precision    0.888889\n",
      "7           Sensitivity    0.891304\n",
      "8           Specificity    0.819400\n",
      "9              F1 score    0.890095\n",
      "10  F1 score (weighted)    0.863808\n",
      "11     F1 score (macro)    0.855644\n",
      "12    Balanced Accuracy    0.855344\n",
      "13                  MCC    0.711292\n",
      "14                  NPV    0.823000\n",
      "15              ROC_AUC    0.855344\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "y_pred_xgb_0_cat = np.where((y_pred_xgb_0 >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 22:26:13,554] Trial 50 finished with value: 0.6873332375975195 and parameters: {'n_estimators': 761, 'eta': 0.09535055124800566, 'max_depth': 9, 'alpha': 0.5621, 'lambda': 31.67312208849351, 'max_bin': 479}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:26:26,861] Trial 51 finished with value: 0.681803634711763 and parameters: {'n_estimators': 710, 'eta': 0.09177929548668795, 'max_depth': 9, 'alpha': 0.6001000000000001, 'lambda': 28.464231328523564, 'max_bin': 454}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:26:42,053] Trial 52 finished with value: 0.6844253975345047 and parameters: {'n_estimators': 843, 'eta': 0.08015765741265451, 'max_depth': 7, 'alpha': 0.6489, 'lambda': 27.92718234937739, 'max_bin': 441}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:26:55,943] Trial 53 finished with value: 0.6816851400264312 and parameters: {'n_estimators': 761, 'eta': 0.08687689388058235, 'max_depth': 9, 'alpha': 0.7054, 'lambda': 23.071147533982135, 'max_bin': 472}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:27:07,550] Trial 54 finished with value: 0.6808813129885772 and parameters: {'n_estimators': 625, 'eta': 0.09595487921348186, 'max_depth': 8, 'alpha': 0.41050000000000003, 'lambda': 27.169592766733057, 'max_bin': 458}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:27:22,118] Trial 55 finished with value: 0.687496205423183 and parameters: {'n_estimators': 683, 'eta': 0.091171821342165, 'max_depth': 7, 'alpha': 0.47790000000000005, 'lambda': 30.333482683030446, 'max_bin': 496}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:27:37,028] Trial 56 finished with value: 0.6858168816892843 and parameters: {'n_estimators': 520, 'eta': 0.08149788430611349, 'max_depth': 8, 'alpha': 0.5313, 'lambda': 33.07239551010461, 'max_bin': 425}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:27:52,467] Trial 57 finished with value: 0.6786274271925362 and parameters: {'n_estimators': 594, 'eta': 0.08710490997297712, 'max_depth': 11, 'alpha': 0.7492000000000001, 'lambda': 25.494462733069764, 'max_bin': 485}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:28:06,560] Trial 58 finished with value: 0.6858486798853153 and parameters: {'n_estimators': 766, 'eta': 0.07644608963524138, 'max_depth': 6, 'alpha': 0.6863, 'lambda': 29.00588650636752, 'max_bin': 409}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:28:18,580] Trial 59 finished with value: 0.6832976336721881 and parameters: {'n_estimators': 459, 'eta': 0.09731912221707266, 'max_depth': 7, 'alpha': 0.8125, 'lambda': 23.76657618186475, 'max_bin': 443}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:28:31,468] Trial 60 finished with value: 0.6830717078272723 and parameters: {'n_estimators': 564, 'eta': 0.09285858421895207, 'max_depth': 8, 'alpha': 0.313, 'lambda': 27.64660221900494, 'max_bin': 471}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:28:49,168] Trial 61 finished with value: 0.6825050452754541 and parameters: {'n_estimators': 623, 'eta': 0.07858776489718049, 'max_depth': 9, 'alpha': 0.7728, 'lambda': 33.57347698590665, 'max_bin': 411}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:29:04,800] Trial 62 finished with value: 0.6839700324174238 and parameters: {'n_estimators': 648, 'eta': 0.08426661626355192, 'max_depth': 8, 'alpha': 0.885, 'lambda': 31.44335031964356, 'max_bin': 419}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:29:22,671] Trial 63 finished with value: 0.685515997728135 and parameters: {'n_estimators': 706, 'eta': 0.0785751107111593, 'max_depth': 10, 'alpha': 0.5839, 'lambda': 34.25562450474493, 'max_bin': 399}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:29:37,715] Trial 64 finished with value: 0.6891193316193379 and parameters: {'n_estimators': 784, 'eta': 0.08187119361427957, 'max_depth': 7, 'alpha': 0.8439000000000001, 'lambda': 37.198109200507446, 'max_bin': 434}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:29:52,133] Trial 65 finished with value: 0.6847914829121035 and parameters: {'n_estimators': 778, 'eta': 0.08763157366571289, 'max_depth': 7, 'alpha': 0.9372, 'lambda': 38.831252055761716, 'max_bin': 433}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:30:07,957] Trial 66 finished with value: 0.6888586568910156 and parameters: {'n_estimators': 816, 'eta': 0.07360001959697964, 'max_depth': 6, 'alpha': 0.8516, 'lambda': 36.53968650314271, 'max_bin': 449}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:30:23,911] Trial 67 finished with value: 0.6848341612360539 and parameters: {'n_estimators': 864, 'eta': 0.07291116167041517, 'max_depth': 5, 'alpha': 0.8473, 'lambda': 35.67867676353201, 'max_bin': 451}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:30:42,276] Trial 68 finished with value: 0.6879763070984971 and parameters: {'n_estimators': 818, 'eta': 0.06428712439778221, 'max_depth': 6, 'alpha': 0.9472, 'lambda': 37.09033746794845, 'max_bin': 462}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:30:58,344] Trial 69 finished with value: 0.6852832263157379 and parameters: {'n_estimators': 889, 'eta': 0.08167753124470115, 'max_depth': 6, 'alpha': 0.8139000000000001, 'lambda': 36.79069998009716, 'max_bin': 482}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:31:15,317] Trial 70 finished with value: 0.6867173143571441 and parameters: {'n_estimators': 811, 'eta': 0.07492461282383962, 'max_depth': 6, 'alpha': 0.8514, 'lambda': 37.49377743851662, 'max_bin': 447}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:31:30,515] Trial 71 finished with value: 0.6876827949890925 and parameters: {'n_estimators': 734, 'eta': 0.08529238283870325, 'max_depth': 7, 'alpha': 0.6996, 'lambda': 39.916954509347875, 'max_bin': 457}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:31:44,724] Trial 72 finished with value: 0.6853686244367053 and parameters: {'n_estimators': 792, 'eta': 0.08234358077280815, 'max_depth': 7, 'alpha': 0.538, 'lambda': 32.40801010811033, 'max_bin': 437}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:31:59,390] Trial 73 finished with value: 0.6844340937621137 and parameters: {'n_estimators': 844, 'eta': 0.07904126979010749, 'max_depth': 7, 'alpha': 0.6214000000000001, 'lambda': 29.41632191448988, 'max_bin': 468}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:32:11,919] Trial 74 finished with value: 0.6808983907751476 and parameters: {'n_estimators': 682, 'eta': 0.06962411836686204, 'max_depth': 5, 'alpha': 0.6581, 'lambda': 34.25484460642949, 'max_bin': 427}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:32:27,349] Trial 75 finished with value: 0.6839176565586238 and parameters: {'n_estimators': 726, 'eta': 0.09015960791495974, 'max_depth': 7, 'alpha': 0.7369, 'lambda': 30.93811459855144, 'max_bin': 493}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:32:38,971] Trial 76 finished with value: 0.6819951775236076 and parameters: {'n_estimators': 548, 'eta': 0.08855240568702401, 'max_depth': 6, 'alpha': 0.871, 'lambda': 36.16478869046521, 'max_bin': 380}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:32:55,285] Trial 77 finished with value: 0.6872402626248603 and parameters: {'n_estimators': 782, 'eta': 0.08314352465023403, 'max_depth': 8, 'alpha': 0.9198000000000001, 'lambda': 29.363939812752292, 'max_bin': 447}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:33:10,340] Trial 78 finished with value: 0.6847609651788651 and parameters: {'n_estimators': 750, 'eta': 0.09434162741286001, 'max_depth': 7, 'alpha': 0.6374000000000001, 'lambda': 38.32979623788154, 'max_bin': 475}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:33:26,386] Trial 79 finished with value: 0.6833528490906415 and parameters: {'n_estimators': 497, 'eta': 0.07578680997233239, 'max_depth': 9, 'alpha': 0.4786, 'lambda': 31.15752567772399, 'max_bin': 461}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:33:30,183] Trial 80 finished with value: 0.659688971527485 and parameters: {'n_estimators': 120, 'eta': 0.0855666139825684, 'max_depth': 8, 'alpha': 0.7906000000000001, 'lambda': 26.09579120484942, 'max_bin': 421}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:33:46,540] Trial 81 finished with value: 0.6831408791058128 and parameters: {'n_estimators': 640, 'eta': 0.08053400084445259, 'max_depth': 8, 'alpha': 0.7544000000000001, 'lambda': 34.99792988547854, 'max_bin': 403}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:34:01,516] Trial 82 finished with value: 0.6846546928935134 and parameters: {'n_estimators': 580, 'eta': 0.07823390535969049, 'max_depth': 8, 'alpha': 0.6705, 'lambda': 32.44291897626345, 'max_bin': 430}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:34:19,819] Trial 83 finished with value: 0.6862562306696117 and parameters: {'n_estimators': 535, 'eta': 0.07181078575388387, 'max_depth': 9, 'alpha': 0.8313, 'lambda': 33.704246820853335, 'max_bin': 413}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:34:33,935] Trial 84 finished with value: 0.6872466285762255 and parameters: {'n_estimators': 601, 'eta': 0.08997570779679155, 'max_depth': 7, 'alpha': 0.5806, 'lambda': 35.64961848841258, 'max_bin': 438}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:34:49,100] Trial 85 finished with value: 0.6852649495655986 and parameters: {'n_estimators': 658, 'eta': 0.08371517678736408, 'max_depth': 7, 'alpha': 0.7911, 'lambda': 27.73043587849371, 'max_bin': 450}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:35:01,987] Trial 86 finished with value: 0.6842228228789288 and parameters: {'n_estimators': 621, 'eta': 0.06688131123728833, 'max_depth': 6, 'alpha': 0.7115, 'lambda': 30.172477969396848, 'max_bin': 397}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:35:16,271] Trial 87 finished with value: 0.6855205351079523 and parameters: {'n_estimators': 699, 'eta': 0.08718215243665105, 'max_depth': 7, 'alpha': 0.612, 'lambda': 33.123246270837086, 'max_bin': 406}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:35:33,735] Trial 88 finished with value: 0.6813032755077775 and parameters: {'n_estimators': 671, 'eta': 0.07437590774969371, 'max_depth': 9, 'alpha': 0.993, 'lambda': 24.454454825543475, 'max_bin': 298}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:35:49,224] Trial 89 finished with value: 0.6884925876862467 and parameters: {'n_estimators': 718, 'eta': 0.0772744925540318, 'max_depth': 8, 'alpha': 0.6846, 'lambda': 28.37954978574457, 'max_bin': 416}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:36:03,769] Trial 90 finished with value: 0.6866035535756265 and parameters: {'n_estimators': 829, 'eta': 0.07664556802743087, 'max_depth': 7, 'alpha': 0.5525, 'lambda': 28.36998145732741, 'max_bin': 443}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:36:18,560] Trial 91 finished with value: 0.6812987374341992 and parameters: {'n_estimators': 728, 'eta': 0.0802149608472918, 'max_depth': 8, 'alpha': 0.6859000000000001, 'lambda': 27.030436814373672, 'max_bin': 425}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:36:33,182] Trial 92 finished with value: 0.6830320861830275 and parameters: {'n_estimators': 747, 'eta': 0.08318935309434525, 'max_depth': 8, 'alpha': 0.7669, 'lambda': 25.43778923749888, 'max_bin': 415}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:36:49,261] Trial 93 finished with value: 0.6858292211429662 and parameters: {'n_estimators': 776, 'eta': 0.077332596230942, 'max_depth': 8, 'alpha': 0.7451, 'lambda': 31.730379924677933, 'max_bin': 433}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:37:06,536] Trial 94 finished with value: 0.6849138311739613 and parameters: {'n_estimators': 713, 'eta': 0.07080463402810395, 'max_depth': 9, 'alpha': 0.5119, 'lambda': 29.51985059097535, 'max_bin': 393}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:37:23,107] Trial 95 finished with value: 0.6899217883970772 and parameters: {'n_estimators': 801, 'eta': 0.07400572859259091, 'max_depth': 7, 'alpha': 0.7211000000000001, 'lambda': 28.615216802445815, 'max_bin': 420}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:37:37,901] Trial 96 finished with value: 0.6843682154064411 and parameters: {'n_estimators': 800, 'eta': 0.07396759528784393, 'max_depth': 7, 'alpha': 0.655, 'lambda': 28.847000812578894, 'max_bin': 457}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:37:51,404] Trial 97 finished with value: 0.6835625438376645 and parameters: {'n_estimators': 754, 'eta': 0.09103902011028231, 'max_depth': 6, 'alpha': 0.6237, 'lambda': 26.803366886675636, 'max_bin': 466}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:38:07,104] Trial 98 finished with value: 0.6873833343808242 and parameters: {'n_estimators': 875, 'eta': 0.07277197409039908, 'max_depth': 7, 'alpha': 0.5755, 'lambda': 28.304094640838734, 'max_bin': 386}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:38:22,403] Trial 99 finished with value: 0.6848423536175944 and parameters: {'n_estimators': 836, 'eta': 0.0806010270831492, 'max_depth': 10, 'alpha': 0.7201000000000001, 'lambda': 25.94753111277768, 'max_bin': 421}. Best is trial 31 with value: 0.6909394963503777.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6909\n",
      "\tBest params:\n",
      "\t\tn_estimators: 606\n",
      "\t\teta: 0.08140645804057761\n",
      "\t\tmax_depth: 7\n",
      "\t\talpha: 0.6611\n",
      "\t\tlambda: 26.15370889233347\n",
      "\t\tmax_bin: 451\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.714082    0.725363\n",
      "1                    TP  328.000000  331.000000\n",
      "2                    TN  186.000000  187.000000\n",
      "3                    FP   41.000000   36.000000\n",
      "4                    FN   40.000000   41.000000\n",
      "5              Accuracy    0.863866    0.870588\n",
      "6             Precision    0.888889    0.901907\n",
      "7           Sensitivity    0.891304    0.889785\n",
      "8           Specificity    0.819400    0.838600\n",
      "9              F1 score    0.890095    0.895805\n",
      "10  F1 score (weighted)    0.863808    0.870868\n",
      "11     F1 score (macro)    0.855644    0.862537\n",
      "12    Balanced Accuracy    0.855344    0.864175\n",
      "13                  MCC    0.711292    0.725210\n",
      "14                  NPV    0.823000    0.820200\n",
      "15              ROC_AUC    0.855344    0.864175\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_1_cat = np.where((y_pred_xgb_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 22:38:39,012] Trial 100 finished with value: 0.6888707404668317 and parameters: {'n_estimators': 816, 'eta': 0.085950567436612, 'max_depth': 6, 'alpha': 0.8274, 'lambda': 30.503070949346167, 'max_bin': 440}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:38:53,551] Trial 101 finished with value: 0.6861526943421156 and parameters: {'n_estimators': 811, 'eta': 0.08511333452849563, 'max_depth': 6, 'alpha': 0.8715, 'lambda': 30.198512036335583, 'max_bin': 439}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:39:08,318] Trial 102 finished with value: 0.6898561316926541 and parameters: {'n_estimators': 863, 'eta': 0.08693425926449588, 'max_depth': 6, 'alpha': 0.8349000000000001, 'lambda': 30.70299287234463, 'max_bin': 430}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:39:19,140] Trial 103 finished with value: 0.6826712944165663 and parameters: {'n_estimators': 860, 'eta': 0.09314899855687364, 'max_depth': 5, 'alpha': 0.8176, 'lambda': 30.75066619466658, 'max_bin': 452}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:39:33,184] Trial 104 finished with value: 0.6848225036443555 and parameters: {'n_estimators': 896, 'eta': 0.08921101719751265, 'max_depth': 6, 'alpha': 0.8366, 'lambda': 31.233146400789774, 'max_bin': 444}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:39:47,483] Trial 105 finished with value: 0.6856040214216578 and parameters: {'n_estimators': 862, 'eta': 0.08579243513493616, 'max_depth': 6, 'alpha': 0.8984000000000001, 'lambda': 29.668779925275725, 'max_bin': 458}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:40:01,479] Trial 106 finished with value: 0.6869543063678321 and parameters: {'n_estimators': 776, 'eta': 0.08185413062400615, 'max_depth': 6, 'alpha': 0.7898000000000001, 'lambda': 32.422989525769374, 'max_bin': 429}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:40:12,538] Trial 107 finished with value: 0.6781811866204532 and parameters: {'n_estimators': 797, 'eta': 0.09794441275511379, 'max_depth': 5, 'alpha': 0.8303, 'lambda': 28.052375619090736, 'max_bin': 435}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:40:27,393] Trial 108 finished with value: 0.6886293570053217 and parameters: {'n_estimators': 818, 'eta': 0.0879118198478324, 'max_depth': 6, 'alpha': 0.9500000000000001, 'lambda': 29.256721082621368, 'max_bin': 474}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:40:39,778] Trial 109 finished with value: 0.6824615250447958 and parameters: {'n_estimators': 825, 'eta': 0.08719284217028866, 'max_depth': 6, 'alpha': 0.9706, 'lambda': 27.158698772546842, 'max_bin': 471}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:40:51,855] Trial 110 finished with value: 0.6826228166057681 and parameters: {'n_estimators': 874, 'eta': 0.09434665439558405, 'max_depth': 5, 'alpha': 0.9232, 'lambda': 28.808960834515595, 'max_bin': 464}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:41:05,738] Trial 111 finished with value: 0.684561754291926 and parameters: {'n_estimators': 850, 'eta': 0.07733718042752867, 'max_depth': 6, 'alpha': 0.0413, 'lambda': 30.673831907784447, 'max_bin': 485}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:41:20,323] Trial 112 finished with value: 0.6897552661616091 and parameters: {'n_estimators': 765, 'eta': 0.08857984772712459, 'max_depth': 6, 'alpha': 0.9652000000000001, 'lambda': 31.546799433320526, 'max_bin': 450}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:41:34,467] Trial 113 finished with value: 0.6861975952925075 and parameters: {'n_estimators': 765, 'eta': 0.0894843214014399, 'max_depth': 6, 'alpha': 0.9666, 'lambda': 29.269180981271415, 'max_bin': 448}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:41:49,038] Trial 114 finished with value: 0.6826814912053852 and parameters: {'n_estimators': 816, 'eta': 0.07955461689047369, 'max_depth': 6, 'alpha': 0.8724000000000001, 'lambda': 31.994718449667662, 'max_bin': 477}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:42:02,635] Trial 115 finished with value: 0.6878939832443616 and parameters: {'n_estimators': 786, 'eta': 0.09121195584564949, 'max_depth': 6, 'alpha': 0.9571000000000001, 'lambda': 29.81182614424382, 'max_bin': 452}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:42:14,072] Trial 116 finished with value: 0.6817375081741786 and parameters: {'n_estimators': 806, 'eta': 0.08693197131196964, 'max_depth': 5, 'alpha': 0.914, 'lambda': 27.442539232291075, 'max_bin': 441}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:42:29,488] Trial 117 finished with value: 0.6878061452542791 and parameters: {'n_estimators': 837, 'eta': 0.08287251011875618, 'max_depth': 6, 'alpha': 0.8989, 'lambda': 31.40009502529542, 'max_bin': 467}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:42:41,032] Trial 118 finished with value: 0.6835287330977955 and parameters: {'n_estimators': 738, 'eta': 0.09587131573918439, 'max_depth': 6, 'alpha': 0.9832000000000001, 'lambda': 33.77453974021177, 'max_bin': 490}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:42:52,961] Trial 119 finished with value: 0.6779713794716947 and parameters: {'n_estimators': 420, 'eta': 0.06077633421641027, 'max_depth': 7, 'alpha': 0.9351, 'lambda': 34.718682796097546, 'max_bin': 456}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:43:07,087] Trial 120 finished with value: 0.6823799866104147 and parameters: {'n_estimators': 900, 'eta': 0.08460905645325148, 'max_depth': 5, 'alpha': 0.8543000000000001, 'lambda': 32.673590538814516, 'max_bin': 480}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:43:19,182] Trial 121 finished with value: 0.6832587705975721 and parameters: {'n_estimators': 764, 'eta': 0.09325489662491131, 'max_depth': 7, 'alpha': 0.7036, 'lambda': 30.977330505568226, 'max_bin': 423}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:43:36,191] Trial 122 finished with value: 0.6833113887484388 and parameters: {'n_estimators': 717, 'eta': 0.08845907702204153, 'max_depth': 7, 'alpha': 0.7289, 'lambda': 30.06441925472311, 'max_bin': 437}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:43:50,465] Trial 123 finished with value: 0.6861390005554803 and parameters: {'n_estimators': 693, 'eta': 0.08065099950936339, 'max_depth': 6, 'alpha': 0.7993, 'lambda': 28.769795639945823, 'max_bin': 430}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:44:02,727] Trial 124 finished with value: 0.6866085759686475 and parameters: {'n_estimators': 789, 'eta': 0.09190557336155483, 'max_depth': 7, 'alpha': 0.7623000000000001, 'lambda': 26.401681831940127, 'max_bin': 445}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:44:16,326] Trial 125 finished with value: 0.6870029937907932 and parameters: {'n_estimators': 738, 'eta': 0.08362518328114132, 'max_depth': 7, 'alpha': 0.8824000000000001, 'lambda': 33.168798879081784, 'max_bin': 462}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:44:33,154] Trial 126 finished with value: 0.6857718846819209 and parameters: {'n_estimators': 824, 'eta': 0.07515203140831252, 'max_depth': 6, 'alpha': 0.6985, 'lambda': 27.870144782080864, 'max_bin': 419}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:44:50,596] Trial 127 finished with value: 0.6872187730498193 and parameters: {'n_estimators': 848, 'eta': 0.07885498407292216, 'max_depth': 9, 'alpha': 0.6401, 'lambda': 31.82540014921893, 'max_bin': 471}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:45:08,444] Trial 128 finished with value: 0.6889362468757829 and parameters: {'n_estimators': 775, 'eta': 0.08229844778932884, 'max_depth': 7, 'alpha': 0.6763, 'lambda': 37.53264105083713, 'max_bin': 255}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:45:21,660] Trial 129 finished with value: 0.6838409372424415 and parameters: {'n_estimators': 314, 'eta': 0.08605085324708128, 'max_depth': 11, 'alpha': 0.6528, 'lambda': 37.93934328803918, 'max_bin': 285}. Best is trial 31 with value: 0.6909394963503777.\n",
      "[I 2023-12-11 22:45:42,206] Trial 130 finished with value: 0.6916016296468966 and parameters: {'n_estimators': 806, 'eta': 0.08200877815151014, 'max_depth': 8, 'alpha': 0.9987, 'lambda': 38.87100685733104, 'max_bin': 260}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:45:58,081] Trial 131 finished with value: 0.686951873515443 and parameters: {'n_estimators': 772, 'eta': 0.0828308952391247, 'max_depth': 8, 'alpha': 0.6732, 'lambda': 37.115173106927514, 'max_bin': 316}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:46:15,048] Trial 132 finished with value: 0.6883437070235929 and parameters: {'n_estimators': 803, 'eta': 0.0817435918718191, 'max_depth': 8, 'alpha': 0.9837, 'lambda': 39.207666528200114, 'max_bin': 280}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:46:31,973] Trial 133 finished with value: 0.6852674786257993 and parameters: {'n_estimators': 786, 'eta': 0.07695255701476912, 'max_depth': 8, 'alpha': 0.996, 'lambda': 36.74474343997848, 'max_bin': 255}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:46:47,419] Trial 134 finished with value: 0.687270610409356 and parameters: {'n_estimators': 753, 'eta': 0.08744152128909208, 'max_depth': 8, 'alpha': 0.464, 'lambda': 37.938171176058496, 'max_bin': 259}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:47:02,858] Trial 135 finished with value: 0.6807780793647618 and parameters: {'n_estimators': 354, 'eta': 0.07304051003462209, 'max_depth': 12, 'alpha': 0.9552, 'lambda': 39.94440406039024, 'max_bin': 262}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:47:18,337] Trial 136 finished with value: 0.6870643693055298 and parameters: {'n_estimators': 813, 'eta': 0.07953088302436571, 'max_depth': 7, 'alpha': 0.6002000000000001, 'lambda': 36.115811043473215, 'max_bin': 250}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:47:34,266] Trial 137 finished with value: 0.6843184530172357 and parameters: {'n_estimators': 830, 'eta': 0.07104857797122138, 'max_depth': 6, 'alpha': 0.9375, 'lambda': 38.11955344197745, 'max_bin': 351}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:47:54,513] Trial 138 finished with value: 0.6883447339763269 and parameters: {'n_estimators': 858, 'eta': 0.0679487373951643, 'max_depth': 7, 'alpha': 0.6769000000000001, 'lambda': 37.42731040990329, 'max_bin': 450}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:48:13,243] Trial 139 finished with value: 0.6860086871469548 and parameters: {'n_estimators': 793, 'eta': 0.07603909061759467, 'max_depth': 9, 'alpha': 0.4107, 'lambda': 39.22254970250219, 'max_bin': 270}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:48:28,763] Trial 140 finished with value: 0.6886950878045395 and parameters: {'n_estimators': 761, 'eta': 0.08443774880452821, 'max_depth': 8, 'alpha': 0.9146000000000001, 'lambda': 35.19075448190054, 'max_bin': 459}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:48:45,697] Trial 141 finished with value: 0.688943923127121 and parameters: {'n_estimators': 755, 'eta': 0.08450156748142486, 'max_depth': 8, 'alpha': 0.9272, 'lambda': 35.875599539373525, 'max_bin': 455}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:49:03,476] Trial 142 finished with value: 0.6889393424973318 and parameters: {'n_estimators': 756, 'eta': 0.08493441246328956, 'max_depth': 8, 'alpha': 0.9096000000000001, 'lambda': 35.34553293352743, 'max_bin': 361}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:49:19,663] Trial 143 finished with value: 0.688526524968957 and parameters: {'n_estimators': 757, 'eta': 0.08473474312040279, 'max_depth': 8, 'alpha': 0.9311, 'lambda': 35.22457162324311, 'max_bin': 460}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:49:35,192] Trial 144 finished with value: 0.6884402172270337 and parameters: {'n_estimators': 776, 'eta': 0.08826727594500088, 'max_depth': 8, 'alpha': 0.906, 'lambda': 35.471756738077275, 'max_bin': 455}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:49:49,531] Trial 145 finished with value: 0.6868918258955633 and parameters: {'n_estimators': 736, 'eta': 0.08553186104533009, 'max_depth': 8, 'alpha': 0.8881, 'lambda': 36.713116111517394, 'max_bin': 447}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:50:06,181] Trial 146 finished with value: 0.6880526748274257 and parameters: {'n_estimators': 873, 'eta': 0.08189111063510492, 'max_depth': 8, 'alpha': 0.9506, 'lambda': 36.22621705959733, 'max_bin': 300}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:50:20,492] Trial 147 finished with value: 0.6857207061261914 and parameters: {'n_estimators': 806, 'eta': 0.08958995040479968, 'max_depth': 7, 'alpha': 0.8622000000000001, 'lambda': 34.359925988904834, 'max_bin': 335}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:50:36,149] Trial 148 finished with value: 0.6873264126012874 and parameters: {'n_estimators': 830, 'eta': 0.08403283713251267, 'max_depth': 8, 'alpha': 0.9673, 'lambda': 38.46926685484416, 'max_bin': 365}. Best is trial 130 with value: 0.6916016296468966.\n",
      "[I 2023-12-11 22:50:44,503] Trial 149 finished with value: 0.6789722545737273 and parameters: {'n_estimators': 250, 'eta': 0.08655668528508538, 'max_depth': 8, 'alpha': 0.9155000000000001, 'lambda': 35.85608482649317, 'max_bin': 442}. Best is trial 130 with value: 0.6916016296468966.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6916\n",
      "\tBest params:\n",
      "\t\tn_estimators: 806\n",
      "\t\teta: 0.08200877815151014\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.9987\n",
      "\t\tlambda: 38.87100685733104\n",
      "\t\tmax_bin: 260\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.714082    0.725363    0.713134\n",
      "1                    TP  328.000000  331.000000  311.000000\n",
      "2                    TN  186.000000  187.000000  194.000000\n",
      "3                    FP   41.000000   36.000000   59.000000\n",
      "4                    FN   40.000000   41.000000   31.000000\n",
      "5              Accuracy    0.863866    0.870588    0.848739\n",
      "6             Precision    0.888889    0.901907    0.840541\n",
      "7           Sensitivity    0.891304    0.889785    0.909357\n",
      "8           Specificity    0.819400    0.838600    0.766800\n",
      "9              F1 score    0.890095    0.895805    0.873596\n",
      "10  F1 score (weighted)    0.863808    0.870868    0.847283\n",
      "11     F1 score (macro)    0.855644    0.862537    0.842655\n",
      "12    Balanced Accuracy    0.855344    0.864175    0.838078\n",
      "13                  MCC    0.711292    0.725210    0.689331\n",
      "14                  NPV    0.823000    0.820200    0.862200\n",
      "15              ROC_AUC    0.855344    0.864175    0.838078\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_2_cat = np.where((y_pred_xgb_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 22:51:01,484] Trial 150 finished with value: 0.6958712768001999 and parameters: {'n_estimators': 748, 'eta': 0.07964945776267479, 'max_depth': 6, 'alpha': 0.8527, 'lambda': 34.03671376330611, 'max_bin': 338}. Best is trial 150 with value: 0.6958712768001999.\n",
      "[I 2023-12-11 22:51:16,737] Trial 151 finished with value: 0.6975781740455131 and parameters: {'n_estimators': 754, 'eta': 0.08086337099765775, 'max_depth': 6, 'alpha': 0.8510000000000001, 'lambda': 33.84509206547516, 'max_bin': 359}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:51:31,133] Trial 152 finished with value: 0.6961307544839339 and parameters: {'n_estimators': 753, 'eta': 0.08018402487453233, 'max_depth': 6, 'alpha': 0.8428, 'lambda': 34.056647119007344, 'max_bin': 361}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:51:45,483] Trial 153 finished with value: 0.6957535820796696 and parameters: {'n_estimators': 743, 'eta': 0.07971172562879122, 'max_depth': 6, 'alpha': 0.8278000000000001, 'lambda': 34.009210108199866, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:51:59,723] Trial 154 finished with value: 0.6954298762690089 and parameters: {'n_estimators': 725, 'eta': 0.08013951880199056, 'max_depth': 6, 'alpha': 0.8369000000000001, 'lambda': 33.8807300597758, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:52:13,271] Trial 155 finished with value: 0.6965101981607741 and parameters: {'n_estimators': 725, 'eta': 0.08017664855426242, 'max_depth': 6, 'alpha': 0.8402000000000001, 'lambda': 33.88398413417431, 'max_bin': 357}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:52:27,264] Trial 156 finished with value: 0.6964554234657256 and parameters: {'n_estimators': 685, 'eta': 0.07934157724367187, 'max_depth': 6, 'alpha': 0.8162, 'lambda': 33.88513885453963, 'max_bin': 360}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:52:40,862] Trial 157 finished with value: 0.695893220580562 and parameters: {'n_estimators': 681, 'eta': 0.0794219588100098, 'max_depth': 6, 'alpha': 0.8121, 'lambda': 33.90875389074276, 'max_bin': 375}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:52:54,508] Trial 158 finished with value: 0.6966390851265423 and parameters: {'n_estimators': 691, 'eta': 0.08017409515526619, 'max_depth': 6, 'alpha': 0.8063, 'lambda': 33.927598170916966, 'max_bin': 356}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:53:09,136] Trial 159 finished with value: 0.6942415957809043 and parameters: {'n_estimators': 679, 'eta': 0.07905421012083091, 'max_depth': 6, 'alpha': 0.8167000000000001, 'lambda': 34.23326355804892, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:53:22,701] Trial 160 finished with value: 0.6935860563873943 and parameters: {'n_estimators': 680, 'eta': 0.07829526568893495, 'max_depth': 6, 'alpha': 0.8144, 'lambda': 33.779560741209025, 'max_bin': 344}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:53:37,044] Trial 161 finished with value: 0.6947880337327565 and parameters: {'n_estimators': 694, 'eta': 0.0784923022505256, 'max_depth': 6, 'alpha': 0.8095, 'lambda': 33.67627673319472, 'max_bin': 374}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:53:51,391] Trial 162 finished with value: 0.6934789780985062 and parameters: {'n_estimators': 677, 'eta': 0.07842320352864382, 'max_depth': 6, 'alpha': 0.8097000000000001, 'lambda': 33.60390876485084, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:54:05,446] Trial 163 finished with value: 0.6944619299592295 and parameters: {'n_estimators': 682, 'eta': 0.07974511216015187, 'max_depth': 6, 'alpha': 0.804, 'lambda': 33.91909689709978, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:54:20,434] Trial 164 finished with value: 0.6936748214029402 and parameters: {'n_estimators': 687, 'eta': 0.07924904122247962, 'max_depth': 6, 'alpha': 0.8089000000000001, 'lambda': 33.81921140794992, 'max_bin': 374}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:54:35,181] Trial 165 finished with value: 0.6951835016749637 and parameters: {'n_estimators': 687, 'eta': 0.07898668694471059, 'max_depth': 6, 'alpha': 0.8054, 'lambda': 33.70794762027766, 'max_bin': 374}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:54:49,212] Trial 166 finished with value: 0.6936105141371484 and parameters: {'n_estimators': 676, 'eta': 0.07831528010629644, 'max_depth': 6, 'alpha': 0.8025, 'lambda': 33.793909237446876, 'max_bin': 375}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:55:03,176] Trial 167 finished with value: 0.6961868059045387 and parameters: {'n_estimators': 672, 'eta': 0.07912537098151293, 'max_depth': 6, 'alpha': 0.8116, 'lambda': 33.9099115704231, 'max_bin': 374}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:55:17,000] Trial 168 finished with value: 0.6934384076479703 and parameters: {'n_estimators': 659, 'eta': 0.07926139651572135, 'max_depth': 6, 'alpha': 0.7791, 'lambda': 34.07268271393604, 'max_bin': 378}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:55:31,682] Trial 169 finished with value: 0.6958934661609526 and parameters: {'n_estimators': 693, 'eta': 0.07645116838201783, 'max_depth': 6, 'alpha': 0.8003, 'lambda': 33.21694059113155, 'max_bin': 356}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:55:45,848] Trial 170 finished with value: 0.6936168286376472 and parameters: {'n_estimators': 696, 'eta': 0.07576528502813126, 'max_depth': 6, 'alpha': 0.7932, 'lambda': 33.094610276035986, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:55:59,082] Trial 171 finished with value: 0.6955349862362867 and parameters: {'n_estimators': 694, 'eta': 0.07632752535209392, 'max_depth': 6, 'alpha': 0.7923, 'lambda': 33.18468101283202, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:56:12,044] Trial 172 finished with value: 0.6927113627043127 and parameters: {'n_estimators': 694, 'eta': 0.07585238597762999, 'max_depth': 6, 'alpha': 0.7816000000000001, 'lambda': 32.77413071921756, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:56:25,844] Trial 173 finished with value: 0.6969558531246871 and parameters: {'n_estimators': 703, 'eta': 0.08018251374422389, 'max_depth': 6, 'alpha': 0.8430000000000001, 'lambda': 34.537465065293205, 'max_bin': 367}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:56:39,819] Trial 174 finished with value: 0.6962855095769279 and parameters: {'n_estimators': 639, 'eta': 0.07988867489845408, 'max_depth': 6, 'alpha': 0.8523000000000001, 'lambda': 34.6475802556413, 'max_bin': 366}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:56:52,560] Trial 175 finished with value: 0.6947727367488652 and parameters: {'n_estimators': 634, 'eta': 0.08029029706501996, 'max_depth': 6, 'alpha': 0.8467, 'lambda': 34.59482109376366, 'max_bin': 365}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:57:05,157] Trial 176 finished with value: 0.6954886006444017 and parameters: {'n_estimators': 644, 'eta': 0.08038057173999669, 'max_depth': 6, 'alpha': 0.8474, 'lambda': 34.70402332908687, 'max_bin': 365}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:57:17,997] Trial 177 finished with value: 0.6927826791448903 and parameters: {'n_estimators': 639, 'eta': 0.07713684578078554, 'max_depth': 6, 'alpha': 0.8490000000000001, 'lambda': 34.73251483583955, 'max_bin': 365}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:57:32,490] Trial 178 finished with value: 0.6940133703572399 and parameters: {'n_estimators': 710, 'eta': 0.08051774231610541, 'max_depth': 6, 'alpha': 0.8613000000000001, 'lambda': 34.9560605011706, 'max_bin': 345}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:57:46,889] Trial 179 finished with value: 0.6949943770602367 and parameters: {'n_estimators': 656, 'eta': 0.07419622806117847, 'max_depth': 6, 'alpha': 0.8287, 'lambda': 32.979513195142374, 'max_bin': 353}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:58:00,199] Trial 180 finished with value: 0.6924750466044233 and parameters: {'n_estimators': 656, 'eta': 0.07366777777285508, 'max_depth': 6, 'alpha': 0.8289000000000001, 'lambda': 32.84358099071012, 'max_bin': 354}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:58:14,285] Trial 181 finished with value: 0.6945495521767155 and parameters: {'n_estimators': 635, 'eta': 0.07646830102596554, 'max_depth': 6, 'alpha': 0.8414, 'lambda': 34.61007709675701, 'max_bin': 367}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:58:28,142] Trial 182 finished with value: 0.6957770271894274 and parameters: {'n_estimators': 719, 'eta': 0.08037174214279977, 'max_depth': 6, 'alpha': 0.8758, 'lambda': 33.04522975632684, 'max_bin': 384}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:58:44,166] Trial 183 finished with value: 0.6941869377741833 and parameters: {'n_estimators': 725, 'eta': 0.07488357683744352, 'max_depth': 6, 'alpha': 0.8665, 'lambda': 32.343828886711805, 'max_bin': 383}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:58:57,607] Trial 184 finished with value: 0.693843038546215 and parameters: {'n_estimators': 709, 'eta': 0.08103957547085307, 'max_depth': 6, 'alpha': 0.8293, 'lambda': 32.97618937893701, 'max_bin': 339}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:59:10,410] Trial 185 finished with value: 0.6915652215123984 and parameters: {'n_estimators': 660, 'eta': 0.07771022598710997, 'max_depth': 6, 'alpha': 0.883, 'lambda': 33.161912225896415, 'max_bin': 351}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:59:24,302] Trial 186 finished with value: 0.6953603094052523 and parameters: {'n_estimators': 701, 'eta': 0.07697676063656035, 'max_depth': 6, 'alpha': 0.7802, 'lambda': 32.222284332647284, 'max_bin': 359}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:59:37,514] Trial 187 finished with value: 0.6924469535343771 and parameters: {'n_estimators': 720, 'eta': 0.0747982045932498, 'max_depth': 6, 'alpha': 0.7613000000000001, 'lambda': 32.42912388093084, 'max_bin': 361}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 22:59:52,575] Trial 188 finished with value: 0.6936382262483827 and parameters: {'n_estimators': 706, 'eta': 0.07196297878021563, 'max_depth': 6, 'alpha': 0.7862, 'lambda': 34.481758282380305, 'max_bin': 355}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:00:05,853] Trial 189 finished with value: 0.6921668642317462 and parameters: {'n_estimators': 730, 'eta': 0.08091024635194163, 'max_depth': 5, 'alpha': 0.8614, 'lambda': 32.139221451775676, 'max_bin': 361}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:00:18,768] Trial 190 finished with value: 0.6939880129775255 and parameters: {'n_estimators': 669, 'eta': 0.07653579944747281, 'max_depth': 6, 'alpha': 0.8225, 'lambda': 33.40836239730639, 'max_bin': 348}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:00:31,692] Trial 191 finished with value: 0.6917840861277428 and parameters: {'n_estimators': 692, 'eta': 0.07802884977057092, 'max_depth': 6, 'alpha': 0.8417, 'lambda': 33.61107006371918, 'max_bin': 380}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:00:45,149] Trial 192 finished with value: 0.6948747543579914 and parameters: {'n_estimators': 703, 'eta': 0.08020882099486151, 'max_depth': 6, 'alpha': 0.8280000000000001, 'lambda': 35.12021644571726, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:00:58,759] Trial 193 finished with value: 0.6942635641834491 and parameters: {'n_estimators': 649, 'eta': 0.08209861034205179, 'max_depth': 6, 'alpha': 0.8752000000000001, 'lambda': 35.18141303193091, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:01:11,742] Trial 194 finished with value: 0.6940010990411586 and parameters: {'n_estimators': 711, 'eta': 0.08073760179280026, 'max_depth': 6, 'alpha': 0.7764000000000001, 'lambda': 34.29091205582203, 'max_bin': 356}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:01:25,021] Trial 195 finished with value: 0.694816461767459 and parameters: {'n_estimators': 725, 'eta': 0.07660290656586134, 'max_depth': 6, 'alpha': 0.8333, 'lambda': 35.15059690092676, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:01:39,399] Trial 196 finished with value: 0.6953654794575636 and parameters: {'n_estimators': 661, 'eta': 0.07375639332512536, 'max_depth': 6, 'alpha': 0.8581000000000001, 'lambda': 32.91571357541161, 'max_bin': 387}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:01:53,044] Trial 197 finished with value: 0.6898016564200105 and parameters: {'n_estimators': 664, 'eta': 0.07030107915624415, 'max_depth': 6, 'alpha': 0.8526, 'lambda': 32.002498253361324, 'max_bin': 388}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:02:06,161] Trial 198 finished with value: 0.693784966548147 and parameters: {'n_estimators': 611, 'eta': 0.07287124212193882, 'max_depth': 6, 'alpha': 0.8769, 'lambda': 32.93265008922304, 'max_bin': 352}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:02:20,548] Trial 199 finished with value: 0.6948669434198871 and parameters: {'n_estimators': 740, 'eta': 0.07347080634021862, 'max_depth': 6, 'alpha': 0.7949, 'lambda': 32.59863322488533, 'max_bin': 340}. Best is trial 151 with value: 0.6975781740455131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tn_estimators: 754\n",
      "\t\teta: 0.08086337099765775\n",
      "\t\tmax_depth: 6\n",
      "\t\talpha: 0.8510000000000001\n",
      "\t\tlambda: 33.84509206547516\n",
      "\t\tmax_bin: 359\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.714082    0.725363    0.713134    0.688499\n",
      "1                    TP  328.000000  331.000000  311.000000  319.000000\n",
      "2                    TN  186.000000  187.000000  194.000000  183.000000\n",
      "3                    FP   41.000000   36.000000   59.000000   59.000000\n",
      "4                    FN   40.000000   41.000000   31.000000   34.000000\n",
      "5              Accuracy    0.863866    0.870588    0.848739    0.843697\n",
      "6             Precision    0.888889    0.901907    0.840541    0.843915\n",
      "7           Sensitivity    0.891304    0.889785    0.909357    0.903683\n",
      "8           Specificity    0.819400    0.838600    0.766800    0.756200\n",
      "9              F1 score    0.890095    0.895805    0.873596    0.872777\n",
      "10  F1 score (weighted)    0.863808    0.870868    0.847283    0.842114\n",
      "11     F1 score (macro)    0.855644    0.862537    0.842655    0.835081\n",
      "12    Balanced Accuracy    0.855344    0.864175    0.838078    0.829941\n",
      "13                  MCC    0.711292    0.725210    0.689331    0.673418\n",
      "14                  NPV    0.823000    0.820200    0.862200    0.843300\n",
      "15              ROC_AUC    0.855344    0.864175    0.838078    0.829941\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_3_cat = np.where((y_pred_xgb_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:02:34,355] Trial 200 finished with value: 0.6884058336123223 and parameters: {'n_estimators': 671, 'eta': 0.07522010339310731, 'max_depth': 6, 'alpha': 0.8551000000000001, 'lambda': 34.25990486841354, 'max_bin': 384}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:02:48,081] Trial 201 finished with value: 0.6865147875679934 and parameters: {'n_estimators': 701, 'eta': 0.07965316903696806, 'max_depth': 6, 'alpha': 0.8276, 'lambda': 35.8294123005056, 'max_bin': 369}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:03:01,308] Trial 202 finished with value: 0.689227593080469 and parameters: {'n_estimators': 720, 'eta': 0.0825649467700035, 'max_depth': 6, 'alpha': 0.8227, 'lambda': 33.310304136305895, 'max_bin': 359}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:03:14,065] Trial 203 finished with value: 0.6895690495298504 and parameters: {'n_estimators': 649, 'eta': 0.07746046399301372, 'max_depth': 6, 'alpha': 0.8408, 'lambda': 34.57370283968229, 'max_bin': 347}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:03:28,988] Trial 204 finished with value: 0.6890510321085352 and parameters: {'n_estimators': 698, 'eta': 0.08067018406438298, 'max_depth': 6, 'alpha': 0.8909, 'lambda': 35.57667243787945, 'max_bin': 378}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:03:42,092] Trial 205 finished with value: 0.6883796068956354 and parameters: {'n_estimators': 734, 'eta': 0.08277833072937467, 'max_depth': 6, 'alpha': 0.7547, 'lambda': 31.754741945637498, 'max_bin': 367}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:03:56,586] Trial 206 finished with value: 0.6890227188260296 and parameters: {'n_estimators': 687, 'eta': 0.0775804448335358, 'max_depth': 6, 'alpha': 0.7927000000000001, 'lambda': 33.335482321755116, 'max_bin': 325}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:04:10,451] Trial 207 finished with value: 0.689893610173509 and parameters: {'n_estimators': 711, 'eta': 0.0800610018959325, 'max_depth': 6, 'alpha': 0.8644000000000001, 'lambda': 34.86100425606792, 'max_bin': 354}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:04:23,763] Trial 208 finished with value: 0.6911695005167007 and parameters: {'n_estimators': 666, 'eta': 0.07538229378826586, 'max_depth': 6, 'alpha': 0.8145, 'lambda': 34.196865561631874, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:04:37,422] Trial 209 finished with value: 0.6931051177967997 and parameters: {'n_estimators': 744, 'eta': 0.07851986461942904, 'max_depth': 6, 'alpha': 0.7724000000000001, 'lambda': 32.508613592876756, 'max_bin': 393}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:04:50,088] Trial 210 finished with value: 0.6887552445570898 and parameters: {'n_estimators': 646, 'eta': 0.08276025461938574, 'max_depth': 6, 'alpha': 0.8463, 'lambda': 36.32749208746279, 'max_bin': 369}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:05:04,080] Trial 211 finished with value: 0.6904499925669703 and parameters: {'n_estimators': 740, 'eta': 0.07373580881684001, 'max_depth': 6, 'alpha': 0.7979, 'lambda': 32.648046231703255, 'max_bin': 341}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:05:17,458] Trial 212 finished with value: 0.6875137794263546 and parameters: {'n_estimators': 721, 'eta': 0.07211200077496203, 'max_depth': 6, 'alpha': 0.2222, 'lambda': 33.63364518159156, 'max_bin': 333}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:05:30,151] Trial 213 finished with value: 0.6870844958371005 and parameters: {'n_estimators': 701, 'eta': 0.0743783501790322, 'max_depth': 6, 'alpha': 0.7994, 'lambda': 32.0418870587307, 'max_bin': 357}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:05:44,096] Trial 214 finished with value: 0.6895668080911541 and parameters: {'n_estimators': 680, 'eta': 0.07652693518583549, 'max_depth': 6, 'alpha': 0.8252, 'lambda': 34.15247668794469, 'max_bin': 351}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:05:56,749] Trial 215 finished with value: 0.6873735455349876 and parameters: {'n_estimators': 619, 'eta': 0.08047941747532124, 'max_depth': 6, 'alpha': 0.8416, 'lambda': 33.31576313200997, 'max_bin': 376}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:06:11,586] Trial 216 finished with value: 0.6911186566360865 and parameters: {'n_estimators': 740, 'eta': 0.0694334866162914, 'max_depth': 6, 'alpha': 0.7791, 'lambda': 35.24341091825499, 'max_bin': 359}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:06:25,402] Trial 217 finished with value: 0.689878697035316 and parameters: {'n_estimators': 691, 'eta': 0.07869238356564122, 'max_depth': 6, 'alpha': 0.8784000000000001, 'lambda': 32.67962352715025, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:06:40,608] Trial 218 finished with value: 0.689938277931546 and parameters: {'n_estimators': 712, 'eta': 0.08124805912435469, 'max_depth': 6, 'alpha': 0.8056000000000001, 'lambda': 34.626293756124696, 'max_bin': 370}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:06:52,986] Trial 219 finished with value: 0.6855376002330561 and parameters: {'n_estimators': 669, 'eta': 0.07715209689100698, 'max_depth': 5, 'alpha': 0.8233, 'lambda': 31.416465197307662, 'max_bin': 381}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:07:05,825] Trial 220 finished with value: 0.689914577781306 and parameters: {'n_estimators': 728, 'eta': 0.07477221486881688, 'max_depth': 6, 'alpha': 0.8550000000000001, 'lambda': 33.87171234778457, 'max_bin': 338}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:07:20,213] Trial 221 finished with value: 0.6893497699680735 and parameters: {'n_estimators': 727, 'eta': 0.07673556168112301, 'max_depth': 6, 'alpha': 0.8348, 'lambda': 35.18589246236647, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:07:32,618] Trial 222 finished with value: 0.6847910432081488 and parameters: {'n_estimators': 700, 'eta': 0.07956239196597865, 'max_depth': 6, 'alpha': 0.8225, 'lambda': 35.727468348928035, 'max_bin': 347}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:07:46,982] Trial 223 finished with value: 0.6898560162129062 and parameters: {'n_estimators': 744, 'eta': 0.07653044819226719, 'max_depth': 6, 'alpha': 0.8655, 'lambda': 33.12935431092012, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:08:01,756] Trial 224 finished with value: 0.6880990058138463 and parameters: {'n_estimators': 719, 'eta': 0.07170183743784939, 'max_depth': 6, 'alpha': 0.7938000000000001, 'lambda': 34.79686766578719, 'max_bin': 355}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:08:13,891] Trial 225 finished with value: 0.6890233942686106 and parameters: {'n_estimators': 686, 'eta': 0.08312013693624412, 'max_depth': 6, 'alpha': 0.8344, 'lambda': 34.13787148212911, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:08:27,390] Trial 226 finished with value: 0.6901664064222057 and parameters: {'n_estimators': 661, 'eta': 0.07872946601655041, 'max_depth': 6, 'alpha': 0.8909, 'lambda': 32.406020521052696, 'max_bin': 331}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:08:43,483] Trial 227 finished with value: 0.6818769799873154 and parameters: {'n_estimators': 707, 'eta': 0.03911055075818612, 'max_depth': 6, 'alpha': 0.8055, 'lambda': 33.58215449646303, 'max_bin': 366}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:08:56,162] Trial 228 finished with value: 0.6889187725436254 and parameters: {'n_estimators': 747, 'eta': 0.08147874523783942, 'max_depth': 6, 'alpha': 0.8486, 'lambda': 35.20862526374842, 'max_bin': 375}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:09:08,903] Trial 229 finished with value: 0.6882315796144205 and parameters: {'n_estimators': 631, 'eta': 0.07360591390217897, 'max_depth': 6, 'alpha': 0.7426, 'lambda': 33.07970257417898, 'max_bin': 350}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:09:22,280] Trial 230 finished with value: 0.6878012076427832 and parameters: {'n_estimators': 678, 'eta': 0.07905193792332647, 'max_depth': 6, 'alpha': 0.7671, 'lambda': 34.38782239501366, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:09:35,763] Trial 231 finished with value: 0.6890483878135221 and parameters: {'n_estimators': 694, 'eta': 0.07768658498424673, 'max_depth': 6, 'alpha': 0.8165, 'lambda': 33.86442282881751, 'max_bin': 376}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:09:48,827] Trial 232 finished with value: 0.6891315191105212 and parameters: {'n_estimators': 726, 'eta': 0.08035598454894262, 'max_depth': 6, 'alpha': 0.7915000000000001, 'lambda': 36.40031654889928, 'max_bin': 386}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:10:04,275] Trial 233 finished with value: 0.6894444517903676 and parameters: {'n_estimators': 695, 'eta': 0.07546641147303688, 'max_depth': 6, 'alpha': 0.8114, 'lambda': 33.5453621337218, 'max_bin': 370}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:10:17,040] Trial 234 finished with value: 0.686299452879044 and parameters: {'n_estimators': 652, 'eta': 0.07870240405477649, 'max_depth': 6, 'alpha': 0.8344, 'lambda': 34.93235089695652, 'max_bin': 361}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:10:33,609] Trial 235 finished with value: 0.6863446427647616 and parameters: {'n_estimators': 714, 'eta': 0.049466699640131825, 'max_depth': 6, 'alpha': 0.8564, 'lambda': 32.015906567753504, 'max_bin': 343}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:10:47,384] Trial 236 finished with value: 0.6886304360085018 and parameters: {'n_estimators': 679, 'eta': 0.08246125241994276, 'max_depth': 6, 'alpha': 0.8722000000000001, 'lambda': 33.220329527354096, 'max_bin': 367}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:11:01,743] Trial 237 finished with value: 0.6876485907494552 and parameters: {'n_estimators': 733, 'eta': 0.07666264089058858, 'max_depth': 6, 'alpha': 0.811, 'lambda': 34.16995028416585, 'max_bin': 380}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:11:15,188] Trial 238 finished with value: 0.6913306421083192 and parameters: {'n_estimators': 701, 'eta': 0.08062186918249001, 'max_depth': 6, 'alpha': 0.8364, 'lambda': 35.474946446037, 'max_bin': 355}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:11:28,482] Trial 239 finished with value: 0.6891667844434566 and parameters: {'n_estimators': 665, 'eta': 0.07844585490967886, 'max_depth': 6, 'alpha': 0.7974, 'lambda': 32.62158544862576, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:11:42,613] Trial 240 finished with value: 0.6888747915013991 and parameters: {'n_estimators': 752, 'eta': 0.08316117284630158, 'max_depth': 6, 'alpha': 0.7756000000000001, 'lambda': 31.46506537740836, 'max_bin': 371}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:11:56,754] Trial 241 finished with value: 0.6884880497581428 and parameters: {'n_estimators': 688, 'eta': 0.07996950768811782, 'max_depth': 6, 'alpha': 0.8566, 'lambda': 34.67615311410855, 'max_bin': 366}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:12:10,997] Trial 242 finished with value: 0.6882431895923906 and parameters: {'n_estimators': 630, 'eta': 0.08087774391502289, 'max_depth': 6, 'alpha': 0.8411000000000001, 'lambda': 33.83519200468305, 'max_bin': 359}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:12:25,435] Trial 243 finished with value: 0.6886406973576723 and parameters: {'n_estimators': 639, 'eta': 0.06546412682218428, 'max_depth': 6, 'alpha': 0.8228000000000001, 'lambda': 34.521915113493456, 'max_bin': 375}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:12:38,050] Trial 244 finished with value: 0.6899743306770433 and parameters: {'n_estimators': 658, 'eta': 0.0778861853428748, 'max_depth': 6, 'alpha': 0.8433, 'lambda': 35.914995590680455, 'max_bin': 364}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:12:52,722] Trial 245 finished with value: 0.6920726569246443 and parameters: {'n_estimators': 714, 'eta': 0.07611713829168984, 'max_depth': 6, 'alpha': 0.8773000000000001, 'lambda': 33.4356740216934, 'max_bin': 350}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:13:06,864] Trial 246 finished with value: 0.6863108953543409 and parameters: {'n_estimators': 676, 'eta': 0.05631355492245746, 'max_depth': 6, 'alpha': 0.8146, 'lambda': 34.76780468312158, 'max_bin': 356}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:13:18,946] Trial 247 finished with value: 0.6898732732063769 and parameters: {'n_estimators': 645, 'eta': 0.08022915183186585, 'max_depth': 6, 'alpha': 0.8567, 'lambda': 32.668796568601685, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:13:26,842] Trial 248 finished with value: -7.170832693882164 and parameters: {'n_estimators': 730, 'eta': 0.0010249663087277122, 'max_depth': 6, 'alpha': 0.8375, 'lambda': 33.96815639594241, 'max_bin': 361}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:13:39,755] Trial 249 finished with value: 0.6891856904914647 and parameters: {'n_estimators': 704, 'eta': 0.07492857389099197, 'max_depth': 6, 'alpha': 0.7859, 'lambda': 35.22623786862862, 'max_bin': 383}. Best is trial 151 with value: 0.6975781740455131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tn_estimators: 754\n",
      "\t\teta: 0.08086337099765775\n",
      "\t\tmax_depth: 6\n",
      "\t\talpha: 0.8510000000000001\n",
      "\t\tlambda: 33.84509206547516\n",
      "\t\tmax_bin: 359\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.714082    0.725363    0.713134    0.688499   \n",
      "1                    TP  328.000000  331.000000  311.000000  319.000000   \n",
      "2                    TN  186.000000  187.000000  194.000000  183.000000   \n",
      "3                    FP   41.000000   36.000000   59.000000   59.000000   \n",
      "4                    FN   40.000000   41.000000   31.000000   34.000000   \n",
      "5              Accuracy    0.863866    0.870588    0.848739    0.843697   \n",
      "6             Precision    0.888889    0.901907    0.840541    0.843915   \n",
      "7           Sensitivity    0.891304    0.889785    0.909357    0.903683   \n",
      "8           Specificity    0.819400    0.838600    0.766800    0.756200   \n",
      "9              F1 score    0.890095    0.895805    0.873596    0.872777   \n",
      "10  F1 score (weighted)    0.863808    0.870868    0.847283    0.842114   \n",
      "11     F1 score (macro)    0.855644    0.862537    0.842655    0.835081   \n",
      "12    Balanced Accuracy    0.855344    0.864175    0.838078    0.829941   \n",
      "13                  MCC    0.711292    0.725210    0.689331    0.673418   \n",
      "14                  NPV    0.823000    0.820200    0.862200    0.843300   \n",
      "15              ROC_AUC    0.855344    0.864175    0.838078    0.829941   \n",
      "\n",
      "          Set4  \n",
      "0     0.704908  \n",
      "1   339.000000  \n",
      "2   167.000000  \n",
      "3    57.000000  \n",
      "4    32.000000  \n",
      "5     0.850420  \n",
      "6     0.856061  \n",
      "7     0.913747  \n",
      "8     0.745500  \n",
      "9     0.883963  \n",
      "10    0.848438  \n",
      "11    0.836781  \n",
      "12    0.829641  \n",
      "13    0.677031  \n",
      "14    0.839200  \n",
      "15    0.829641  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_4_cat = np.where((y_pred_xgb_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c6c1fb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_xgb_4_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:13:54,572] Trial 250 finished with value: 0.6915293358968834 and parameters: {'n_estimators': 587, 'eta': 0.08288802806543835, 'max_depth': 6, 'alpha': 0.8228000000000001, 'lambda': 22.068346832297717, 'max_bin': 373}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:14:08,245] Trial 251 finished with value: 0.6884202830665173 and parameters: {'n_estimators': 613, 'eta': 0.0784724334238476, 'max_depth': 6, 'alpha': 0.8021, 'lambda': 33.05070172326349, 'max_bin': 354}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:14:23,545] Trial 252 finished with value: 0.6925521117196729 and parameters: {'n_estimators': 745, 'eta': 0.07297168282829995, 'max_depth': 6, 'alpha': 0.8947, 'lambda': 34.064312767755844, 'max_bin': 378}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:14:38,152] Trial 253 finished with value: 0.6905585611640143 and parameters: {'n_estimators': 688, 'eta': 0.06790222659890977, 'max_depth': 6, 'alpha': 0.8628, 'lambda': 32.29701663615538, 'max_bin': 365}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:14:55,884] Trial 254 finished with value: 0.6778804176767647 and parameters: {'n_estimators': 720, 'eta': 0.02888307614389475, 'max_depth': 6, 'alpha': 0.8441000000000001, 'lambda': 36.28258670616329, 'max_bin': 360}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:15:12,754] Trial 255 finished with value: 0.6931066052388253 and parameters: {'n_estimators': 671, 'eta': 0.061026462220653915, 'max_depth': 6, 'alpha': 0.8208000000000001, 'lambda': 34.74201072357765, 'max_bin': 392}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:15:25,637] Trial 256 finished with value: 0.6933184849848043 and parameters: {'n_estimators': 764, 'eta': 0.0813622813489289, 'max_depth': 6, 'alpha': 0.7828, 'lambda': 16.779156409410838, 'max_bin': 346}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:15:40,686] Trial 257 finished with value: 0.6902019334303962 and parameters: {'n_estimators': 703, 'eta': 0.084034729794288, 'max_depth': 6, 'alpha': 0.878, 'lambda': 33.364638079939276, 'max_bin': 371}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:15:53,355] Trial 258 finished with value: 0.6882399773169396 and parameters: {'n_estimators': 654, 'eta': 0.07690101769955875, 'max_depth': 6, 'alpha': 0.8332, 'lambda': 31.877094512066183, 'max_bin': 351}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:16:07,158] Trial 259 finished with value: 0.6935059259475449 and parameters: {'n_estimators': 730, 'eta': 0.07964707396573124, 'max_depth': 6, 'alpha': 0.8112, 'lambda': 11.935565528327526, 'max_bin': 386}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:16:22,889] Trial 260 finished with value: 0.6936667265354788 and parameters: {'n_estimators': 688, 'eta': 0.07148656792788091, 'max_depth': 6, 'alpha': 0.8579, 'lambda': 35.50166668316801, 'max_bin': 366}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:16:38,541] Trial 261 finished with value: 0.694489317530009 and parameters: {'n_estimators': 713, 'eta': 0.07468654029873777, 'max_depth': 6, 'alpha': 0.7508, 'lambda': 34.143690232279276, 'max_bin': 375}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:16:50,821] Trial 262 finished with value: 0.6865977082812426 and parameters: {'n_estimators': 630, 'eta': 0.07774551387520955, 'max_depth': 5, 'alpha': 0.7936000000000001, 'lambda': 32.92645192474154, 'max_bin': 326}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:17:04,534] Trial 263 finished with value: 0.6919468314268756 and parameters: {'n_estimators': 745, 'eta': 0.08110028869378828, 'max_depth': 6, 'alpha': 0.1413, 'lambda': 31.0613168073462, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:17:19,384] Trial 264 finished with value: 0.6927019203502711 and parameters: {'n_estimators': 671, 'eta': 0.0703021581690366, 'max_depth': 6, 'alpha': 0.7683, 'lambda': 33.55099354012937, 'max_bin': 370}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:17:33,074] Trial 265 finished with value: 0.6903906765966128 and parameters: {'n_estimators': 702, 'eta': 0.07604660955165932, 'max_depth': 6, 'alpha': 0.8471000000000001, 'lambda': 36.722292287945415, 'max_bin': 365}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:17:51,007] Trial 266 finished with value: 0.6573085590854766 and parameters: {'n_estimators': 723, 'eta': 0.01629876753365976, 'max_depth': 6, 'alpha': 0.8976000000000001, 'lambda': 34.888555970656626, 'max_bin': 355}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:18:04,974] Trial 267 finished with value: 0.2900019743204563 and parameters: {'n_estimators': 685, 'eta': 0.003882378698500555, 'max_depth': 6, 'alpha': 0.8249000000000001, 'lambda': 19.75078427424445, 'max_bin': 342}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:18:20,218] Trial 268 finished with value: 0.6965792960611961 and parameters: {'n_estimators': 651, 'eta': 0.08391926130930293, 'max_depth': 6, 'alpha': 0.8019000000000001, 'lambda': 34.21711521718137, 'max_bin': 378}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:18:33,058] Trial 269 finished with value: 0.6907038558446826 and parameters: {'n_estimators': 662, 'eta': 0.08412659204267461, 'max_depth': 6, 'alpha': 0.7967000000000001, 'lambda': 32.53964043918797, 'max_bin': 380}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:18:46,559] Trial 270 finished with value: 0.6918267203270166 and parameters: {'n_estimators': 763, 'eta': 0.08286035930067981, 'max_depth': 6, 'alpha': 0.812, 'lambda': 35.840033464527366, 'max_bin': 390}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:19:01,361] Trial 271 finished with value: 0.6901417881549492 and parameters: {'n_estimators': 739, 'eta': 0.07880312761256505, 'max_depth': 6, 'alpha': 0.7676000000000001, 'lambda': 24.188214582690776, 'max_bin': 376}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:19:15,242] Trial 272 finished with value: 0.6905024703076265 and parameters: {'n_estimators': 697, 'eta': 0.07331659075233485, 'max_depth': 6, 'alpha': 0.805, 'lambda': 20.8879622208307, 'max_bin': 383}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:19:30,464] Trial 273 finished with value: 0.6947282829255512 and parameters: {'n_estimators': 671, 'eta': 0.08526400482109875, 'max_depth': 6, 'alpha': 0.8667, 'lambda': 33.7274622527335, 'max_bin': 360}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:19:45,491] Trial 274 finished with value: 0.6947511077839822 and parameters: {'n_estimators': 715, 'eta': 0.08182267303386573, 'max_depth': 6, 'alpha': 0.8289000000000001, 'lambda': 31.83882747680577, 'max_bin': 349}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:19:59,220] Trial 275 finished with value: 0.694895776135019 and parameters: {'n_estimators': 654, 'eta': 0.07744830986066227, 'max_depth': 6, 'alpha': 0.7833, 'lambda': 32.96356909872255, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:20:13,885] Trial 276 finished with value: 0.6926008859543273 and parameters: {'n_estimators': 646, 'eta': 0.075985592820675, 'max_depth': 6, 'alpha': 0.7776000000000001, 'lambda': 32.85633704823391, 'max_bin': 371}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:20:25,243] Trial 277 finished with value: 0.6888518395492678 and parameters: {'n_estimators': 654, 'eta': 0.09997136244894123, 'max_depth': 5, 'alpha': 0.7371000000000001, 'lambda': 34.55558874244897, 'max_bin': 354}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:20:40,210] Trial 278 finished with value: 0.693512245607442 and parameters: {'n_estimators': 752, 'eta': 0.07968640211577845, 'max_depth': 6, 'alpha': 0.7899, 'lambda': 31.49616026274846, 'max_bin': 362}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:20:54,752] Trial 279 finished with value: 0.6926532072293972 and parameters: {'n_estimators': 680, 'eta': 0.07729970103430163, 'max_depth': 6, 'alpha': 0.8768, 'lambda': 32.43914792248859, 'max_bin': 378}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:21:09,559] Trial 280 finished with value: 0.6939630078917969 and parameters: {'n_estimators': 640, 'eta': 0.07451732541492839, 'max_depth': 6, 'alpha': 0.8396, 'lambda': 35.30625661746584, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:21:22,962] Trial 281 finished with value: 0.6853366768051142 and parameters: {'n_estimators': 602, 'eta': 0.05177045804610397, 'max_depth': 6, 'alpha': 0.8145, 'lambda': 37.043036134976894, 'max_bin': 360}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:21:36,374] Trial 282 finished with value: 0.6926777715779883 and parameters: {'n_estimators': 664, 'eta': 0.08395363513162644, 'max_depth': 6, 'alpha': 0.7831, 'lambda': 33.968767222308756, 'max_bin': 336}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:21:47,619] Trial 283 finished with value: 0.6872738989769268 and parameters: {'n_estimators': 727, 'eta': 0.08166660668910464, 'max_depth': 6, 'alpha': 0.7547, 'lambda': 5.635960046947323, 'max_bin': 329}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:22:02,645] Trial 284 finished with value: 0.6911243358906662 and parameters: {'n_estimators': 706, 'eta': 0.07948314205068145, 'max_depth': 6, 'alpha': 0.8542000000000001, 'lambda': 32.940117564454155, 'max_bin': 353}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:22:16,022] Trial 285 finished with value: 0.6898473517427106 and parameters: {'n_estimators': 767, 'eta': 0.07727576114116415, 'max_depth': 6, 'alpha': 0.8262, 'lambda': 34.3618122149725, 'max_bin': 344}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:22:30,753] Trial 286 finished with value: 0.6937600692067512 and parameters: {'n_estimators': 734, 'eta': 0.0720007420523696, 'max_depth': 6, 'alpha': 0.8023, 'lambda': 33.33571615894309, 'max_bin': 367}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:22:45,978] Trial 287 finished with value: 0.693090921414359 and parameters: {'n_estimators': 680, 'eta': 0.06888730535160442, 'max_depth': 6, 'alpha': 0.8796, 'lambda': 30.91271461831641, 'max_bin': 395}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:23:00,896] Trial 288 finished with value: 0.6882873263048009 and parameters: {'n_estimators': 621, 'eta': 0.07443621668917207, 'max_depth': 6, 'alpha': 0.8365, 'lambda': 35.693530830028344, 'max_bin': 386}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:23:14,949] Trial 289 finished with value: 0.6930973940530089 and parameters: {'n_estimators': 689, 'eta': 0.08160389403009953, 'max_depth': 6, 'alpha': 0.8608, 'lambda': 34.98263159889451, 'max_bin': 374}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:23:19,483] Trial 290 finished with value: 0.6540524361592126 and parameters: {'n_estimators': 154, 'eta': 0.07841025103019737, 'max_depth': 6, 'alpha': 0.792, 'lambda': 32.13927074258752, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:23:33,545] Trial 291 finished with value: 0.692559630967434 and parameters: {'n_estimators': 650, 'eta': 0.08032058172324173, 'max_depth': 6, 'alpha': 0.8327, 'lambda': 33.61669336832874, 'max_bin': 364}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:23:49,728] Trial 292 finished with value: 0.6943213011137505 and parameters: {'n_estimators': 712, 'eta': 0.0636522600645218, 'max_depth': 6, 'alpha': 0.7721, 'lambda': 34.38678746783811, 'max_bin': 380}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:24:04,719] Trial 293 finished with value: 0.6921473184153952 and parameters: {'n_estimators': 753, 'eta': 0.08512363590622095, 'max_depth': 6, 'alpha': 0.8101, 'lambda': 36.50142177501554, 'max_bin': 349}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:24:18,065] Trial 294 finished with value: 0.6873362035750274 and parameters: {'n_estimators': 670, 'eta': 0.07616065033248483, 'max_depth': 5, 'alpha': 0.8948, 'lambda': 33.33536566182137, 'max_bin': 371}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:24:36,315] Trial 295 finished with value: 0.6851032477579795 and parameters: {'n_estimators': 723, 'eta': 0.04173239245784892, 'max_depth': 6, 'alpha': 0.8500000000000001, 'lambda': 32.70428686730658, 'max_bin': 357}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:24:47,912] Trial 296 finished with value: 0.6925140326642556 and parameters: {'n_estimators': 698, 'eta': 0.09765188085126258, 'max_depth': 6, 'alpha': 0.8175, 'lambda': 23.620896948045498, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:24:59,171] Trial 297 finished with value: 0.6886887181163259 and parameters: {'n_estimators': 474, 'eta': 0.08273260415094834, 'max_depth': 6, 'alpha': 0.8684000000000001, 'lambda': 22.63636494700828, 'max_bin': 369}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:25:00,851] Trial 298 finished with value: 0.5716981144611439 and parameters: {'n_estimators': 57, 'eta': 0.07925432518645054, 'max_depth': 6, 'alpha': 0.36310000000000003, 'lambda': 34.25807393557633, 'max_bin': 353}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:25:16,816] Trial 299 finished with value: 0.6934647690626348 and parameters: {'n_estimators': 772, 'eta': 0.0773108513671727, 'max_depth': 6, 'alpha': 0.3042, 'lambda': 35.268341402726904, 'max_bin': 382}. Best is trial 151 with value: 0.6975781740455131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tn_estimators: 754\n",
      "\t\teta: 0.08086337099765775\n",
      "\t\tmax_depth: 6\n",
      "\t\talpha: 0.8510000000000001\n",
      "\t\tlambda: 33.84509206547516\n",
      "\t\tmax_bin: 359\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.714082    0.725363    0.713134    0.688499   \n",
      "1                    TP  328.000000  331.000000  311.000000  319.000000   \n",
      "2                    TN  186.000000  187.000000  194.000000  183.000000   \n",
      "3                    FP   41.000000   36.000000   59.000000   59.000000   \n",
      "4                    FN   40.000000   41.000000   31.000000   34.000000   \n",
      "5              Accuracy    0.863866    0.870588    0.848739    0.843697   \n",
      "6             Precision    0.888889    0.901907    0.840541    0.843915   \n",
      "7           Sensitivity    0.891304    0.889785    0.909357    0.903683   \n",
      "8           Specificity    0.819400    0.838600    0.766800    0.756200   \n",
      "9              F1 score    0.890095    0.895805    0.873596    0.872777   \n",
      "10  F1 score (weighted)    0.863808    0.870868    0.847283    0.842114   \n",
      "11     F1 score (macro)    0.855644    0.862537    0.842655    0.835081   \n",
      "12    Balanced Accuracy    0.855344    0.864175    0.838078    0.829941   \n",
      "13                  MCC    0.711292    0.725210    0.689331    0.673418   \n",
      "14                  NPV    0.823000    0.820200    0.862200    0.843300   \n",
      "15              ROC_AUC    0.855344    0.864175    0.838078    0.829941   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.704908    0.706308  \n",
      "1   339.000000  322.000000  \n",
      "2   167.000000  185.000000  \n",
      "3    57.000000   47.000000  \n",
      "4    32.000000   41.000000  \n",
      "5     0.850420    0.852101  \n",
      "6     0.856061    0.872629  \n",
      "7     0.913747    0.887052  \n",
      "8     0.745500    0.797400  \n",
      "9     0.883963    0.879781  \n",
      "10    0.848438    0.851738  \n",
      "11    0.836781    0.843821  \n",
      "12    0.829641    0.842233  \n",
      "13    0.677031    0.687831  \n",
      "14    0.839200    0.818600  \n",
      "15    0.829641    0.842233  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "\n",
    "y_pred_xgb_5_cat = np.where((y_pred_xgb_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:25:32,347] Trial 300 finished with value: 0.6892631015180364 and parameters: {'n_estimators': 743, 'eta': 0.07335277972374518, 'max_depth': 6, 'alpha': 0.8328, 'lambda': 21.81522958940362, 'max_bin': 375}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:25:46,476] Trial 301 finished with value: 0.6914794776374394 and parameters: {'n_estimators': 660, 'eta': 0.08131315942668985, 'max_depth': 6, 'alpha': 0.7559, 'lambda': 30.554457000365343, 'max_bin': 361}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:26:00,550] Trial 302 finished with value: 0.6915006158066985 and parameters: {'n_estimators': 636, 'eta': 0.07562027803406117, 'max_depth': 6, 'alpha': 0.7928000000000001, 'lambda': 31.77293819559495, 'max_bin': 342}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:26:16,109] Trial 303 finished with value: 0.6942961399849992 and parameters: {'n_estimators': 684, 'eta': 0.07073543199112155, 'max_depth': 6, 'alpha': 0.8538, 'lambda': 36.09697978689397, 'max_bin': 389}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:26:29,445] Trial 304 finished with value: 0.6917588516934314 and parameters: {'n_estimators': 711, 'eta': 0.08610635930606386, 'max_depth': 6, 'alpha': 0.8193, 'lambda': 24.62310585332429, 'max_bin': 367}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:26:43,189] Trial 305 finished with value: 0.6880835864246972 and parameters: {'n_estimators': 733, 'eta': 0.06681045528845567, 'max_depth': 6, 'alpha': 0.8016000000000001, 'lambda': 33.59140310699407, 'max_bin': 348}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:26:57,636] Trial 306 finished with value: 0.6918494560036332 and parameters: {'n_estimators': 698, 'eta': 0.08325660774464826, 'max_depth': 6, 'alpha': 0.8457, 'lambda': 34.76928412994752, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:27:13,406] Trial 307 finished with value: 0.6906832909859643 and parameters: {'n_estimators': 669, 'eta': 0.05946098023271819, 'max_depth': 6, 'alpha': 0.8865000000000001, 'lambda': 37.5382220139773, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:27:28,312] Trial 308 finished with value: 0.6901748553126827 and parameters: {'n_estimators': 721, 'eta': 0.07922238224649186, 'max_depth': 6, 'alpha': 0.776, 'lambda': 32.7884586902866, 'max_bin': 377}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:27:43,392] Trial 309 finished with value: 0.6814343898090376 and parameters: {'n_estimators': 752, 'eta': 0.04712592587425464, 'max_depth': 5, 'alpha': 0.8262, 'lambda': 34.01964560000718, 'max_bin': 364}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:27:57,335] Trial 310 finished with value: 0.6930767818548258 and parameters: {'n_estimators': 649, 'eta': 0.07752440171357422, 'max_depth': 6, 'alpha': 0.8706, 'lambda': 18.73047441984024, 'max_bin': 338}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:28:12,484] Trial 311 finished with value: 0.6918038630693307 and parameters: {'n_estimators': 684, 'eta': 0.08073234887755526, 'max_depth': 6, 'alpha': 0.8045, 'lambda': 32.328696038231406, 'max_bin': 354}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:28:26,189] Trial 312 finished with value: 0.693197731618861 and parameters: {'n_estimators': 699, 'eta': 0.0744634726099446, 'max_depth': 6, 'alpha': 0.903, 'lambda': 23.211231250771327, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:28:41,433] Trial 313 finished with value: 0.6935227938739106 and parameters: {'n_estimators': 736, 'eta': 0.0837642042092373, 'max_depth': 6, 'alpha': 0.8392000000000001, 'lambda': 35.624238616874855, 'max_bin': 362}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:28:55,746] Trial 314 finished with value: 0.6914155468847772 and parameters: {'n_estimators': 713, 'eta': 0.07824024774172868, 'max_depth': 6, 'alpha': 0.7877000000000001, 'lambda': 33.223116472598896, 'max_bin': 384}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:29:10,005] Trial 315 finished with value: 0.694406501260401 and parameters: {'n_estimators': 619, 'eta': 0.07606414482048439, 'max_depth': 6, 'alpha': 0.8592000000000001, 'lambda': 34.77515544672535, 'max_bin': 373}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:29:24,020] Trial 316 finished with value: 0.6933514290141845 and parameters: {'n_estimators': 672, 'eta': 0.05707661120553932, 'max_depth': 6, 'alpha': 0.8226, 'lambda': 25.402402162293384, 'max_bin': 357}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:29:38,308] Trial 317 finished with value: 0.6925112814912522 and parameters: {'n_estimators': 778, 'eta': 0.08166158100862328, 'max_depth': 6, 'alpha': 0.7615000000000001, 'lambda': 33.910421097398604, 'max_bin': 379}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:29:53,281] Trial 318 finished with value: 0.6943323733278302 and parameters: {'n_estimators': 691, 'eta': 0.079404175039591, 'max_depth': 6, 'alpha': 0.8423, 'lambda': 31.218349875849043, 'max_bin': 347}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:30:05,970] Trial 319 finished with value: 0.690387623779068 and parameters: {'n_estimators': 657, 'eta': 0.07213271541117514, 'max_depth': 6, 'alpha': 0.8131, 'lambda': 32.095741678206075, 'max_bin': 366}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:30:21,128] Trial 320 finished with value: 0.6933422897975038 and parameters: {'n_estimators': 727, 'eta': 0.07661060279018744, 'max_depth': 6, 'alpha': 0.8761, 'lambda': 33.16262795605766, 'max_bin': 353}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:30:35,637] Trial 321 finished with value: 0.6901360409993834 and parameters: {'n_estimators': 753, 'eta': 0.08665725997092531, 'max_depth': 6, 'alpha': 0.7897000000000001, 'lambda': 35.209645439199235, 'max_bin': 359}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:30:49,291] Trial 322 finished with value: 0.6904238856966496 and parameters: {'n_estimators': 633, 'eta': 0.08035020233764754, 'max_depth': 6, 'alpha': 0.8484, 'lambda': 34.04402755571399, 'max_bin': 371}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:31:16,418] Trial 323 finished with value: 0.6884512969108643 and parameters: {'n_estimators': 706, 'eta': 0.022251494329946334, 'max_depth': 10, 'alpha': 0.7362000000000001, 'lambda': 35.910993642825034, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:31:33,025] Trial 324 finished with value: 0.6928971283376935 and parameters: {'n_estimators': 675, 'eta': 0.07373199324055232, 'max_depth': 6, 'alpha': 0.8263, 'lambda': 39.79442922035426, 'max_bin': 377}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:31:49,060] Trial 325 finished with value: 0.6867720067190585 and parameters: {'n_estimators': 738, 'eta': 0.08289669029247373, 'max_depth': 11, 'alpha': 0.8043, 'lambda': 34.403681641720745, 'max_bin': 400}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:32:01,611] Trial 326 finished with value: 0.690512353164774 and parameters: {'n_estimators': 714, 'eta': 0.07835745927164056, 'max_depth': 6, 'alpha': 0.7723, 'lambda': 32.82630317279157, 'max_bin': 316}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:32:14,582] Trial 327 finished with value: 0.6877100608326495 and parameters: {'n_estimators': 646, 'eta': 0.08472042302833241, 'max_depth': 6, 'alpha': 0.8599, 'lambda': 26.7513366417306, 'max_bin': 389}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:32:29,970] Trial 328 finished with value: 0.6933105763029055 and parameters: {'n_estimators': 687, 'eta': 0.0753528997588886, 'max_depth': 6, 'alpha': 0.8323, 'lambda': 38.26221938952705, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:32:43,213] Trial 329 finished with value: 0.688382934411434 and parameters: {'n_estimators': 659, 'eta': 0.08085926460422227, 'max_depth': 5, 'alpha': 0.8075, 'lambda': 36.67450854085596, 'max_bin': 351}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:33:01,106] Trial 330 finished with value: 0.6851701651550297 and parameters: {'n_estimators': 764, 'eta': 0.036308302898651584, 'max_depth': 6, 'alpha': 0.8821, 'lambda': 33.664364807432186, 'max_bin': 359}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:33:15,880] Trial 331 finished with value: 0.6943763617262402 and parameters: {'n_estimators': 700, 'eta': 0.07010458006676915, 'max_depth': 6, 'alpha': 0.7839, 'lambda': 31.612411358876994, 'max_bin': 382}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:33:28,987] Trial 332 finished with value: 0.691187620754199 and parameters: {'n_estimators': 568, 'eta': 0.0621061751053411, 'max_depth': 6, 'alpha': 0.8459, 'lambda': 34.94222415435817, 'max_bin': 365}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:33:45,539] Trial 333 finished with value: 0.6926045484273108 and parameters: {'n_estimators': 719, 'eta': 0.05492752611637428, 'max_depth': 6, 'alpha': 0.8243, 'lambda': 16.03192451137816, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:33:58,620] Trial 334 finished with value: 0.6888373230395116 and parameters: {'n_estimators': 602, 'eta': 0.06603368812673222, 'max_depth': 6, 'alpha': 0.9043, 'lambda': 33.17350230985899, 'max_bin': 345}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:34:13,955] Trial 335 finished with value: 0.6921220063863418 and parameters: {'n_estimators': 786, 'eta': 0.07836308699956969, 'max_depth': 6, 'alpha': 0.8623000000000001, 'lambda': 30.107378166488335, 'max_bin': 357}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:34:28,248] Trial 336 finished with value: 0.6917347143007466 and parameters: {'n_estimators': 676, 'eta': 0.08205138875535203, 'max_depth': 6, 'alpha': 0.8044, 'lambda': 34.34981080465468, 'max_bin': 352}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:34:42,580] Trial 337 finished with value: 0.6914736952213818 and parameters: {'n_estimators': 737, 'eta': 0.07671622457287486, 'max_depth': 6, 'alpha': 0.7599, 'lambda': 32.143043318328836, 'max_bin': 362}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:34:59,451] Trial 338 finished with value: 0.689482261151775 and parameters: {'n_estimators': 693, 'eta': 0.058018407729116366, 'max_depth': 6, 'alpha': 0.8311000000000001, 'lambda': 35.417666221051824, 'max_bin': 375}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:35:15,585] Trial 339 finished with value: 0.6920169169974586 and parameters: {'n_estimators': 750, 'eta': 0.06264366559947725, 'max_depth': 6, 'alpha': 0.7817000000000001, 'lambda': 25.980470956436264, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:35:30,247] Trial 340 finished with value: 0.689804901046427 and parameters: {'n_estimators': 663, 'eta': 0.06799300901404924, 'max_depth': 6, 'alpha': 0.81, 'lambda': 33.36724612614677, 'max_bin': 339}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:35:43,731] Trial 341 finished with value: 0.6917185301255923 and parameters: {'n_estimators': 717, 'eta': 0.08020501101916594, 'max_depth': 6, 'alpha': 0.8484, 'lambda': 32.495099344256566, 'max_bin': 380}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:35:55,578] Trial 342 finished with value: 0.684589936253829 and parameters: {'n_estimators': 624, 'eta': 0.07408634108173318, 'max_depth': 5, 'alpha': 0.8807, 'lambda': 33.899062597982585, 'max_bin': 357}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:36:09,617] Trial 343 finished with value: 0.6892439587429701 and parameters: {'n_estimators': 642, 'eta': 0.08409537200393534, 'max_depth': 6, 'alpha': 0.8208000000000001, 'lambda': 34.752526653593385, 'max_bin': 331}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:36:22,177] Trial 344 finished with value: 0.6897496929164507 and parameters: {'n_estimators': 682, 'eta': 0.07771687526611085, 'max_depth': 6, 'alpha': 0.8672000000000001, 'lambda': 31.126645862246733, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:36:30,836] Trial 345 finished with value: 0.6843639578419145 and parameters: {'n_estimators': 364, 'eta': 0.07914156324712403, 'max_depth': 6, 'alpha': 0.7921, 'lambda': 36.068215079975474, 'max_bin': 386}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:36:42,789] Trial 346 finished with value: 0.6884982999264571 and parameters: {'n_estimators': 704, 'eta': 0.09314917984056692, 'max_depth': 6, 'alpha': 0.8394, 'lambda': 37.09536486807964, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:36:57,807] Trial 347 finished with value: 0.6943305938860497 and parameters: {'n_estimators': 732, 'eta': 0.07277375417479806, 'max_depth': 6, 'alpha': 0.8006000000000001, 'lambda': 34.13949349170107, 'max_bin': 352}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:37:09,437] Trial 348 finished with value: 0.6858111196120789 and parameters: {'n_estimators': 761, 'eta': 0.08150613394106124, 'max_depth': 6, 'alpha': 0.8248000000000001, 'lambda': 14.365663169206334, 'max_bin': 347}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:37:23,328] Trial 349 finished with value: 0.6923132600062345 and parameters: {'n_estimators': 661, 'eta': 0.06464530675234392, 'max_depth': 6, 'alpha': 0.7471, 'lambda': 21.4085534805081, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tn_estimators: 754\n",
      "\t\teta: 0.08086337099765775\n",
      "\t\tmax_depth: 6\n",
      "\t\talpha: 0.8510000000000001\n",
      "\t\tlambda: 33.84509206547516\n",
      "\t\tmax_bin: 359\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.714082    0.725363    0.713134    0.688499   \n",
      "1                    TP  328.000000  331.000000  311.000000  319.000000   \n",
      "2                    TN  186.000000  187.000000  194.000000  183.000000   \n",
      "3                    FP   41.000000   36.000000   59.000000   59.000000   \n",
      "4                    FN   40.000000   41.000000   31.000000   34.000000   \n",
      "5              Accuracy    0.863866    0.870588    0.848739    0.843697   \n",
      "6             Precision    0.888889    0.901907    0.840541    0.843915   \n",
      "7           Sensitivity    0.891304    0.889785    0.909357    0.903683   \n",
      "8           Specificity    0.819400    0.838600    0.766800    0.756200   \n",
      "9              F1 score    0.890095    0.895805    0.873596    0.872777   \n",
      "10  F1 score (weighted)    0.863808    0.870868    0.847283    0.842114   \n",
      "11     F1 score (macro)    0.855644    0.862537    0.842655    0.835081   \n",
      "12    Balanced Accuracy    0.855344    0.864175    0.838078    0.829941   \n",
      "13                  MCC    0.711292    0.725210    0.689331    0.673418   \n",
      "14                  NPV    0.823000    0.820200    0.862200    0.843300   \n",
      "15              ROC_AUC    0.855344    0.864175    0.838078    0.829941   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.704908    0.706308    0.715518  \n",
      "1   339.000000  322.000000  331.000000  \n",
      "2   167.000000  185.000000  172.000000  \n",
      "3    57.000000   47.000000   56.000000  \n",
      "4    32.000000   41.000000   36.000000  \n",
      "5     0.850420    0.852101    0.845378  \n",
      "6     0.856061    0.872629    0.855297  \n",
      "7     0.913747    0.887052    0.901907  \n",
      "8     0.745500    0.797400    0.754400  \n",
      "9     0.883963    0.879781    0.877984  \n",
      "10    0.848438    0.851738    0.843882  \n",
      "11    0.836781    0.843821    0.833487  \n",
      "12    0.829641    0.842233    0.828147  \n",
      "13    0.677031    0.687831    0.669131  \n",
      "14    0.839200    0.818600    0.826900  \n",
      "15    0.829641    0.842233    0.828147  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_6_cat = np.where((y_pred_xgb_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:37:35,743] Trial 350 finished with value: 0.6739856103251564 and parameters: {'n_estimators': 692, 'eta': 0.09518232512061074, 'max_depth': 6, 'alpha': 0.8575, 'lambda': 1.1660572000565779, 'max_bin': 361}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:37:44,654] Trial 351 finished with value: 0.6844536797894886 and parameters: {'n_estimators': 420, 'eta': 0.07568106490136882, 'max_depth': 6, 'alpha': 0.7721, 'lambda': 19.827979310134182, 'max_bin': 371}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:37:58,497] Trial 352 finished with value: 0.6886698518407807 and parameters: {'n_estimators': 717, 'eta': 0.07952910145504566, 'max_depth': 6, 'alpha': 0.8928, 'lambda': 32.805113334911724, 'max_bin': 378}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:38:11,211] Trial 353 finished with value: 0.6876393607070833 and parameters: {'n_estimators': 677, 'eta': 0.08271491152700425, 'max_depth': 6, 'alpha': 0.8371000000000001, 'lambda': 33.68153342684127, 'max_bin': 356}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:38:22,546] Trial 354 finished with value: 0.6838532066024688 and parameters: {'n_estimators': 738, 'eta': 0.08684748674507103, 'max_depth': 6, 'alpha': 0.8132, 'lambda': 22.7606557566358, 'max_bin': 364}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:38:37,271] Trial 355 finished with value: 0.6920664289089405 and parameters: {'n_estimators': 778, 'eta': 0.07733488550538059, 'max_depth': 6, 'alpha': 0.8681000000000001, 'lambda': 35.19625528976363, 'max_bin': 375}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:38:49,519] Trial 356 finished with value: 0.6852792218553484 and parameters: {'n_estimators': 703, 'eta': 0.07103142892042912, 'max_depth': 6, 'alpha': 0.7914, 'lambda': 31.973116707695628, 'max_bin': 360}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:39:00,488] Trial 357 finished with value: 0.6840276881425902 and parameters: {'n_estimators': 638, 'eta': 0.08475650139937305, 'max_depth': 6, 'alpha': 0.8399000000000001, 'lambda': 20.913220130082873, 'max_bin': 341}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:39:12,734] Trial 358 finished with value: 0.6874770331497306 and parameters: {'n_estimators': 670, 'eta': 0.07525195917982662, 'max_depth': 6, 'alpha': 0.0079, 'lambda': 34.58558040682666, 'max_bin': 367}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:39:24,746] Trial 359 finished with value: 0.6840508582026221 and parameters: {'n_estimators': 729, 'eta': 0.0892847144299601, 'max_depth': 5, 'alpha': 0.8163, 'lambda': 33.19947987674922, 'max_bin': 354}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:39:38,207] Trial 360 finished with value: 0.688534760454747 and parameters: {'n_estimators': 750, 'eta': 0.08067413165688211, 'max_depth': 6, 'alpha': 0.8526, 'lambda': 35.89750756716928, 'max_bin': 393}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:39:51,280] Trial 361 finished with value: 0.6809829606671389 and parameters: {'n_estimators': 653, 'eta': 0.05240085568544413, 'max_depth': 6, 'alpha': 0.9062, 'lambda': 18.508376309716233, 'max_bin': 382}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:40:03,365] Trial 362 finished with value: 0.6866456465126685 and parameters: {'n_estimators': 689, 'eta': 0.07802294573648363, 'max_depth': 6, 'alpha': 0.7982, 'lambda': 32.37662044878292, 'max_bin': 348}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:40:15,970] Trial 363 finished with value: 0.6869153949470259 and parameters: {'n_estimators': 708, 'eta': 0.08265507206947123, 'max_depth': 6, 'alpha': 0.7661, 'lambda': 33.7874320387926, 'max_bin': 374}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:40:31,327] Trial 364 finished with value: 0.6819708412758381 and parameters: {'n_estimators': 720, 'eta': 0.04347377039700939, 'max_depth': 6, 'alpha': 0.8264, 'lambda': 34.852641475677274, 'max_bin': 322}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:40:37,198] Trial 365 finished with value: 0.6725497102063107 and parameters: {'n_estimators': 238, 'eta': 0.07961833835983972, 'max_depth': 7, 'alpha': 0.8709, 'lambda': 30.795567021169568, 'max_bin': 365}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:40:49,812] Trial 366 finished with value: 0.6820480852053974 and parameters: {'n_estimators': 617, 'eta': 0.05956288138903963, 'max_depth': 6, 'alpha': 0.8395, 'lambda': 32.8372348694964, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:41:02,175] Trial 367 finished with value: 0.6867652315178578 and parameters: {'n_estimators': 678, 'eta': 0.07334243722851742, 'max_depth': 6, 'alpha': 0.7267, 'lambda': 34.28387478211363, 'max_bin': 369}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:41:13,012] Trial 368 finished with value: 0.6877271410840817 and parameters: {'n_estimators': 745, 'eta': 0.09161318834845304, 'max_depth': 6, 'alpha': 0.7859, 'lambda': 31.69953466441744, 'max_bin': 352}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:41:27,792] Trial 369 finished with value: 0.6684182214952056 and parameters: {'n_estimators': 652, 'eta': 0.03063690880089603, 'max_depth': 6, 'alpha': 0.8157000000000001, 'lambda': 36.613266702291824, 'max_bin': 385}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:41:37,223] Trial 370 finished with value: 0.6826153080487657 and parameters: {'n_estimators': 522, 'eta': 0.07653804005337117, 'max_depth': 6, 'alpha': 0.8896000000000001, 'lambda': 7.998482759022231, 'max_bin': 378}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:41:50,147] Trial 371 finished with value: 0.686804278409266 and parameters: {'n_estimators': 698, 'eta': 0.08058375215226295, 'max_depth': 6, 'alpha': 0.8580000000000001, 'lambda': 29.67139309786686, 'max_bin': 362}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:42:03,597] Trial 372 finished with value: 0.6889022147046272 and parameters: {'n_estimators': 764, 'eta': 0.07797556649871012, 'max_depth': 6, 'alpha': 0.804, 'lambda': 35.31180739505543, 'max_bin': 335}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:42:17,590] Trial 373 finished with value: 0.6853366635698468 and parameters: {'n_estimators': 724, 'eta': 0.06870781521545316, 'max_depth': 6, 'alpha': 0.9229, 'lambda': 25.352950671417123, 'max_bin': 371}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:42:27,908] Trial 374 finished with value: 0.6849936541897435 and parameters: {'n_estimators': 668, 'eta': 0.09820464243507426, 'max_depth': 5, 'alpha': 0.8404, 'lambda': 33.39037131976255, 'max_bin': 356}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:42:43,280] Trial 375 finished with value: 0.682217657347128 and parameters: {'n_estimators': 690, 'eta': 0.08358669091897375, 'max_depth': 11, 'alpha': 0.7764000000000001, 'lambda': 37.50797079220179, 'max_bin': 344}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:42:56,739] Trial 376 finished with value: 0.6883355076847566 and parameters: {'n_estimators': 705, 'eta': 0.07525591752174388, 'max_depth': 6, 'alpha': 0.8227, 'lambda': 24.521287087563238, 'max_bin': 364}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:43:08,142] Trial 377 finished with value: 0.6870509376850731 and parameters: {'n_estimators': 632, 'eta': 0.08552558682775553, 'max_depth': 6, 'alpha': 0.8547, 'lambda': 39.054143661370745, 'max_bin': 389}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:43:21,248] Trial 378 finished with value: 0.6863553839610408 and parameters: {'n_estimators': 744, 'eta': 0.08177441436217735, 'max_depth': 6, 'alpha': 0.8783000000000001, 'lambda': 34.390523664692196, 'max_bin': 350}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:43:32,990] Trial 379 finished with value: 0.6860345546704181 and parameters: {'n_estimators': 719, 'eta': 0.07957528865052911, 'max_depth': 6, 'alpha': 0.796, 'lambda': 32.67427259811359, 'max_bin': 360}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:43:45,349] Trial 380 finished with value: 0.6851906080857746 and parameters: {'n_estimators': 678, 'eta': 0.07681918789183188, 'max_depth': 6, 'alpha': 0.8271000000000001, 'lambda': 33.656075514123145, 'max_bin': 375}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:44:00,929] Trial 381 finished with value: 0.6879893723747689 and parameters: {'n_estimators': 771, 'eta': 0.07277765915461346, 'max_depth': 6, 'alpha': 0.7547, 'lambda': 35.57873382410052, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:44:14,628] Trial 382 finished with value: 0.6881170204113319 and parameters: {'n_estimators': 657, 'eta': 0.07849928450370369, 'max_depth': 6, 'alpha': 0.8126, 'lambda': 31.328253031921253, 'max_bin': 381}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:44:29,227] Trial 383 finished with value: 0.6878324260298616 and parameters: {'n_estimators': 733, 'eta': 0.08171637398029992, 'max_depth': 6, 'alpha': 0.8436, 'lambda': 34.72957842163562, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:44:46,463] Trial 384 finished with value: 0.6776859530392005 and parameters: {'n_estimators': 699, 'eta': 0.07474756701208252, 'max_depth': 12, 'alpha': 0.7908000000000001, 'lambda': 20.13749781632726, 'max_bin': 365}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:45:00,142] Trial 385 finished with value: 0.6876161353952237 and parameters: {'n_estimators': 644, 'eta': 0.07996441792285143, 'max_depth': 6, 'alpha': 0.8580000000000001, 'lambda': 28.4339812825305, 'max_bin': 354}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:45:14,362] Trial 386 finished with value: 0.6894704322801399 and parameters: {'n_estimators': 689, 'eta': 0.08307015269991215, 'max_depth': 6, 'alpha': 0.8312, 'lambda': 27.78830901967836, 'max_bin': 371}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:45:27,728] Trial 387 finished with value: 0.6863418823495063 and parameters: {'n_estimators': 756, 'eta': 0.07013381118133565, 'max_depth': 6, 'alpha': 0.8104, 'lambda': 33.0853282811005, 'max_bin': 348}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:45:43,038] Trial 388 finished with value: 0.6891867781226546 and parameters: {'n_estimators': 715, 'eta': 0.08740070816023744, 'max_depth': 7, 'alpha': 0.8831, 'lambda': 27.14910862504066, 'max_bin': 396}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:45:58,272] Trial 389 finished with value: 0.6856808844280085 and parameters: {'n_estimators': 788, 'eta': 0.07575208392569455, 'max_depth': 6, 'alpha': 0.7803, 'lambda': 34.01974216628603, 'max_bin': 377}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:46:10,204] Trial 390 finished with value: 0.687094738433051 and parameters: {'n_estimators': 667, 'eta': 0.07764326308024297, 'max_depth': 6, 'alpha': 0.8474, 'lambda': 36.23396187214618, 'max_bin': 364}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:46:25,406] Trial 391 finished with value: 0.680692672260257 and parameters: {'n_estimators': 733, 'eta': 0.06202926860552591, 'max_depth': 6, 'alpha': 0.8661000000000001, 'lambda': 32.07656517990016, 'max_bin': 359}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:46:42,390] Trial 392 finished with value: 0.6794154434867634 and parameters: {'n_estimators': 709, 'eta': 0.046349025746472455, 'max_depth': 6, 'alpha': 0.8099000000000001, 'lambda': 33.683908481916376, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:46:54,658] Trial 393 finished with value: 0.6879710215342599 and parameters: {'n_estimators': 608, 'eta': 0.08469243326840639, 'max_depth': 6, 'alpha': 0.7568, 'lambda': 30.5324801388821, 'max_bin': 383}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:47:08,500] Trial 394 finished with value: 0.5645805212076636 and parameters: {'n_estimators': 683, 'eta': 0.006548917754542853, 'max_depth': 6, 'alpha': 0.8363, 'lambda': 35.19855660483298, 'max_bin': 343}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:47:19,119] Trial 395 finished with value: 0.6817601890553688 and parameters: {'n_estimators': 664, 'eta': 0.07969532992560546, 'max_depth': 6, 'alpha': 0.7976000000000001, 'lambda': 17.31568335088088, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:47:31,710] Trial 396 finished with value: 0.687725649362739 and parameters: {'n_estimators': 746, 'eta': 0.07242119980818139, 'max_depth': 6, 'alpha': 0.9, 'lambda': 32.6570997597976, 'max_bin': 355}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:47:46,421] Trial 397 finished with value: 0.6835120454160455 and parameters: {'n_estimators': 698, 'eta': 0.06425338437934643, 'max_depth': 6, 'alpha': 0.8274, 'lambda': 34.368707140008645, 'max_bin': 361}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:47:59,665] Trial 398 finished with value: 0.6899303776641031 and parameters: {'n_estimators': 633, 'eta': 0.08220757908898296, 'max_depth': 6, 'alpha': 0.7704000000000001, 'lambda': 33.234085138961106, 'max_bin': 378}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:48:15,324] Trial 399 finished with value: 0.689665878647115 and parameters: {'n_estimators': 724, 'eta': 0.077160605380175, 'max_depth': 6, 'alpha': 0.8643000000000001, 'lambda': 35.676439398045, 'max_bin': 351}. Best is trial 151 with value: 0.6975781740455131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tn_estimators: 754\n",
      "\t\teta: 0.08086337099765775\n",
      "\t\tmax_depth: 6\n",
      "\t\talpha: 0.8510000000000001\n",
      "\t\tlambda: 33.84509206547516\n",
      "\t\tmax_bin: 359\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.714082    0.725363    0.713134    0.688499   \n",
      "1                    TP  328.000000  331.000000  311.000000  319.000000   \n",
      "2                    TN  186.000000  187.000000  194.000000  183.000000   \n",
      "3                    FP   41.000000   36.000000   59.000000   59.000000   \n",
      "4                    FN   40.000000   41.000000   31.000000   34.000000   \n",
      "5              Accuracy    0.863866    0.870588    0.848739    0.843697   \n",
      "6             Precision    0.888889    0.901907    0.840541    0.843915   \n",
      "7           Sensitivity    0.891304    0.889785    0.909357    0.903683   \n",
      "8           Specificity    0.819400    0.838600    0.766800    0.756200   \n",
      "9              F1 score    0.890095    0.895805    0.873596    0.872777   \n",
      "10  F1 score (weighted)    0.863808    0.870868    0.847283    0.842114   \n",
      "11     F1 score (macro)    0.855644    0.862537    0.842655    0.835081   \n",
      "12    Balanced Accuracy    0.855344    0.864175    0.838078    0.829941   \n",
      "13                  MCC    0.711292    0.725210    0.689331    0.673418   \n",
      "14                  NPV    0.823000    0.820200    0.862200    0.843300   \n",
      "15              ROC_AUC    0.855344    0.864175    0.838078    0.829941   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.704908    0.706308    0.715518    0.695919  \n",
      "1   339.000000  322.000000  331.000000  314.000000  \n",
      "2   167.000000  185.000000  172.000000  179.000000  \n",
      "3    57.000000   47.000000   56.000000   56.000000  \n",
      "4    32.000000   41.000000   36.000000   46.000000  \n",
      "5     0.850420    0.852101    0.845378    0.828571  \n",
      "6     0.856061    0.872629    0.855297    0.848649  \n",
      "7     0.913747    0.887052    0.901907    0.872222  \n",
      "8     0.745500    0.797400    0.754400    0.761700  \n",
      "9     0.883963    0.879781    0.877984    0.860274  \n",
      "10    0.848438    0.851738    0.843882    0.827882  \n",
      "11    0.836781    0.843821    0.833487    0.819267  \n",
      "12    0.829641    0.842233    0.828147    0.816962  \n",
      "13    0.677031    0.687831    0.669131    0.639044  \n",
      "14    0.839200    0.818600    0.826900    0.795600  \n",
      "15    0.829641    0.842233    0.828147    0.816962  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "y_pred_xgb_7_cat = np.where((y_pred_xgb_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:48:28,218] Trial 400 finished with value: 0.6498847997313497 and parameters: {'n_estimators': 584, 'eta': 0.025684758554375797, 'max_depth': 5, 'alpha': 0.8196, 'lambda': 29.015694877952846, 'max_bin': 366}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:48:41,552] Trial 401 finished with value: 0.6813447533228475 and parameters: {'n_estimators': 682, 'eta': 0.07386035791165968, 'max_depth': 6, 'alpha': 0.8437, 'lambda': 34.78337662795177, 'max_bin': 336}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:48:55,683] Trial 402 finished with value: 0.6410182385753972 and parameters: {'n_estimators': 649, 'eta': 0.014028763759752962, 'max_depth': 6, 'alpha': 0.24630000000000002, 'lambda': 32.221799287549715, 'max_bin': 373}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:49:07,245] Trial 403 finished with value: 0.6836991749266955 and parameters: {'n_estimators': 763, 'eta': 0.08077400786630741, 'max_depth': 6, 'alpha': 0.7981, 'lambda': 33.55711489466722, 'max_bin': 386}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:49:25,505] Trial 404 finished with value: 0.6786228077599663 and parameters: {'n_estimators': 703, 'eta': 0.032800698398743555, 'max_depth': 7, 'alpha': 0.8803000000000001, 'lambda': 36.89641986348361, 'max_bin': 361}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:49:37,288] Trial 405 finished with value: 0.6818698435140279 and parameters: {'n_estimators': 728, 'eta': 0.07912712524776282, 'max_depth': 6, 'alpha': 0.7791, 'lambda': 34.18165992224964, 'max_bin': 356}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:49:48,784] Trial 406 finished with value: 0.6799411824834184 and parameters: {'n_estimators': 667, 'eta': 0.06569125269789822, 'max_depth': 6, 'alpha': 0.736, 'lambda': 22.112110185424463, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:50:01,513] Trial 407 finished with value: 0.6815813068635425 and parameters: {'n_estimators': 711, 'eta': 0.07599392425922903, 'max_depth': 6, 'alpha': 0.8192, 'lambda': 32.903975926365035, 'max_bin': 377}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:50:13,126] Trial 408 finished with value: 0.6816127798079118 and parameters: {'n_estimators': 686, 'eta': 0.08560263209024749, 'max_depth': 6, 'alpha': 0.8515, 'lambda': 31.609979249194033, 'max_bin': 347}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:50:23,913] Trial 409 finished with value: 0.6834065895068058 and parameters: {'n_estimators': 748, 'eta': 0.08192959210947003, 'max_depth': 6, 'alpha': 0.804, 'lambda': 38.72307159583408, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:50:36,415] Trial 410 finished with value: 0.6806630876311914 and parameters: {'n_estimators': 775, 'eta': 0.0781530535030943, 'max_depth': 6, 'alpha': 0.8375, 'lambda': 35.37890686676783, 'max_bin': 370}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:50:47,858] Trial 411 finished with value: 0.608209126477668 and parameters: {'n_estimators': 648, 'eta': 0.011380731995298522, 'max_depth': 5, 'alpha': 0.8642000000000001, 'lambda': 37.95556119539077, 'max_bin': 307}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:51:00,249] Trial 412 finished with value: 0.6809994675972589 and parameters: {'n_estimators': 675, 'eta': 0.07139843510192412, 'max_depth': 6, 'alpha': 0.8255, 'lambda': 33.92638485715268, 'max_bin': 354}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:51:19,498] Trial 413 finished with value: 0.6851016019256428 and parameters: {'n_estimators': 627, 'eta': 0.04880292234074249, 'max_depth': 10, 'alpha': 0.917, 'lambda': 30.090595153205122, 'max_bin': 389}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:51:28,630] Trial 414 finished with value: 0.68061350151747 and parameters: {'n_estimators': 727, 'eta': 0.08914746166384169, 'max_depth': 6, 'alpha': 0.0678, 'lambda': 36.26296789775709, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:51:38,872] Trial 415 finished with value: 0.6786885528009411 and parameters: {'n_estimators': 695, 'eta': 0.08072824576654615, 'max_depth': 6, 'alpha': 0.7863, 'lambda': 25.82930941665255, 'max_bin': 374}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:51:51,251] Trial 416 finished with value: 0.6837200014481548 and parameters: {'n_estimators': 705, 'eta': 0.07512358669866176, 'max_depth': 6, 'alpha': 0.4298, 'lambda': 34.76834222827932, 'max_bin': 405}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:52:08,488] Trial 417 finished with value: 0.6636272239170555 and parameters: {'n_estimators': 660, 'eta': 0.017817355758551465, 'max_depth': 7, 'alpha': 0.8877, 'lambda': 32.536439656708524, 'max_bin': 341}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:52:19,890] Trial 418 finished with value: 0.682860320996288 and parameters: {'n_estimators': 739, 'eta': 0.08413687786689861, 'max_depth': 6, 'alpha': 0.8473, 'lambda': 33.54718050437042, 'max_bin': 381}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:52:33,408] Trial 419 finished with value: 0.6833953553747741 and parameters: {'n_estimators': 718, 'eta': 0.07857830301674455, 'max_depth': 6, 'alpha': 0.8059000000000001, 'lambda': 34.336371395990604, 'max_bin': 364}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:52:45,990] Trial 420 finished with value: 0.6839310621647126 and parameters: {'n_estimators': 684, 'eta': 0.07748654918019872, 'max_depth': 6, 'alpha': 0.76, 'lambda': 31.630561406121867, 'max_bin': 350}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:52:55,673] Trial 421 finished with value: 0.6768948876526624 and parameters: {'n_estimators': 759, 'eta': 0.05776300645359447, 'max_depth': 6, 'alpha': 0.52, 'lambda': 3.106666113359843, 'max_bin': 368}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:53:05,814] Trial 422 finished with value: 0.6805494979140426 and parameters: {'n_estimators': 671, 'eta': 0.09132706997172163, 'max_depth': 6, 'alpha': 0.8696, 'lambda': 19.04451768625697, 'max_bin': 360}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:53:14,832] Trial 423 finished with value: 0.6814797677877316 and parameters: {'n_estimators': 710, 'eta': 0.0832348834680587, 'max_depth': 6, 'alpha': 0.8297, 'lambda': 11.982495126473836, 'max_bin': 327}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:53:25,277] Trial 424 finished with value: 0.6819021649904199 and parameters: {'n_estimators': 645, 'eta': 0.08020361687840079, 'max_depth': 6, 'alpha': 0.8108000000000001, 'lambda': 33.01984185090495, 'max_bin': 376}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:53:39,091] Trial 425 finished with value: 0.6843366128255635 and parameters: {'n_estimators': 744, 'eta': 0.060366533270449135, 'max_depth': 6, 'alpha': 0.8508, 'lambda': 20.1740579529796, 'max_bin': 355}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:53:51,844] Trial 426 finished with value: 0.6813618367004708 and parameters: {'n_estimators': 696, 'eta': 0.07418830687350617, 'max_depth': 6, 'alpha': 0.1522, 'lambda': 35.23479330716276, 'max_bin': 365}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:54:03,901] Trial 427 finished with value: 0.680122326934713 and parameters: {'n_estimators': 725, 'eta': 0.07645159812610575, 'max_depth': 6, 'alpha': 0.7803, 'lambda': 14.885468640650956, 'max_bin': 372}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:54:14,782] Trial 428 finished with value: 0.6806324849009714 and parameters: {'n_estimators': 782, 'eta': 0.08180221572004945, 'max_depth': 6, 'alpha': 0.8285, 'lambda': 33.6222674181533, 'max_bin': 382}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:54:26,703] Trial 429 finished with value: 0.683702633155771 and parameters: {'n_estimators': 660, 'eta': 0.07894947427863135, 'max_depth': 6, 'alpha': 0.7965, 'lambda': 32.32776959523, 'max_bin': 362}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:54:37,930] Trial 430 finished with value: 0.6863585184779792 and parameters: {'n_estimators': 619, 'eta': 0.08672312099387271, 'max_depth': 6, 'alpha': 0.878, 'lambda': 31.08070482828762, 'max_bin': 345}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:54:52,383] Trial 431 finished with value: 0.6833794407457306 and parameters: {'n_estimators': 689, 'eta': 0.06696143891574252, 'max_depth': 7, 'alpha': 0.8451000000000001, 'lambda': 36.08127096165742, 'max_bin': 321}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:55:05,987] Trial 432 finished with value: 0.6843541045124549 and parameters: {'n_estimators': 733, 'eta': 0.06910306238504973, 'max_depth': 6, 'alpha': 0.8067000000000001, 'lambda': 34.674311088034, 'max_bin': 351}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:55:19,136] Trial 433 finished with value: 0.6856712611162532 and parameters: {'n_estimators': 674, 'eta': 0.0735285078043553, 'max_depth': 6, 'alpha': 0.9002, 'lambda': 34.093727332090516, 'max_bin': 369}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:55:30,538] Trial 434 finished with value: 0.681373814583994 and parameters: {'n_estimators': 710, 'eta': 0.07611291927005082, 'max_depth': 6, 'alpha': 0.8253, 'lambda': 20.640465189664877, 'max_bin': 358}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:55:40,748] Trial 435 finished with value: 0.6799270575669069 and parameters: {'n_estimators': 641, 'eta': 0.09706314211755282, 'max_depth': 6, 'alpha': 0.7706000000000001, 'lambda': 32.858185816524326, 'max_bin': 332}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:55:52,843] Trial 436 finished with value: 0.6795638251181517 and parameters: {'n_estimators': 762, 'eta': 0.08055018217638175, 'max_depth': 5, 'alpha': 0.7141000000000001, 'lambda': 35.29722402095439, 'max_bin': 391}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:56:07,865] Trial 437 finished with value: 0.6802751980088811 and parameters: {'n_estimators': 697, 'eta': 0.050305271858311795, 'max_depth': 6, 'alpha': 0.8629, 'lambda': 33.37646264131727, 'max_bin': 373}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:56:21,507] Trial 438 finished with value: 0.6807970155518607 and parameters: {'n_estimators': 720, 'eta': 0.051903569788583556, 'max_depth': 6, 'alpha': 0.7932, 'lambda': 23.21932852311992, 'max_bin': 366}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:56:32,589] Trial 439 finished with value: 0.6848756644723746 and parameters: {'n_estimators': 673, 'eta': 0.08347937722769151, 'max_depth': 6, 'alpha': 0.8169000000000001, 'lambda': 32.036527792502966, 'max_bin': 355}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:56:49,186] Trial 440 finished with value: 0.6791099543461376 and parameters: {'n_estimators': 752, 'eta': 0.03692006586248387, 'max_depth': 6, 'alpha': 0.8404, 'lambda': 34.39200759620223, 'max_bin': 379}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:57:00,400] Trial 441 finished with value: 0.6780498521555298 and parameters: {'n_estimators': 499, 'eta': 0.05487670908212483, 'max_depth': 6, 'alpha': 0.7464000000000001, 'lambda': 26.86259701647822, 'max_bin': 361}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:57:16,541] Trial 442 finished with value: 0.6621759416810251 and parameters: {'n_estimators': 736, 'eta': 0.020327557226495454, 'max_depth': 6, 'alpha': 0.8657, 'lambda': 37.16062914804859, 'max_bin': 385}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:57:18,864] Trial 443 finished with value: 0.6276690088881545 and parameters: {'n_estimators': 98, 'eta': 0.0785201861099359, 'max_depth': 6, 'alpha': 0.7844, 'lambda': 33.438358660841395, 'max_bin': 371}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:57:32,611] Trial 444 finished with value: 0.6821375235845577 and parameters: {'n_estimators': 657, 'eta': 0.07083807871976536, 'max_depth': 6, 'alpha': 0.8324, 'lambda': 29.26561178016553, 'max_bin': 351}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:57:48,651] Trial 445 finished with value: 0.6657223513224103 and parameters: {'n_estimators': 686, 'eta': 0.02532020564931019, 'max_depth': 6, 'alpha': 0.8061, 'lambda': 39.730964351856876, 'max_bin': 366}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:58:01,647] Trial 446 finished with value: 0.6826786625188868 and parameters: {'n_estimators': 604, 'eta': 0.07715278266265058, 'max_depth': 6, 'alpha': 0.8573000000000001, 'lambda': 35.82908867083987, 'max_bin': 359}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:58:09,030] Trial 447 finished with value: 0.6746684887641067 and parameters: {'n_estimators': 329, 'eta': 0.08149556794713789, 'max_depth': 6, 'alpha': 0.8832000000000001, 'lambda': 19.306919836345283, 'max_bin': 376}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:58:21,405] Trial 448 finished with value: 0.68622268253211 and parameters: {'n_estimators': 710, 'eta': 0.0794705503897186, 'max_depth': 7, 'alpha': 0.8286, 'lambda': 34.65644706044333, 'max_bin': 346}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:58:32,853] Trial 449 finished with value: 0.6838606576024449 and parameters: {'n_estimators': 632, 'eta': 0.08536829519090987, 'max_depth': 6, 'alpha': 0.8494, 'lambda': 23.393693835009646, 'max_bin': 363}. Best is trial 151 with value: 0.6975781740455131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6976\n",
      "\tBest params:\n",
      "\t\tn_estimators: 754\n",
      "\t\teta: 0.08086337099765775\n",
      "\t\tmax_depth: 6\n",
      "\t\talpha: 0.8510000000000001\n",
      "\t\tlambda: 33.84509206547516\n",
      "\t\tmax_bin: 359\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.714082    0.725363    0.713134    0.688499   \n",
      "1                    TP  328.000000  331.000000  311.000000  319.000000   \n",
      "2                    TN  186.000000  187.000000  194.000000  183.000000   \n",
      "3                    FP   41.000000   36.000000   59.000000   59.000000   \n",
      "4                    FN   40.000000   41.000000   31.000000   34.000000   \n",
      "5              Accuracy    0.863866    0.870588    0.848739    0.843697   \n",
      "6             Precision    0.888889    0.901907    0.840541    0.843915   \n",
      "7           Sensitivity    0.891304    0.889785    0.909357    0.903683   \n",
      "8           Specificity    0.819400    0.838600    0.766800    0.756200   \n",
      "9              F1 score    0.890095    0.895805    0.873596    0.872777   \n",
      "10  F1 score (weighted)    0.863808    0.870868    0.847283    0.842114   \n",
      "11     F1 score (macro)    0.855644    0.862537    0.842655    0.835081   \n",
      "12    Balanced Accuracy    0.855344    0.864175    0.838078    0.829941   \n",
      "13                  MCC    0.711292    0.725210    0.689331    0.673418   \n",
      "14                  NPV    0.823000    0.820200    0.862200    0.843300   \n",
      "15              ROC_AUC    0.855344    0.864175    0.838078    0.829941   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.704908    0.706308    0.715518    0.695919    0.720062  \n",
      "1   339.000000  322.000000  331.000000  314.000000  324.000000  \n",
      "2   167.000000  185.000000  172.000000  179.000000  176.000000  \n",
      "3    57.000000   47.000000   56.000000   56.000000   45.000000  \n",
      "4    32.000000   41.000000   36.000000   46.000000   50.000000  \n",
      "5     0.850420    0.852101    0.845378    0.828571    0.840336  \n",
      "6     0.856061    0.872629    0.855297    0.848649    0.878049  \n",
      "7     0.913747    0.887052    0.901907    0.872222    0.866310  \n",
      "8     0.745500    0.797400    0.754400    0.761700    0.796400  \n",
      "9     0.883963    0.879781    0.877984    0.860274    0.872140  \n",
      "10    0.848438    0.851738    0.843882    0.827882    0.840692  \n",
      "11    0.836781    0.843821    0.833487    0.819267    0.829806  \n",
      "12    0.829641    0.842233    0.828147    0.816962    0.831345  \n",
      "13    0.677031    0.687831    0.669131    0.639044    0.659743  \n",
      "14    0.839200    0.818600    0.826900    0.795600    0.778800  \n",
      "15    0.829641    0.842233    0.828147    0.816962    0.831345  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "y_pred_xgb_8_cat = np.where((y_pred_xgb_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:58:41,140] Trial 450 finished with value: -13.124903902163734 and parameters: {'n_estimators': 690, 'eta': 0.000635266558754434, 'max_depth': 5, 'alpha': 0.8143, 'lambda': 30.286182070819173, 'max_bin': 338}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:58:56,771] Trial 451 finished with value: 0.6947371307753858 and parameters: {'n_estimators': 724, 'eta': 0.07392526659366973, 'max_depth': 6, 'alpha': 0.7664000000000001, 'lambda': 32.63826290881803, 'max_bin': 355}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:59:06,652] Trial 452 finished with value: 0.6915596500554901 and parameters: {'n_estimators': 656, 'eta': 0.09402243890292487, 'max_depth': 6, 'alpha': 0.7832, 'lambda': 33.95549760100094, 'max_bin': 369}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:59:19,167] Trial 453 finished with value: 0.6935838915705336 and parameters: {'n_estimators': 793, 'eta': 0.07597070282189826, 'max_depth': 6, 'alpha': 0.799, 'lambda': 24.713291398343014, 'max_bin': 381}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:59:29,723] Trial 454 finished with value: 0.691600353247209 and parameters: {'n_estimators': 554, 'eta': 0.08281756094723473, 'max_depth': 6, 'alpha': 0.1825, 'lambda': 31.30856635483399, 'max_bin': 374}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:59:41,404] Trial 455 finished with value: 0.6938301593233848 and parameters: {'n_estimators': 747, 'eta': 0.0792541657800341, 'max_depth': 6, 'alpha': 0.8427, 'lambda': 35.0079982558943, 'max_bin': 294}. Best is trial 151 with value: 0.6975781740455131.\n",
      "[I 2023-12-11 23:59:54,942] Trial 456 finished with value: 0.6979647969051316 and parameters: {'n_estimators': 703, 'eta': 0.07178368700458576, 'max_depth': 6, 'alpha': 0.9033, 'lambda': 36.557853720715386, 'max_bin': 360}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:00:08,859] Trial 457 finished with value: 0.5917634050762353 and parameters: {'n_estimators': 673, 'eta': 0.007566105805092864, 'max_depth': 6, 'alpha': 0.9022, 'lambda': 38.59848275066035, 'max_bin': 358}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:00:20,630] Trial 458 finished with value: 0.6921640053381963 and parameters: {'n_estimators': 703, 'eta': 0.07182430284817336, 'max_depth': 6, 'alpha': 0.9344, 'lambda': 18.346757805052075, 'max_bin': 398}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:00:34,681] Trial 459 finished with value: 0.6933354241263399 and parameters: {'n_estimators': 682, 'eta': 0.06976828118011755, 'max_depth': 6, 'alpha': 0.93, 'lambda': 32.219386069028566, 'max_bin': 351}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:00:50,660] Trial 460 finished with value: 0.6920916010947664 and parameters: {'n_estimators': 703, 'eta': 0.05341026791426008, 'max_depth': 6, 'alpha': 0.9021, 'lambda': 36.99548589675069, 'max_bin': 344}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:01:05,266] Trial 461 finished with value: 0.6926893097891055 and parameters: {'n_estimators': 660, 'eta': 0.06732418019171821, 'max_depth': 6, 'alpha': 0.8848, 'lambda': 37.74673494468245, 'max_bin': 366}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:01:17,148] Trial 462 finished with value: 0.6921700549192218 and parameters: {'n_estimators': 642, 'eta': 0.07310606378868577, 'max_depth': 6, 'alpha': 0.8715, 'lambda': 36.31413202900193, 'max_bin': 360}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:01:31,330] Trial 463 finished with value: 0.692872546283237 and parameters: {'n_estimators': 696, 'eta': 0.056972364091358224, 'max_depth': 6, 'alpha': 0.9056000000000001, 'lambda': 33.18207298545073, 'max_bin': 387}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:01:42,548] Trial 464 finished with value: 0.6946838384522442 and parameters: {'n_estimators': 672, 'eta': 0.08138301568070065, 'max_depth': 6, 'alpha': 0.8669, 'lambda': 16.31928821047515, 'max_bin': 353}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:01:56,176] Trial 465 finished with value: 0.6929989270817047 and parameters: {'n_estimators': 771, 'eta': 0.08747175551283329, 'max_depth': 6, 'alpha': 0.7523000000000001, 'lambda': 33.85754835592769, 'max_bin': 371}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:02:10,133] Trial 466 finished with value: 0.694839231562198 and parameters: {'n_estimators': 715, 'eta': 0.07474794232771896, 'max_depth': 6, 'alpha': 0.9149, 'lambda': 35.718627375713815, 'max_bin': 378}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:02:22,481] Trial 467 finished with value: 0.692753230928671 and parameters: {'n_estimators': 684, 'eta': 0.07759495190138521, 'max_depth': 6, 'alpha': 0.8159000000000001, 'lambda': 33.022170104612364, 'max_bin': 363}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:02:32,737] Trial 468 finished with value: 0.6900802713219145 and parameters: {'n_estimators': 735, 'eta': 0.08311411136046995, 'max_depth': 6, 'alpha': 0.8889, 'lambda': 8.272851563928377, 'max_bin': 358}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:02:45,926] Trial 469 finished with value: 0.6950181706323942 and parameters: {'n_estimators': 629, 'eta': 0.08005022163305016, 'max_depth': 6, 'alpha': 0.8515, 'lambda': 34.318203980218485, 'max_bin': 339}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:03:01,031] Trial 470 finished with value: 0.6841038972885582 and parameters: {'n_estimators': 631, 'eta': 0.03935488383902412, 'max_depth': 6, 'alpha': 0.8497, 'lambda': 34.86027883582131, 'max_bin': 368}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:03:12,859] Trial 471 finished with value: 0.6941165339718497 and parameters: {'n_estimators': 619, 'eta': 0.08019216699143575, 'max_depth': 6, 'alpha': 0.871, 'lambda': 36.72580335066524, 'max_bin': 349}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:03:24,592] Trial 472 finished with value: 0.6697373875362194 and parameters: {'n_estimators': 597, 'eta': 0.03489197050309831, 'max_depth': 5, 'alpha': 0.8549, 'lambda': 34.28204660476869, 'max_bin': 394}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:03:35,298] Trial 473 finished with value: 0.6943898740737319 and parameters: {'n_estimators': 646, 'eta': 0.08449719061823073, 'max_depth': 6, 'alpha': 0.8823000000000001, 'lambda': 21.656195155405378, 'max_bin': 375}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:03:48,551] Trial 474 finished with value: 0.6941810420965532 and parameters: {'n_estimators': 660, 'eta': 0.08156246452939246, 'max_depth': 6, 'alpha': 0.9429000000000001, 'lambda': 35.73321032156041, 'max_bin': 356}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:04:01,954] Trial 475 finished with value: 0.6945680721999145 and parameters: {'n_estimators': 611, 'eta': 0.07874134164577327, 'max_depth': 7, 'alpha': 0.8337, 'lambda': 33.96134503755815, 'max_bin': 384}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:04:16,041] Trial 476 finished with value: 0.689251321514958 and parameters: {'n_estimators': 637, 'eta': 0.04269780342907382, 'max_depth': 6, 'alpha': 0.8582000000000001, 'lambda': 17.214696639095735, 'max_bin': 363}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:04:31,155] Trial 477 finished with value: 0.6947632122197147 and parameters: {'n_estimators': 653, 'eta': 0.06246921985756445, 'max_depth': 6, 'alpha': 0.8344, 'lambda': 26.30210390874442, 'max_bin': 369}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:04:43,256] Trial 478 finished with value: 0.6953044860087851 and parameters: {'n_estimators': 674, 'eta': 0.08039762808336576, 'max_depth': 6, 'alpha': 0.9202, 'lambda': 34.85877140007968, 'max_bin': 379}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:04:55,337] Trial 479 finished with value: 0.69225711684781 and parameters: {'n_estimators': 667, 'eta': 0.08565943152998219, 'max_depth': 6, 'alpha': 0.9049, 'lambda': 38.4674717593571, 'max_bin': 380}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:05:06,880] Trial 480 finished with value: 0.6925418869900605 and parameters: {'n_estimators': 629, 'eta': 0.07737382852033836, 'max_depth': 6, 'alpha': 0.9187000000000001, 'lambda': 22.672917005116815, 'max_bin': 385}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:05:19,240] Trial 481 finished with value: 0.6952577508338904 and parameters: {'n_estimators': 648, 'eta': 0.08323424544050939, 'max_depth': 6, 'alpha': 0.9203, 'lambda': 33.49268592560358, 'max_bin': 377}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:05:30,652] Trial 482 finished with value: 0.6881449221889905 and parameters: {'n_estimators': 623, 'eta': 0.0838147979800242, 'max_depth': 6, 'alpha': 0.8969, 'lambda': 25.313119143469347, 'max_bin': 389}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:05:44,033] Trial 483 finished with value: 0.6943966918890084 and parameters: {'n_estimators': 676, 'eta': 0.08839700708527555, 'max_depth': 7, 'alpha': 0.9532, 'lambda': 34.70233812294138, 'max_bin': 379}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:05:56,867] Trial 484 finished with value: 0.6942351262102255 and parameters: {'n_estimators': 651, 'eta': 0.08226431136717702, 'max_depth': 6, 'alpha': 0.9336000000000001, 'lambda': 33.61712647124763, 'max_bin': 392}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:06:11,567] Trial 485 finished with value: 0.6977725461661493 and parameters: {'n_estimators': 683, 'eta': 0.08568181481723963, 'max_depth': 6, 'alpha': 0.9524, 'lambda': 35.27576380949072, 'max_bin': 380}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:06:23,567] Trial 486 finished with value: 0.6914152762394921 and parameters: {'n_estimators': 687, 'eta': 0.09043524413829677, 'max_depth': 6, 'alpha': 0.9829, 'lambda': 27.94927054773745, 'max_bin': 382}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:06:34,457] Trial 487 finished with value: 0.6920252832454453 and parameters: {'n_estimators': 689, 'eta': 0.08571084663123603, 'max_depth': 5, 'alpha': 0.9469000000000001, 'lambda': 23.866552113061292, 'max_bin': 385}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:06:47,432] Trial 488 finished with value: 0.6946701418236677 and parameters: {'n_estimators': 671, 'eta': 0.08733839117402331, 'max_depth': 6, 'alpha': 0.9546, 'lambda': 35.04439240774505, 'max_bin': 388}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:06:57,190] Trial 489 finished with value: 0.6905765303417708 and parameters: {'n_estimators': 399, 'eta': 0.08449936450982015, 'max_depth': 6, 'alpha': 0.9382, 'lambda': 36.160054305328856, 'max_bin': 377}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:07:10,734] Trial 490 finished with value: 0.6949778030589312 and parameters: {'n_estimators': 702, 'eta': 0.08281931860801137, 'max_depth': 6, 'alpha': 0.9363, 'lambda': 37.40154738635057, 'max_bin': 380}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:07:24,044] Trial 491 finished with value: 0.6948998309336991 and parameters: {'n_estimators': 677, 'eta': 0.08099230584976556, 'max_depth': 6, 'alpha': 0.926, 'lambda': 35.723090680737194, 'max_bin': 373}. Best is trial 456 with value: 0.6979647969051316.\n",
      "[I 2023-12-12 00:07:35,446] Trial 492 finished with value: 0.6999906456694454 and parameters: {'n_estimators': 714, 'eta': 0.08662185348353768, 'max_depth': 6, 'alpha': 0.9122, 'lambda': 14.275291688204845, 'max_bin': 376}. Best is trial 492 with value: 0.6999906456694454.\n",
      "[I 2023-12-12 00:07:42,108] Trial 493 finished with value: 0.6864148919607433 and parameters: {'n_estimators': 267, 'eta': 0.08942258894641808, 'max_depth': 6, 'alpha': 0.9205000000000001, 'lambda': 12.362062467231997, 'max_bin': 382}. Best is trial 492 with value: 0.6999906456694454.\n",
      "[I 2023-12-12 00:07:51,992] Trial 494 finished with value: 0.688586576417843 and parameters: {'n_estimators': 717, 'eta': 0.08548313412577058, 'max_depth': 6, 'alpha': 0.9201, 'lambda': 9.685829116216386, 'max_bin': 376}. Best is trial 492 with value: 0.6999906456694454.\n",
      "[I 2023-12-12 00:08:03,318] Trial 495 finished with value: 0.6976929059322263 and parameters: {'n_estimators': 711, 'eta': 0.08745502470304724, 'max_depth': 6, 'alpha': 0.9665, 'lambda': 14.571791725690506, 'max_bin': 387}. Best is trial 492 with value: 0.6999906456694454.\n",
      "[I 2023-12-12 00:08:13,596] Trial 496 finished with value: 0.695859137091168 and parameters: {'n_estimators': 713, 'eta': 0.08871822138654274, 'max_depth': 6, 'alpha': 0.9773000000000001, 'lambda': 13.752352149093756, 'max_bin': 392}. Best is trial 492 with value: 0.6999906456694454.\n",
      "[I 2023-12-12 00:08:24,883] Trial 497 finished with value: 0.6949315196872543 and parameters: {'n_estimators': 720, 'eta': 0.08765036423163362, 'max_depth': 6, 'alpha': 0.9994000000000001, 'lambda': 13.657536392243134, 'max_bin': 398}. Best is trial 492 with value: 0.6999906456694454.\n",
      "[I 2023-12-12 00:08:35,230] Trial 498 finished with value: 0.6921421663233944 and parameters: {'n_estimators': 711, 'eta': 0.08776153875189058, 'max_depth': 6, 'alpha': 0.9619000000000001, 'lambda': 13.395921076529923, 'max_bin': 395}. Best is trial 492 with value: 0.6999906456694454.\n",
      "[I 2023-12-12 00:08:44,474] Trial 499 finished with value: 0.6941193365792285 and parameters: {'n_estimators': 732, 'eta': 0.09038288443864073, 'max_depth': 6, 'alpha': 0.9780000000000001, 'lambda': 15.534281617993877, 'max_bin': 400}. Best is trial 492 with value: 0.6999906456694454.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7000\n",
      "\tBest params:\n",
      "\t\tn_estimators: 714\n",
      "\t\teta: 0.08662185348353768\n",
      "\t\tmax_depth: 6\n",
      "\t\talpha: 0.9122\n",
      "\t\tlambda: 14.275291688204845\n",
      "\t\tmax_bin: 376\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.714082    0.725363    0.713134    0.688499   \n",
      "1                    TP  328.000000  331.000000  311.000000  319.000000   \n",
      "2                    TN  186.000000  187.000000  194.000000  183.000000   \n",
      "3                    FP   41.000000   36.000000   59.000000   59.000000   \n",
      "4                    FN   40.000000   41.000000   31.000000   34.000000   \n",
      "5              Accuracy    0.863866    0.870588    0.848739    0.843697   \n",
      "6             Precision    0.888889    0.901907    0.840541    0.843915   \n",
      "7           Sensitivity    0.891304    0.889785    0.909357    0.903683   \n",
      "8           Specificity    0.819400    0.838600    0.766800    0.756200   \n",
      "9              F1 score    0.890095    0.895805    0.873596    0.872777   \n",
      "10  F1 score (weighted)    0.863808    0.870868    0.847283    0.842114   \n",
      "11     F1 score (macro)    0.855644    0.862537    0.842655    0.835081   \n",
      "12    Balanced Accuracy    0.855344    0.864175    0.838078    0.829941   \n",
      "13                  MCC    0.711292    0.725210    0.689331    0.673418   \n",
      "14                  NPV    0.823000    0.820200    0.862200    0.843300   \n",
      "15              ROC_AUC    0.855344    0.864175    0.838078    0.829941   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.704908    0.706308    0.715518    0.695919    0.720062    0.710447  \n",
      "1   339.000000  322.000000  331.000000  314.000000  324.000000  329.000000  \n",
      "2   167.000000  185.000000  172.000000  179.000000  176.000000  171.000000  \n",
      "3    57.000000   47.000000   56.000000   56.000000   45.000000   56.000000  \n",
      "4    32.000000   41.000000   36.000000   46.000000   50.000000   39.000000  \n",
      "5     0.850420    0.852101    0.845378    0.828571    0.840336    0.840336  \n",
      "6     0.856061    0.872629    0.855297    0.848649    0.878049    0.854545  \n",
      "7     0.913747    0.887052    0.901907    0.872222    0.866310    0.894022  \n",
      "8     0.745500    0.797400    0.754400    0.761700    0.796400    0.753300  \n",
      "9     0.883963    0.879781    0.877984    0.860274    0.872140    0.873838  \n",
      "10    0.848438    0.851738    0.843882    0.827882    0.840692    0.839033  \n",
      "11    0.836781    0.843821    0.833487    0.819267    0.829806    0.828223  \n",
      "12    0.829641    0.842233    0.828147    0.816962    0.831345    0.823663  \n",
      "13    0.677031    0.687831    0.669131    0.639044    0.659743    0.657991  \n",
      "14    0.839200    0.818600    0.826900    0.795600    0.778800    0.814300  \n",
      "15    0.829641    0.842233    0.828147    0.816962    0.831345    0.823663  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_9_cat = np.where((y_pred_xgb_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.708270</td>\n",
       "      <td>0.030854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>164.400000</td>\n",
       "      <td>5.601587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>89.200000</td>\n",
       "      <td>6.762642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>5.731007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>6.019413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.853596</td>\n",
       "      <td>0.031440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.872116</td>\n",
       "      <td>0.027538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.895148</td>\n",
       "      <td>0.031817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.786420</td>\n",
       "      <td>0.051263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.883208</td>\n",
       "      <td>0.024770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.852882</td>\n",
       "      <td>0.031528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.843339</td>\n",
       "      <td>0.034124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.840784</td>\n",
       "      <td>0.034203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.688225</td>\n",
       "      <td>0.068038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.822970</td>\n",
       "      <td>0.051303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.840784</td>\n",
       "      <td>0.034203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.708270     0.030854\n",
       "1                    TP       164.400000     5.601587\n",
       "2                    TN        89.200000     6.762642\n",
       "3                    FP        24.200000     5.731007\n",
       "4                    FN        19.300000     6.019413\n",
       "5              Accuracy         0.853596     0.031440\n",
       "6             Precision         0.872116     0.027538\n",
       "7           Sensitivity         0.895148     0.031817\n",
       "8           Specificity         0.786420     0.051263\n",
       "9              F1 score         0.883208     0.024770\n",
       "10  F1 score (weighted)         0.852882     0.031528\n",
       "11     F1 score (macro)         0.843339     0.034124\n",
       "12    Balanced Accuracy         0.840784     0.034203\n",
       "13                  MCC         0.688225     0.068038\n",
       "14                  NPV         0.822970     0.051303\n",
       "15              ROC_AUC         0.840784     0.034203"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.714082</td>\n",
       "      <td>0.725363</td>\n",
       "      <td>0.713134</td>\n",
       "      <td>0.688499</td>\n",
       "      <td>0.704908</td>\n",
       "      <td>0.706308</td>\n",
       "      <td>0.715518</td>\n",
       "      <td>0.695919</td>\n",
       "      <td>0.720062</td>\n",
       "      <td>0.710447</td>\n",
       "      <td>0.709424</td>\n",
       "      <td>0.011011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>324.800000</td>\n",
       "      <td>8.534896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>8.472177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>51.200000</td>\n",
       "      <td>8.270429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>6.018490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.863866</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.852101</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.848403</td>\n",
       "      <td>0.012036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.901907</td>\n",
       "      <td>0.840541</td>\n",
       "      <td>0.843915</td>\n",
       "      <td>0.856061</td>\n",
       "      <td>0.872629</td>\n",
       "      <td>0.855297</td>\n",
       "      <td>0.848649</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.864048</td>\n",
       "      <td>0.020401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.889785</td>\n",
       "      <td>0.909357</td>\n",
       "      <td>0.903683</td>\n",
       "      <td>0.913747</td>\n",
       "      <td>0.887052</td>\n",
       "      <td>0.901907</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.866310</td>\n",
       "      <td>0.894022</td>\n",
       "      <td>0.892939</td>\n",
       "      <td>0.015221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.819400</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.766800</td>\n",
       "      <td>0.756200</td>\n",
       "      <td>0.745500</td>\n",
       "      <td>0.797400</td>\n",
       "      <td>0.754400</td>\n",
       "      <td>0.761700</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0.753300</td>\n",
       "      <td>0.778970</td>\n",
       "      <td>0.031940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.890095</td>\n",
       "      <td>0.895805</td>\n",
       "      <td>0.873596</td>\n",
       "      <td>0.872777</td>\n",
       "      <td>0.883963</td>\n",
       "      <td>0.879781</td>\n",
       "      <td>0.877984</td>\n",
       "      <td>0.860274</td>\n",
       "      <td>0.872140</td>\n",
       "      <td>0.873838</td>\n",
       "      <td>0.878025</td>\n",
       "      <td>0.010081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.863808</td>\n",
       "      <td>0.870868</td>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.842114</td>\n",
       "      <td>0.848438</td>\n",
       "      <td>0.851738</td>\n",
       "      <td>0.843882</td>\n",
       "      <td>0.827882</td>\n",
       "      <td>0.840692</td>\n",
       "      <td>0.839033</td>\n",
       "      <td>0.847574</td>\n",
       "      <td>0.012364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.855644</td>\n",
       "      <td>0.862537</td>\n",
       "      <td>0.842655</td>\n",
       "      <td>0.835081</td>\n",
       "      <td>0.836781</td>\n",
       "      <td>0.843821</td>\n",
       "      <td>0.833487</td>\n",
       "      <td>0.819267</td>\n",
       "      <td>0.829806</td>\n",
       "      <td>0.828223</td>\n",
       "      <td>0.838730</td>\n",
       "      <td>0.012942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.855344</td>\n",
       "      <td>0.864175</td>\n",
       "      <td>0.838078</td>\n",
       "      <td>0.829941</td>\n",
       "      <td>0.829641</td>\n",
       "      <td>0.842233</td>\n",
       "      <td>0.828147</td>\n",
       "      <td>0.816962</td>\n",
       "      <td>0.831345</td>\n",
       "      <td>0.823663</td>\n",
       "      <td>0.835953</td>\n",
       "      <td>0.014477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.711292</td>\n",
       "      <td>0.725210</td>\n",
       "      <td>0.689331</td>\n",
       "      <td>0.673418</td>\n",
       "      <td>0.677031</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>0.639044</td>\n",
       "      <td>0.659743</td>\n",
       "      <td>0.657991</td>\n",
       "      <td>0.679002</td>\n",
       "      <td>0.025612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.820200</td>\n",
       "      <td>0.862200</td>\n",
       "      <td>0.843300</td>\n",
       "      <td>0.839200</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.826900</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.778800</td>\n",
       "      <td>0.814300</td>\n",
       "      <td>0.822210</td>\n",
       "      <td>0.023635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.855344</td>\n",
       "      <td>0.864175</td>\n",
       "      <td>0.838078</td>\n",
       "      <td>0.829941</td>\n",
       "      <td>0.829641</td>\n",
       "      <td>0.842233</td>\n",
       "      <td>0.828147</td>\n",
       "      <td>0.816962</td>\n",
       "      <td>0.831345</td>\n",
       "      <td>0.823663</td>\n",
       "      <td>0.835953</td>\n",
       "      <td>0.014477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.714082    0.725363    0.713134    0.688499   \n",
       "1                    TP  328.000000  331.000000  311.000000  319.000000   \n",
       "2                    TN  186.000000  187.000000  194.000000  183.000000   \n",
       "3                    FP   41.000000   36.000000   59.000000   59.000000   \n",
       "4                    FN   40.000000   41.000000   31.000000   34.000000   \n",
       "5              Accuracy    0.863866    0.870588    0.848739    0.843697   \n",
       "6             Precision    0.888889    0.901907    0.840541    0.843915   \n",
       "7           Sensitivity    0.891304    0.889785    0.909357    0.903683   \n",
       "8           Specificity    0.819400    0.838600    0.766800    0.756200   \n",
       "9              F1 score    0.890095    0.895805    0.873596    0.872777   \n",
       "10  F1 score (weighted)    0.863808    0.870868    0.847283    0.842114   \n",
       "11     F1 score (macro)    0.855644    0.862537    0.842655    0.835081   \n",
       "12    Balanced Accuracy    0.855344    0.864175    0.838078    0.829941   \n",
       "13                  MCC    0.711292    0.725210    0.689331    0.673418   \n",
       "14                  NPV    0.823000    0.820200    0.862200    0.843300   \n",
       "15              ROC_AUC    0.855344    0.864175    0.838078    0.829941   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.704908    0.706308    0.715518    0.695919    0.720062    0.710447   \n",
       "1   339.000000  322.000000  331.000000  314.000000  324.000000  329.000000   \n",
       "2   167.000000  185.000000  172.000000  179.000000  176.000000  171.000000   \n",
       "3    57.000000   47.000000   56.000000   56.000000   45.000000   56.000000   \n",
       "4    32.000000   41.000000   36.000000   46.000000   50.000000   39.000000   \n",
       "5     0.850420    0.852101    0.845378    0.828571    0.840336    0.840336   \n",
       "6     0.856061    0.872629    0.855297    0.848649    0.878049    0.854545   \n",
       "7     0.913747    0.887052    0.901907    0.872222    0.866310    0.894022   \n",
       "8     0.745500    0.797400    0.754400    0.761700    0.796400    0.753300   \n",
       "9     0.883963    0.879781    0.877984    0.860274    0.872140    0.873838   \n",
       "10    0.848438    0.851738    0.843882    0.827882    0.840692    0.839033   \n",
       "11    0.836781    0.843821    0.833487    0.819267    0.829806    0.828223   \n",
       "12    0.829641    0.842233    0.828147    0.816962    0.831345    0.823663   \n",
       "13    0.677031    0.687831    0.669131    0.639044    0.659743    0.657991   \n",
       "14    0.839200    0.818600    0.826900    0.795600    0.778800    0.814300   \n",
       "15    0.829641    0.842233    0.828147    0.816962    0.831345    0.823663   \n",
       "\n",
       "           ave       std  \n",
       "0     0.709424  0.011011  \n",
       "1   324.800000  8.534896  \n",
       "2   180.000000  8.472177  \n",
       "3    51.200000  8.270429  \n",
       "4    39.000000  6.018490  \n",
       "5     0.848403  0.012036  \n",
       "6     0.864048  0.020401  \n",
       "7     0.892939  0.015221  \n",
       "8     0.778970  0.031940  \n",
       "9     0.878025  0.010081  \n",
       "10    0.847574  0.012364  \n",
       "11    0.838730  0.012942  \n",
       "12    0.835953  0.014477  \n",
       "13    0.679002  0.025612  \n",
       "14    0.822210  0.023635  \n",
       "15    0.835953  0.014477  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_xgb0</th>\n",
       "      <th>y_pred_xgb1</th>\n",
       "      <th>y_pred_xgb2</th>\n",
       "      <th>y_pred_xgb3</th>\n",
       "      <th>y_pred_xgb4</th>\n",
       "      <th>y_pred_xgb_ave</th>\n",
       "      <th>y_pred_xgb_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2047687</td>\n",
       "      <td>0</td>\n",
       "      <td>5.48</td>\n",
       "      <td>6.082468</td>\n",
       "      <td>6.025534</td>\n",
       "      <td>5.981858</td>\n",
       "      <td>6.136372</td>\n",
       "      <td>5.973534</td>\n",
       "      <td>5.946628</td>\n",
       "      <td>0.216174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1164212</td>\n",
       "      <td>1</td>\n",
       "      <td>5.76</td>\n",
       "      <td>4.686630</td>\n",
       "      <td>4.841517</td>\n",
       "      <td>4.711684</td>\n",
       "      <td>4.786704</td>\n",
       "      <td>4.935638</td>\n",
       "      <td>4.953696</td>\n",
       "      <td>0.369869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2337873</td>\n",
       "      <td>2</td>\n",
       "      <td>6.07</td>\n",
       "      <td>6.233112</td>\n",
       "      <td>6.027457</td>\n",
       "      <td>6.164878</td>\n",
       "      <td>6.154288</td>\n",
       "      <td>6.288994</td>\n",
       "      <td>6.156455</td>\n",
       "      <td>0.089101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4577419</td>\n",
       "      <td>3</td>\n",
       "      <td>7.90</td>\n",
       "      <td>7.604960</td>\n",
       "      <td>7.560972</td>\n",
       "      <td>7.607971</td>\n",
       "      <td>7.574067</td>\n",
       "      <td>7.627919</td>\n",
       "      <td>7.645982</td>\n",
       "      <td>0.115742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL515452</td>\n",
       "      <td>4</td>\n",
       "      <td>6.22</td>\n",
       "      <td>6.680586</td>\n",
       "      <td>6.535979</td>\n",
       "      <td>6.930906</td>\n",
       "      <td>6.835907</td>\n",
       "      <td>6.753607</td>\n",
       "      <td>6.659497</td>\n",
       "      <td>0.231840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL3693800</td>\n",
       "      <td>2966</td>\n",
       "      <td>8.22</td>\n",
       "      <td>8.234132</td>\n",
       "      <td>8.180608</td>\n",
       "      <td>8.260927</td>\n",
       "      <td>8.164945</td>\n",
       "      <td>8.180732</td>\n",
       "      <td>8.206891</td>\n",
       "      <td>0.034079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL2431917</td>\n",
       "      <td>2967</td>\n",
       "      <td>7.46</td>\n",
       "      <td>7.242373</td>\n",
       "      <td>7.143218</td>\n",
       "      <td>7.284688</td>\n",
       "      <td>7.334032</td>\n",
       "      <td>7.127889</td>\n",
       "      <td>7.265367</td>\n",
       "      <td>0.113519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL2413298</td>\n",
       "      <td>2968</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.062501</td>\n",
       "      <td>5.891378</td>\n",
       "      <td>6.170100</td>\n",
       "      <td>5.952351</td>\n",
       "      <td>5.935059</td>\n",
       "      <td>6.045231</td>\n",
       "      <td>0.133199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3656016</td>\n",
       "      <td>2969</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.094202</td>\n",
       "      <td>8.107736</td>\n",
       "      <td>8.025042</td>\n",
       "      <td>7.927164</td>\n",
       "      <td>7.985439</td>\n",
       "      <td>8.109930</td>\n",
       "      <td>0.193441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL4643138</td>\n",
       "      <td>2970</td>\n",
       "      <td>6.38</td>\n",
       "      <td>6.397174</td>\n",
       "      <td>6.377121</td>\n",
       "      <td>6.309494</td>\n",
       "      <td>6.261768</td>\n",
       "      <td>6.279437</td>\n",
       "      <td>6.334166</td>\n",
       "      <td>0.052854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_xgb0  y_pred_xgb1  \\\n",
       "0         CHEMBL2047687            0     5.48     6.082468     6.025534   \n",
       "1         CHEMBL1164212            1     5.76     4.686630     4.841517   \n",
       "2         CHEMBL2337873            2     6.07     6.233112     6.027457   \n",
       "3         CHEMBL4577419            3     7.90     7.604960     7.560972   \n",
       "4          CHEMBL515452            4     6.22     6.680586     6.535979   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "2966      CHEMBL3693800         2966     8.22     8.234132     8.180608   \n",
       "2967      CHEMBL2431917         2967     7.46     7.242373     7.143218   \n",
       "2968      CHEMBL2413298         2968     6.26     6.062501     5.891378   \n",
       "2969      CHEMBL3656016         2969     8.52     8.094202     8.107736   \n",
       "2970      CHEMBL4643138         2970     6.38     6.397174     6.377121   \n",
       "\n",
       "      y_pred_xgb2  y_pred_xgb3  y_pred_xgb4  y_pred_xgb_ave  y_pred_xgb_std  \n",
       "0        5.981858     6.136372     5.973534        5.946628        0.216174  \n",
       "1        4.711684     4.786704     4.935638        4.953696        0.369869  \n",
       "2        6.164878     6.154288     6.288994        6.156455        0.089101  \n",
       "3        7.607971     7.574067     7.627919        7.645982        0.115742  \n",
       "4        6.930906     6.835907     6.753607        6.659497        0.231840  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "2966     8.260927     8.164945     8.180732        8.206891        0.034079  \n",
       "2967     7.284688     7.334032     7.127889        7.265367        0.113519  \n",
       "2968     6.170100     5.952351     5.935059        6.045231        0.133199  \n",
       "2969     8.025042     7.927164     7.985439        8.109930        0.193441  \n",
       "2970     6.309494     6.261768     6.279437        6.334166        0.052854  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where((y_pred_optimized_xgb >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id, xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "xgb_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6635 with a standard deviation of 0.0450\n",
      "XGBoost optimized model r2_score 0.7038 with a standard deviation of 0.0450\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb.joblib']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"OUTPUT/xgb_reg.joblib\")\n",
    "#joblib.dump(optimized_xgb, \"OUTPUT/optimized_xgb.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.647056     0.045117\n",
      "1                    TP       164.800000     4.917090\n",
      "2                    TN        83.400000     6.077280\n",
      "3                    FP        30.000000     6.480741\n",
      "4                    FN        18.900000     4.532598\n",
      "5              Accuracy         0.835418     0.023106\n",
      "6             Precision         0.846747     0.029393\n",
      "7           Sensitivity         0.897213     0.023832\n",
      "8           Specificity         0.735840     0.055371\n",
      "9              F1 score         0.870839     0.017703\n",
      "10  F1 score (weighted)         0.833425     0.023783\n",
      "11     F1 score (macro)         0.821788     0.025695\n",
      "12    Balanced Accuracy         0.816532     0.027699\n",
      "13                  MCC         0.647830     0.051016\n",
      "14                  NPV         0.816470     0.038970\n",
      "15              ROC_AUC         0.816532     0.027699\n",
      "CPU times: user 2.93 s, sys: 10.5 s, total: 13.5 s\n",
      "Wall time: 665 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:16:31,398] A new study created in memory with name: KNNregressor\n",
      "[I 2023-12-12 00:16:31,761] Trial 0 finished with value: 0.5367313693190472 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 0 with value: 0.5367313693190472.\n",
      "[I 2023-12-12 00:16:31,969] Trial 1 finished with value: 0.5692816425165323 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 1 with value: 0.5692816425165323.\n",
      "[I 2023-12-12 00:16:32,476] Trial 2 finished with value: 0.5336867824216596 and parameters: {'n_neighbors': 23, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 1 with value: 0.5692816425165323.\n",
      "[I 2023-12-12 00:16:32,745] Trial 3 finished with value: 0.6116212712561784 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 60}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:33,069] Trial 4 finished with value: 0.552004600934799 and parameters: {'n_neighbors': 18, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 63}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:33,620] Trial 5 finished with value: 0.574948182817361 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:34,139] Trial 6 finished with value: 0.574948182817361 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:34,652] Trial 7 finished with value: 0.5257451488270459 and parameters: {'n_neighbors': 26, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:34,883] Trial 8 finished with value: 0.5366987686314266 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:35,177] Trial 9 finished with value: 0.6053701196900646 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:35,507] Trial 10 finished with value: 0.6116212712561784 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 48}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:35,847] Trial 11 finished with value: 0.6116212712561784 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:36,193] Trial 12 finished with value: 0.5846527930519528 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 51}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:36,481] Trial 13 finished with value: 0.6098605927632901 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:36,812] Trial 14 finished with value: 0.5753733895727976 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:37,150] Trial 15 finished with value: 0.5993835105972649 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:37,510] Trial 16 finished with value: 0.5713316310805694 and parameters: {'n_neighbors': 14, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 61}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:37,925] Trial 17 finished with value: 0.6116212712561784 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:38,261] Trial 18 finished with value: 0.5672906919662025 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 56}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:38,593] Trial 19 finished with value: 0.593108778510646 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:38,931] Trial 20 finished with value: 0.5846527930519528 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:39,242] Trial 21 finished with value: 0.6116212712561784 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 54}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:39,462] Trial 22 finished with value: 0.602841071698563 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:39,680] Trial 23 finished with value: 0.5846527930519528 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 65}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:39,920] Trial 24 finished with value: 0.6116212712561784 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:40,142] Trial 25 finished with value: 0.5993835105972649 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 3 with value: 0.6116212712561784.\n",
      "[I 2023-12-12 00:16:40,361] Trial 26 finished with value: 0.6127418017166176 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 57}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:40,590] Trial 27 finished with value: 0.5776459947844321 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 58}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:40,957] Trial 28 finished with value: 0.6023028820862183 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 77}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:41,522] Trial 29 finished with value: 0.5870887727309645 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:41,908] Trial 30 finished with value: 0.6127418017166176 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 59}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:42,236] Trial 31 finished with value: 0.6127418017166176 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 59}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:42,506] Trial 32 finished with value: 0.6127418017166176 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 59}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:42,817] Trial 33 finished with value: 0.5902840569813292 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 55}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:43,079] Trial 34 finished with value: 0.6127418017166176 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 68}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:43,347] Trial 35 finished with value: 0.6127418017166176 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 61}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:43,616] Trial 36 finished with value: 0.6053701196900646 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 76}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:43,884] Trial 37 finished with value: 0.5902840569813292 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:44,194] Trial 38 finished with value: 0.5611129670915218 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:44,459] Trial 39 finished with value: 0.6023028820862183 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:44,803] Trial 40 finished with value: 0.6127418017166176 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 59}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:45,152] Trial 41 finished with value: 0.6127418017166176 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:45,504] Trial 42 finished with value: 0.6101555776616007 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:45,837] Trial 43 finished with value: 0.6023028820862183 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 53}. Best is trial 26 with value: 0.6127418017166176.\n",
      "[I 2023-12-12 00:16:46,129] Trial 44 finished with value: 0.6170768049675897 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 44 with value: 0.6170768049675897.\n",
      "[I 2023-12-12 00:16:46,675] Trial 45 finished with value: 0.6240882702021777 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 45 with value: 0.6240882702021777.\n",
      "[I 2023-12-12 00:16:47,181] Trial 46 finished with value: 0.6240882702021777 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 45 with value: 0.6240882702021777.\n",
      "[I 2023-12-12 00:16:47,699] Trial 47 finished with value: 0.6240882702021777 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 45 with value: 0.6240882702021777.\n",
      "[I 2023-12-12 00:16:48,215] Trial 48 finished with value: 0.6240882702021777 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 45 with value: 0.6240882702021777.\n",
      "[I 2023-12-12 00:16:48,736] Trial 49 finished with value: 0.5650127950058721 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 45 with value: 0.6240882702021777.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6241\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 63\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.643294\n",
      "1                    TP  331.000000\n",
      "2                    TN  175.000000\n",
      "3                    FP   52.000000\n",
      "4                    FN   37.000000\n",
      "5              Accuracy    0.850420\n",
      "6             Precision    0.864230\n",
      "7           Sensitivity    0.899457\n",
      "8           Specificity    0.770900\n",
      "9              F1 score    0.881491\n",
      "10  F1 score (weighted)    0.849359\n",
      "11     F1 score (macro)    0.839379\n",
      "12    Balanced Accuracy    0.835191\n",
      "13                  MCC    0.679973\n",
      "14                  NPV    0.825500\n",
      "15              ROC_AUC    0.835191\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_0_cat = np.where((y_pred_knn_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:16:49,393] Trial 50 finished with value: 0.5927535988037752 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 45 with value: 0.6240882702021777.\n",
      "[I 2023-12-12 00:16:49,925] Trial 51 finished with value: 0.6387997871460858 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:50,457] Trial 52 finished with value: 0.6387997871460858 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:50,988] Trial 53 finished with value: 0.6387997871460858 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:51,519] Trial 54 finished with value: 0.6372795397594865 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:52,048] Trial 55 finished with value: 0.6372795397594865 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:52,581] Trial 56 finished with value: 0.6271405088777542 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:53,111] Trial 57 finished with value: 0.6271405088777542 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:53,630] Trial 58 finished with value: 0.6271405088777542 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:54,157] Trial 59 finished with value: 0.6372795397594865 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:54,691] Trial 60 finished with value: 0.6372795397594865 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:55,211] Trial 61 finished with value: 0.6372795397594865 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:55,745] Trial 62 finished with value: 0.6372795397594865 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 51 with value: 0.6387997871460858.\n",
      "[I 2023-12-12 00:16:56,277] Trial 63 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:16:56,799] Trial 64 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:16:57,330] Trial 65 finished with value: 0.6352779469934685 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:16:57,864] Trial 66 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:16:58,392] Trial 67 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:16:58,914] Trial 68 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:16:59,438] Trial 69 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:16:59,948] Trial 70 finished with value: 0.6289270437803843 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:00,428] Trial 71 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:00,950] Trial 72 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:01,480] Trial 73 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:02,014] Trial 74 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:02,547] Trial 75 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:03,046] Trial 76 finished with value: 0.6329220348006623 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:03,517] Trial 77 finished with value: 0.6289270437803843 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:03,996] Trial 78 finished with value: 0.6101192109179484 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:04,477] Trial 79 finished with value: 0.6329220348006623 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:04,921] Trial 80 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:05,444] Trial 81 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:05,983] Trial 82 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:06,487] Trial 83 finished with value: 0.6329220348006623 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:07,005] Trial 84 finished with value: 0.6403210818033198 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 63 with value: 0.6403210818033198.\n",
      "[I 2023-12-12 00:17:07,525] Trial 85 finished with value: 0.6419118091617892 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:08,051] Trial 86 finished with value: 0.5859427458536137 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:08,579] Trial 87 finished with value: 0.6352779469934685 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:09,108] Trial 88 finished with value: 0.6329220348006623 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:09,633] Trial 89 finished with value: 0.616047427413877 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:10,162] Trial 90 finished with value: 0.6204269198073857 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:10,682] Trial 91 finished with value: 0.6419118091617892 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:11,210] Trial 92 finished with value: 0.6419118091617892 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:11,730] Trial 93 finished with value: 0.6419118091617892 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:12,257] Trial 94 finished with value: 0.6419118091617892 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:12,770] Trial 95 finished with value: 0.6419118091617892 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:13,287] Trial 96 finished with value: 0.6419118091617892 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:13,806] Trial 97 finished with value: 0.6352779469934685 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:14,330] Trial 98 finished with value: 0.6419118091617892 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:14,845] Trial 99 finished with value: 0.6419118091617892 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6419\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.643294    0.635706\n",
      "1                    TP  331.000000  324.000000\n",
      "2                    TN  175.000000  170.000000\n",
      "3                    FP   52.000000   53.000000\n",
      "4                    FN   37.000000   48.000000\n",
      "5              Accuracy    0.850420    0.830252\n",
      "6             Precision    0.864230    0.859416\n",
      "7           Sensitivity    0.899457    0.870968\n",
      "8           Specificity    0.770900    0.762300\n",
      "9              F1 score    0.881491    0.865154\n",
      "10  F1 score (weighted)    0.849359    0.829856\n",
      "11     F1 score (macro)    0.839379    0.818064\n",
      "12    Balanced Accuracy    0.835191    0.816650\n",
      "13                  MCC    0.679973    0.636259\n",
      "14                  NPV    0.825500    0.779800\n",
      "15              ROC_AUC    0.835191    0.816650\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_1_cat = np.where((y_pred_knn_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:17:15,490] Trial 100 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:16,021] Trial 101 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:16,552] Trial 102 finished with value: 0.6293258946280771 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:17,082] Trial 103 finished with value: 0.6256089909239044 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:17,611] Trial 104 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:18,137] Trial 105 finished with value: 0.6293258946280771 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:18,659] Trial 106 finished with value: 0.6117350066123806 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:19,175] Trial 107 finished with value: 0.6256089909239044 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:19,690] Trial 108 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:20,219] Trial 109 finished with value: 0.6293258946280771 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:20,750] Trial 110 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:21,272] Trial 111 finished with value: 0.6239921430556413 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:21,784] Trial 112 finished with value: 0.5931310737848797 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:22,299] Trial 113 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:22,822] Trial 114 finished with value: 0.6239921430556413 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:23,340] Trial 115 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:23,742] Trial 116 finished with value: 0.6193297168499106 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:24,287] Trial 117 finished with value: 0.6293258946280771 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:24,805] Trial 118 finished with value: 0.6293258946280771 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:25,330] Trial 119 finished with value: 0.6239921430556413 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:25,850] Trial 120 finished with value: 0.6039423252484443 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:26,346] Trial 121 finished with value: 0.6255492335206485 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:26,864] Trial 122 finished with value: 0.5954772885719641 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:27,390] Trial 123 finished with value: 0.6255492335206485 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:27,918] Trial 124 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:28,427] Trial 125 finished with value: 0.6293258946280771 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:28,937] Trial 126 finished with value: 0.6255492335206485 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:29,464] Trial 127 finished with value: 0.6239921430556413 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:29,996] Trial 128 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:30,528] Trial 129 finished with value: 0.6215390418642941 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:31,058] Trial 130 finished with value: 0.6239921430556413 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:31,583] Trial 131 finished with value: 0.6255492335206485 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:32,095] Trial 132 finished with value: 0.6255492335206485 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:32,597] Trial 133 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:33,129] Trial 134 finished with value: 0.6255492335206485 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:33,657] Trial 135 finished with value: 0.6293258946280771 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:34,098] Trial 136 finished with value: 0.5564794567434453 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:34,648] Trial 137 finished with value: 0.6293258946280771 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:35,178] Trial 138 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:35,675] Trial 139 finished with value: 0.6239921430556413 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:36,134] Trial 140 finished with value: 0.6255492335206485 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:36,636] Trial 141 finished with value: 0.6255492335206485 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:37,154] Trial 142 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:37,674] Trial 143 finished with value: 0.6255492335206485 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:38,177] Trial 144 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:38,702] Trial 145 finished with value: 0.6239921430556413 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:39,235] Trial 146 finished with value: 0.6117350066123806 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:39,770] Trial 147 finished with value: 0.6256089909239044 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:40,301] Trial 148 finished with value: 0.6276991420109073 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:40,833] Trial 149 finished with value: 0.6239921430556413 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6419\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.643294    0.635706    0.665874\n",
      "1                    TP  331.000000  324.000000  309.000000\n",
      "2                    TN  175.000000  170.000000  180.000000\n",
      "3                    FP   52.000000   53.000000   73.000000\n",
      "4                    FN   37.000000   48.000000   33.000000\n",
      "5              Accuracy    0.850420    0.830252    0.821849\n",
      "6             Precision    0.864230    0.859416    0.808901\n",
      "7           Sensitivity    0.899457    0.870968    0.903509\n",
      "8           Specificity    0.770900    0.762300    0.711500\n",
      "9              F1 score    0.881491    0.865154    0.853591\n",
      "10  F1 score (weighted)    0.849359    0.829856    0.819124\n",
      "11     F1 score (macro)    0.839379    0.818064    0.813062\n",
      "12    Balanced Accuracy    0.835191    0.816650    0.807486\n",
      "13                  MCC    0.679973    0.636259    0.634171\n",
      "14                  NPV    0.825500    0.779800    0.845100\n",
      "15              ROC_AUC    0.835191    0.816650    0.807486\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_2_cat = np.where((y_pred_knn_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:17:41,510] Trial 150 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:42,007] Trial 151 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:42,474] Trial 152 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:42,953] Trial 153 finished with value: 0.6319137191569496 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:43,436] Trial 154 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:43,958] Trial 155 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:44,483] Trial 156 finished with value: 0.639768080543323 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:44,979] Trial 157 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:45,507] Trial 158 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:45,931] Trial 159 finished with value: 0.6285259306272988 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:46,499] Trial 160 finished with value: 0.6381868131698831 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:47,028] Trial 161 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:47,560] Trial 162 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:48,094] Trial 163 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:48,625] Trial 164 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:49,160] Trial 165 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:49,681] Trial 166 finished with value: 0.639768080543323 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:50,214] Trial 167 finished with value: 0.6151289660650564 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:50,704] Trial 168 finished with value: 0.6319137191569496 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:51,222] Trial 169 finished with value: 0.6381868131698831 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:51,744] Trial 170 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:52,250] Trial 171 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:52,777] Trial 172 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:53,304] Trial 173 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:53,826] Trial 174 finished with value: 0.6381868131698831 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:54,348] Trial 175 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:54,750] Trial 176 finished with value: 0.6117517660377279 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:55,265] Trial 177 finished with value: 0.639768080543323 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:55,758] Trial 178 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:56,290] Trial 179 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:56,807] Trial 180 finished with value: 0.639768080543323 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:57,315] Trial 181 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:57,841] Trial 182 finished with value: 0.6381868131698831 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:58,363] Trial 183 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:58,884] Trial 184 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:59,409] Trial 185 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:17:59,943] Trial 186 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:00,459] Trial 187 finished with value: 0.6381868131698831 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:00,995] Trial 188 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:01,529] Trial 189 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:02,060] Trial 190 finished with value: 0.639768080543323 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:02,589] Trial 191 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:03,101] Trial 192 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:03,609] Trial 193 finished with value: 0.641031956290048 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:04,127] Trial 194 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:04,648] Trial 195 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:05,181] Trial 196 finished with value: 0.6381868131698831 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:05,520] Trial 197 finished with value: 0.6285259306272988 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:06,062] Trial 198 finished with value: 0.6381868131698831 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:06,579] Trial 199 finished with value: 0.6374309192535278 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6419\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.643294    0.635706    0.665874    0.646857\n",
      "1                    TP  331.000000  324.000000  309.000000  313.000000\n",
      "2                    TN  175.000000  170.000000  180.000000  176.000000\n",
      "3                    FP   52.000000   53.000000   73.000000   66.000000\n",
      "4                    FN   37.000000   48.000000   33.000000   40.000000\n",
      "5              Accuracy    0.850420    0.830252    0.821849    0.821849\n",
      "6             Precision    0.864230    0.859416    0.808901    0.825858\n",
      "7           Sensitivity    0.899457    0.870968    0.903509    0.886686\n",
      "8           Specificity    0.770900    0.762300    0.711500    0.727300\n",
      "9              F1 score    0.881491    0.865154    0.853591    0.855191\n",
      "10  F1 score (weighted)    0.849359    0.829856    0.819124    0.819956\n",
      "11     F1 score (macro)    0.839379    0.818064    0.813062    0.811875\n",
      "12    Balanced Accuracy    0.835191    0.816650    0.807486    0.806979\n",
      "13                  MCC    0.679973    0.636259    0.634171    0.627173\n",
      "14                  NPV    0.825500    0.779800    0.845100    0.814800\n",
      "15              ROC_AUC    0.835191    0.816650    0.807486    0.806979\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_3_cat = np.where((y_pred_knn_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:18:07,238] Trial 200 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:07,765] Trial 201 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:08,295] Trial 202 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:08,821] Trial 203 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:09,344] Trial 204 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:09,869] Trial 205 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:10,400] Trial 206 finished with value: 0.629258285186193 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:10,916] Trial 207 finished with value: 0.6152034036029205 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:11,447] Trial 208 finished with value: 0.636041670405654 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:11,823] Trial 209 finished with value: 0.5572275693562541 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:12,390] Trial 210 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:12,937] Trial 211 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:13,475] Trial 212 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:14,013] Trial 213 finished with value: 0.629258285186193 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:14,557] Trial 214 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:15,035] Trial 215 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:15,539] Trial 216 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:16,062] Trial 217 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:16,588] Trial 218 finished with value: 0.636041670405654 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:17,111] Trial 219 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:17,576] Trial 220 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:18,069] Trial 221 finished with value: 0.629258285186193 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:18,548] Trial 222 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:19,044] Trial 223 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:19,523] Trial 224 finished with value: 0.5786289190380648 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:20,059] Trial 225 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:20,595] Trial 226 finished with value: 0.629258285186193 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:21,124] Trial 227 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:21,640] Trial 228 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:22,144] Trial 229 finished with value: 0.629258285186193 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:22,519] Trial 230 finished with value: 0.6244309828573373 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:23,086] Trial 231 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:23,605] Trial 232 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:24,120] Trial 233 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:24,636] Trial 234 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:25,152] Trial 235 finished with value: 0.636041670405654 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:25,673] Trial 236 finished with value: 0.5970847498954737 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:26,197] Trial 237 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:26,719] Trial 238 finished with value: 0.629258285186193 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:27,237] Trial 239 finished with value: 0.6096695116037241 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:27,725] Trial 240 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:28,235] Trial 241 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:28,756] Trial 242 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:29,264] Trial 243 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:29,798] Trial 244 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:30,328] Trial 245 finished with value: 0.629258285186193 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:30,853] Trial 246 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:31,376] Trial 247 finished with value: 0.6362317919983441 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:31,894] Trial 248 finished with value: 0.6338084614466115 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:32,326] Trial 249 finished with value: 0.6136300913851714 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6419\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.643294    0.635706    0.665874    0.646857   \n",
      "1                    TP  331.000000  324.000000  309.000000  313.000000   \n",
      "2                    TN  175.000000  170.000000  180.000000  176.000000   \n",
      "3                    FP   52.000000   53.000000   73.000000   66.000000   \n",
      "4                    FN   37.000000   48.000000   33.000000   40.000000   \n",
      "5              Accuracy    0.850420    0.830252    0.821849    0.821849   \n",
      "6             Precision    0.864230    0.859416    0.808901    0.825858   \n",
      "7           Sensitivity    0.899457    0.870968    0.903509    0.886686   \n",
      "8           Specificity    0.770900    0.762300    0.711500    0.727300   \n",
      "9              F1 score    0.881491    0.865154    0.853591    0.855191   \n",
      "10  F1 score (weighted)    0.849359    0.829856    0.819124    0.819956   \n",
      "11     F1 score (macro)    0.839379    0.818064    0.813062    0.811875   \n",
      "12    Balanced Accuracy    0.835191    0.816650    0.807486    0.806979   \n",
      "13                  MCC    0.679973    0.636259    0.634171    0.627173   \n",
      "14                  NPV    0.825500    0.779800    0.845100    0.814800   \n",
      "15              ROC_AUC    0.835191    0.816650    0.807486    0.806979   \n",
      "\n",
      "          Set4  \n",
      "0     0.661726  \n",
      "1   339.000000  \n",
      "2   158.000000  \n",
      "3    66.000000  \n",
      "4    32.000000  \n",
      "5     0.835294  \n",
      "6     0.837037  \n",
      "7     0.913747  \n",
      "8     0.705400  \n",
      "9     0.873711  \n",
      "10    0.832139  \n",
      "11    0.818498  \n",
      "12    0.809552  \n",
      "13    0.643384  \n",
      "14    0.831600  \n",
      "15    0.809552  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_4_cat = np.where((y_pred_knn_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:18:33,046] Trial 250 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:33,572] Trial 251 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:34,083] Trial 252 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:34,582] Trial 253 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:35,096] Trial 254 finished with value: 0.6309002056068518 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:35,617] Trial 255 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:36,138] Trial 256 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:36,668] Trial 257 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:37,199] Trial 258 finished with value: 0.6107673668008584 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:37,731] Trial 259 finished with value: 0.6248464983804106 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:38,260] Trial 260 finished with value: 0.6309002056068518 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:38,789] Trial 261 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:39,155] Trial 262 finished with value: 0.6094298408984515 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:39,720] Trial 263 finished with value: 0.5970777766908457 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:40,249] Trial 264 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:40,753] Trial 265 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:41,279] Trial 266 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:41,799] Trial 267 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:42,322] Trial 268 finished with value: 0.6248464983804106 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:42,852] Trial 269 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:43,255] Trial 270 finished with value: 0.6229426893722543 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 80}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:43,799] Trial 271 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:44,317] Trial 272 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:44,835] Trial 273 finished with value: 0.6248464983804106 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:45,330] Trial 274 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:45,822] Trial 275 finished with value: 0.6309002056068518 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:46,321] Trial 276 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:46,833] Trial 277 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:47,354] Trial 278 finished with value: 0.6013855854016912 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:47,873] Trial 279 finished with value: 0.6248464983804106 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:48,396] Trial 280 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:48,927] Trial 281 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:49,455] Trial 282 finished with value: 0.5921821569435105 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:49,969] Trial 283 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:50,487] Trial 284 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:51,017] Trial 285 finished with value: 0.6309002056068518 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:51,455] Trial 286 finished with value: 0.6151858380925818 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:52,007] Trial 287 finished with value: 0.6248464983804106 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:52,535] Trial 288 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:53,072] Trial 289 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:53,582] Trial 290 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:54,093] Trial 291 finished with value: 0.6248464983804106 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:54,620] Trial 292 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:54,982] Trial 293 finished with value: 0.6229426893722543 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:55,534] Trial 294 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:56,068] Trial 295 finished with value: 0.6256633986144724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:56,603] Trial 296 finished with value: 0.6293848889144427 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:57,141] Trial 297 finished with value: 0.6227629841138663 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:57,678] Trial 298 finished with value: 0.572367641066514 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:58,203] Trial 299 finished with value: 0.6248464983804106 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6419\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.643294    0.635706    0.665874    0.646857   \n",
      "1                    TP  331.000000  324.000000  309.000000  313.000000   \n",
      "2                    TN  175.000000  170.000000  180.000000  176.000000   \n",
      "3                    FP   52.000000   53.000000   73.000000   66.000000   \n",
      "4                    FN   37.000000   48.000000   33.000000   40.000000   \n",
      "5              Accuracy    0.850420    0.830252    0.821849    0.821849   \n",
      "6             Precision    0.864230    0.859416    0.808901    0.825858   \n",
      "7           Sensitivity    0.899457    0.870968    0.903509    0.886686   \n",
      "8           Specificity    0.770900    0.762300    0.711500    0.727300   \n",
      "9              F1 score    0.881491    0.865154    0.853591    0.855191   \n",
      "10  F1 score (weighted)    0.849359    0.829856    0.819124    0.819956   \n",
      "11     F1 score (macro)    0.839379    0.818064    0.813062    0.811875   \n",
      "12    Balanced Accuracy    0.835191    0.816650    0.807486    0.806979   \n",
      "13                  MCC    0.679973    0.636259    0.634171    0.627173   \n",
      "14                  NPV    0.825500    0.779800    0.845100    0.814800   \n",
      "15              ROC_AUC    0.835191    0.816650    0.807486    0.806979   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.661726    0.640378  \n",
      "1   339.000000  326.000000  \n",
      "2   158.000000  174.000000  \n",
      "3    66.000000   58.000000  \n",
      "4    32.000000   37.000000  \n",
      "5     0.835294    0.840336  \n",
      "6     0.837037    0.848958  \n",
      "7     0.913747    0.898072  \n",
      "8     0.705400    0.750000  \n",
      "9     0.873711    0.872825  \n",
      "10    0.832139    0.838796  \n",
      "11    0.818498    0.829189  \n",
      "12    0.809552    0.824036  \n",
      "13    0.643384    0.660714  \n",
      "14    0.831600    0.824600  \n",
      "15    0.809552    0.824036  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_5_cat = np.where((y_pred_knn_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:18:58,903] Trial 300 finished with value: 0.6328529627968172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:59,447] Trial 301 finished with value: 0.6347605684684735 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:18:59,964] Trial 302 finished with value: 0.6109667681052542 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:00,496] Trial 303 finished with value: 0.6328529627968172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:01,034] Trial 304 finished with value: 0.6314603611552239 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:01,571] Trial 305 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:01,906] Trial 306 finished with value: 0.6216109872511764 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:02,487] Trial 307 finished with value: 0.6299703677269083 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:03,023] Trial 308 finished with value: 0.6347605684684735 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:03,569] Trial 309 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:04,111] Trial 310 finished with value: 0.6328529627968172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:04,658] Trial 311 finished with value: 0.6117245804933804 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:05,204] Trial 312 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:05,746] Trial 313 finished with value: 0.6299703677269083 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:06,279] Trial 314 finished with value: 0.6328529627968172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:06,496] Trial 315 finished with value: 0.6216109872511764 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:07,038] Trial 316 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:07,584] Trial 317 finished with value: 0.6347605684684735 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:08,123] Trial 318 finished with value: 0.6299703677269083 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:08,653] Trial 319 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:09,198] Trial 320 finished with value: 0.6181830483536025 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:09,736] Trial 321 finished with value: 0.6070164857941414 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:10,270] Trial 322 finished with value: 0.6328529627968172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:10,811] Trial 323 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:11,352] Trial 324 finished with value: 0.6328529627968172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:11,890] Trial 325 finished with value: 0.5895476051552887 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:12,420] Trial 326 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:12,948] Trial 327 finished with value: 0.6293932972204561 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:13,462] Trial 328 finished with value: 0.6299703677269083 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:13,846] Trial 329 finished with value: 0.6216109872511764 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:14,409] Trial 330 finished with value: 0.6347605684684735 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:14,939] Trial 331 finished with value: 0.6314603611552239 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:15,464] Trial 332 finished with value: 0.6328529627968172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:16,000] Trial 333 finished with value: 0.6299703677269083 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:16,545] Trial 334 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:17,091] Trial 335 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:17,626] Trial 336 finished with value: 0.6328529627968172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:18,170] Trial 337 finished with value: 0.6347605684684735 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:18,581] Trial 338 finished with value: 0.6216109872511764 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:19,150] Trial 339 finished with value: 0.6299703677269083 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:19,669] Trial 340 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:20,184] Trial 341 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:20,710] Trial 342 finished with value: 0.6070164857941414 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:21,247] Trial 343 finished with value: 0.6299703677269083 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:21,769] Trial 344 finished with value: 0.6328529627968172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:22,302] Trial 345 finished with value: 0.6347605684684735 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:22,834] Trial 346 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:23,373] Trial 347 finished with value: 0.6328529627968172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:23,900] Trial 348 finished with value: 0.6299703677269083 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:24,444] Trial 349 finished with value: 0.6351521187721327 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 85 with value: 0.6419118091617892.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6419\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.643294    0.635706    0.665874    0.646857   \n",
      "1                    TP  331.000000  324.000000  309.000000  313.000000   \n",
      "2                    TN  175.000000  170.000000  180.000000  176.000000   \n",
      "3                    FP   52.000000   53.000000   73.000000   66.000000   \n",
      "4                    FN   37.000000   48.000000   33.000000   40.000000   \n",
      "5              Accuracy    0.850420    0.830252    0.821849    0.821849   \n",
      "6             Precision    0.864230    0.859416    0.808901    0.825858   \n",
      "7           Sensitivity    0.899457    0.870968    0.903509    0.886686   \n",
      "8           Specificity    0.770900    0.762300    0.711500    0.727300   \n",
      "9              F1 score    0.881491    0.865154    0.853591    0.855191   \n",
      "10  F1 score (weighted)    0.849359    0.829856    0.819124    0.819956   \n",
      "11     F1 score (macro)    0.839379    0.818064    0.813062    0.811875   \n",
      "12    Balanced Accuracy    0.835191    0.816650    0.807486    0.806979   \n",
      "13                  MCC    0.679973    0.636259    0.634171    0.627173   \n",
      "14                  NPV    0.825500    0.779800    0.845100    0.814800   \n",
      "15              ROC_AUC    0.835191    0.816650    0.807486    0.806979   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.661726    0.640378    0.678259  \n",
      "1   339.000000  326.000000  334.000000  \n",
      "2   158.000000  174.000000  168.000000  \n",
      "3    66.000000   58.000000   60.000000  \n",
      "4    32.000000   37.000000   33.000000  \n",
      "5     0.835294    0.840336    0.843697  \n",
      "6     0.837037    0.848958    0.847716  \n",
      "7     0.913747    0.898072    0.910082  \n",
      "8     0.705400    0.750000    0.736800  \n",
      "9     0.873711    0.872825    0.877792  \n",
      "10    0.832139    0.838796    0.841552  \n",
      "11    0.818498    0.829189    0.830505  \n",
      "12    0.809552    0.824036    0.823462  \n",
      "13    0.643384    0.660714    0.664978  \n",
      "14    0.831600    0.824600    0.835800  \n",
      "15    0.809552    0.824036    0.823462  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_6_cat = np.where((y_pred_knn_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:19:25,057] Trial 350 finished with value: 0.5868629506680632 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:25,622] Trial 351 finished with value: 0.6392740057003838 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:26,152] Trial 352 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:26,669] Trial 353 finished with value: 0.6344271998430969 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:27,176] Trial 354 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:27,696] Trial 355 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:28,225] Trial 356 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:28,727] Trial 357 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:29,261] Trial 358 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:29,815] Trial 359 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:30,365] Trial 360 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:30,915] Trial 361 finished with value: 0.62322882998805 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:31,253] Trial 362 finished with value: 0.6309851890830043 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 58}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:31,827] Trial 363 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:32,315] Trial 364 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:32,865] Trial 365 finished with value: 0.6356559009666016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:33,394] Trial 366 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:33,933] Trial 367 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:34,472] Trial 368 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:34,977] Trial 369 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:35,469] Trial 370 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:35,967] Trial 371 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:36,484] Trial 372 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:37,028] Trial 373 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:37,567] Trial 374 finished with value: 0.6356559009666016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:38,104] Trial 375 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:38,637] Trial 376 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:38,952] Trial 377 finished with value: 0.5956655087769278 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:39,492] Trial 378 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:40,022] Trial 379 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:40,567] Trial 380 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:41,087] Trial 381 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:41,624] Trial 382 finished with value: 0.6356559009666016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:42,151] Trial 383 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:42,641] Trial 384 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:43,184] Trial 385 finished with value: 0.62322882998805 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:43,726] Trial 386 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:44,273] Trial 387 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:44,824] Trial 388 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:45,120] Trial 389 finished with value: 0.6320211244596853 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:45,684] Trial 390 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:46,216] Trial 391 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:46,766] Trial 392 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:47,317] Trial 393 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:47,869] Trial 394 finished with value: 0.6404007922405693 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:48,421] Trial 395 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:48,971] Trial 396 finished with value: 0.6356559009666016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:49,520] Trial 397 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:50,068] Trial 398 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:50,619] Trial 399 finished with value: 0.6411176207984672 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 85 with value: 0.6419118091617892.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6419\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.643294    0.635706    0.665874    0.646857   \n",
      "1                    TP  331.000000  324.000000  309.000000  313.000000   \n",
      "2                    TN  175.000000  170.000000  180.000000  176.000000   \n",
      "3                    FP   52.000000   53.000000   73.000000   66.000000   \n",
      "4                    FN   37.000000   48.000000   33.000000   40.000000   \n",
      "5              Accuracy    0.850420    0.830252    0.821849    0.821849   \n",
      "6             Precision    0.864230    0.859416    0.808901    0.825858   \n",
      "7           Sensitivity    0.899457    0.870968    0.903509    0.886686   \n",
      "8           Specificity    0.770900    0.762300    0.711500    0.727300   \n",
      "9              F1 score    0.881491    0.865154    0.853591    0.855191   \n",
      "10  F1 score (weighted)    0.849359    0.829856    0.819124    0.819956   \n",
      "11     F1 score (macro)    0.839379    0.818064    0.813062    0.811875   \n",
      "12    Balanced Accuracy    0.835191    0.816650    0.807486    0.806979   \n",
      "13                  MCC    0.679973    0.636259    0.634171    0.627173   \n",
      "14                  NPV    0.825500    0.779800    0.845100    0.814800   \n",
      "15              ROC_AUC    0.835191    0.816650    0.807486    0.806979   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.661726    0.640378    0.678259    0.636796  \n",
      "1   339.000000  326.000000  334.000000  325.000000  \n",
      "2   158.000000  174.000000  168.000000  175.000000  \n",
      "3    66.000000   58.000000   60.000000   60.000000  \n",
      "4    32.000000   37.000000   33.000000   35.000000  \n",
      "5     0.835294    0.840336    0.843697    0.840336  \n",
      "6     0.837037    0.848958    0.847716    0.844156  \n",
      "7     0.913747    0.898072    0.910082    0.902778  \n",
      "8     0.705400    0.750000    0.736800    0.744700  \n",
      "9     0.873711    0.872825    0.877792    0.872483  \n",
      "10    0.832139    0.838796    0.841552    0.838530  \n",
      "11    0.818498    0.829189    0.830505    0.829500  \n",
      "12    0.809552    0.824036    0.823462    0.823729  \n",
      "13    0.643384    0.660714    0.664978    0.662304  \n",
      "14    0.831600    0.824600    0.835800    0.833300  \n",
      "15    0.809552    0.824036    0.823462    0.823729  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_7_cat = np.where((y_pred_knn_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:19:51,327] Trial 400 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:51,862] Trial 401 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:52,395] Trial 402 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:52,823] Trial 403 finished with value: 0.6254713760302932 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 45}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:53,368] Trial 404 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:53,901] Trial 405 finished with value: 0.618522424457901 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:54,435] Trial 406 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:54,972] Trial 407 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:55,503] Trial 408 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:56,025] Trial 409 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:56,562] Trial 410 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:57,131] Trial 411 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:57,668] Trial 412 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:57,987] Trial 413 finished with value: 0.6195105015742328 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:58,542] Trial 414 finished with value: 0.6300928294777297 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:59,080] Trial 415 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:19:59,632] Trial 416 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:00,183] Trial 417 finished with value: 0.6194748114151054 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:00,734] Trial 418 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:01,284] Trial 419 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:01,827] Trial 420 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:02,371] Trial 421 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:02,908] Trial 422 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:03,445] Trial 423 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:03,981] Trial 424 finished with value: 0.6093790668488763 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:04,536] Trial 425 finished with value: 0.5755608769662068 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:05,088] Trial 426 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:05,468] Trial 427 finished with value: 0.6254713760302932 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 46}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:06,026] Trial 428 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:06,577] Trial 429 finished with value: 0.6300928294777297 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:07,128] Trial 430 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:07,678] Trial 431 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:08,219] Trial 432 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:08,771] Trial 433 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:09,041] Trial 434 finished with value: 0.6195105015742328 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 51}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:09,588] Trial 435 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:10,138] Trial 436 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:10,679] Trial 437 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:11,230] Trial 438 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:11,781] Trial 439 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:12,332] Trial 440 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:12,882] Trial 441 finished with value: 0.6300928294777297 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:13,425] Trial 442 finished with value: 0.6322644613993542 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:13,960] Trial 443 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:14,499] Trial 444 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:15,035] Trial 445 finished with value: 0.618522424457901 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:15,538] Trial 446 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:15,837] Trial 447 finished with value: 0.6254713760302932 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:16,363] Trial 448 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 85 with value: 0.6419118091617892.\n",
      "[I 2023-12-12 00:20:16,862] Trial 449 finished with value: 0.6292558267883952 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 85 with value: 0.6419118091617892.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6419\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.643294    0.635706    0.665874    0.646857   \n",
      "1                    TP  331.000000  324.000000  309.000000  313.000000   \n",
      "2                    TN  175.000000  170.000000  180.000000  176.000000   \n",
      "3                    FP   52.000000   53.000000   73.000000   66.000000   \n",
      "4                    FN   37.000000   48.000000   33.000000   40.000000   \n",
      "5              Accuracy    0.850420    0.830252    0.821849    0.821849   \n",
      "6             Precision    0.864230    0.859416    0.808901    0.825858   \n",
      "7           Sensitivity    0.899457    0.870968    0.903509    0.886686   \n",
      "8           Specificity    0.770900    0.762300    0.711500    0.727300   \n",
      "9              F1 score    0.881491    0.865154    0.853591    0.855191   \n",
      "10  F1 score (weighted)    0.849359    0.829856    0.819124    0.819956   \n",
      "11     F1 score (macro)    0.839379    0.818064    0.813062    0.811875   \n",
      "12    Balanced Accuracy    0.835191    0.816650    0.807486    0.806979   \n",
      "13                  MCC    0.679973    0.636259    0.634171    0.627173   \n",
      "14                  NPV    0.825500    0.779800    0.845100    0.814800   \n",
      "15              ROC_AUC    0.835191    0.816650    0.807486    0.806979   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.661726    0.640378    0.678259    0.636796    0.669328  \n",
      "1   339.000000  326.000000  334.000000  325.000000  330.000000  \n",
      "2   158.000000  174.000000  168.000000  175.000000  166.000000  \n",
      "3    66.000000   58.000000   60.000000   60.000000   55.000000  \n",
      "4    32.000000   37.000000   33.000000   35.000000   44.000000  \n",
      "5     0.835294    0.840336    0.843697    0.840336    0.833613  \n",
      "6     0.837037    0.848958    0.847716    0.844156    0.857143  \n",
      "7     0.913747    0.898072    0.910082    0.902778    0.882353  \n",
      "8     0.705400    0.750000    0.736800    0.744700    0.751100  \n",
      "9     0.873711    0.872825    0.877792    0.872483    0.869565  \n",
      "10    0.832139    0.838796    0.841552    0.838530    0.832696  \n",
      "11    0.818498    0.829189    0.830505    0.829500    0.819933  \n",
      "12    0.809552    0.824036    0.823462    0.823729    0.816742  \n",
      "13    0.643384    0.660714    0.664978    0.662304    0.640513  \n",
      "14    0.831600    0.824600    0.835800    0.833300    0.790500  \n",
      "15    0.809552    0.824036    0.823462    0.823729    0.816742  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_8_cat = np.where((y_pred_knn_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:20:17,520] Trial 450 finished with value: 0.6468399769920511 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 450 with value: 0.6468399769920511.\n",
      "[I 2023-12-12 00:20:17,961] Trial 451 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:18,423] Trial 452 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:18,963] Trial 453 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:19,503] Trial 454 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:20,047] Trial 455 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:20,363] Trial 456 finished with value: 0.6397999397132403 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:20,922] Trial 457 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:21,453] Trial 458 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:21,966] Trial 459 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:22,481] Trial 460 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:22,990] Trial 461 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:23,507] Trial 462 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:24,017] Trial 463 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:24,542] Trial 464 finished with value: 0.6332036179989229 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:25,064] Trial 465 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:25,592] Trial 466 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:26,110] Trial 467 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:26,644] Trial 468 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:27,177] Trial 469 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:27,719] Trial 470 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:28,238] Trial 471 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:28,757] Trial 472 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:29,276] Trial 473 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:29,804] Trial 474 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:30,347] Trial 475 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:30,891] Trial 476 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:31,175] Trial 477 finished with value: 0.6397999397132403 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:31,746] Trial 478 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:32,287] Trial 479 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:32,832] Trial 480 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:33,377] Trial 481 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:33,921] Trial 482 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:34,416] Trial 483 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:34,936] Trial 484 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:35,451] Trial 485 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:35,865] Trial 486 finished with value: 0.6397999397132403 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:36,443] Trial 487 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:36,978] Trial 488 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:37,513] Trial 489 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:38,055] Trial 490 finished with value: 0.6332036179989229 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:38,599] Trial 491 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:39,119] Trial 492 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:39,658] Trial 493 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:40,206] Trial 494 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:40,713] Trial 495 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:41,244] Trial 496 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:41,775] Trial 497 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:42,300] Trial 498 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 451 with value: 0.6487624101296955.\n",
      "[I 2023-12-12 00:20:42,838] Trial 499 finished with value: 0.6487624101296955 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 451 with value: 0.6487624101296955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6488\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 41\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.643294    0.635706    0.665874    0.646857   \n",
      "1                    TP  331.000000  324.000000  309.000000  313.000000   \n",
      "2                    TN  175.000000  170.000000  180.000000  176.000000   \n",
      "3                    FP   52.000000   53.000000   73.000000   66.000000   \n",
      "4                    FN   37.000000   48.000000   33.000000   40.000000   \n",
      "5              Accuracy    0.850420    0.830252    0.821849    0.821849   \n",
      "6             Precision    0.864230    0.859416    0.808901    0.825858   \n",
      "7           Sensitivity    0.899457    0.870968    0.903509    0.886686   \n",
      "8           Specificity    0.770900    0.762300    0.711500    0.727300   \n",
      "9              F1 score    0.881491    0.865154    0.853591    0.855191   \n",
      "10  F1 score (weighted)    0.849359    0.829856    0.819124    0.819956   \n",
      "11     F1 score (macro)    0.839379    0.818064    0.813062    0.811875   \n",
      "12    Balanced Accuracy    0.835191    0.816650    0.807486    0.806979   \n",
      "13                  MCC    0.679973    0.636259    0.634171    0.627173   \n",
      "14                  NPV    0.825500    0.779800    0.845100    0.814800   \n",
      "15              ROC_AUC    0.835191    0.816650    0.807486    0.806979   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.661726    0.640378    0.678259    0.636796    0.669328    0.621768  \n",
      "1   339.000000  326.000000  334.000000  325.000000  330.000000  331.000000  \n",
      "2   158.000000  174.000000  168.000000  175.000000  166.000000  166.000000  \n",
      "3    66.000000   58.000000   60.000000   60.000000   55.000000   61.000000  \n",
      "4    32.000000   37.000000   33.000000   35.000000   44.000000   37.000000  \n",
      "5     0.835294    0.840336    0.843697    0.840336    0.833613    0.835294  \n",
      "6     0.837037    0.848958    0.847716    0.844156    0.857143    0.844388  \n",
      "7     0.913747    0.898072    0.910082    0.902778    0.882353    0.899457  \n",
      "8     0.705400    0.750000    0.736800    0.744700    0.751100    0.731300  \n",
      "9     0.873711    0.872825    0.877792    0.872483    0.869565    0.871053  \n",
      "10    0.832139    0.838796    0.841552    0.838530    0.832696    0.833298  \n",
      "11    0.818498    0.829189    0.830505    0.829500    0.819933    0.821573  \n",
      "12    0.809552    0.824036    0.823462    0.823729    0.816742    0.815367  \n",
      "13    0.643384    0.660714    0.664978    0.662304    0.640513    0.646237  \n",
      "14    0.831600    0.824600    0.835800    0.833300    0.790500    0.817700  \n",
      "15    0.809552    0.824036    0.823462    0.823729    0.816742    0.815367  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_9_cat = np.where((y_pred_knn_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.658818</td>\n",
       "      <td>0.043759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>4.570436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>85.300000</td>\n",
       "      <td>6.961002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>6.838616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>3.713339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.845852</td>\n",
       "      <td>0.024708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.856097</td>\n",
       "      <td>0.031244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.903702</td>\n",
       "      <td>0.019556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.059780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.878894</td>\n",
       "      <td>0.018315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.844066</td>\n",
       "      <td>0.025629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.833197</td>\n",
       "      <td>0.028026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.828046</td>\n",
       "      <td>0.030562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.670115</td>\n",
       "      <td>0.055039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.828570</td>\n",
       "      <td>0.034377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.828046</td>\n",
       "      <td>0.030562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.658818     0.043759\n",
       "1                    TP       166.000000     4.570436\n",
       "2                    TN        85.300000     6.961002\n",
       "3                    FP        28.100000     6.838616\n",
       "4                    FN        17.700000     3.713339\n",
       "5              Accuracy         0.845852     0.024708\n",
       "6             Precision         0.856097     0.031244\n",
       "7           Sensitivity         0.903702     0.019556\n",
       "8           Specificity         0.752400     0.059780\n",
       "9              F1 score         0.878894     0.018315\n",
       "10  F1 score (weighted)         0.844066     0.025629\n",
       "11     F1 score (macro)         0.833197     0.028026\n",
       "12    Balanced Accuracy         0.828046     0.030562\n",
       "13                  MCC         0.670115     0.055039\n",
       "14                  NPV         0.828570     0.034377\n",
       "15              ROC_AUC         0.828046     0.030562"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.643294</td>\n",
       "      <td>0.635706</td>\n",
       "      <td>0.665874</td>\n",
       "      <td>0.646857</td>\n",
       "      <td>0.661726</td>\n",
       "      <td>0.640378</td>\n",
       "      <td>0.678259</td>\n",
       "      <td>0.636796</td>\n",
       "      <td>0.669328</td>\n",
       "      <td>0.621768</td>\n",
       "      <td>0.649999</td>\n",
       "      <td>0.017903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>326.200000</td>\n",
       "      <td>9.199034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>170.800000</td>\n",
       "      <td>6.460134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>60.400000</td>\n",
       "      <td>6.518350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.600000</td>\n",
       "      <td>5.125102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.821849</td>\n",
       "      <td>0.821849</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.833613</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.009068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.864230</td>\n",
       "      <td>0.859416</td>\n",
       "      <td>0.808901</td>\n",
       "      <td>0.825858</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.847716</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.844388</td>\n",
       "      <td>0.843780</td>\n",
       "      <td>0.016557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.899457</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.886686</td>\n",
       "      <td>0.913747</td>\n",
       "      <td>0.898072</td>\n",
       "      <td>0.910082</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.899457</td>\n",
       "      <td>0.896711</td>\n",
       "      <td>0.013074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.770900</td>\n",
       "      <td>0.762300</td>\n",
       "      <td>0.711500</td>\n",
       "      <td>0.727300</td>\n",
       "      <td>0.705400</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.736800</td>\n",
       "      <td>0.744700</td>\n",
       "      <td>0.751100</td>\n",
       "      <td>0.731300</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.020940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.881491</td>\n",
       "      <td>0.865154</td>\n",
       "      <td>0.853591</td>\n",
       "      <td>0.855191</td>\n",
       "      <td>0.873711</td>\n",
       "      <td>0.872825</td>\n",
       "      <td>0.877792</td>\n",
       "      <td>0.872483</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.871053</td>\n",
       "      <td>0.869286</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.849359</td>\n",
       "      <td>0.829856</td>\n",
       "      <td>0.819124</td>\n",
       "      <td>0.819956</td>\n",
       "      <td>0.832139</td>\n",
       "      <td>0.838796</td>\n",
       "      <td>0.841552</td>\n",
       "      <td>0.838530</td>\n",
       "      <td>0.832696</td>\n",
       "      <td>0.833298</td>\n",
       "      <td>0.833531</td>\n",
       "      <td>0.009283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.839379</td>\n",
       "      <td>0.818064</td>\n",
       "      <td>0.813062</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.818498</td>\n",
       "      <td>0.829189</td>\n",
       "      <td>0.830505</td>\n",
       "      <td>0.829500</td>\n",
       "      <td>0.819933</td>\n",
       "      <td>0.821573</td>\n",
       "      <td>0.823158</td>\n",
       "      <td>0.008715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.835191</td>\n",
       "      <td>0.816650</td>\n",
       "      <td>0.807486</td>\n",
       "      <td>0.806979</td>\n",
       "      <td>0.809552</td>\n",
       "      <td>0.824036</td>\n",
       "      <td>0.823462</td>\n",
       "      <td>0.823729</td>\n",
       "      <td>0.816742</td>\n",
       "      <td>0.815367</td>\n",
       "      <td>0.817919</td>\n",
       "      <td>0.008871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.679973</td>\n",
       "      <td>0.636259</td>\n",
       "      <td>0.634171</td>\n",
       "      <td>0.627173</td>\n",
       "      <td>0.643384</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.664978</td>\n",
       "      <td>0.662304</td>\n",
       "      <td>0.640513</td>\n",
       "      <td>0.646237</td>\n",
       "      <td>0.649571</td>\n",
       "      <td>0.016651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.825500</td>\n",
       "      <td>0.779800</td>\n",
       "      <td>0.845100</td>\n",
       "      <td>0.814800</td>\n",
       "      <td>0.831600</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.835800</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>0.817700</td>\n",
       "      <td>0.819870</td>\n",
       "      <td>0.020442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.835191</td>\n",
       "      <td>0.816650</td>\n",
       "      <td>0.807486</td>\n",
       "      <td>0.806979</td>\n",
       "      <td>0.809552</td>\n",
       "      <td>0.824036</td>\n",
       "      <td>0.823462</td>\n",
       "      <td>0.823729</td>\n",
       "      <td>0.816742</td>\n",
       "      <td>0.815367</td>\n",
       "      <td>0.817919</td>\n",
       "      <td>0.008871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.643294    0.635706    0.665874    0.646857   \n",
       "1                    TP  331.000000  324.000000  309.000000  313.000000   \n",
       "2                    TN  175.000000  170.000000  180.000000  176.000000   \n",
       "3                    FP   52.000000   53.000000   73.000000   66.000000   \n",
       "4                    FN   37.000000   48.000000   33.000000   40.000000   \n",
       "5              Accuracy    0.850420    0.830252    0.821849    0.821849   \n",
       "6             Precision    0.864230    0.859416    0.808901    0.825858   \n",
       "7           Sensitivity    0.899457    0.870968    0.903509    0.886686   \n",
       "8           Specificity    0.770900    0.762300    0.711500    0.727300   \n",
       "9              F1 score    0.881491    0.865154    0.853591    0.855191   \n",
       "10  F1 score (weighted)    0.849359    0.829856    0.819124    0.819956   \n",
       "11     F1 score (macro)    0.839379    0.818064    0.813062    0.811875   \n",
       "12    Balanced Accuracy    0.835191    0.816650    0.807486    0.806979   \n",
       "13                  MCC    0.679973    0.636259    0.634171    0.627173   \n",
       "14                  NPV    0.825500    0.779800    0.845100    0.814800   \n",
       "15              ROC_AUC    0.835191    0.816650    0.807486    0.806979   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.661726    0.640378    0.678259    0.636796    0.669328    0.621768   \n",
       "1   339.000000  326.000000  334.000000  325.000000  330.000000  331.000000   \n",
       "2   158.000000  174.000000  168.000000  175.000000  166.000000  166.000000   \n",
       "3    66.000000   58.000000   60.000000   60.000000   55.000000   61.000000   \n",
       "4    32.000000   37.000000   33.000000   35.000000   44.000000   37.000000   \n",
       "5     0.835294    0.840336    0.843697    0.840336    0.833613    0.835294   \n",
       "6     0.837037    0.848958    0.847716    0.844156    0.857143    0.844388   \n",
       "7     0.913747    0.898072    0.910082    0.902778    0.882353    0.899457   \n",
       "8     0.705400    0.750000    0.736800    0.744700    0.751100    0.731300   \n",
       "9     0.873711    0.872825    0.877792    0.872483    0.869565    0.871053   \n",
       "10    0.832139    0.838796    0.841552    0.838530    0.832696    0.833298   \n",
       "11    0.818498    0.829189    0.830505    0.829500    0.819933    0.821573   \n",
       "12    0.809552    0.824036    0.823462    0.823729    0.816742    0.815367   \n",
       "13    0.643384    0.660714    0.664978    0.662304    0.640513    0.646237   \n",
       "14    0.831600    0.824600    0.835800    0.833300    0.790500    0.817700   \n",
       "15    0.809552    0.824036    0.823462    0.823729    0.816742    0.815367   \n",
       "\n",
       "           ave       std  \n",
       "0     0.649999  0.017903  \n",
       "1   326.200000  9.199034  \n",
       "2   170.800000  6.460134  \n",
       "3    60.400000  6.518350  \n",
       "4    37.600000  5.125102  \n",
       "5     0.835294  0.009068  \n",
       "6     0.843780  0.016557  \n",
       "7     0.896711  0.013074  \n",
       "8     0.739130  0.020940  \n",
       "9     0.869286  0.009000  \n",
       "10    0.833531  0.009283  \n",
       "11    0.823158  0.008715  \n",
       "12    0.817919  0.008871  \n",
       "13    0.649571  0.016651  \n",
       "14    0.819870  0.020442  \n",
       "15    0.817919  0.008871  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_knn0</th>\n",
       "      <th>y_pred_knn1</th>\n",
       "      <th>y_pred_knn2</th>\n",
       "      <th>y_pred_knn3</th>\n",
       "      <th>y_pred_knn4</th>\n",
       "      <th>y_pred_knn_ave</th>\n",
       "      <th>y_pred_knn_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2047687</td>\n",
       "      <td>0</td>\n",
       "      <td>5.48</td>\n",
       "      <td>5.767736</td>\n",
       "      <td>5.767736</td>\n",
       "      <td>5.848059</td>\n",
       "      <td>5.877176</td>\n",
       "      <td>5.767736</td>\n",
       "      <td>5.751407</td>\n",
       "      <td>0.128855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1164212</td>\n",
       "      <td>1</td>\n",
       "      <td>5.76</td>\n",
       "      <td>4.721115</td>\n",
       "      <td>4.721115</td>\n",
       "      <td>4.721115</td>\n",
       "      <td>4.721115</td>\n",
       "      <td>4.717495</td>\n",
       "      <td>4.893659</td>\n",
       "      <td>0.387442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2337873</td>\n",
       "      <td>2</td>\n",
       "      <td>6.07</td>\n",
       "      <td>6.185194</td>\n",
       "      <td>6.112129</td>\n",
       "      <td>6.139218</td>\n",
       "      <td>6.185194</td>\n",
       "      <td>6.261499</td>\n",
       "      <td>6.158872</td>\n",
       "      <td>0.061084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4577419</td>\n",
       "      <td>3</td>\n",
       "      <td>7.90</td>\n",
       "      <td>7.753628</td>\n",
       "      <td>7.737701</td>\n",
       "      <td>7.703214</td>\n",
       "      <td>7.703214</td>\n",
       "      <td>7.703214</td>\n",
       "      <td>7.750162</td>\n",
       "      <td>0.069799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL515452</td>\n",
       "      <td>4</td>\n",
       "      <td>6.22</td>\n",
       "      <td>6.561392</td>\n",
       "      <td>6.606488</td>\n",
       "      <td>6.606488</td>\n",
       "      <td>6.606488</td>\n",
       "      <td>6.590732</td>\n",
       "      <td>6.531932</td>\n",
       "      <td>0.140418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL3693800</td>\n",
       "      <td>2966</td>\n",
       "      <td>8.22</td>\n",
       "      <td>8.156009</td>\n",
       "      <td>8.156009</td>\n",
       "      <td>8.352267</td>\n",
       "      <td>8.156009</td>\n",
       "      <td>8.156009</td>\n",
       "      <td>8.199384</td>\n",
       "      <td>0.072254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL2431917</td>\n",
       "      <td>2967</td>\n",
       "      <td>7.46</td>\n",
       "      <td>7.383905</td>\n",
       "      <td>7.383905</td>\n",
       "      <td>7.297824</td>\n",
       "      <td>7.346097</td>\n",
       "      <td>7.383905</td>\n",
       "      <td>7.375940</td>\n",
       "      <td>0.048732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL2413298</td>\n",
       "      <td>2968</td>\n",
       "      <td>6.26</td>\n",
       "      <td>5.967708</td>\n",
       "      <td>5.971729</td>\n",
       "      <td>5.967708</td>\n",
       "      <td>5.967708</td>\n",
       "      <td>5.967708</td>\n",
       "      <td>6.017094</td>\n",
       "      <td>0.108641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3656016</td>\n",
       "      <td>2969</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.335466</td>\n",
       "      <td>8.335466</td>\n",
       "      <td>8.364407</td>\n",
       "      <td>8.427198</td>\n",
       "      <td>8.364407</td>\n",
       "      <td>8.391157</td>\n",
       "      <td>0.065235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL4643138</td>\n",
       "      <td>2970</td>\n",
       "      <td>6.38</td>\n",
       "      <td>7.048583</td>\n",
       "      <td>7.048583</td>\n",
       "      <td>7.048583</td>\n",
       "      <td>6.953459</td>\n",
       "      <td>7.048583</td>\n",
       "      <td>6.921298</td>\n",
       "      <td>0.244555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_knn0  y_pred_knn1  \\\n",
       "0         CHEMBL2047687            0     5.48     5.767736     5.767736   \n",
       "1         CHEMBL1164212            1     5.76     4.721115     4.721115   \n",
       "2         CHEMBL2337873            2     6.07     6.185194     6.112129   \n",
       "3         CHEMBL4577419            3     7.90     7.753628     7.737701   \n",
       "4          CHEMBL515452            4     6.22     6.561392     6.606488   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "2966      CHEMBL3693800         2966     8.22     8.156009     8.156009   \n",
       "2967      CHEMBL2431917         2967     7.46     7.383905     7.383905   \n",
       "2968      CHEMBL2413298         2968     6.26     5.967708     5.971729   \n",
       "2969      CHEMBL3656016         2969     8.52     8.335466     8.335466   \n",
       "2970      CHEMBL4643138         2970     6.38     7.048583     7.048583   \n",
       "\n",
       "      y_pred_knn2  y_pred_knn3  y_pred_knn4  y_pred_knn_ave  y_pred_knn_std  \n",
       "0        5.848059     5.877176     5.767736        5.751407        0.128855  \n",
       "1        4.721115     4.721115     4.717495        4.893659        0.387442  \n",
       "2        6.139218     6.185194     6.261499        6.158872        0.061084  \n",
       "3        7.703214     7.703214     7.703214        7.750162        0.069799  \n",
       "4        6.606488     6.606488     6.590732        6.531932        0.140418  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "2966     8.352267     8.156009     8.156009        8.199384        0.072254  \n",
       "2967     7.297824     7.346097     7.383905        7.375940        0.048732  \n",
       "2968     5.967708     5.967708     5.967708        6.017094        0.108641  \n",
       "2969     8.364407     8.427198     8.364407        8.391157        0.065235  \n",
       "2970     7.048583     6.953459     7.048583        6.921298        0.244555  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where((y_pred_optimized_knn >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id, knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "knn_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6393 with a standard deviation of 0.0400\n",
      "KNN optimized model r2_score 0.6499 with a standard deviation of 0.0402\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn.joblib']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"OUTPUT/knn_reg.joblib\")\n",
    "#joblib.dump(optimized_knn, \"OUTPUT/optimized_knn.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn.joblib\")\n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.692719     0.028995\n",
      "1                    TP       165.100000     5.506562\n",
      "2                    TN        86.500000     6.293736\n",
      "3                    FP        26.900000     4.653553\n",
      "4                    FN        18.600000     5.440588\n",
      "5              Accuracy         0.846865     0.025608\n",
      "6             Precision         0.860226     0.020991\n",
      "7           Sensitivity         0.898886     0.028983\n",
      "8           Specificity         0.762410     0.042845\n",
      "9              F1 score         0.878901     0.020021\n",
      "10  F1 score (weighted)         0.845591     0.025700\n",
      "11     F1 score (macro)         0.835083     0.028162\n",
      "12    Balanced Accuracy         0.830643     0.027811\n",
      "13                  MCC         0.672603     0.056613\n",
      "14                  NPV         0.824060     0.047798\n",
      "15              ROC_AUC         0.830643     0.027811\n",
      "CPU times: user 11.4 s, sys: 0 ns, total: 11.4 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:21:02,103] A new study created in memory with name: SVM_regressor_CV\n",
      "[I 2023-12-12 00:21:09,152] Trial 0 finished with value: 0.4468836525627882 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 0 with value: 0.4468836525627882.\n",
      "[I 2023-12-12 00:21:16,147] Trial 1 finished with value: 0.07729306308617256 and parameters: {'C': 0.0625, 'gamma': 0.00048828125}. Best is trial 0 with value: 0.4468836525627882.\n",
      "[I 2023-12-12 00:21:22,860] Trial 2 finished with value: 0.6179559922110088 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 2 with value: 0.6179559922110088.\n",
      "[I 2023-12-12 00:21:30,339] Trial 3 finished with value: 0.0210508713213186 and parameters: {'C': 0.5, 'gamma': 0.5}. Best is trial 2 with value: 0.6179559922110088.\n",
      "[I 2023-12-12 00:21:37,534] Trial 4 finished with value: 0.27285814231161837 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 2 with value: 0.6179559922110088.\n",
      "[I 2023-12-12 00:21:45,053] Trial 5 finished with value: 0.012852732577699599 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 2 with value: 0.6179559922110088.\n",
      "[I 2023-12-12 00:21:51,516] Trial 6 finished with value: 0.5056451079467453 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 2 with value: 0.6179559922110088.\n",
      "[I 2023-12-12 00:21:58,156] Trial 7 finished with value: 0.37695841341582115 and parameters: {'C': 0.0625, 'gamma': 0.0078125}. Best is trial 2 with value: 0.6179559922110088.\n",
      "[I 2023-12-12 00:22:07,238] Trial 8 finished with value: 0.6398407914119546 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 8 with value: 0.6398407914119546.\n",
      "[I 2023-12-12 00:22:14,043] Trial 9 finished with value: 0.24623757195566212 and parameters: {'C': 0.5, 'gamma': 0.000244140625}. Best is trial 8 with value: 0.6398407914119546.\n",
      "[I 2023-12-12 00:22:21,158] Trial 10 finished with value: 0.2010622379129436 and parameters: {'C': 0.03125, 'gamma': 0.00390625}. Best is trial 8 with value: 0.6398407914119546.\n",
      "[I 2023-12-12 00:22:28,713] Trial 11 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:22:35,763] Trial 12 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:22:42,860] Trial 13 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:22:49,656] Trial 14 finished with value: 0.6649942277875195 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:22:56,753] Trial 15 finished with value: 0.0915708882466266 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:23:03,832] Trial 16 finished with value: -0.005017855497487189 and parameters: {'C': 0.015625, 'gamma': 0.25}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:23:12,635] Trial 17 finished with value: 0.008843095085509655 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:23:19,594] Trial 18 finished with value: 0.3640374146129071 and parameters: {'C': 0.25, 'gamma': 0.0625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:23:27,564] Trial 19 finished with value: 0.00936013218168924 and parameters: {'C': 64.0, 'gamma': 4.0}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:23:34,595] Trial 20 finished with value: 0.35488259817871826 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:23:41,539] Trial 21 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:23:48,609] Trial 22 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:23:55,410] Trial 23 finished with value: 0.5816212326175625 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:24:02,400] Trial 24 finished with value: 0.5528937178280924 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:24:09,554] Trial 25 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:24:17,486] Trial 26 finished with value: 0.02129366954838181 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:24:24,449] Trial 27 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:24:31,371] Trial 28 finished with value: 0.0915708882466266 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:24:38,612] Trial 29 finished with value: 0.15012799040888955 and parameters: {'C': 1.0, 'gamma': 6.103515625e-05}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:24:45,775] Trial 30 finished with value: 0.4468836525627882 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:24:53,107] Trial 31 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:25:00,239] Trial 32 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:25:07,092] Trial 33 finished with value: 0.6179559922110088 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:25:14,150] Trial 34 finished with value: 0.5851943658127678 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:25:21,535] Trial 35 finished with value: -0.004544758529628856 and parameters: {'C': 0.03125, 'gamma': 0.5}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:25:29,240] Trial 36 finished with value: 0.2729569403394113 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:25:36,361] Trial 37 finished with value: 0.5967595041991244 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:25:44,303] Trial 38 finished with value: 0.012852846519239392 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:25:51,285] Trial 39 finished with value: 0.37695841341582115 and parameters: {'C': 0.0625, 'gamma': 0.0078125}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:25:58,307] Trial 40 finished with value: 0.5698715116307859 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:26:05,444] Trial 41 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:26:12,656] Trial 42 finished with value: 0.6866339253789416 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:26:20,651] Trial 43 finished with value: 0.585225669418136 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:26:27,447] Trial 44 finished with value: 0.6294779793069019 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:26:34,675] Trial 45 finished with value: 0.03523004220303311 and parameters: {'C': 0.015625, 'gamma': 0.0625}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:26:42,219] Trial 46 finished with value: 0.07793267119110661 and parameters: {'C': 2.0, 'gamma': 0.25}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:26:50,800] Trial 47 finished with value: -0.0023275883763470896 and parameters: {'C': 0.125, 'gamma': 8.0}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:26:58,655] Trial 48 finished with value: 0.00936052314700504 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:27:06,748] Trial 49 finished with value: 0.02129366954838181 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 11 with value: 0.6866339253789416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6866\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.713191\n",
      "1                    TP  335.000000\n",
      "2                    TN  188.000000\n",
      "3                    FP   39.000000\n",
      "4                    FN   33.000000\n",
      "5              Accuracy    0.878992\n",
      "6             Precision    0.895722\n",
      "7           Sensitivity    0.910326\n",
      "8           Specificity    0.828200\n",
      "9              F1 score    0.902965\n",
      "10  F1 score (weighted)    0.878671\n",
      "11     F1 score (macro)    0.871125\n",
      "12    Balanced Accuracy    0.869260\n",
      "13                  MCC    0.742450\n",
      "14                  NPV    0.850700\n",
      "15              ROC_AUC    0.869260\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_0_cat = np.where((y_pred_svm_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:27:14,774] Trial 50 finished with value: 0.2478747403615106 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 11 with value: 0.6866339253789416.\n",
      "[I 2023-12-12 00:27:21,771] Trial 51 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:27:28,571] Trial 52 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:27:35,293] Trial 53 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:27:41,782] Trial 54 finished with value: 0.5852969393291907 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:27:49,152] Trial 55 finished with value: 0.6478905918102722 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:27:55,979] Trial 56 finished with value: 0.2813325276698341 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:28:02,521] Trial 57 finished with value: 0.3346794755542496 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:28:08,652] Trial 58 finished with value: 0.5521103689355428 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:28:15,633] Trial 59 finished with value: 0.031866965694130445 and parameters: {'C': 0.0078125, 'gamma': 0.001953125}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:28:21,708] Trial 60 finished with value: 0.6707167413234918 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:28:28,170] Trial 61 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:28:34,593] Trial 62 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:28:41,008] Trial 63 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:28:47,441] Trial 64 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:28:54,464] Trial 65 finished with value: 0.038264571623600564 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:29:00,428] Trial 66 finished with value: 0.6326846769400454 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:29:07,370] Trial 67 finished with value: 0.28034812788324004 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:29:13,925] Trial 68 finished with value: -0.0021157003698083375 and parameters: {'C': 0.015625, 'gamma': 0.0001220703125}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:29:20,866] Trial 69 finished with value: 0.019018016232837685 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:29:26,886] Trial 70 finished with value: 0.5730920884354661 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:29:33,326] Trial 71 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:29:39,876] Trial 72 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:29:46,446] Trial 73 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:29:53,235] Trial 74 finished with value: 0.5801504833723515 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:30:00,032] Trial 75 finished with value: 0.6742416957825593 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:30:06,341] Trial 76 finished with value: 0.5511244692061925 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:30:13,602] Trial 77 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:30:20,808] Trial 78 finished with value: 0.6798115643829814 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:30:28,003] Trial 79 finished with value: 0.07945241133850253 and parameters: {'C': 2.0, 'gamma': 0.25}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:30:34,494] Trial 80 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:30:41,006] Trial 81 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:30:47,457] Trial 82 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:30:54,012] Trial 83 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:31:00,574] Trial 84 finished with value: 0.5628600194587838 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:31:07,663] Trial 85 finished with value: 6.408799644698204e-05 and parameters: {'C': 0.125, 'gamma': 4.0}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:31:14,194] Trial 86 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:31:22,468] Trial 87 finished with value: 0.016081121337230476 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:31:28,914] Trial 88 finished with value: 0.6435028040150275 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:31:35,494] Trial 89 finished with value: 0.35415333796027865 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:31:42,454] Trial 90 finished with value: -0.007490454088762644 and parameters: {'C': 0.0078125, 'gamma': 1.0}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:31:49,148] Trial 91 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:31:55,720] Trial 92 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:32:02,142] Trial 93 finished with value: 0.690101305102947 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:32:08,676] Trial 94 finished with value: 0.2813325276698341 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:32:15,536] Trial 95 finished with value: 0.13884287929089947 and parameters: {'C': 0.0625, 'gamma': 0.0009765625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:32:21,622] Trial 96 finished with value: 0.6707167413234918 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:32:28,381] Trial 97 finished with value: 0.6753879684948123 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:32:34,806] Trial 98 finished with value: 0.5521103689355428 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:32:40,995] Trial 99 finished with value: 0.6326846769400454 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 51 with value: 0.690101305102947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6901\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.713191    0.718417\n",
      "1                    TP  335.000000  335.000000\n",
      "2                    TN  188.000000  178.000000\n",
      "3                    FP   39.000000   45.000000\n",
      "4                    FN   33.000000   37.000000\n",
      "5              Accuracy    0.878992    0.862185\n",
      "6             Precision    0.895722    0.881579\n",
      "7           Sensitivity    0.910326    0.900538\n",
      "8           Specificity    0.828200    0.798200\n",
      "9              F1 score    0.902965    0.890957\n",
      "10  F1 score (weighted)    0.878671    0.861659\n",
      "11     F1 score (macro)    0.871125    0.851871\n",
      "12    Balanced Accuracy    0.869260    0.849372\n",
      "13                  MCC    0.742450    0.704094\n",
      "14                  NPV    0.850700    0.827900\n",
      "15              ROC_AUC    0.869260    0.849372\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_1_cat = np.where((y_pred_svm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:32:49,532] Trial 100 finished with value: 0.62536697473798 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 51 with value: 0.690101305102947.\n",
      "[I 2023-12-12 00:32:56,365] Trial 101 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:33:03,116] Trial 102 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:33:10,181] Trial 103 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:33:17,707] Trial 104 finished with value: 0.04865562169139011 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:33:24,145] Trial 105 finished with value: 0.5647191669173635 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:33:31,613] Trial 106 finished with value: 0.28132582121523886 and parameters: {'C': 64.0, 'gamma': 0.125}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:33:39,028] Trial 107 finished with value: 0.025526876270347342 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:33:46,435] Trial 108 finished with value: 0.16299574270465356 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:33:53,571] Trial 109 finished with value: 0.5894935845328042 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:34:01,073] Trial 110 finished with value: 0.6673565296465014 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:34:07,826] Trial 111 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:34:14,732] Trial 112 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:34:21,740] Trial 113 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:34:28,550] Trial 114 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:34:35,334] Trial 115 finished with value: 0.3408994791258029 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:34:42,385] Trial 116 finished with value: 0.42639304384182186 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:34:49,662] Trial 117 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:34:56,756] Trial 118 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:35:03,906] Trial 119 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:35:10,499] Trial 120 finished with value: 0.4817891128152125 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:35:17,316] Trial 121 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:35:23,995] Trial 122 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:35:31,896] Trial 123 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:35:38,981] Trial 124 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:35:46,349] Trial 125 finished with value: 0.08967520169344834 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:35:53,325] Trial 126 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:36:00,416] Trial 127 finished with value: 0.558967325808964 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:36:09,569] Trial 128 finished with value: 0.021443066413110345 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:36:17,348] Trial 129 finished with value: 0.0219768280644431 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:36:24,521] Trial 130 finished with value: 0.26978962881238894 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:36:31,561] Trial 131 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:36:38,547] Trial 132 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:36:46,154] Trial 133 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:36:53,678] Trial 134 finished with value: 0.055352022391247965 and parameters: {'C': 0.0078125, 'gamma': 0.00390625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:37:00,926] Trial 135 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:37:08,099] Trial 136 finished with value: 0.0292994741185564 and parameters: {'C': 1.0, 'gamma': 1.0}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:37:15,341] Trial 137 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:37:22,242] Trial 138 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:37:29,171] Trial 139 finished with value: 0.5864041708425097 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:37:36,133] Trial 140 finished with value: -0.002218042911261997 and parameters: {'C': 0.0625, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:37:42,966] Trial 141 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:37:50,003] Trial 142 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:37:56,970] Trial 143 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:38:03,867] Trial 144 finished with value: 0.6908565532162256 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:38:10,962] Trial 145 finished with value: 0.6124419470810333 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 101 with value: 0.6908565532162256.\n",
      "[I 2023-12-12 00:38:17,951] Trial 146 finished with value: 0.6916913268243857 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 146 with value: 0.6916913268243857.\n",
      "[I 2023-12-12 00:38:25,150] Trial 147 finished with value: 0.6916913268243857 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 146 with value: 0.6916913268243857.\n",
      "[I 2023-12-12 00:38:32,856] Trial 148 finished with value: 0.04865562169139011 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 146 with value: 0.6916913268243857.\n",
      "[I 2023-12-12 00:38:40,331] Trial 149 finished with value: 0.62536697473798 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 146 with value: 0.6916913268243857.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6917\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.713191    0.718417    0.708531\n",
      "1                    TP  335.000000  335.000000  310.000000\n",
      "2                    TN  188.000000  178.000000  192.000000\n",
      "3                    FP   39.000000   45.000000   61.000000\n",
      "4                    FN   33.000000   37.000000   32.000000\n",
      "5              Accuracy    0.878992    0.862185    0.843697\n",
      "6             Precision    0.895722    0.881579    0.835580\n",
      "7           Sensitivity    0.910326    0.900538    0.906433\n",
      "8           Specificity    0.828200    0.798200    0.758900\n",
      "9              F1 score    0.902965    0.890957    0.869565\n",
      "10  F1 score (weighted)    0.878671    0.861659    0.842125\n",
      "11     F1 score (macro)    0.871125    0.851871    0.837298\n",
      "12    Balanced Accuracy    0.869260    0.849372    0.832663\n",
      "13                  MCC    0.742450    0.704094    0.678886\n",
      "14                  NPV    0.850700    0.827900    0.857100\n",
      "15              ROC_AUC    0.869260    0.849372    0.832663\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_2_cat = np.where((y_pred_svm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:38:47,675] Trial 150 finished with value: 0.6052139710693882 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 146 with value: 0.6916913268243857.\n",
      "[I 2023-12-12 00:38:54,272] Trial 151 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:39:00,757] Trial 152 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:39:07,413] Trial 153 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:39:13,877] Trial 154 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:39:20,349] Trial 155 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:39:27,333] Trial 156 finished with value: 0.021769279950525376 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:39:34,431] Trial 157 finished with value: 0.2868467963304262 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:39:40,567] Trial 158 finished with value: 0.5575764302635902 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:39:47,241] Trial 159 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:39:54,046] Trial 160 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:40:01,001] Trial 161 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:40:07,825] Trial 162 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:40:14,525] Trial 163 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:40:22,589] Trial 164 finished with value: 0.6795115826356493 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:40:29,181] Trial 165 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:40:35,585] Trial 166 finished with value: 0.5037549918399629 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:40:42,195] Trial 167 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:40:48,800] Trial 168 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:40:55,676] Trial 169 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:41:02,597] Trial 170 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:41:09,251] Trial 171 finished with value: 0.5882820865523165 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:41:16,312] Trial 172 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:41:22,805] Trial 173 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:41:29,861] Trial 174 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:41:36,520] Trial 175 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:41:43,552] Trial 176 finished with value: 0.08863236030780744 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:41:50,219] Trial 177 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:41:56,905] Trial 178 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:42:03,600] Trial 179 finished with value: 0.5635383959128359 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:42:10,230] Trial 180 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:42:16,828] Trial 181 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:42:23,378] Trial 182 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:42:31,691] Trial 183 finished with value: 0.018873820283481134 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:42:38,495] Trial 184 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:42:45,276] Trial 185 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:42:52,438] Trial 186 finished with value: 0.01923505259248103 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:42:59,704] Trial 187 finished with value: 0.6634347203636516 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:43:06,086] Trial 188 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:43:12,050] Trial 189 finished with value: 0.4377525645223105 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:43:18,578] Trial 190 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:43:25,314] Trial 191 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:43:31,989] Trial 192 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:43:39,494] Trial 193 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:43:46,803] Trial 194 finished with value: 0.02910312584980268 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:43:53,506] Trial 195 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:44:00,127] Trial 196 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:44:06,680] Trial 197 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:44:13,170] Trial 198 finished with value: 0.6974624975241241 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:44:20,708] Trial 199 finished with value: 0.6795931768657129 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 151 with value: 0.6974624975241241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6975\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.713191    0.718417    0.708531    0.678131\n",
      "1                    TP  335.000000  335.000000  310.000000  317.000000\n",
      "2                    TN  188.000000  178.000000  192.000000  181.000000\n",
      "3                    FP   39.000000   45.000000   61.000000   61.000000\n",
      "4                    FN   33.000000   37.000000   32.000000   36.000000\n",
      "5              Accuracy    0.878992    0.862185    0.843697    0.836975\n",
      "6             Precision    0.895722    0.881579    0.835580    0.838624\n",
      "7           Sensitivity    0.910326    0.900538    0.906433    0.898017\n",
      "8           Specificity    0.828200    0.798200    0.758900    0.747900\n",
      "9              F1 score    0.902965    0.890957    0.869565    0.867305\n",
      "10  F1 score (weighted)    0.878671    0.861659    0.842125    0.835323\n",
      "11     F1 score (macro)    0.871125    0.851871    0.837298    0.827988\n",
      "12    Balanced Accuracy    0.869260    0.849372    0.832663    0.822975\n",
      "13                  MCC    0.742450    0.704094    0.678886    0.659202\n",
      "14                  NPV    0.850700    0.827900    0.857100    0.834100\n",
      "15              ROC_AUC    0.869260    0.849372    0.832663    0.822975\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_3_cat = np.where((y_pred_svm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:44:28,816] Trial 200 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:44:36,077] Trial 201 finished with value: 0.6222115684220875 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:44:43,457] Trial 202 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:44:50,465] Trial 203 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:44:57,604] Trial 204 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:45:04,822] Trial 205 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:45:11,898] Trial 206 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:45:18,960] Trial 207 finished with value: 0.6054880068561794 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:45:26,751] Trial 208 finished with value: 0.6414557944090162 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:45:34,056] Trial 209 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:45:41,052] Trial 210 finished with value: 0.5742194589367129 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:45:48,196] Trial 211 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:45:55,333] Trial 212 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:46:03,419] Trial 213 finished with value: 0.04254601529693206 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:46:10,620] Trial 214 finished with value: 0.6805339554362245 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:46:17,936] Trial 215 finished with value: 0.16898826344006573 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:46:25,850] Trial 216 finished with value: 0.2768697494297311 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:46:33,986] Trial 217 finished with value: 0.01914805330773679 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:46:40,959] Trial 218 finished with value: 0.6803528500243801 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:46:47,702] Trial 219 finished with value: 0.5847301966999485 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:46:55,597] Trial 220 finished with value: 0.6661294896345366 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:47:02,663] Trial 221 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:47:09,783] Trial 222 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:47:16,865] Trial 223 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:47:24,005] Trial 224 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:47:30,913] Trial 225 finished with value: 0.6884180589413726 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:47:38,083] Trial 226 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:47:45,347] Trial 227 finished with value: 0.014946159652279679 and parameters: {'C': 0.125, 'gamma': 6.103515625e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:47:52,206] Trial 228 finished with value: 0.5856803516757781 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:47:59,524] Trial 229 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:48:06,623] Trial 230 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:48:13,785] Trial 231 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:48:20,974] Trial 232 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:48:28,210] Trial 233 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:48:35,431] Trial 234 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:48:42,602] Trial 235 finished with value: 0.6840059983433737 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:48:49,634] Trial 236 finished with value: 0.5582854861995186 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:48:57,676] Trial 237 finished with value: 0.08190703728438233 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:49:04,873] Trial 238 finished with value: 0.2792764421787679 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:49:13,678] Trial 239 finished with value: -0.009062660369972497 and parameters: {'C': 0.0078125, 'gamma': 8.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:49:20,345] Trial 240 finished with value: 0.6709860714263745 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:49:27,534] Trial 241 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:49:34,736] Trial 242 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:49:41,868] Trial 243 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:49:49,068] Trial 244 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:49:56,563] Trial 245 finished with value: 0.3953800518002151 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:50:04,574] Trial 246 finished with value: 0.6590396994725699 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:50:12,321] Trial 247 finished with value: 0.014906168709820412 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:50:19,609] Trial 248 finished with value: 0.6828094309763 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:50:26,494] Trial 249 finished with value: 0.6333224548408612 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6975\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.713191    0.718417    0.708531    0.678131   \n",
      "1                    TP  335.000000  335.000000  310.000000  317.000000   \n",
      "2                    TN  188.000000  178.000000  192.000000  181.000000   \n",
      "3                    FP   39.000000   45.000000   61.000000   61.000000   \n",
      "4                    FN   33.000000   37.000000   32.000000   36.000000   \n",
      "5              Accuracy    0.878992    0.862185    0.843697    0.836975   \n",
      "6             Precision    0.895722    0.881579    0.835580    0.838624   \n",
      "7           Sensitivity    0.910326    0.900538    0.906433    0.898017   \n",
      "8           Specificity    0.828200    0.798200    0.758900    0.747900   \n",
      "9              F1 score    0.902965    0.890957    0.869565    0.867305   \n",
      "10  F1 score (weighted)    0.878671    0.861659    0.842125    0.835323   \n",
      "11     F1 score (macro)    0.871125    0.851871    0.837298    0.827988   \n",
      "12    Balanced Accuracy    0.869260    0.849372    0.832663    0.822975   \n",
      "13                  MCC    0.742450    0.704094    0.678886    0.659202   \n",
      "14                  NPV    0.850700    0.827900    0.857100    0.834100   \n",
      "15              ROC_AUC    0.869260    0.849372    0.832663    0.822975   \n",
      "\n",
      "          Set4  \n",
      "0     0.716960  \n",
      "1   336.000000  \n",
      "2   170.000000  \n",
      "3    54.000000  \n",
      "4    35.000000  \n",
      "5     0.850420  \n",
      "6     0.861538  \n",
      "7     0.905660  \n",
      "8     0.758900  \n",
      "9     0.883049  \n",
      "10    0.848975  \n",
      "11    0.837795  \n",
      "12    0.832294  \n",
      "13    0.677571  \n",
      "14    0.829300  \n",
      "15    0.832294  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_4_cat = np.where((y_pred_svm_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:50:34,392] Trial 250 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:50:41,235] Trial 251 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:50:48,798] Trial 252 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:50:55,132] Trial 253 finished with value: 0.43250620194338507 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:51:02,412] Trial 254 finished with value: 0.02466573337923238 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:51:09,099] Trial 255 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:51:15,910] Trial 256 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:51:22,498] Trial 257 finished with value: 0.5401795189323237 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:51:29,069] Trial 258 finished with value: 0.6832952566889233 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:51:36,045] Trial 259 finished with value: 0.01251838241371892 and parameters: {'C': 0.015625, 'gamma': 0.00048828125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:51:43,515] Trial 260 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:51:50,818] Trial 261 finished with value: 0.6163151743142631 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:51:57,893] Trial 262 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:52:04,599] Trial 263 finished with value: 0.6845714155421748 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:52:11,796] Trial 264 finished with value: 0.037582715285980686 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:52:18,609] Trial 265 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:52:25,615] Trial 266 finished with value: 0.017644554115298562 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:52:31,984] Trial 267 finished with value: 0.6882357729215685 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:52:39,183] Trial 268 finished with value: 0.27696616409420405 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:52:45,768] Trial 269 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:52:52,763] Trial 270 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:52:59,943] Trial 271 finished with value: 0.631839081261859 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:53:06,582] Trial 272 finished with value: 0.5533278495332974 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:53:13,893] Trial 273 finished with value: 0.6698963257512907 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:53:20,630] Trial 274 finished with value: 0.6880792902004174 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:53:27,417] Trial 275 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:53:34,414] Trial 276 finished with value: 0.013319501763178776 and parameters: {'C': 0.125, 'gamma': 6.103515625e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:53:40,902] Trial 277 finished with value: 0.5864430278601308 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:53:47,536] Trial 278 finished with value: 0.27188592384609384 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:53:54,523] Trial 279 finished with value: 0.08620408498911661 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:54:01,115] Trial 280 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:54:07,858] Trial 281 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:54:14,740] Trial 282 finished with value: 0.06460317722203865 and parameters: {'C': 1.0, 'gamma': 0.25}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:54:21,595] Trial 283 finished with value: 0.3885629875847848 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:54:29,791] Trial 284 finished with value: 0.014343434044689019 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:54:36,426] Trial 285 finished with value: 0.5544414925731036 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:54:43,369] Trial 286 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:54:50,584] Trial 287 finished with value: 0.01477122911828307 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:54:57,331] Trial 288 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:55:04,000] Trial 289 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:55:10,044] Trial 290 finished with value: 0.6325141711975094 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:55:17,385] Trial 291 finished with value: 0.6497513023671513 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:55:24,143] Trial 292 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:55:31,432] Trial 293 finished with value: 0.02466573337923238 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:55:38,124] Trial 294 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:55:45,070] Trial 295 finished with value: 0.013375859476602603 and parameters: {'C': 0.25, 'gamma': 3.0517578125e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:55:52,102] Trial 296 finished with value: 0.6832952566889233 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:55:58,960] Trial 297 finished with value: 0.6863614860951411 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:56:05,810] Trial 298 finished with value: 0.16182786106125463 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:56:12,667] Trial 299 finished with value: 0.6163151743142631 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 151 with value: 0.6974624975241241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6975\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.713191    0.718417    0.708531    0.678131   \n",
      "1                    TP  335.000000  335.000000  310.000000  317.000000   \n",
      "2                    TN  188.000000  178.000000  192.000000  181.000000   \n",
      "3                    FP   39.000000   45.000000   61.000000   61.000000   \n",
      "4                    FN   33.000000   37.000000   32.000000   36.000000   \n",
      "5              Accuracy    0.878992    0.862185    0.843697    0.836975   \n",
      "6             Precision    0.895722    0.881579    0.835580    0.838624   \n",
      "7           Sensitivity    0.910326    0.900538    0.906433    0.898017   \n",
      "8           Specificity    0.828200    0.798200    0.758900    0.747900   \n",
      "9              F1 score    0.902965    0.890957    0.869565    0.867305   \n",
      "10  F1 score (weighted)    0.878671    0.861659    0.842125    0.835323   \n",
      "11     F1 score (macro)    0.871125    0.851871    0.837298    0.827988   \n",
      "12    Balanced Accuracy    0.869260    0.849372    0.832663    0.822975   \n",
      "13                  MCC    0.742450    0.704094    0.678886    0.659202   \n",
      "14                  NPV    0.850700    0.827900    0.857100    0.834100   \n",
      "15              ROC_AUC    0.869260    0.849372    0.832663    0.822975   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.716960    0.693906  \n",
      "1   336.000000  323.000000  \n",
      "2   170.000000  191.000000  \n",
      "3    54.000000   41.000000  \n",
      "4    35.000000   40.000000  \n",
      "5     0.850420    0.863866  \n",
      "6     0.861538    0.887363  \n",
      "7     0.905660    0.889807  \n",
      "8     0.758900    0.823300  \n",
      "9     0.883049    0.888583  \n",
      "10    0.848975    0.863812  \n",
      "11    0.837795    0.856819  \n",
      "12    0.832294    0.856542  \n",
      "13    0.677571    0.713643  \n",
      "14    0.829300    0.826800  \n",
      "15    0.832294    0.856542  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_5_cat = np.where((y_pred_svm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:56:20,530] Trial 300 finished with value: 0.6675669604737855 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:56:27,483] Trial 301 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:56:34,657] Trial 302 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:56:41,932] Trial 303 finished with value: 0.49595693839092486 and parameters: {'C': 2.0, 'gamma': 0.00048828125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:56:48,942] Trial 304 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:56:55,932] Trial 305 finished with value: 0.6806940353989054 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:57:03,207] Trial 306 finished with value: 0.04139401899197277 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:57:10,755] Trial 307 finished with value: 0.6356170496188535 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:57:17,999] Trial 308 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:57:25,208] Trial 309 finished with value: 0.4842301698113724 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:57:33,007] Trial 310 finished with value: 0.02090116428446275 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:57:40,527] Trial 311 finished with value: 0.27546101213153873 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:57:47,654] Trial 312 finished with value: 0.6849120104706263 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:57:54,835] Trial 313 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:58:01,614] Trial 314 finished with value: 0.5496233896203041 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:58:08,730] Trial 315 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:58:16,093] Trial 316 finished with value: 0.08168836815319272 and parameters: {'C': 0.0078125, 'gamma': 0.0078125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:58:23,184] Trial 317 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:58:30,023] Trial 318 finished with value: 0.49705121887417975 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:58:37,413] Trial 319 finished with value: 0.27222313732241654 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:58:44,560] Trial 320 finished with value: 0.3404498590509597 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:58:51,786] Trial 321 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:58:58,924] Trial 322 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:59:05,889] Trial 323 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:59:12,675] Trial 324 finished with value: 0.38568757781560253 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:59:20,273] Trial 325 finished with value: 0.08117981086267877 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:59:26,750] Trial 326 finished with value: 0.6337378433149299 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:59:33,804] Trial 327 finished with value: 0.5531571405316003 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:59:41,094] Trial 328 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:59:49,877] Trial 329 finished with value: 0.01734538435843592 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 00:59:57,721] Trial 330 finished with value: 0.017809488632540938 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:00:04,859] Trial 331 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:00:12,012] Trial 332 finished with value: 0.6815257168570036 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:00:19,015] Trial 333 finished with value: 0.5682650586311346 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:00:26,690] Trial 334 finished with value: 0.6565893507367722 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:00:33,893] Trial 335 finished with value: 0.1633227446487904 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:00:40,486] Trial 336 finished with value: 0.42859746096107676 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:00:48,211] Trial 337 finished with value: 0.028310208007899195 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:00:55,470] Trial 338 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:01:02,580] Trial 339 finished with value: 0.6806940353989054 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:01:09,665] Trial 340 finished with value: 0.6824134635674546 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:01:16,709] Trial 341 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:01:23,277] Trial 342 finished with value: 0.5514477975794334 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:01:29,877] Trial 343 finished with value: 0.6685200278133123 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:01:36,734] Trial 344 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:01:43,817] Trial 345 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:01:50,549] Trial 346 finished with value: 0.607806522948217 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:01:57,541] Trial 347 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:02:04,603] Trial 348 finished with value: 0.6836712075701689 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:02:11,516] Trial 349 finished with value: 0.6849120104706263 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6975\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.713191    0.718417    0.708531    0.678131   \n",
      "1                    TP  335.000000  335.000000  310.000000  317.000000   \n",
      "2                    TN  188.000000  178.000000  192.000000  181.000000   \n",
      "3                    FP   39.000000   45.000000   61.000000   61.000000   \n",
      "4                    FN   33.000000   37.000000   32.000000   36.000000   \n",
      "5              Accuracy    0.878992    0.862185    0.843697    0.836975   \n",
      "6             Precision    0.895722    0.881579    0.835580    0.838624   \n",
      "7           Sensitivity    0.910326    0.900538    0.906433    0.898017   \n",
      "8           Specificity    0.828200    0.798200    0.758900    0.747900   \n",
      "9              F1 score    0.902965    0.890957    0.869565    0.867305   \n",
      "10  F1 score (weighted)    0.878671    0.861659    0.842125    0.835323   \n",
      "11     F1 score (macro)    0.871125    0.851871    0.837298    0.827988   \n",
      "12    Balanced Accuracy    0.869260    0.849372    0.832663    0.822975   \n",
      "13                  MCC    0.742450    0.704094    0.678886    0.659202   \n",
      "14                  NPV    0.850700    0.827900    0.857100    0.834100   \n",
      "15              ROC_AUC    0.869260    0.849372    0.832663    0.822975   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.716960    0.693906    0.721868  \n",
      "1   336.000000  323.000000  331.000000  \n",
      "2   170.000000  191.000000  169.000000  \n",
      "3    54.000000   41.000000   59.000000  \n",
      "4    35.000000   40.000000   36.000000  \n",
      "5     0.850420    0.863866    0.840336  \n",
      "6     0.861538    0.887363    0.848718  \n",
      "7     0.905660    0.889807    0.901907  \n",
      "8     0.758900    0.823300    0.741200  \n",
      "9     0.883049    0.888583    0.874505  \n",
      "10    0.848975    0.863812    0.838521  \n",
      "11    0.837795    0.856819    0.827553  \n",
      "12    0.832294    0.856542    0.821568  \n",
      "13    0.677571    0.713643    0.657951  \n",
      "14    0.829300    0.826800    0.824400  \n",
      "15    0.832294    0.856542    0.821568  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_6_cat = np.where((y_pred_svm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:02:19,084] Trial 350 finished with value: 0.34283542600580846 and parameters: {'C': 0.125, 'gamma': 0.001953125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:02:25,706] Trial 351 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:02:32,707] Trial 352 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:02:39,431] Trial 353 finished with value: 0.09180438765879817 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:02:46,259] Trial 354 finished with value: -0.0074239700421417365 and parameters: {'C': 0.03125, 'gamma': 2.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:02:54,154] Trial 355 finished with value: 0.03849084670775514 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:03:01,696] Trial 356 finished with value: 0.27599717772635496 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:03:08,365] Trial 357 finished with value: 0.2543111684252512 and parameters: {'C': 1.0, 'gamma': 0.0001220703125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:03:15,440] Trial 358 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:03:22,404] Trial 359 finished with value: 0.38361506272931756 and parameters: {'C': 0.0625, 'gamma': 0.0078125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:03:29,291] Trial 360 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:03:35,704] Trial 361 finished with value: 0.5156399362208395 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:03:42,420] Trial 362 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:03:49,049] Trial 363 finished with value: 0.2526662182717031 and parameters: {'C': 0.5, 'gamma': 0.000244140625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:03:55,785] Trial 364 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:04:02,664] Trial 365 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:04:09,461] Trial 366 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:04:16,622] Trial 367 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:04:23,341] Trial 368 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:04:29,785] Trial 369 finished with value: 0.5540881564816933 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:04:37,989] Trial 370 finished with value: 0.0023219233976700003 and parameters: {'C': 0.25, 'gamma': 8.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:04:44,986] Trial 371 finished with value: 0.08008083523925645 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:04:51,889] Trial 372 finished with value: 0.17159568267395148 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:04:58,576] Trial 373 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:05:05,930] Trial 374 finished with value: 0.01621472156959445 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:05:12,709] Trial 375 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:05:19,547] Trial 376 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:05:26,305] Trial 377 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:05:34,810] Trial 378 finished with value: 0.6175719585210903 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:05:43,795] Trial 379 finished with value: 0.025375621340166965 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:05:50,361] Trial 380 finished with value: 0.1531033054037712 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:05:57,170] Trial 381 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:06:04,269] Trial 382 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:06:10,976] Trial 383 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:06:17,538] Trial 384 finished with value: 0.6170395508450086 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:06:24,238] Trial 385 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:06:30,887] Trial 386 finished with value: 0.44814582424687827 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:06:37,552] Trial 387 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:06:44,201] Trial 388 finished with value: 0.6856697250889765 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:06:50,878] Trial 389 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:06:57,161] Trial 390 finished with value: 0.6039639596375557 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:07:04,340] Trial 391 finished with value: 0.031033627579792965 and parameters: {'C': 0.0078125, 'gamma': 0.001953125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:07:11,515] Trial 392 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:07:18,436] Trial 393 finished with value: 0.2856651886745215 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:07:25,590] Trial 394 finished with value: 0.03220913185636859 and parameters: {'C': 1.0, 'gamma': 0.5}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:07:32,645] Trial 395 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:07:39,985] Trial 396 finished with value: 0.018838073436411796 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:07:47,168] Trial 397 finished with value: 0.021676824200497213 and parameters: {'C': 0.0625, 'gamma': 0.125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:07:53,952] Trial 398 finished with value: 0.6839842879153459 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:08:00,606] Trial 399 finished with value: 0.5625336234898883 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 151 with value: 0.6974624975241241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6975\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.713191    0.718417    0.708531    0.678131   \n",
      "1                    TP  335.000000  335.000000  310.000000  317.000000   \n",
      "2                    TN  188.000000  178.000000  192.000000  181.000000   \n",
      "3                    FP   39.000000   45.000000   61.000000   61.000000   \n",
      "4                    FN   33.000000   37.000000   32.000000   36.000000   \n",
      "5              Accuracy    0.878992    0.862185    0.843697    0.836975   \n",
      "6             Precision    0.895722    0.881579    0.835580    0.838624   \n",
      "7           Sensitivity    0.910326    0.900538    0.906433    0.898017   \n",
      "8           Specificity    0.828200    0.798200    0.758900    0.747900   \n",
      "9              F1 score    0.902965    0.890957    0.869565    0.867305   \n",
      "10  F1 score (weighted)    0.878671    0.861659    0.842125    0.835323   \n",
      "11     F1 score (macro)    0.871125    0.851871    0.837298    0.827988   \n",
      "12    Balanced Accuracy    0.869260    0.849372    0.832663    0.822975   \n",
      "13                  MCC    0.742450    0.704094    0.678886    0.659202   \n",
      "14                  NPV    0.850700    0.827900    0.857100    0.834100   \n",
      "15              ROC_AUC    0.869260    0.849372    0.832663    0.822975   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.716960    0.693906    0.721868    0.701108  \n",
      "1   336.000000  323.000000  331.000000  321.000000  \n",
      "2   170.000000  191.000000  169.000000  183.000000  \n",
      "3    54.000000   41.000000   59.000000   52.000000  \n",
      "4    35.000000   40.000000   36.000000   39.000000  \n",
      "5     0.850420    0.863866    0.840336    0.847059  \n",
      "6     0.861538    0.887363    0.848718    0.860590  \n",
      "7     0.905660    0.889807    0.901907    0.891667  \n",
      "8     0.758900    0.823300    0.741200    0.778700  \n",
      "9     0.883049    0.888583    0.874505    0.875853  \n",
      "10    0.848975    0.863812    0.838521    0.846240  \n",
      "11    0.837795    0.856819    0.827553    0.838364  \n",
      "12    0.832294    0.856542    0.821568    0.835195  \n",
      "13    0.677571    0.713643    0.657951    0.677613  \n",
      "14    0.829300    0.826800    0.824400    0.824300  \n",
      "15    0.832294    0.856542    0.821568    0.835195  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_7_cat = np.where((y_pred_svm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:08:08,560] Trial 400 finished with value: 0.6334940651537473 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:08:16,234] Trial 401 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:08:23,275] Trial 402 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:08:30,067] Trial 403 finished with value: 0.5081411548029009 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:08:36,905] Trial 404 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:08:43,743] Trial 405 finished with value: 0.5830587490189424 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:08:51,082] Trial 406 finished with value: 0.6662871630935818 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:08:58,200] Trial 407 finished with value: 0.5753462457671195 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:09:05,524] Trial 408 finished with value: 0.6788748282166013 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:09:12,668] Trial 409 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:09:20,167] Trial 410 finished with value: 0.17133284499324647 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:09:27,421] Trial 411 finished with value: 0.558302601742229 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:09:34,544] Trial 412 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:09:42,042] Trial 413 finished with value: 0.0906987286001876 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:09:50,772] Trial 414 finished with value: 0.020462677325758193 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:09:57,799] Trial 415 finished with value: 0.6786597319674093 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:10:04,785] Trial 416 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:10:11,786] Trial 417 finished with value: 0.6898002455441865 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:10:19,504] Trial 418 finished with value: 0.020910260895889952 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:10:26,608] Trial 419 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:10:33,706] Trial 420 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:10:41,099] Trial 421 finished with value: 0.4470101947059266 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:10:48,871] Trial 422 finished with value: 0.6475664199604647 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:10:55,754] Trial 423 finished with value: 0.4977990073760744 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:11:02,878] Trial 424 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:11:10,974] Trial 425 finished with value: 0.0319859516879666 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:11:18,417] Trial 426 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:11:26,092] Trial 427 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:11:33,906] Trial 428 finished with value: 0.016786043736209944 and parameters: {'C': 0.0078125, 'gamma': 0.0009765625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:11:41,337] Trial 429 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:11:48,486] Trial 430 finished with value: 0.2844744092285402 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:11:55,621] Trial 431 finished with value: 0.6705182373494154 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:12:02,602] Trial 432 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:12:09,596] Trial 433 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:12:16,460] Trial 434 finished with value: 0.6706635624146109 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:12:23,357] Trial 435 finished with value: 0.5962535217350915 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:12:30,943] Trial 436 finished with value: 0.623475088052244 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:12:37,872] Trial 437 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:12:46,184] Trial 438 finished with value: 0.18774051549901644 and parameters: {'C': 0.5, 'gamma': 0.125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:12:53,406] Trial 439 finished with value: 0.40152388489924384 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:13:01,233] Trial 440 finished with value: 0.047368480446771576 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:13:08,692] Trial 441 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:13:15,534] Trial 442 finished with value: 0.553063790104963 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:13:22,552] Trial 443 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:13:30,514] Trial 444 finished with value: 0.009048495088380914 and parameters: {'C': 0.25, 'gamma': 2.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:13:38,356] Trial 445 finished with value: 0.6662871630935818 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:13:45,685] Trial 446 finished with value: 0.6788748282166013 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:13:52,784] Trial 447 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:14:00,235] Trial 448 finished with value: 0.006801304122975771 and parameters: {'C': 0.015625, 'gamma': 0.000244140625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:14:07,351] Trial 449 finished with value: 0.6813280026064432 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6975\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.713191    0.718417    0.708531    0.678131   \n",
      "1                    TP  335.000000  335.000000  310.000000  317.000000   \n",
      "2                    TN  188.000000  178.000000  192.000000  181.000000   \n",
      "3                    FP   39.000000   45.000000   61.000000   61.000000   \n",
      "4                    FN   33.000000   37.000000   32.000000   36.000000   \n",
      "5              Accuracy    0.878992    0.862185    0.843697    0.836975   \n",
      "6             Precision    0.895722    0.881579    0.835580    0.838624   \n",
      "7           Sensitivity    0.910326    0.900538    0.906433    0.898017   \n",
      "8           Specificity    0.828200    0.798200    0.758900    0.747900   \n",
      "9              F1 score    0.902965    0.890957    0.869565    0.867305   \n",
      "10  F1 score (weighted)    0.878671    0.861659    0.842125    0.835323   \n",
      "11     F1 score (macro)    0.871125    0.851871    0.837298    0.827988   \n",
      "12    Balanced Accuracy    0.869260    0.849372    0.832663    0.822975   \n",
      "13                  MCC    0.742450    0.704094    0.678886    0.659202   \n",
      "14                  NPV    0.850700    0.827900    0.857100    0.834100   \n",
      "15              ROC_AUC    0.869260    0.849372    0.832663    0.822975   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.716960    0.693906    0.721868    0.701108    0.714266  \n",
      "1   336.000000  323.000000  331.000000  321.000000  327.000000  \n",
      "2   170.000000  191.000000  169.000000  183.000000  176.000000  \n",
      "3    54.000000   41.000000   59.000000   52.000000   45.000000  \n",
      "4    35.000000   40.000000   36.000000   39.000000   47.000000  \n",
      "5     0.850420    0.863866    0.840336    0.847059    0.845378  \n",
      "6     0.861538    0.887363    0.848718    0.860590    0.879032  \n",
      "7     0.905660    0.889807    0.901907    0.891667    0.874332  \n",
      "8     0.758900    0.823300    0.741200    0.778700    0.796400  \n",
      "9     0.883049    0.888583    0.874505    0.875853    0.876676  \n",
      "10    0.848975    0.863812    0.838521    0.846240    0.845519  \n",
      "11    0.837795    0.856819    0.827553    0.838364    0.834734  \n",
      "12    0.832294    0.856542    0.821568    0.835195    0.835356  \n",
      "13    0.677571    0.713643    0.657951    0.677613    0.669490  \n",
      "14    0.829300    0.826800    0.824400    0.824300    0.789200  \n",
      "15    0.832294    0.856542    0.821568    0.835195    0.835356  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_8_cat = np.where((y_pred_svm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:14:14,910] Trial 450 finished with value: 0.5081649683734353 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:14:21,708] Trial 451 finished with value: 0.6940845876356374 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:14:28,502] Trial 452 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:14:35,168] Trial 453 finished with value: 0.6914123725669139 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:14:41,899] Trial 454 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:14:48,800] Trial 455 finished with value: 0.08308905648416517 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:14:55,553] Trial 456 finished with value: 0.5688258740754539 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:15:04,045] Trial 457 finished with value: 0.014400009617939701 and parameters: {'C': 2.0, 'gamma': 8.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:15:10,904] Trial 458 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:15:17,282] Trial 459 finished with value: 0.49739353084193355 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:15:23,859] Trial 460 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:15:31,281] Trial 461 finished with value: 0.014727592169690584 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:15:38,755] Trial 462 finished with value: 0.6541536037252158 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:15:45,481] Trial 463 finished with value: 0.6952880534259274 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:15:51,980] Trial 464 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:15:58,138] Trial 465 finished with value: 0.44286578744163857 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:16:05,050] Trial 466 finished with value: 0.025757786641084558 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:16:11,796] Trial 467 finished with value: 0.09164988317791525 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:16:18,506] Trial 468 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:16:25,148] Trial 469 finished with value: 0.2811543066075342 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:16:31,596] Trial 470 finished with value: 0.5060178609176798 and parameters: {'C': 1.0, 'gamma': 0.0009765625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:16:38,383] Trial 471 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:16:45,045] Trial 472 finished with value: 0.3341676398495906 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:16:51,760] Trial 473 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:16:58,386] Trial 474 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:17:05,031] Trial 475 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:17:11,347] Trial 476 finished with value: 0.6370092397421232 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:17:17,650] Trial 477 finished with value: 0.6102403698649457 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:17:24,818] Trial 478 finished with value: 0.6376986964124505 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:17:31,762] Trial 479 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:17:38,359] Trial 480 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:17:45,615] Trial 481 finished with value: 0.2862226957157238 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:17:52,307] Trial 482 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:17:59,163] Trial 483 finished with value: 0.011864979249111274 and parameters: {'C': 0.25, 'gamma': 0.5}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:18:06,467] Trial 484 finished with value: 0.018112863750251305 and parameters: {'C': 64.0, 'gamma': 2.0}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:18:13,189] Trial 485 finished with value: -0.0013635285119394825 and parameters: {'C': 0.015625, 'gamma': 0.0001220703125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:18:19,735] Trial 486 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:18:26,287] Trial 487 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:18:33,270] Trial 488 finished with value: 0.6719992776257767 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:18:39,836] Trial 489 finished with value: 0.6914123725669139 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:18:46,571] Trial 490 finished with value: 0.6940845876356374 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:18:53,235] Trial 491 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:18:59,428] Trial 492 finished with value: 0.5081649683734353 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:19:06,037] Trial 493 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:19:12,642] Trial 494 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:19:19,390] Trial 495 finished with value: 0.08282443915122656 and parameters: {'C': 2.0, 'gamma': 0.25}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:19:25,357] Trial 496 finished with value: 0.5947067751789484 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:19:31,943] Trial 497 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:19:38,400] Trial 498 finished with value: 0.2530985818857804 and parameters: {'C': 0.125, 'gamma': 0.0625}. Best is trial 151 with value: 0.6974624975241241.\n",
      "[I 2023-12-12 01:19:44,960] Trial 499 finished with value: 0.6946275781074274 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.6974624975241241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6975\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.713191    0.718417    0.708531    0.678131   \n",
      "1                    TP  335.000000  335.000000  310.000000  317.000000   \n",
      "2                    TN  188.000000  178.000000  192.000000  181.000000   \n",
      "3                    FP   39.000000   45.000000   61.000000   61.000000   \n",
      "4                    FN   33.000000   37.000000   32.000000   36.000000   \n",
      "5              Accuracy    0.878992    0.862185    0.843697    0.836975   \n",
      "6             Precision    0.895722    0.881579    0.835580    0.838624   \n",
      "7           Sensitivity    0.910326    0.900538    0.906433    0.898017   \n",
      "8           Specificity    0.828200    0.798200    0.758900    0.747900   \n",
      "9              F1 score    0.902965    0.890957    0.869565    0.867305   \n",
      "10  F1 score (weighted)    0.878671    0.861659    0.842125    0.835323   \n",
      "11     F1 score (macro)    0.871125    0.851871    0.837298    0.827988   \n",
      "12    Balanced Accuracy    0.869260    0.849372    0.832663    0.822975   \n",
      "13                  MCC    0.742450    0.704094    0.678886    0.659202   \n",
      "14                  NPV    0.850700    0.827900    0.857100    0.834100   \n",
      "15              ROC_AUC    0.869260    0.849372    0.832663    0.822975   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.716960    0.693906    0.721868    0.701108    0.714266    0.719360  \n",
      "1   336.000000  323.000000  331.000000  321.000000  327.000000  326.000000  \n",
      "2   170.000000  191.000000  169.000000  183.000000  176.000000  183.000000  \n",
      "3    54.000000   41.000000   59.000000   52.000000   45.000000   44.000000  \n",
      "4    35.000000   40.000000   36.000000   39.000000   47.000000   42.000000  \n",
      "5     0.850420    0.863866    0.840336    0.847059    0.845378    0.855462  \n",
      "6     0.861538    0.887363    0.848718    0.860590    0.879032    0.881081  \n",
      "7     0.905660    0.889807    0.901907    0.891667    0.874332    0.885870  \n",
      "8     0.758900    0.823300    0.741200    0.778700    0.796400    0.806200  \n",
      "9     0.883049    0.888583    0.874505    0.875853    0.876676    0.883469  \n",
      "10    0.848975    0.863812    0.838521    0.846240    0.845519    0.855338  \n",
      "11    0.837795    0.856819    0.827553    0.838364    0.834734    0.846602  \n",
      "12    0.832294    0.856542    0.821568    0.835195    0.835356    0.846018  \n",
      "13    0.677571    0.713643    0.657951    0.677613    0.669490    0.693225  \n",
      "14    0.829300    0.826800    0.824400    0.824300    0.789200    0.813300  \n",
      "15    0.832294    0.856542    0.821568    0.835195    0.835356    0.846018  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_9_cat = np.where((y_pred_svm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6975\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.711331</td>\n",
       "      <td>0.028449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>165.400000</td>\n",
       "      <td>4.273952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>5.734884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>4.971027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>4.423423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.856290</td>\n",
       "      <td>0.024177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.871815</td>\n",
       "      <td>0.023880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.900547</td>\n",
       "      <td>0.022982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.784800</td>\n",
       "      <td>0.043967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.885744</td>\n",
       "      <td>0.018649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.855427</td>\n",
       "      <td>0.024370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.845914</td>\n",
       "      <td>0.026611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.842679</td>\n",
       "      <td>0.027253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.693374</td>\n",
       "      <td>0.053095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.829770</td>\n",
       "      <td>0.039123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.842679</td>\n",
       "      <td>0.027253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.711331     0.028449\n",
       "1                    TP       165.400000     4.273952\n",
       "2                    TN        89.000000     5.734884\n",
       "3                    FP        24.400000     4.971027\n",
       "4                    FN        18.300000     4.423423\n",
       "5              Accuracy         0.856290     0.024177\n",
       "6             Precision         0.871815     0.023880\n",
       "7           Sensitivity         0.900547     0.022982\n",
       "8           Specificity         0.784800     0.043967\n",
       "9              F1 score         0.885744     0.018649\n",
       "10  F1 score (weighted)         0.855427     0.024370\n",
       "11     F1 score (macro)         0.845914     0.026611\n",
       "12    Balanced Accuracy         0.842679     0.027253\n",
       "13                  MCC         0.693374     0.053095\n",
       "14                  NPV         0.829770     0.039123\n",
       "15              ROC_AUC         0.842679     0.027253"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.713191</td>\n",
       "      <td>0.718417</td>\n",
       "      <td>0.708531</td>\n",
       "      <td>0.678131</td>\n",
       "      <td>0.716960</td>\n",
       "      <td>0.693906</td>\n",
       "      <td>0.721868</td>\n",
       "      <td>0.701108</td>\n",
       "      <td>0.714266</td>\n",
       "      <td>0.719360</td>\n",
       "      <td>0.708574</td>\n",
       "      <td>0.013791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>326.100000</td>\n",
       "      <td>8.556349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>181.100000</td>\n",
       "      <td>8.006248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>50.100000</td>\n",
       "      <td>8.372574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>37.700000</td>\n",
       "      <td>4.473378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.878992</td>\n",
       "      <td>0.862185</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.836975</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.863866</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.855462</td>\n",
       "      <td>0.852437</td>\n",
       "      <td>0.012844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.895722</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.835580</td>\n",
       "      <td>0.838624</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.887363</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.860590</td>\n",
       "      <td>0.879032</td>\n",
       "      <td>0.881081</td>\n",
       "      <td>0.866983</td>\n",
       "      <td>0.021063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.910326</td>\n",
       "      <td>0.900538</td>\n",
       "      <td>0.906433</td>\n",
       "      <td>0.898017</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.889807</td>\n",
       "      <td>0.901907</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.874332</td>\n",
       "      <td>0.885870</td>\n",
       "      <td>0.896456</td>\n",
       "      <td>0.011026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.828200</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.758900</td>\n",
       "      <td>0.747900</td>\n",
       "      <td>0.758900</td>\n",
       "      <td>0.823300</td>\n",
       "      <td>0.741200</td>\n",
       "      <td>0.778700</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>0.783790</td>\n",
       "      <td>0.031214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.902965</td>\n",
       "      <td>0.890957</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.867305</td>\n",
       "      <td>0.883049</td>\n",
       "      <td>0.888583</td>\n",
       "      <td>0.874505</td>\n",
       "      <td>0.875853</td>\n",
       "      <td>0.876676</td>\n",
       "      <td>0.883469</td>\n",
       "      <td>0.881293</td>\n",
       "      <td>0.010823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.878671</td>\n",
       "      <td>0.861659</td>\n",
       "      <td>0.842125</td>\n",
       "      <td>0.835323</td>\n",
       "      <td>0.848975</td>\n",
       "      <td>0.863812</td>\n",
       "      <td>0.838521</td>\n",
       "      <td>0.846240</td>\n",
       "      <td>0.845519</td>\n",
       "      <td>0.855338</td>\n",
       "      <td>0.851618</td>\n",
       "      <td>0.013315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.871125</td>\n",
       "      <td>0.851871</td>\n",
       "      <td>0.837298</td>\n",
       "      <td>0.827988</td>\n",
       "      <td>0.837795</td>\n",
       "      <td>0.856819</td>\n",
       "      <td>0.827553</td>\n",
       "      <td>0.838364</td>\n",
       "      <td>0.834734</td>\n",
       "      <td>0.846602</td>\n",
       "      <td>0.843015</td>\n",
       "      <td>0.013691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.869260</td>\n",
       "      <td>0.849372</td>\n",
       "      <td>0.832663</td>\n",
       "      <td>0.822975</td>\n",
       "      <td>0.832294</td>\n",
       "      <td>0.856542</td>\n",
       "      <td>0.821568</td>\n",
       "      <td>0.835195</td>\n",
       "      <td>0.835356</td>\n",
       "      <td>0.846018</td>\n",
       "      <td>0.840124</td>\n",
       "      <td>0.015060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.742450</td>\n",
       "      <td>0.704094</td>\n",
       "      <td>0.678886</td>\n",
       "      <td>0.659202</td>\n",
       "      <td>0.677571</td>\n",
       "      <td>0.713643</td>\n",
       "      <td>0.657951</td>\n",
       "      <td>0.677613</td>\n",
       "      <td>0.669490</td>\n",
       "      <td>0.693225</td>\n",
       "      <td>0.687412</td>\n",
       "      <td>0.026415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>0.827900</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.834100</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>0.826800</td>\n",
       "      <td>0.824400</td>\n",
       "      <td>0.824300</td>\n",
       "      <td>0.789200</td>\n",
       "      <td>0.813300</td>\n",
       "      <td>0.827710</td>\n",
       "      <td>0.018708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.869260</td>\n",
       "      <td>0.849372</td>\n",
       "      <td>0.832663</td>\n",
       "      <td>0.822975</td>\n",
       "      <td>0.832294</td>\n",
       "      <td>0.856542</td>\n",
       "      <td>0.821568</td>\n",
       "      <td>0.835195</td>\n",
       "      <td>0.835356</td>\n",
       "      <td>0.846018</td>\n",
       "      <td>0.840124</td>\n",
       "      <td>0.015060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.713191    0.718417    0.708531    0.678131   \n",
       "1                    TP  335.000000  335.000000  310.000000  317.000000   \n",
       "2                    TN  188.000000  178.000000  192.000000  181.000000   \n",
       "3                    FP   39.000000   45.000000   61.000000   61.000000   \n",
       "4                    FN   33.000000   37.000000   32.000000   36.000000   \n",
       "5              Accuracy    0.878992    0.862185    0.843697    0.836975   \n",
       "6             Precision    0.895722    0.881579    0.835580    0.838624   \n",
       "7           Sensitivity    0.910326    0.900538    0.906433    0.898017   \n",
       "8           Specificity    0.828200    0.798200    0.758900    0.747900   \n",
       "9              F1 score    0.902965    0.890957    0.869565    0.867305   \n",
       "10  F1 score (weighted)    0.878671    0.861659    0.842125    0.835323   \n",
       "11     F1 score (macro)    0.871125    0.851871    0.837298    0.827988   \n",
       "12    Balanced Accuracy    0.869260    0.849372    0.832663    0.822975   \n",
       "13                  MCC    0.742450    0.704094    0.678886    0.659202   \n",
       "14                  NPV    0.850700    0.827900    0.857100    0.834100   \n",
       "15              ROC_AUC    0.869260    0.849372    0.832663    0.822975   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.716960    0.693906    0.721868    0.701108    0.714266    0.719360   \n",
       "1   336.000000  323.000000  331.000000  321.000000  327.000000  326.000000   \n",
       "2   170.000000  191.000000  169.000000  183.000000  176.000000  183.000000   \n",
       "3    54.000000   41.000000   59.000000   52.000000   45.000000   44.000000   \n",
       "4    35.000000   40.000000   36.000000   39.000000   47.000000   42.000000   \n",
       "5     0.850420    0.863866    0.840336    0.847059    0.845378    0.855462   \n",
       "6     0.861538    0.887363    0.848718    0.860590    0.879032    0.881081   \n",
       "7     0.905660    0.889807    0.901907    0.891667    0.874332    0.885870   \n",
       "8     0.758900    0.823300    0.741200    0.778700    0.796400    0.806200   \n",
       "9     0.883049    0.888583    0.874505    0.875853    0.876676    0.883469   \n",
       "10    0.848975    0.863812    0.838521    0.846240    0.845519    0.855338   \n",
       "11    0.837795    0.856819    0.827553    0.838364    0.834734    0.846602   \n",
       "12    0.832294    0.856542    0.821568    0.835195    0.835356    0.846018   \n",
       "13    0.677571    0.713643    0.657951    0.677613    0.669490    0.693225   \n",
       "14    0.829300    0.826800    0.824400    0.824300    0.789200    0.813300   \n",
       "15    0.832294    0.856542    0.821568    0.835195    0.835356    0.846018   \n",
       "\n",
       "           ave       std  \n",
       "0     0.708574  0.013791  \n",
       "1   326.100000  8.556349  \n",
       "2   181.100000  8.006248  \n",
       "3    50.100000  8.372574  \n",
       "4    37.700000  4.473378  \n",
       "5     0.852437  0.012844  \n",
       "6     0.866983  0.021063  \n",
       "7     0.896456  0.011026  \n",
       "8     0.783790  0.031214  \n",
       "9     0.881293  0.010823  \n",
       "10    0.851618  0.013315  \n",
       "11    0.843015  0.013691  \n",
       "12    0.840124  0.015060  \n",
       "13    0.687412  0.026415  \n",
       "14    0.827710  0.018708  \n",
       "15    0.840124  0.015060  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_svm0</th>\n",
       "      <th>y_pred_svm1</th>\n",
       "      <th>y_pred_svm2</th>\n",
       "      <th>y_pred_svm3</th>\n",
       "      <th>y_pred_svm4</th>\n",
       "      <th>y_pred_svm_ave</th>\n",
       "      <th>y_pred_svm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2047687</td>\n",
       "      <td>0</td>\n",
       "      <td>5.48</td>\n",
       "      <td>5.791406</td>\n",
       "      <td>5.790756</td>\n",
       "      <td>5.809844</td>\n",
       "      <td>5.774863</td>\n",
       "      <td>5.802858</td>\n",
       "      <td>5.741621</td>\n",
       "      <td>0.117507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1164212</td>\n",
       "      <td>1</td>\n",
       "      <td>5.76</td>\n",
       "      <td>5.235682</td>\n",
       "      <td>5.284850</td>\n",
       "      <td>5.180544</td>\n",
       "      <td>5.094217</td>\n",
       "      <td>5.203748</td>\n",
       "      <td>5.293173</td>\n",
       "      <td>0.216629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2337873</td>\n",
       "      <td>2</td>\n",
       "      <td>6.07</td>\n",
       "      <td>6.213333</td>\n",
       "      <td>6.060871</td>\n",
       "      <td>6.135316</td>\n",
       "      <td>6.129850</td>\n",
       "      <td>6.237609</td>\n",
       "      <td>6.141163</td>\n",
       "      <td>0.066060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4577419</td>\n",
       "      <td>3</td>\n",
       "      <td>7.90</td>\n",
       "      <td>7.728884</td>\n",
       "      <td>7.665766</td>\n",
       "      <td>7.681916</td>\n",
       "      <td>7.670734</td>\n",
       "      <td>7.659775</td>\n",
       "      <td>7.717846</td>\n",
       "      <td>0.084555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL515452</td>\n",
       "      <td>4</td>\n",
       "      <td>6.22</td>\n",
       "      <td>6.594512</td>\n",
       "      <td>6.708106</td>\n",
       "      <td>6.724834</td>\n",
       "      <td>6.720258</td>\n",
       "      <td>6.655570</td>\n",
       "      <td>6.603880</td>\n",
       "      <td>0.177558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL3693800</td>\n",
       "      <td>2966</td>\n",
       "      <td>8.22</td>\n",
       "      <td>8.165879</td>\n",
       "      <td>8.209120</td>\n",
       "      <td>8.197620</td>\n",
       "      <td>8.180154</td>\n",
       "      <td>8.203111</td>\n",
       "      <td>8.195981</td>\n",
       "      <td>0.018072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL2431917</td>\n",
       "      <td>2967</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.906326</td>\n",
       "      <td>6.963545</td>\n",
       "      <td>6.954576</td>\n",
       "      <td>6.728151</td>\n",
       "      <td>6.881785</td>\n",
       "      <td>6.982397</td>\n",
       "      <td>0.227224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL2413298</td>\n",
       "      <td>2968</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.015856</td>\n",
       "      <td>6.015853</td>\n",
       "      <td>6.075710</td>\n",
       "      <td>6.031398</td>\n",
       "      <td>6.076037</td>\n",
       "      <td>6.079142</td>\n",
       "      <td>0.084677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3656016</td>\n",
       "      <td>2969</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.045797</td>\n",
       "      <td>8.139489</td>\n",
       "      <td>7.961229</td>\n",
       "      <td>8.058423</td>\n",
       "      <td>8.084253</td>\n",
       "      <td>8.134865</td>\n",
       "      <td>0.180203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL4643138</td>\n",
       "      <td>2970</td>\n",
       "      <td>6.38</td>\n",
       "      <td>6.593245</td>\n",
       "      <td>6.531594</td>\n",
       "      <td>6.647270</td>\n",
       "      <td>6.196612</td>\n",
       "      <td>6.568077</td>\n",
       "      <td>6.486133</td>\n",
       "      <td>0.153463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_svm0  y_pred_svm1  \\\n",
       "0         CHEMBL2047687            0     5.48     5.791406     5.790756   \n",
       "1         CHEMBL1164212            1     5.76     5.235682     5.284850   \n",
       "2         CHEMBL2337873            2     6.07     6.213333     6.060871   \n",
       "3         CHEMBL4577419            3     7.90     7.728884     7.665766   \n",
       "4          CHEMBL515452            4     6.22     6.594512     6.708106   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "2966      CHEMBL3693800         2966     8.22     8.165879     8.209120   \n",
       "2967      CHEMBL2431917         2967     7.46     6.906326     6.963545   \n",
       "2968      CHEMBL2413298         2968     6.26     6.015856     6.015853   \n",
       "2969      CHEMBL3656016         2969     8.52     8.045797     8.139489   \n",
       "2970      CHEMBL4643138         2970     6.38     6.593245     6.531594   \n",
       "\n",
       "      y_pred_svm2  y_pred_svm3  y_pred_svm4  y_pred_svm_ave  y_pred_svm_std  \n",
       "0        5.809844     5.774863     5.802858        5.741621        0.117507  \n",
       "1        5.180544     5.094217     5.203748        5.293173        0.216629  \n",
       "2        6.135316     6.129850     6.237609        6.141163        0.066060  \n",
       "3        7.681916     7.670734     7.659775        7.717846        0.084555  \n",
       "4        6.724834     6.720258     6.655570        6.603880        0.177558  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "2966     8.197620     8.180154     8.203111        8.195981        0.018072  \n",
       "2967     6.954576     6.728151     6.881785        6.982397        0.227224  \n",
       "2968     6.075710     6.031398     6.076037        6.079142        0.084677  \n",
       "2969     7.961229     8.058423     8.084253        8.134865        0.180203  \n",
       "2970     6.647270     6.196612     6.568077        6.486133        0.153463  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where((y_pred_optimized_svm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id, svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "svm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.6852 with a standard deviation of 0.0405\n",
      "SVM optimized model r2_score 0.7052 with a standard deviation of 0.0358\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm.joblib']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, \"OUTPUT/svm_reg.joblib\")\n",
    "#joblib.dump(optimized_svm, \"OUTPUT/optimized_svm.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f66294e1-0f38-417c-a4cd-08c62b383269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9ee06d03-ed96-4b17-8147-bbc6648b7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4d6ff474-48d0-4e5c-bbe1-5be8c018975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/PredResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    rf_5preds.to_excel(writer, sheet_name=\"RF\", )\n",
    "    lgbm_5preds.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    xgb_5preds.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    knn_5preds.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    svm_5preds.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
